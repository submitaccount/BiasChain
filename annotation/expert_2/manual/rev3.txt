Paper Title: Multi-Source Unsupervised Hyperparameter Optimization

Paper Abstract: How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and \u2018somewhat relevant\u2019 source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization. 


Target Review: "I thank the authors for their answer. However I want to challenge the following point

\"the aim of this paper is to obtain the optimal set of hyperparameters\"

How HPO is generally motivated is: in practice there are many situations where model X or Y is used, but the HPs are suboptimal, or tuning them manually takes a lot of time or effort. So the general aim for supervised ML is to produce a good model according to some metric, and one way to improve many of the best models (the hyperparameters) is by HPO.

We should not forget the overall goal (providing a good model), when looking at the subgoal (finding good HPs to further improve a model). The final goal in the task described in the paper appears to be to provide a regression model on unlabeled data, where labeled transfer learning data is not available, but labeled source tasks are. I don't understand why it is not simply framed as thus, and then (if it is the case) motivate why HPO is important in that setting.

Once the problem is framed as such, better baselines (that do not need to use HPO) can come to mind. For example, just train a model for each source task, and predict their average or median, on the target task. A slight variation: train a single model where the source task is one hot encoded as a feature; then at prediction again one can use the mean or median of what would be the prediction for the various source tasks. An even simpler idea: concatenate all source tasks and just train a model on these, use the obtained model directly for predicting on the target. (Or is this Naive? My understanding from the paper is that only the HP configuration found this way is used, and then a new model is trained only using the target dataset and f_hat as an objective, but I found the section \"Experimental Procedure\" hard to follow). In each of these cases there is no need to perform HPO, just either use the default parameters or perform HPO on the source tasks."

Tone and Sentiment Analysis:
"Sentiment": "Constructive",
"Sentiment_reason": "Challenges framing and suggests simpler alternatives.",
"Tone": "Critical",
"Tone_reason": "Directly questions assumptions and offers concrete alternatives."

Internal Consistency Check:
"Consistent_in_itself": true,
"Consistency_reason": "Maintains a focused critique on framing and baselines."

Inter-Review Comparision:
"is_consistent_with_others": true,
"alignment_score": 4/5,
"contradictory_points": "Shares baseline concerns but dives deeper into problem framing.",
"possible_bias_flags": "None",
"summary_of_differences": "More questioning of the paper’s core framing than the brief baseline note in Review 2."

Bias Detection:
"bias_detected": false,
"bias_types": [],
"confidence_score": 8,
"evidence": "Offers reasoned challenge without dismissive language.",
"suggestion_for_improvements": "Could clarify how proposed baseline experiments compare quantitatively."

