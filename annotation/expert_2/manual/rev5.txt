Paper Title: Multi-Source Unsupervised Hyperparameter Optimization

Paper Abstract: How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and \u2018somewhat relevant\u2019 source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization. 


Target Review: "The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.

The main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets.

### Questions / Comments
The paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.

In this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.

The results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?

As the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).

The description of the baseline they call "Naive" is also not very clear. If would be good to have more details on this baseline.

In Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.

To summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.

### Typos
Figure 1b:
Comapring -> Comparing

Table 1 caption:
performs almost the same with naive in Parkinson given
their standard errors -> grammar should be improved"

Tone and Sentiment Analysis:
"Sentiment": "Neutral",
"Sentiment_reason": "Descriptive overview with substantive questions, no overt emotion.",
"Tone": "Analytical",
"Tone_reason": "Focuses on framing and comparison methodology."

Internal Consistency Check:
"Consistent_in_itself": true,
"Consistency_reason": "Keeps a single analytical thread challenging comparisons."

Inter-Review Comparision:
"is_consistent_with_others": true,
"alignment_score": 4/5,
"contradictory_points": "Shares unsupervised framing concerns with some peers but adds clarity questions on evaluation.",
"possible_bias_flags": "None",
"summary_of_differences": "More emphasis on unsupervised vs. hyperparameter tuning distinction than most reviews."

Bias Detection:
"bias_detected": false,
"bias_types": [],
"confidence_score": 8,
"evidence": "Asks for clarifications without undue criticism.",
"suggestion_for_improvements": "Cite specific unsupervised baselines to strengthen comparison."
