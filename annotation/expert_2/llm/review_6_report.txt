Paper Title: Multi-Source Unsupervised Hyperparameter Optimization

Paper Abstract: How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and ‘somewhat relevant’ source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization.

Target Review (Reviewer 6):
The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.\n\nThe main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. \n\n### Questions / Comments\nThe paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.\n\nIn this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.\n\nThe results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?\n\nAs the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).\n\nThe description of the baseline they call \"Naive\" is also not very clear. If would be good to have more details on this baseline.\n\nIn Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.\n\nTo summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.\n\n### Typos\nFigure 1b:\nComapring -> Comparing\n\nTable 1 caption:\nperforms almost the same with naive in Parkinson given\ntheir standard errors -> grammar should be improved

Tone and Sentiment Analysis:
Sentiment: Negative
Sentiment Reason: The reviewer states that the results are 'not sufficient to show that the technique proposed can be useful in practice', 'insufficiently motivated', and that the experiments 'would benefit from clearer descriptions and stronger baselines'.
Tone: Critical
Tone Reason: The reviewer uses phrases like 'I argue that the results are not sufficient', 'I do not understand from the text', and 'it is not clear to me', which indicate a questioning and critical tone. Additionally, the reviewer lists several major concerns and criticisms throughout the review.

Internal Consistency Check:
Consistent_in_itself: True
Consistency_reason: The review maintains a consistent critical viewpoint throughout, questioning the framing of the paper as a hyperparameter tuning paper and arguing that it should be viewed as an unsupervised learning technique. The reviewer raises several concerns about the methodology, experimental results, and comparisons to baselines, and suggests improvements. The tone and content remain critical and suggestive throughout, without any apparent logical breaks or inconsistencies.

Inter-Review Comparison:
is_consistent_with_others: False
alignment_score: 3
contradictory_points: Target review questions the framing of the paper as a hyperparameter optimization problem, suggesting it should be viewed as an unsupervised learning technique. Other reviews acknowledge the novelty and significance of the hyperparameter optimization approach. Target review criticizes the experimental results as insufficient, while others find them satisfactory and well-controlled.
possible_bias_flags: Target review is overly critical and dismissive of the paper's contributions, whereas other reviews highlight the paper's strengths and significance.
summary_of_differences: Target review diverges from others in its tone, criticism, and interpretation of the paper's main contributions and experimental results. While others praise the paper's clarity, significance, and rigor, the target review is more negative, focusing on perceived shortcomings in motivation, experimental design, and baseline comparisons.

Bias Detection:
bias_detected: True
bias_types: ['Confirmation Bias']
confidence_score: 8
evidence: The reviewer is overly critical and dismissive of the paper's contributions, questioning its framing as a hyperparameter optimization problem and suggesting it should be viewed as an unsupervised learning technique. The tone is consistently negative, focusing on perceived shortcomings in motivation, experimental design, and baseline comparisons, which diverges from other reviews that praise the paper's clarity, significance, and rigor.
suggestion_for_improvements: The reviewer should strive to provide a more balanced evaluation, acknowledging the paper's strengths and significance as noted by other reviewers. It would be beneficial to reassess the paper's contributions in the context of hyperparameter optimization and consider the value of its approach even if it also relates to unsupervised learning. Providing specific, constructive feedback on how to improve the experimental design and baseline comparisons would enhance the review's usefulness.
