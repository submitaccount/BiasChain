Paper Title: Multi-Source Unsupervised Hyperparameter Optimization

Paper Abstract: How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and ‘somewhat relevant’ source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization.

Target Review (Reviewer 5):
The paper introduces multi-source unsupervised hyperparameter optimization (MSU-HPO), a novel BO framework where a range of related tasks are available but labels cannot be accessed for the target task. As ground truth on the target task is unavailable, the work introduces two estimators to approximate the target task objective. This enables HPO to be run to optimize the hyperparameters on the target task, converging faster to a good hyperparameter configuration.\n\n\nPositive\n\n1. *Significance.* To my knowledge, this is the first paper to investigate a transfer learning setting for HPO where labels are unavailable for the target task. This is of interest in several practical applications (e.g., advertising, as the authors discuss). The exploration of a new problem together with the introduction of principled estimators make both the paper's goal and methodology significant.\u2028\n2. *Clarity.* The paper was a pleasure to read. Very clear and well structured. I am also confident about reproducibility as enough details about the algorithm and experimental setup are provided.\u2028\n3. *Rigorous evaluation.* I appreciated the solid theoretical analysis paired with fully-controlled synthetic experiments where the degree of task similarity can be regulated and results compared against ground truth. All figures are based on multiple runs and have clearly detailed error bars.\u2028\n\nNegative\n\n1. *Easy experiments.* The experiments are run on synthetic data and on real-data with only two ML models (SVM and LightGBM). This is secondary considering that the theoretical analysis is solid, but tuning a wider range of ML algorithms (such as neural networks/NAS) would have made the case even stronger by showing that transfer learning is possible across a diverse class of models. The dimensionality of the optimized hyperparameter spaces is also very small, with respectively two and four tuned hyperparameters for SVM and LightGBM. Applying the method to more challenging scenarios would further demonstrate the benefits of the proposed approach.\u2028\n2. *Not many baselines.* The main baselines the method is compared against are LI and DISTBO. While many transfer learning baselines are inapplicable as most assume target labels to be available (as discussed in the related work), this is not the case for all of them. An example is Feurer, et al.: Initializing Bayesian hyperparameter optimization via meta-learning, AAAI, 2015. This is not referenced but should be discussed, as it only uses hyperparameter configurations from previous related tasks to warm-start the new optimization. As this does not look at the labels of the target task, it could be compared against. This is also the case for Perrone et al., 2019, which learn a search space purely based on previous tasks and does not require target labels. Another search space pruning method that is not compared against nor discussed is Wistuba, et al.: Hyperparameter search space pruning\u2013a new component for sequential model-based hyperparameter optimization, in Machine Learning and Knowledge Discovery in Databases, 2015.\n\nOverall, I am inclined towards accepting this paper due to the rigorous theoretical evaluation, the significance of the problem, and the fairly satisfactory experimental evaluation (although more baselines and more challenging experiments would be needed to make a stronger case).\n\nAdditional questions\n\na. Related to the point above, how does the proposed approach compare to the transfer learning baselines references above that are purely based on learning an initialization or a good search space?\n\nb. The naive method performs on part with LI in the synthetic experiments and clearly better than DISTBO in the real-world ones. Is this expected? Does this indicate that the chosen baselines (LI and DISTBO) may be too weak? In appendix D3 the authors discuss that this is probably due to the fact that only 50 hyperparameter evaluations on source tasks are available. Would it then not be useful to re-run the comparison under a varying number of hyperparameter evaluations available from source tasks? If the proposed method only outperforms baselines when a small number of prior evaluations are available, this should be clearly stated and shown.\u2028\n\nc. Considering the relatively poor results of the unbiased estimator, would it not be better to re-frame the narrative to focus on the variance-reduced estimator? The unbiased one is an interesting ablation study, but given the results it might be better to clearly state from the beginning that the variance reduced estimator is the recommended choice.\u2028\n\nd. Will code be made available? \n\nTypos:\n1. Page 12: \"omited\" ---> \"omitted\"\n2. Figure 1b: \"Comapring\" ----> \"Comparing\

Tone and Sentiment Analysis:
Sentiment: Positive
Sentiment Reason: The reviewer states 'I am inclined towards accepting this paper due to the rigorous theoretical evaluation, the significance of the problem, and the fairly satisfactory experimental evaluation'.
Tone: Balanced
Tone Reason: The reviewer provides both positive and negative points about the paper, showing a balanced tone, e.g., 'The paper was a pleasure to read. Very clear and well structured.' and 'The experiments are run on synthetic data and on real-data with only two ML models (SVM and LightGBM).'

Internal Consistency Check:
Consistent_in_itself: True
Consistency_reason: The review maintains a consistent critical viewpoint throughout, praising the paper's significance, clarity, and rigorous evaluation while also pointing out areas for improvement such as the limited range of experiments and baselines. The reviewer's suggestions for improvement are logically connected to their overall positive assessment, and their criticisms are constructive and specific.

Inter-Review Comparison:
is_consistent_with_others: False
alignment_score: 3
contradictory_points: The target review is more critical of the experiments and baselines compared to other reviews. It suggests that the experiments are not challenging enough and that more baselines should be considered, whereas other reviews seem to accept the experimental setup and findings more positively.
possible_bias_flags: The target review appears to be more demanding in terms of experimental scope and baseline comparisons, potentially indicating a bias towards requiring more stringent validation.
summary_of_differences: The target review is more critical and detailed in its assessment, focusing on the limitations of the experimental setup and the need for more baselines. Other reviews are generally more positive and accepting of the paper's contributions, with some suggesting minor revisions and clarifications.

Bias Detection:
bias_detected: True
bias_types: ['Methodology Bias', 'Confirmation Bias']
confidence_score: 8
evidence: The reviewer is more critical of the experimental setup and the limited range of baselines compared to other reviews. The tone is balanced, but the reviewer appears to be demanding more stringent validation, potentially indicating a bias towards requiring more comprehensive experiments and baselines.
suggestion_for_improvements: The reviewer should consider acknowledging the potential limitations of their own critical viewpoint and strive to provide a more balanced assessment that recognizes the contributions of the paper while still offering constructive criticisms.
