Reviewer 6: 
            "The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.\n\nThe main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. \n\n### Questions / Comments\nThe paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.\n\nIn this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.\n\nThe results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?\n\nAs the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).\n\nThe description of the baseline they call \"Naive\" is also not very clear. If would be good to have more details on this baseline.\n\nIn Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.\n\nTo summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.\n\n### Typos\nFigure 1b:\nComapring -> Comparing\n\nTable 1 caption:\nperforms almost the same with naive in Parkinson given\ntheir standard errors -> grammar should be improved" 

Sentiment Label: Neutral
Sentiment Reason: The reviewer seems to be confused by the paper. He wants to understand the paper but seems to miss the original intention of writing the paper. 
Tone Label: Formal
Tone Reason: The reviewer is quite particular and formal in his tone. 

Internal Consistency Label: Yes
Internal Consistency Reason: There is no inconsistent / contradictory statement in the review. 

Bias Detected: 
Bias Types: 
Confidence Score: 
Evidence: 
Suggestion for improvements: 