Paper titles:

1. Towards Fairer Peer Review: Structured Bias Detection Using LLM Agents
2. From Text to Justification: Detecting and Explaining Peer Review Bias with Structured Multi-Agent LLM Reasoning
3. BiasChain: A Multi-Agent LLM Framework for Justified Peer Review Bias Detection  
4. ReviewMind: A Chain-of-Agents Framework for Detecting and Explaining Bias in Peer Feedback


===========================================================================================================================================

Review 1:
"**Summary**\nIn the situation where a given objective is computed with samples from a distribution, e.g. loss on validation data in hyperparameter optimization, this paper proposes a method to construct a surrogate objective using objectives computed on sets of samples each of which is from a different distribution. Basic idea is to use a linear combination of importance sampling estimators. Moreover, the optimal linear combination coefficients are identified in a sense of being optimal in a certain family of convex combination coefficients. This approach has an interesting application that hyperparameters of machine learning deployed on an unseen dataset, importantly, without labels(output) can be optimized as long as the distribution of the input of the unseen dataset is samplable.\n\n\n**Strengths**\n1. The paper provides an algorithm that can estimate an objective computed on data without requiring access to labels in some optimal sense.\n2. Based on the proposed estimator, a transfer hyperparameter optimization algorithm to solve interesting problems is introduced.\n\n\n**Weaknesses**\n1. Maybe the novelty of the paper mainly lies in the combination not on inventing something new, which is, however, not certain since my coverage of relevant literature was not extensive.\n2. I can imagine that someone may ask for more experiments of a large scale or of the type exemplified in the intro. On the other hand, the experiments can be regarded as designed concisely to demonstrate the authors' main points.\n\n\n**Recommendation**\nOverall, I am willing to defend the acceptance of this paper. This combination of transfer HPO and importance sampling estimator seem novel, interesting, and well-demonstrated, with which many interesting applications can be imagined. \n\n\n**Questions**\n- On the line right below eq.(2), the loss function L is assumed to be bounded. Is this condition is necessary in proofs of any theoretical ones? It seems that, in all experiments, all losses are unbounded.\n\n\n**Additional feedback**\n- Explicitly emphasizing that Def 3 is motivated by eq.(6) may guide readers better.\n- While reading the paper, the questions arose were mostly answered after a few lines. The reading was pleasant for me and the paper is well-structured.\n"

Sentiment - Positive
Reason - Reviewer supports the paper, and good points are highlighted by them.
Tone - Supportive/Balanced
Reason - Both pros and cons are discussed with being supportive
Consistent with itself? - Yes
Reason - Start to end, the Reviewer supports the paper. Discusses strengths, weaknesses and Recommendation, but still in favour from start.

////////////////////////////////////////////////////////////////////////////////////////////////////

Review 2: 
"I'd like to thank the authors for the clarifications. I hope the authors will discuss and compare to more baselines and I look forward to the paper revision."

Sentiment - Positive
Reason - No use/intention of negative comment. Just thanking.
Tone - Supportve/Formal
Reason - Proper supportive language is used and no extra info.
Consistent in itself? - Yes
Reason - Same tone, sentiment and meaning throughout (only one thing discussed and i.e. suggestion)

////////////////////////////////////////////////////////////////////////////////////////////////////

Review 3:
"I thank the authors for their answer. However I want to challenge the following point \n\n\"the aim of this paper is to obtain the optimal set of hyperparameters\"\n\nHow HPO is generally motivated is: in practice there are many situations where model X or Y is used, but the HPs are suboptimal, or tuning them manually takes a lot of time or effort. So the general aim for supervised ML is to produce a good model according to some metric, and one way to improve many of the best models (the hyperparameters) is by HPO.\n\nWe should not forget the overall goal (providing a good model), when looking at the subgoal (finding good HPs to further improve a model). The final goal in the task described in the paper appears to be to provide a regression model on unlabeled data, where labeled transfer learning data is not available, but labeled source tasks are. I don't understand why it is not simply framed as thus, and then (if it is the case) motivate why HPO is important in that setting.\n\nOnce the problem is framed as such, better baselines (that do not need to use HPO) can come to mind. For example, just train a model for each source task, and predict their average or median, on the target task. A slight variation: train a single model where the source task is one hot encoded as a feature; then at prediction again one can use the mean or median of what would be the prediction for the various source tasks. An even simpler idea: concatenate all source tasks and just train a model on these, use the obtained model directly for predicting on the target. (Or is this Naive? My understanding from the paper is that only the HP configuration found this way is used, and then a new model is trained only using the target dataset and f_hat as an objective, but I found the section \"Experimental Procedure\" hard to follow). In each of these cases there is no need to perform HPO, just either use the default parameters or perform HPO on the source tasks."

Sentiment - Negative
Reason - Only negative aspects is mainly discussed
Tone - Critical
Reason - I want to challenge, baseline using alternative approaches, experiments Procedure section confusing, etc
Consistent in itself? - Yes
Reason - The paper is challenged by the author throughout and is on the negative side

/////////////////////////////////////////////////////////////////////////////////////////////////////

Review 4: 
"Strong points:\n- Well-written and easy to read and follow.\n- Results show the method does what it promises to.\n\nWeaknesses:\n- While it was easy to follow the paper's rationale, I found it difficult to motivate. Until the final experiment, I was left wondering what kind of application would have these constraints but also where the assumptions would be reasonable. This point I think should be easy to address given a small rework of the intro or perhaps a running example to periodically come back to.\n- After reading about the last experiment, I found myself wondering why this solution is billed as a hyperparameter optimization solution; it sounds to me that the parameters of the model are also being optimized along the way. Again, this is a question of clarification and can easily be addressed.\n- The fact that the Naive method beats the other two baselines and performs comparably to the unbiased proposed method makes me wonder whether (a) these are challenging enough tasks, or (b) those are competitive baselines.\n- The density estimator and the divergence estimator are moving parts the errors of which perhaps warrant quantifying. For example, in both experiments the true labels were known and the authors can measure the error in the divergence estimate.\n\nI'd be happy to increase my score if the above points are addressed."

Sentiment - Negative
Reason - Greater weaknesses and dominant. 
Tone - Critical
Reason - Many suggestive changes required
Consistent in itself? - Yes
Reason - The review comment doesnt contradict itself. All the sentences are consistent

/////////////////////////////////////////////////////////////////////////////////////////////////////

Review 5:
"The paper introduces multi-source unsupervised hyperparameter optimization (MSU-HPO), a novel BO framework where a range of related tasks are available but labels cannot be accessed for the target task. As ground truth on the target task is unavailable, the work introduces two estimators to approximate the target task objective. This enables HPO to be run to optimize the hyperparameters on the target task, converging faster to a good hyperparameter configuration.\n\n\nPositive\n\n1. **Significance.** To my knowledge, this is the first paper to investigate a transfer learning setting for HPO where labels are unavailable for the target task. This is of interest in several practical applications (e.g., advertising, as the authors discuss). The exploration of a new problem together with the introduction of principled estimators make both the paper's goal and methodology significant.\u2028\n2. **Clarity.** The paper was a pleasure to read. Very clear and well structured. I am also confident about reproducibility as enough details about the algorithm and experimental setup are provided.\u2028\n3. **Rigorous evaluation.** I appreciated the solid theoretical analysis paired with fully-controlled synthetic experiments where the degree of task similarity can be regulated and results compared against ground truth. All figures are based on multiple runs and have clearly detailed error bars.\u2028\n\nNegative\n\n1. **Easy experiments.** The experiments are run on synthetic data and on real-data with only two ML models (SVM and LightGBM). This is secondary considering that the theoretical analysis is solid, but tuning a wider range of ML algorithms (such as neural networks/NAS) would have made the case even stronger by showing that transfer learning is possible across a diverse class of models. The dimensionality of the optimized hyperparameter spaces is also very small, with respectively two and four tuned hyperparameters for SVM and LightGBM. Applying the method to more challenging scenarios would further demonstrate the benefits of the proposed approach.\u2028\n2. **Not many baselines.** The main baselines the method is compared against are LI and DISTBO. While many transfer learning baselines are inapplicable as most assume target labels to be available (as discussed in the related work), this is not the case for all of them. An example is Feurer, et al.: Initializing Bayesian hyperparameter optimization via meta-learning, AAAI, 2015. This is not referenced but should be discussed, as it only uses hyperparameter configurations from previous related tasks to warm-start the new optimization. As this does not look at the labels of the target task, it could be compared against. This is also the case for Perrone et al., 2019, which learn a search space purely based on previous tasks and does not require target labels. Another search space pruning method that is not compared against nor discussed is Wistuba, et al.: Hyperparameter search space pruning\u2013a new component for sequential model-based hyperparameter optimization, in Machine Learning and Knowledge Discovery in Databases, 2015.\n\nOverall, I am inclined towards accepting this paper due to the rigorous theoretical evaluation, the significance of the problem, and the fairly satisfactory experimental evaluation (although more baselines and more challenging experiments would be needed to make a stronger case).\n\nAdditional questions\n\na. Related to the point above, how does the proposed approach compare to the transfer learning baselines references above that are purely based on learning an initialization or a good search space?\n\nb. The naive method performs on part with LI in the synthetic experiments and clearly better than DISTBO in the real-world ones. Is this expected? Does this indicate that the chosen baselines (LI and DISTBO) may be too weak? In appendix D3 the authors discuss that this is probably due to the fact that only 50 hyperparameter evaluations on source tasks are available. Would it then not be useful to re-run the comparison under a varying number of hyperparameter evaluations available from source tasks? If the proposed method only outperforms baselines when a small number of prior evaluations are available, this should be clearly stated and shown.\u2028\n\nc. Considering the relatively poor results of the unbiased estimator, would it not be better to re-frame the narrative to focus on the variance-reduced estimator? The unbiased one is an interesting ablation study, but given the results it might be better to clearly state from the beginning that the variance reduced estimator is the recommended choice.\u2028\n\nd. Will code be made available? \n\nTypos:\n1. Page 12: \"omited\" ---> \"omitted\"\n2. Figure 1b: \"Comapring\" ----> \"Comparing\""

Sentiment: Positive
Sentiment Reason: Reviewer is inclined towards the acceptance of the paper
Tone: Balanced
Tone Reason: Fairly both strengths and weaknesses are described, with praising and criticizing where needed
Consistent in itself? - Yes
Reason - Statements logically align with each other and no contradiction present


/////////////////////////////////////////////////////////////////////////////////////////////////////

Review 6:
"The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.\n\nThe main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. \n\n### Questions / Comments\nThe paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.\n\nIn this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.\n\nThe results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?\n\nAs the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).\n\nThe description of the baseline they call \"Naive\" is also not very clear. If would be good to have more details on this baseline.\n\nIn Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.\n\nTo summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.\n\n### Typos\nFigure 1b:\nComapring -> Comparing\n\nTable 1 caption:\nperforms almost the same with naive in Parkinson given\ntheir standard errors -> grammar should be improved"

Sentiment: Negative
Sentiment Reason: Review is mainly of doubts and criticism
Tone: Critical
Tone Reason: Full criticism with words like 'I might be missing something, it is not clear to me'
Consistent in itself? - Yes
Reason - Confusion and concerns are logically structured


===========================================================================================================================================

Summary of every review (hinglish):

1. Reviewer ko idea acha laga. Bola ki bina labels ke target pe HPO karna importance sampling ke saath interesting hai. Experiments concise hain par sufficient lagte hain. Thoda doubt hai ki kya idea bilkul naya hai ya sirf existing concepts ka combo, but fir bhi acceptance ko support karega. Paper clearly written hai.

2. Bas itna bola ki clarification ke liye thanks, aur umeed hai ki authors aur baselines add karein paper revision me. Zyada detail nahi diya, par tone positive hai.

3. Ye reviewer confused hai ki paper ko HPO paper kyu bola gaya. Unka kehna hai ki problem unsupervised regression ki tarah dikh rahi hai, aur aise mein aur simple baselines ho sakte the jo bina HPO ke kaam kar lete. Suggest kiya ki problem framing aur baselines dono better ho sakte the. Experiments section bhi unclear laga.

4. Paper clearly written hai aur experiments kaafi straightforward hain. Par reviewer ko motivation weak laga — matlab real world application properly samjha nahi gaya. Naive method strong perform kar gaya toh baselines pe bhi sawal uthaye. Bola ki agar kuch clarifications de diye gaye toh wo score badha sakte hai.

5. Ye reviewer impressed hai — bola ki naya setting (MSU-HPO) introduce kiya gaya, theory strong hai, aur paper clearly likha gaya. Experiments controlled hain. Par kuch cheezein missing hai jaise: zyada challenging datasets/models aur strong baselines. Suggested ki aur kuch transfer baselines compare karo, jaise meta-learning wale. Overall, accept karne ke favor me hai.

6. Reviewer bol raha hai ki ye paper HPO ke naam pe hai, par actual me unsupervised prediction kar raha hai using label estimators. HPO bas ek part lagta hai. Suggest kiya ki paper ko unsupervised transfer learning ke perspective se dekhna chahiye. Baselines weak hai aur clarity missing hai experimental procedure me. Bola ki idea interesting hai, par motivation aur setup aur strong hona chahiye.


===========================================================================================================================================


### Review-wise

Is consistent with others:
1. True
--> Review 3 and 6 challenges the framing of the problem but aligns with 2, 4, 5. Majority alignment
Alignment Score: 6

2. False
--> While others talk of doubts and refinement, this review simply says thanks for clarification
Alignment Score: 3

3. True
--> 3,4,6 all doubt the framing of the PS, but 1, and 5 are somewhat impressed (opposite). 2 only appreciates the changes required. Since majority is doubting the framing, i'll go with True 
Alignment Score: 6

4. True
--> Somewhat aligned with all other reviews, shares concerns but balanced.
Alignment Score: 8

5. False
--> Mostly aligned with 1 and 4, but others dominate doubting the framing
Alignment Score: 5

6. True
--> Strong consistency with 3, 4 for baselines and strongly with 3 for framing.
Alignment Score: 7



Biases:

1. Novelty Bias
--> Reviewer questions if the work is novel or just increamental

2. None

3. Methodology Bias
--> Biased towards using simple baselines, reframing as per themselves

4. None

5. Novelty Bias
--> Reviewer needs stronger real world Justification

6. Methodology Bias
--> Reframing the paper as unsupervised learning and not HPO