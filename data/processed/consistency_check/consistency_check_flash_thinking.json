[
    {
        "paper_id": "iclr_2021_IFqrg1p5Bc",
        "paper_title": "Distance-Based Regularisation of Deep Networks for Fine-Tuning",
        "paper_abstract": "We investigate approaches to regularisation during fine-tuning of deep neural networks. First we provide a neural network generalisation bound based on Rademacher complexity that uses the distance the weights have moved from their initial values. This bound has no direct dependence on the number of weights and compares favourably to other bounds when applied to convolutional networks. Our bound is highly relevant for fine-tuning, because providing a network with a good initialisation based on transfer learning means that learning can modify the weights less, and hence achieve tighter generalisation. Inspired by this, we develop a simple yet effective fine-tuning algorithm that constrains the hypothesis class to a small sphere centred on the initial pre-trained weights, thus obtaining provably better generalisation performance than conventional transfer learning. Empirical evaluation shows that our algorithm works well, corroborating our theoretical results. It outperforms both state of the art fine-tuning competitors, and penalty-based alternatives that we show do not directly constrain the radius of the search space.",
        "review_ids": [
            "GUlmylQMl3",
            "pc1JzK8Hlux",
            "TdQmSVH7w_w",
            "LKQ3ydPIzdw",
            "f2loxf--1wI",
            "xcI5x9eToXG",
            "T4CF1Ux4unB",
            "Hjv1qXuZX8s",
            "45MnKjVIYKF",
            "vnVDQEFdYDN",
            "WdO-NXHXhZf",
            "OJVhTLAicgv"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes new regularization methods for fine-tuning deep neural networks based on matrix $\\infty$-norm distance. The authors claim that their choice of matrix $\\infty$-norm distance is more suitable than commonly used Frobenius norm distance (a.k.a., Euclidean distance) when measuring the distance in the parameter space of convolutional networks by a comparison of two generalization bounds. Moreover, the authors empirically show that enforcing a hard constraint on the weights by projected methods throughout the training process is more effective in regularizing neural networks than widely used strategy of adding a penalty term to the objective function.\n\nOverall, the paper is well written and has a nice logical flow. The problem of finding a proper distance metric for fine-tuning is interesting, though I have a few concerns outlined below regarding their theoretical analysis of using generalization bound to guide the choice of distance metric, especially the proof of the theorems. \n\nConcerns:\n1. The authors try to modify the peeling technique of prior work to prove two generalization bounds, i.e., Theorem 1 and Theorem 2. A key step in proving the two theorems is to prove Lemma 2 given in the Appendix. However, from the proof of Lemma 2, if I understand correctly, the second equality and the fourth equality seem to interchange the order of sum and supremum freely, i.e., $\\sum_{j=1}^n v_j \\sup_{W_{1:k}} \u2026=\\sup_{W_{1:k}} \\sum_{j=1}^n v_j\u2026$, which of course does not hold in general. It should be stated clearly on why the two equalities hold here.\n\n2. The authors provide two generalization bounds for fine-tuning. The two bounds are almost the same except for the norm used. The authors then claim that a comparison of the two bounds suggests that matrix $\\infty$-norm is more effective than Frobenius norm when measuring distance in weight space of neural networks just because matrix $\\infty$-norm itself is independent of the feature map size. This is misleading in the sense that matrix $\\infty$-norm and Frobenius norm are actually equivalent, i.e., for an arbitrary matrix, its matrix $\\infty$-norm is not strictly smaller than its Frobenius norm and vice versa, and thus the two bounds are also equivalent and cannot be used to tell which norm is better. Therefore, I do not think that their choice of matrix $\\infty$-norm  as the distance metric can be theoretically justified by comparing the two generalization bounds as in the paper, despite that empirical results show that their method performs well in practice.\n\n3. In section 5.3, the authors hope to demonstrate the ability of the distance-based regularization methods to control model capacity by sweeping through a range of hyperparameter values and plotting the corresponding predictive performance. The authors claim that Figure 2 shows that the PGM methods behave as the theoretical analysis predicts and the penalty-based approaches are not able to influence the model capacity as much as the constraint based approaches. This statement is inaccurate in several ways. First, the symbol $\\lambda_j$ in the third line is confusing. It seems to represent the hyperparameter for both the constraint based methods and penalty methods. However, $\\lambda_j$ first appears in equation (5) where it represents the hyperparameter for penalty methods. Second, from Figure 2, as $c$ becomes larger and larger, there is only a very small drop of accuracy for the PGM methods. So, it does not lead to overfitting, and the PGM methods do not behave exactly as the generalization bound predicts. Third, small $c$ for PGM methods corresponds to large $c$ for penalty methods by the equivalence of constraint methods and penalty methods. Therefore, Figure 2 shows that the penalty-based approaches actually have the same influence on the model capacity as the constraint based methods.\n\nMinor comments:\n- From the proof of Theorem 2, the term $\\sqrt{c}$ in the bound should be $c$. Therefore, the bounds in Theorem 1 and Theorem 2 exhibit the same dependence on the number of classes.\n\n- I am a little confused by the sentence \u201cIn the case of the final classification layer, $W_L^0$ can be randomly initialized.\u201d in 7th line of Section 3. Do you mean that $W_j^0$s ($j<L$) are pre-defined and fixed, but $W_L^0$ is random? However, when proving the upper bound for empirical Rademacher complexity, especially the last step where the rightmost term evaluates to zero, it seems that you assume that all these matrices $W_j^0$ ($1\\leq j\\leq L$) are fixed. It would be better if this can be clarified.\n\n- In section 4 and Appendix E, to support the claim that projection based methods are better than penalty based methods, the authors state that penalty methods have weaker assurance on whether a constraint is being forced. However, Figure 1 shows that for ResNet101 model penalty-based method is actually more effective in enforcing the constraints in the sense that not only it successfully constraints weight distance to be less than $\\gamma_j$, but also the number of weights which have small distance is larger. Therefore, more evidence might be needed to support their claim.\n\nSome typos:\n\n(1) In line 6 of Page 2, best way restrict -> best way to restrict\n\n(2) In the last line of Page 5, change the $l^1$ distance-> change the MARS distance\n\n(3) In the third line of the proof of Lemma 2, $\\varphi_j$-> $\\varphi$\n\n(4) In the third formula of the proof of Theorem 2, $sqrt{2}$-> $\\sqrt{2}$",
            "The reviewer thanks the authors for the response. My concerns have been partially addressed. Therefore I have updated my score to reflect the change. Thanks.",
            "This paper studies regularization for neural network fine-tuning, motivated by limiting deviation of the final model from the initialization states. The provide a generalization bound that utilizes a novel Rademacher complexity term built on the layer weights and their deviation from the initial weights. This bound relates particularly to fine-tuning, since a part of the bound can be fixed to the pre-trained weights, providing an alternative regularization objective specific to fine-tuning. Using this objective, the authors provide several fine-tuning benchmark experiments and demonstrate competitive performance.\n\nStrengths of the paper:\n- Well written, easy to follows.\n- Motivation for the algorithm stems directly from the analysis, as opposed to heuristic-style arguments that typically dominate the field of CV /deep learning research, especially for fine-tuning. Moreover, the generalization bounds are derived such that they lead to an optimization objective (as opposed to conventional approaches that typically have not led directly to an effective algorithm).\n- The analysis appears to be general, without any particularly strong assumptions.\n- Two different norms are considered with corresponding algorithms and experiments.\n- Extensive ablations are performed on vision tasks.\n\nWeaknesses:\n- Only tested on computer vision benchmarks. If the paper claims this approach to be a general technique then it is necessary that the methods do well on other tasks (e.g., language), otherwise the experimental claims rely too much on the convolutional inductive biases.\n- If the paper is in fact framed as a CV paper, then it is natural that a comparison be made with respect to prior (albeit heuristic) computer vision research, e.g., label-smoothing regularization, entropy regularization and so on.\n- An empirical comparison of the tightness of the bounds is warranted given the deviation of this analysis from PAC-Bayesian (Neyshabur 2018) or spectral norms (Bartlett and Long).",
            "Thank you for the update! I have updated my review to reflect accordingly.",
            "Thanks for all the clarifications. \n\nTo clarify my comment on suboptimal experimental setups, I mean that the learning protocol used here does not lead to the performances obtained elsewhere. For example, the results obtained in the l2-SP and DELTA papers for subsets of Caltech are much better than those reported in Table 1. Also [Kornblith et al.: Do Better ImageNet Models Transfer Better? CVPR 2019: 2661-2671], which is not about improving transfer learning, reports results with their fine-tuning baseline (that would be your \"None\" in Table 1) that are better than the best results reported here among all the methods for ResNet 101, and this by a wide margin: Aircraft 10.4%, Flowers 7.5%, Pets 3.6%, DTD 2.6%, Caltech 8.3%. \n\nI understood that the purpose of Table 3 was to show the performance that would be achieved with your approach on a more SOTA transfer learning scheme, so it should be about genuine transfer. Reproducing past results on benchmarks that are now known to be flawed is not helpful in this regard.\n\n",
            "Thank you all for the discussion which has already raised and clarified important issues. I found the document rather confusing on some points, so there are still a number of points I would like to raise with all of you (somewhat rewording some of the reviewers' questions):\n\nFocus on fine-tuning:\nWhy does the paper only discusses fine-tuning? The main theorem provides a bound on the risk without stating anything about the origin of the initial (pre-trained) weights. Thus, this theorem is no less relevant for standard learning with initial random parameters than it is for fine-tuning from pre-trained parameters. Why do the authors only consider fine-tuning, and why is this theorem not related to the ones pertaining to \"standard\" learning from scratch?\n\nExperiments:\nIt is quite peculiar to compare algorithms on sub-optimal setups (Table 1) and better setups on flawed experimental benchmarks (Table 3). A simple corrected version of Stanford Dogs is provided in [Li et al.: A baseline regularization scheme for transfer learning with convolutional neural networks. Pattern Recognit. 98 (2020)], avoiding the overlap between the pre-training set of ImageNet and the fine-tuning training set for transfer. I suppose the same protocol could be applied to CUB-2011.\n\nHyper-parameters:\nStill on the subject of experiments experiments, I think that the paper should be more precise regarding the tuning of hyper-parameters, which is crucial here because of the importance given in the paper to the difference between the formulations relying on hard constraints and penalties. Giving the same number of trials in HyperOpt is not sufficient to ensure fairness; the parameters given to HyperOpt should be given, with an assessment of the relevance of the given intervals for gamma and lambda. \nAlso regarding hyper-parameters, I don't get the message of Section 5.3: what should be inferred from Figure 2 that compares the effects of the variations of the hyper-parameters \\gamma and \\lambda? These hyper-parameters are not commensurable; why a multiplicative update of the optimal gamma should be expected to correspond to the same (or inverse) multiplicative update of \\lambda?\n\nConvergence of penalty-based approaches:\nA last point for me very debatable, is the assessment of convergence given in Figure 3. The penalty of MARS-SP being not differentiable, why should the norm of the gradient be relevant for assessing convergence? If the solution is, as expected,  at a non-differentiable point, the norm of all subgradients is not expected to go to zero, and points in the vicinity of the solution are not expected to have small gradients.  \nAlso regarding this figure: the number of epochs reported in the DELTA paper is several thousands, here the experiment shows only 30 epochs: is this representative of all the experiments carried out elsewhere in this paper?",
            "This is exactly what I was asking for. \n\nSo basically the key in your sketch of proof is that the bound only depends on the norm constraints instead of W_0, so it holds uniformly for all W_0s that satisfy the norm constraints. \n\nCan you add a formal proof to the appendix and extend your theorem from a fixed W_0 to the uniform bound? \n\nI am changing my rating to 6 to reflect the change. \n",
            "The work proposes a Rademacher type bound for the fine-tuned models based on the distance between the fine-tuned weights and the pre-trained weights. Since the distance term shows up in the upper bound on the generalization gap, the authors further propose to adopt it as the regularization term to boost the generalization performance of the model during the fine-tuning process. Some experiments are also done to show the effectiveness of the proposed regularization.\n\nI am seriously concerned about the correctness of the Rademacher-type bound the authors have proposed. The bound does not seem correct to me.\n\nThe flaw comes from the function class F_* defined in section 3 of the draft. The function class F_*, by definition, depends on the pre-trained weights W_j^0. However, W_j^0 is not fixed, it is random! This is because W_j^0 depends on the data (W_j^0 is pre-trained using the data), which by the assumption of the draft, is random. As a consequence you cannot assume W_j^0 as fixed. The randomness of the hypothesis class F_* destroys almost all the derivations the authors are currently using in their proof. \n\nAnother minor bug is the second term in the bound for theorem 1 seems to have some subscript issues. To me the product term related to B_j^\\infty should go from i=1 to j instead of from j=1 to L. I may have missed something in this point but could the authors double check if the subscript of the B_j^\\infty is correct? In particular the derivation from the second inequality to the third on page 13 of the appendix. \n\nThe second issue is easy to fix. However the first issue seems like a fundamental flaw. I do not have a good way to handle it for now. ",
            "Thanks R2 for the discussion. Actually what I suggested for Q1 was very much aligned with R2\u2019s comments but with a different opinion. \n\nTo claim a 1-\\delta probability bound the authors can do either of the following:\n\n1). take into account the randomness of W_0 by averaging all possible W_0 of the pre-training procedure. I suggested a union bound but that is an overkill. looks like the authors are not going this route but to me this may be the desired way. \n\n2). assume W_0 is fixed. With a fixed W_0 one can follow the normal Rademacher calculus to get some generalization guarantee. However this is implicitly assuming that a) the pre-training data are all fixed. b) the pre-training procedure is deterministic. This is usually NOT the case in the pre-training process. \n\nAs R2 says, it all boils down to the assumptions the authors have made. To me the assumption that W_0 is fixed is too strong and the problem is not interesting any more.  I would prefer the authors do 1) in their theorem instead.  \n",
            "Thanks R1 and authors for the discussion. \n\nI do not understand Q1 completely, however. The authors assume a norm bound on the initial weights as well as a norm bound on the final weights and the distance between initial and final weights. In learning theory these bounds seem to be perfectly in line with standard assumptions in the field. \n\nApplying a union bound (as R1 suggests) will not further any insight as the results already hold with high probability for *any* initial weights W_0 that satisfy the bounded norm constraint. Moreover, the results also hold for *any* final weights W_t that satisfy their norm constraint and the distance constraint.\n\nSure, having a bound that would *simultaneously* be true for all W_0 and W_t within the respective regions can be obtained, and a simple way would be by a covering argument and union bound over the respective balls, leading to an additional covering radius (radii?) in the bound. But what exactly is the utility of such a result? \n\nWhenever we perform fine-tuning or transfer learning in practice, we are always provided with the initial weights, therefore it does not add much value to have a result that is needlessly diluted to hold simultaneously over all possible initial weights. The essence of Theorem 1 is to provide a control over model capacity that is usable in practice, and the 1-\\delta is to account for the randomness in the data, and not the model space itself.\n\nOne can further this line of reasoning and ask for bounds that are averaged over all possible initial weights for the first training procedure, since they also are sampled at random (in fact, it would make more sense there, since those weights are actually initialized randomly). One can also question the norm bound, since we could be given a W_0 that does not obey the norm bound itself, and then we must have to make another covering argument.\n\nIn summary, it boils down to the assumptions that the authors make, and given that every transfer learning algorithm till date assumes knowledge of initial weights (I have yet to see a transfer learning algorithm that uses random initial weights, but I may be wrong), I do not see any reason why that cannot be assumed for this result as well.",
            "Thanks very much for the clarification. In particular I would like to thank the authors for double checking the subscripts for Q2. \n\nSomewhat the authors\u2019 comment on Q1 does not fully address my concern.\n\nPretrained weights are independent of the training data used for fine-tuning, but that does not lead to the conclusion that they are not random. They can still be random variables and independent of the fine-tuning process. If so what the authors have proved is a generalization bound conditioned on W_0. To give a full claim on the 1-\\delta probability you may want to work on a theorem for all W_0s by either a union bound or other types of more delicate methods. Otherwise the probability 1-\\delta does not seem right.\n\nTo me the only way that you can claim W_0 is fixed is by assuming the data in the pretraining process are all fixed. This assumption seems so strong that the problem is not interesting any more. \n",
            "In this manuscript the authors derive a bound on the rademacher complexity of neural network models which can be written as a funciton of the MARS norms of the weights in the network. This motivates the authors to put a regularization on the MARS norm of the network weights instead of the more typical L2 norm. Here the authors implement this regularization as a hard bound on the weights, which they enforce by projecting the weights back on the allowed ball. They use their regularization for transfer learning of ResNet-101 and EfficientNetB0 from ImageNet onto the set of smaller image classification tasks. On these tasks, the projection methods and to a smaller degree the MARS based methods generalize better. \n\nOverall I vote for acceptance. This is an interesting contribution to the literature, providing both a theoretical insight and an experimental test that this theoretical insight is relevant for applications. However there is a certain disconnect between the theory and the experimental observations. Performance  benefits more from the projection methods than from the switch of norm although the switch of norm has a much stronger theoretical justification.\n\nPros: \n1) Well structured paper with interesting results\n2) Theoretical results are well justified to be more helpful than existing bounds.\n3) There is an empirical test that the switch in bound is helpful for practice.\n4) Overall the generalization is actually improved.\n\nCons:\n1) Empirically the less justified change has a larger impact, indicating that there might be another more important theoretical insight\n2) The hyperparameter setting procedure remains opaque. The authors always talk about gamma_i/ lambda_i parameters changing the strength of regularization per layer, but only test how scaling all regularizations up or down affects performance. A description how the values were chosen is really necessary I think and some analysis to convince us that the worse performance of the regularization is not caused by a bad hyperparameter choice would definitely be a plus.\n3) I think there is a bit of a missed opportunity here for the scaling over layers as the bound suggests an unequal weighting of the layer norms. I think directly regularization of the bound which would allow layers to compensate for each other or giving each layer an equal budget in terms of raising the bound would be interesting variants here."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Positive",
            "Negative",
            "Negative",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the paper's theoretical justification and experimental validation. The reviewer points out potential flaws in the proofs of the theorems, questions the validity of comparing generalization bounds based on different norms, and raises concerns about the interpretation of experimental results. The reviewer concludes that the choice of matrix $\\infty$-norm cannot be theoretically justified by the paper's analysis.",
            "The reviewer explicitly states that their concerns have been 'partially addressed' and they have 'updated my score to reflect the change,' indicating a positive shift in their assessment.",
            "The review expresses overall positive feedback, highlighting the paper's strengths such as being well-written, having a strong motivation, general analysis, consideration of different norms, and extensive ablations. While weaknesses are pointed out, they are framed as areas for improvement rather than fundamental flaws.",
            "The reviewer expresses gratitude ('Thank you') and indicates a positive action ('updated my review to reflect accordingly').",
            "The reviewer points out discrepancies in the results compared to other papers, suggesting the experimental setup is \"suboptimal\" and the benchmarks used are \"flawed\", indicating a negative assessment of the research methodology and results.",
            "The review expresses confusion and disagreement with several aspects of the paper, including its focus, experimental design, and analysis. The reviewer uses phrases like 'rather confusing,' 'peculiar,' 'flawed experimental benchmarks,' 'not sufficient to ensure fairness,' 'I don't get the message,' and 'very debatable' to express their negative assessment.",
            "The reviewer states \"This is exactly what I was asking for,\" indicating satisfaction. They are also increasing their rating, which suggests a positive shift in their assessment.",
            "The reviewer expresses serious concerns about the correctness of the proposed bound, stating it 'does not seem correct' and identifying a 'fundamental flaw.' They also mention a 'minor bug,' further contributing to the negative sentiment.",
            "The reviewer expresses a preference for an alternative approach and finds the current assumption 'too strong' and the problem 'not interesting any more'. This indicates a negative assessment of the current methodology.",
            "The review expresses a mix of agreement and disagreement. While acknowledging the validity of the authors' assumptions, it questions the utility of a suggested alternative approach, indicating a neutral overall sentiment.",
            "The review expresses concerns about the authors' response to a previous comment (Q1) and questions the validity of their generalization bound, suggesting a potential flaw in their methodology. The reviewer uses phrases like 'does not fully address my concern' and 'does not seem right,' indicating disagreement and criticism.",
            "The reviewer votes for acceptance and states the paper is an \"interesting contribution to the literature\" with \"interesting results\" and improved generalization. They also note that the theoretical results are well justified and empirically helpful."
        ],
        "tone": [
            "Critical",
            "Neutral",
            "Balanced",
            "Supportive",
            "Critical",
            "Critical",
            "Supportive",
            "Critical",
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like 'misleading,' 'inaccurate,' and 'does not hold in general' to express strong disagreement with the authors' claims. The reviewer also uses specific examples and detailed explanations to support their criticisms, indicating a thorough and critical evaluation of the paper's content.",
            "The language is polite and professional ('thanks the authors,' 'my concerns,' 'partially addressed,' 'updated my score'), but lacks strong emotional coloring, maintaining a neutral stance.",
            "The review provides both strengths and weaknesses of the paper, presenting a balanced perspective. The use of phrases like 'Strengths of the paper' and 'Weaknesses' explicitly structures the feedback, and the language is objective and constructive.",
            "The reviewer's expression of gratitude and willingness to update the review suggests a supportive and collaborative tone.",
            "The reviewer points out discrepancies in the results compared to other papers, suggesting the experimental setup is \"suboptimal\" and the benchmarks used are \"flawed\", indicating a critical tone.",
            "The review adopts a critical tone by directly questioning the paper's choices and methodology. For instance, the reviewer asks \"Why does the paper only discuss fine-tuning?\" and states \"It is quite peculiar to compare algorithms on sub-optimal setups.\" The use of strong words like 'flawed' and 'debatable' further contributes to the critical tone.",
            "The reviewer's tone is supportive, offering constructive suggestions for improvement (\"Can you add a formal proof to the appendix and extend your theorem...\"). The statement \"This is exactly what I was asking for\" further reinforces this supportive tone.",
            "The review uses strong, critical language such as 'seriously concerned,' 'flaw,' and 'destroys almost all the derivations.' The reviewer directly challenges the validity of the authors' approach and its underlying assumptions.",
            "The reviewer uses phrases like 'an overkill', 'too strong', and 'not interesting any more', which convey a critical evaluation of the authors' approach and assumptions. The reviewer also expresses a preference for an alternative method, suggesting the current one is lacking.",
            "The review presents a balanced perspective, acknowledging the authors' work while also critically examining suggestions and questioning the potential value of alternative approaches. Phrases like 'I do not see any reason why that cannot be assumed' indicate a reasoned and considered tone.",
            "The tone is critical due to phrases like 'does not fully address my concern,' 'does not seem right,' and 'This assumption seems so strong that the problem is not interesting any more.' The reviewer directly challenges the authors' reasoning and assumptions.",
            "The review presents both positive aspects (\"Pros\") and negative aspects (\"Cons\") of the paper, offering constructive criticism alongside praise. The reviewer uses phrases like \"Overall I vote for acceptance\" and \"interesting contribution\" while also pointing out \"a certain disconnect\" and areas for improvement."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critique, focusing on the theoretical weaknesses and misinterpretations of results in the paper. The reviewer raises several concerns regarding the mathematical correctness of proofs, the validity of the theoretical justification for the proposed method, and the interpretation of experimental findings. All points consistently argue against the paper's claims and highlight areas needing improvement or clarification.",
            "The review is consistent because the reviewer expresses gratitude for the authors' response, acknowledges that their concerns have been partially addressed, and logically concludes that they have updated their score to reflect this change. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because the weaknesses pointed out are not contradicting the strengths. The reviewer acknowledges the paper's strengths in theoretical grounding and experimental validation within the CV domain, while suggesting improvements in terms of generalizability and comparison with existing methods. The weaknesses are presented as limitations and suggestions for improvement, not as fundamental flaws that would contradict the acknowledged strengths.",
            "The review is consistent as it expresses a clear and logical action of updating the review based on an update from the authors. There are no contradictory statements or conflicting viewpoints presented.",
            "The review is consistent because the reviewer clearly argues that the experimental setup is suboptimal by comparing the paper's results with existing literature and highlighting the use of potentially flawed benchmarks. The reviewer provides specific examples and references to support their claims, maintaining a logical and coherent argument throughout the review.",
            "The review is consistent because it raises valid and related concerns about different aspects of the paper (focus, experiments, hyperparameters, convergence) without contradicting itself. The reviewer consistently points out weaknesses and suggests improvements.",
            "The review is consistent because the reviewer expresses satisfaction that their previous request has been addressed and provides constructive feedback for further improvement, while also increasing the rating to reflect the positive changes.",
            "The review is consistent because it identifies both a major fundamental flaw related to the randomness of pre-trained weights and a minor bug in the theorem. The reviewer consistently expresses concern about the correctness of the proposed method based on these identified issues.",
            "The review is consistent because the reviewer clearly outlines two possible approaches for the authors to address a problem, discusses the pros and cons of each approach, and consistently expresses a preference for one approach over the other. There are no self-contradictory statements or conflicting opinions within the review.",
            "The reviewer maintains a consistent stance throughout the review, arguing that the authors' assumptions are reasonable and practical in the context of transfer learning. The reviewer logically defends the authors' approach against the suggestion of applying a union bound, and the arguments presented are coherent and do not contradict each other.",
            "The review is consistent because the reviewer acknowledges the authors' effort in addressing their previous comments but maintains a specific concern regarding the randomness of pretrained weights and its implications for the generalization bound. The reviewer's arguments are logically connected and focused on this specific point, without contradicting themselves.",
            "The review is consistent. The reviewer acknowledges the paper's strengths, including theoretical justification and empirical validation, leading to an overall positive assessment and a vote for acceptance. While pointing out weaknesses like the disconnect between theory and experimental results and opacity in hyperparameter settings, these are presented as constructive criticisms and areas for improvement rather than contradictions to the overall positive evaluation of the paper's contribution."
        ]
    },
    {
        "paper_id": "nips_2021_khZGbgRQjjM",
        "paper_title": "Stylized Dialogue Generation with Multi-Pass Dual Learning",
        "paper_abstract": "Stylized dialogue generation, which aims to generate a given-style response for an input context, plays a vital role in intelligent dialogue systems. Considering there is no parallel data between the contexts and the responses of target style S1, existing works mainly use back translation to generate stylized synthetic data for training, where the data about context, target style S1 and an intermediate style S0 is used. However, the interaction among these texts is not fully exploited, and the pseudo contexts are not adequately modeled. To overcome the above difficulties, we propose multi-pass dual learning (MPDL), which leverages the duality among the context, response of style S1 and response of style S_0. MPDL builds mappings among the above three domains, where the context should be reconstructed by the MPDL framework, and the reconstruction error is used as the training signal. To evaluate the quality of synthetic data, we also introduce discriminators that effectively measure how a pseudo sequence matches the specific domain, and the evaluation result is used as the weight for that data. Evaluation results indicate that our method obtains significant improvement over previous baselines.\n",
        "review_ids": [
            "O8vM_cMz9y-",
            "T2ikjIJ68m8",
            "TPQmV9oN9dh",
            "QaXWYZrjQYh",
            "WXT2qN1D6b1",
            "hvp85AgF7a",
            "-0wmPlMB0Wt"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thanks a lot for addressing my concerns.\nThe additional experiments look valuable.\nHence, I have raised the score to 7.",
            "This paper focuses on proposing multi-pass dual learning (MPDL) to build mappings among dialogue context and responses in two styles. The proposed model is reasonable and is the extension of the classical dual learning method, which usually maps between two domains. The idea is reasonable and novel, and the experiments show the improved performance in terms of both automatic and human evaluation for both datasets.\n\nA main concern is about the paired style transfer data $D_{tra}$, which limits the flexibility of usage. Also, an intuitive baseline may be needed for fair comparison. The further suggestion is to remove the \"paired style transfer data\" constraint, considering that the proposed MPDL may still work.  This paper proposes multi-pass dual learning (MPDL) to build mappings among three domains (dialogue context $C$, response of style $S_0$, response of style $S_1$) through duality. The target scenario is lack of $(C, S_1)$ parallel data for training, but there is unlabelled data with the style $S_1$. \n\nIn the proposed MPDL, the main idea is to utilize the reconstruction property of the response with the style $S_1$, which is mapped to the response with the style $S_0$ via a style transfer model, then mapped to the context via an inverse dialogue generation model, and then finally mapped to the original one (the response with the style $S_0$) via a dialogue generation model. In addition to minimizing the reconstruction error through multiple passes, two discriminators are introduced to ensure the good quality of pseudo contexts and responses in style $S_1$. By jointly learning the mixed learning objective, the proposed method outperforms multiple baselines for two benchmark datasets and most metrics in the conducted experiments.\n\nThe idea about using multi-pass dual learning is reasonable and original, where the unlabelled data in style $S_1$ can be effectively utilized and the mappings among three domains can be obtained. However, there are some concerns to be addressed.\n\n- In the experiments, three datasets are used: $D_{dia}$: $(C, S_0)$ paired data, $D_{tra}$: $(S_0, S_1)$ paired data, and $D_{sty}$: unlabelled $S_1$ data. An intuitive baseline can be a pipelined method, where the first model is to generate the response in style $S_0$ and the second model is to transfer it into the response in style $S_1$ in a supervised manner. However, this baseline is not compared in the experiments. Although the unlabelled data $D_{sty}$ is not used, it still needs to be shown in the table so that we can see how much improvement the unlabeled data can bring in. (The S2S+BT baseline uses \"unsupervised\" text style transfer model [33] instead of \"supervised\" learning, where the performance is expected worse than the supervised one.)\n\n- In the proposed MPDL framework, it may be possible to use the unpaired $D_{tra}$ for learning $f_{\\tilde{y}y}$ and $f_{y\\tilde{y}}$ and add a discriminator to check the quality of $y$. Hence, the paired style transfer data may not be required, making the target scenarios more practical. Have the authors tried this setting? (Comparing the results of this setting with S2S+BT is fair.)\n\n- In the discussions, the authors analyze the sensitivity of the size of the parallel data $D_{dial}$. The results are good, but it is also useful to show the impact of the size of $D_{tra}$. Because the main idea is to better utilize unlabelled $S_1$, so the paired style transfer data $D_{tra}$ may not be available/enough. Showing the results with different sizes of $D_{tra}$ may be more informative from my perspective.\n\n- Are the numbers shown in Table 3 for the TCFC dev set? Because the numbers are quite different from ones in Table 1, can they be compared directly?\n\n- For ablation study, removing discriminators shows the degraded performance in Table 3. Can the authors show the results of removing only one discriminator (either $D_x$ or $D_\\tilde{y}$), so that we can see which one contributes more?\n\n- The diversity (Distinct) is sometimes lower in the proposed method. Is it because $f_{yx}$ and $f_{x\\tilde{y}}$ are not 1-1 mappings and the reconstruction $\\tilde{y}'=f_{x\\tilde{y}}(f_{yx}(f_{\\tilde{y}y}(\\tilde{y})))$ forces the mappings to be less diverse? Do the authors have any explanation about this observation?\n\nIn sum, the proposed MPDL is reasonable and seems useful in the conducted experiments. The paper is clear. The experiments can be improved in order to better convince the readers about the model's effectiveness.  Yes.",
            " Thanks for the great response and addressing some of my concerns, and I apologize for my mis-interpretation in Q3.\n\nRegarding Q2, my concern was more on the lines of, \"is it practical to collect a large scale dataset where all responses share the same style? for example, all responses are simplified, or all responses are formal in nature?\" But to be fair, such a dataset can be built by running a style classifier on a large-scale dialogue dataset like the one you mentioned.\n\nI've raised my score to 6, but I encourage the authors to evaluate the non-parallel `D_tra` setting more thoroughly in the next version of the paper (perhaps using human evaluation).",
            "This paper studies the task of stylized dialogue generation, where the goal is to generate dialogue responses in a particular target style (formal / informal / Shakespeare-like). The authors operate in a setting where they do not have access to any dialogue training data in the target style, but instead have access to dialogue data in a complementary style, along with parallel data to transfer between the two styles. For instance, the authors study dialogue generation in formal style, and assume access to dialogue data in informal style, parallel data between formal / informal sentences, and a corpus of unpaired formal sentences.\n\nThe authors model this task as a three-way dual learning problem, a technique which has proven useful for machine translation [1]. They jointly training their model to perform 1) style transfer in both directions; 2) dialogue / inverse-dialogue generation to the complementary style; 3) dialogue / inverse-dialogue generation to the target style. 1 / 2 act as auxiliary tasks to aid (3) via shared models. Since no parallel data is available for (3), the authors perform sampling (using current version of model for 1 / 2) to create pseudo parallel data for (3). Since sampling can be noisy, the authors additionally use discriminators to weight samples, which judge whether the samples are real/fake (akin to adversarial training).\n\nThe authors evaluate their approach on formal dialogue generation and Shakespearan dialogue generation. They compare their approach to 5-6 baselines, along with human upperbounds. Both automatic and human evaluation is conducted. The authors additionally conduct ablation studies, analyze their discriminator, and show the effect of the size of parallel data.\n\n[1] - https://arxiv.org/abs/1611.00179  **Strengths**\n\n1. Style-controlled text generation is an important problem in natural language generation technology, and is far from solved. This has several important applications in chatbots and coversation agents.\n\n2. The authors use several interesting ideas in this paper, which were new to me. The idea to use dual learning to boost style-controlled generation as well as discriminators for weighing poor synthetic data were interesting and could be more widely applicable in style transfer literature.\n\n3. The authors approach beats several strong baselines on automatic & human evaluation. Several good analysis experiments have been conducted in the paper including ablations, analysis of the effect of discriminators and the effect of the size of available parallel data.\n\n**Weaknesses**\n\n1. My biggest issue with the paper is the usage of parallel data. Most style transfer literature [1, 2, 3, 4] operates in an unsupervised / unpaired setting (with access to D_sty only), and recent work [5] is assuming access to only 5-10 exemplars from D_sty. Access to parallel data is a strong assumption --- besides formality, shakespeare, sentiment transfer, almost no parallel resources exist. Moreover, all these datasets are in English, and no parallel style transfer resources exist for other languages. Even assuming access to dialogue data in a single style (`D_dia`) is strong assumption in my opinion. Indeed, the authors show that their method's performance reduces significantly if you don't have access to this parallel data (Table 3 w/o Multi-Pass, Figure 4), which reduces the applicability of this work.\n\n2. The evaluation in this paper could be improved. Here are some specific thoughts ---\n\n* Since there are three orthogonal metrics [7] for both human / automatic evaluation (style accuracy, semantic correctness, fluency), it's hard to compare between systems when only one metric improves (and the other does not), for instance SFusion vs MPDL in Table 1; S2S+BT vs MPDL in Table 2; experiments in Table 3. I suggest using the aggregation strategy from [2] to combine the metrics into a single score, making it easy to compare between systems.\n\n* The BLEU scores are quite low overall for all systems --- which is not surprising given the large space of possible outputs which makes evaluation challenging and easy to game [8]. I suggest using other automatic metrics too, such as those based on natural language inference [9] or the recent BEGIN benchmark [10].\n\n* Since automatic metrics can be flawed, human evaluation is critical. While the authors conduct human evaluation, more details will be helpful. Specifically, i) Were the annotators familiar with the research goals? (ideally they should be unfamiliar to avoid bias) ii) Were they shown outputs from different systems in a blind manner (not aware which system produced which output) & shuffled order? (consecutive samples from different systems) iii) What's the agreement between annotators & variance / error-bars of their annotation? Are the gains statistically significant? iv) How much were the annotators paid for this experiment?\n\n* More qualitative outputs from the system would be very helpful. Currently only two outputs are shown in Table 4. In both examples it's not really clear to me if the outputs are really formal / informal, which makes me doubt the quantiative evaluation in this work (In second example, \"wouldn't\" has an apostrophe which shouldn't be there in an informal style generation).\n\n**Minor**: The notation in Section 3 is confusing, primarily due to lots of inverted commas and tildes. I suggest avoiding these and using subscripts with short natural language identifiers (like you did for `D_dia / D_tra / D_sty`). Also it will be good to mention `BERT / SVM` in Table 1 refer to classification accuracy (my initially incorrect thought reading this was a reference to BERTScore [6], which confused me).\n\n**Overall**\n\nIn terms of the NeurIPS reviewer guidelines, I think the paper has decent originality (strength #2), average quality (strength #3, weakness #2), average clarity (minor #1), and average significance (strength #1, weakness #1). Overall, I liked the ideas in the paper, but I think weakness #1 is the biggest deal-breaker for me which will prevent me from giving a higher score. I encourage the authors to improve this in the next version of the paper.\n\n[1] - https://arxiv.org/abs/2011.00416  \n[2] - https://arxiv.org/abs/2010.05700  \n[3] - https://arxiv.org/abs/1804.09000  \n[4] - https://arxiv.org/abs/1811.00552  \n[5] - https://arxiv.org/abs/2010.03802  \n[6] - https://arxiv.org/abs/1904.09675  \n[7] - https://arxiv.org/abs/1910.03747  \n[8] - https://arxiv.org/abs/2103.06332  \n[9] - https://arxiv.org/abs/1904.03371  \n[10] - https://arxiv.org/abs/2105.00071\n\n-------\n\n**After author response**: Thanks for the excellent response. I've raised my score to 6. Yes",
            " Hi everyone,\nI just wanted to add a quick clarification about my weakness #1, since I noticed I was unclear in my review. By \"parallel data\", I'm referring to `D_tra`, or parallel sentences which share semantics but differ in style. This \"parallel data\" is different from the \"parallel data\" referred to by the authors in their abstract / TL;DR, where they are referring to parallel dialogue utterances differing in style.",
            "This paper studies the stylized dialog generation problem, in which the problem setup contains contexts C, responses with the original style S0, and responses with the desired style S1. The contributions of this paper includes:\n- A novel multiple-pass dual learning framework that explores the interaction between the pairwise relationship among C, S0 and S1.\n- The authors propose an additional discrimination to determine the text quality generated by the backward models, which has been shown to be helpful for the task.\n- Evaluations are compared to state-of-the-arts on two large text style transfer datasets, and show good amounts of improvements on both automatic and manual metrics.\n- Ablation studies further demonstrated the necessity of each component, especially convinced the capacity of discriminator (which is pretty clear from Figure 3). \n- The authors also release their code and data.  - Originality: There have been many work exploring the stylized dialog generation problem without parallel data. But it's new for this paper to use dual learning to explore the interactions between the pairwise relationship between context C, source style S0 and target style S1. The proposed discriminator component is also intuitive and effective (verified by the experiments), though I am wondering if pretraining such discriminator component might be more effective than optimizing it jointly with the dialog generation loss. Overall, the method is novel and technically sound.\n- Quality & Clarity: In general, the paper's writing is clear and easy to follow. There are a few questions to me still: 1) In Equation 9, the optimization loss is defined as the sum of losses from the four components, which gave me the impression that the sum of losses is jointly optimized. However, from Algorithm 1 in the Appendix section, the different components are optimized sequentially. I would love to see the confusion could be resolved. 2) In Table 2, the style consistency scores by auto metrics (e.g., BERT and SVM) don't seem correlate well with the manual judgements, when you compare MPDL with S2S+BT. Is that because the small samples in manual testing, or inaccurate consistency prediction by BERT or SVM?\n- Significance: Evaluations are compared to state-of-the-arts on two large text style transfer datasets, and demonstrate significant improvements over strong baselines on a subset of metrics. Human evaluation confirmed that the model generates more style-constrained responses. Ablation studies also provide insights about why this method work.  There are a few flaws in the evaluation part:\n- Instead of reporting BLEU-1 and BLEU-2, it's better to report BLEU-4, which is much widely adopted. \n- The author could consider to increase the number of samples for human eval to increase the evaluation significance. Also, it would be good to disclose the variance of manual's judgements in each metric.",
            "The authors propose a method for learning stylized dialogue response generation: given a context, generate a response conditioned on a particular style domain (e.g. modern vs. shakespearean english). The method, Multi-Pass Dual Learning (MPDL), leverages large corpora of non-parallel data in tandem. Specifically, the framework involves several forward and backward models between the context and two styles (one which has labeled data from the context to the style, and one that does not); the models produce intermediate data that is used to train the desired style generation model. To make sure that the intermediate data is being used appropriately, the authors introduce two discriminators that evaluate the quality of the data and determine whether it falls in the respective style/domain. The scores of these discriminators weight the importance of the loss computed on that data to ensure that higher-quality pseudo tuples are more impactful in the learning process. The authors evaluate the framework on two style-transfer datasets, and show that the method outperforms 5 baselines, achieving state-of-the-art results in automated metrics and outperforming nearly all models in human evaluations. A case study and ablations are presented as well that highlight and support the authors\u2019 design decisions.  *Originality*: The authors build upon the concept of dual learning and apply it to dialogue response generation; the framework is novel in the sense that the dual learning is required for three domain mappings, and is additionally novel in its use of discriminators to measure the quality of the intermediate data; related work is generally adequately cited, though there are a few cites that are not included (e.g. [1] https://arxiv.org/abs/2009.10855). In general, it is the first paper I have seen applying such a method to dialogue.\n\n*Quality*: The claims are generally well supported, with the presented results clearly identifying MPDL as the preferred method amongst several interesting baselines. Relevant ablations and case studies are considered that highlight the impact of various components in the framework (e.g., the multi-pass and discriminator components are important indeed). Some of the results are not clearly justified or explained, however; see limitations below.\n\n*Clarity*: The theoretical derivations are presented cleanly and concisely; although several pieces of notation are introduced, the proposed method and loss functions are all generally understandable. The results tables and figures are very clearly presented as well. However, I had several questions regarding the presentation of the overall method; see limitations below.\n\n*Significance*: The method is understandable and reproducible and appears to bring strong gains to style generation tasks. What is especially interesting and important is the way that the method can leverage unlabeled data, which makes the method quite generalizable (which is supported by its use on two completely different style-transfer domains). \n\n\n[1] Smith et. al., \"Controlling Style in Generated Dialogue\", https://arxiv.org/abs/2009.10855 - I have several questions for the authors; in general, the presented equations all made sense, but required that I re-read them several times to resolve the notation. \n    - On line 56, the third contribution is stated that they provide a new dataset for this task; it is unclear what dataset they have provided (is it just the sampled set of data?)\n    - The derivation of Pr(y\\~|x, f_xy\\~) in lines 112-113 is not super clear to me (why is y~ only related to x)? (similarly for lines 114-115)\n- The scores presented in figure 2 do not seem to align with the quality of the input/output sentences (are they swapped?)\n- I believe the first line of equation 7 has a typo, which confused me for quite a bit: should the last part read log(1-Dx(Hf(x)), rather than Hf(y)?\n- Where does the informal response in \u201c(context, informal response) dialogue pairs\u201d (line 176) come from? \n- What does TCFC stand for?\n- How does the model seem to have higher performance on the style classifiers than human responses for the formal response target in the TCFC task?\n- Why does SFusion have a much higher Distinct score for the TCFC dataset than it does for SDGC (such that MPDL is now better)?\n- I think a qualitative case study of the shakespeare data would be interesting to evaluate the qualitative performance of the model\n"
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('Thanks a lot') and appreciation ('look valuable') for the changes made by the authors, leading to an increased score.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges the reasonableness and potential usefulness of the proposed method but also raises several concerns regarding the experimental setup and comparisons. The concluding sentence suggests a balanced view.",
            "The reviewer expresses gratitude, acknowledges the authors' efforts in addressing concerns, apologizes for misinterpretations, and raises the score, indicating a positive overall assessment.",
            "The review identifies both strengths and weaknesses, but the final decision to raise the score suggests a positive overall assessment.",
            "The review is providing a clarification and doesn't express positive or negative opinions about the work itself.",
            "The review expresses overall positive feedback, highlighting the novelty, technical soundness, clear writing, and significant improvements demonstrated by the paper. While it points out some areas for improvement, the core assessment is favorable.",
            "The review expresses overall positive sentiment, highlighting the novelty, quality, clarity, and significance of the work. Phrases like \"generally well supported,\" \"clearly identifying MPDL as the preferred method,\" \"strong gains to style generation tasks,\" and \"especially interesting and important\" indicate a positive evaluation."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Balanced",
            "Neutral",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'Thanks a lot' and 'look valuable,' indicating a supportive and encouraging tone. The act of raising the score further emphasizes this support.",
            "The review adopts a balanced tone by acknowledging the strengths of the paper ('The proposed model is reasonable and is the extension of the classical dual learning method', 'The idea about using multi-pass dual learning is reasonable and original') while also providing critical feedback and suggestions for improvement ('A main concern is about the paired style transfer data', 'However, there are some concerns to be addressed', 'The experiments can be improved in order to better convince the readers').",
            "The reviewer uses phrases like 'Thanks for the great response,' 'addressing some of my concerns,' 'to be fair,' and 'I encourage the authors,' which convey a supportive and encouraging tone despite some remaining concerns.",
            "The reviewer provides both positive and negative feedback, using specific examples and references to support their claims. The tone is professional and constructive, aiming to help the authors improve their work.",
            "The language is straightforward and factual, aiming to clarify a point. There's no strong emotion or personal opinion expressed, resulting in a neutral tone.",
            "The review adopts a balanced tone by acknowledging the paper's strengths (novelty, clarity, significance) while also pointing out areas for improvement (evaluation flaws, questions about optimization). It uses phrases like 'Overall, the method is novel and technically sound' to show appreciation, but also poses questions and suggestions for enhancement in a constructive manner.",
            "The review presents both positive aspects of the paper (originality, quality, clarity, significance) and constructive criticism (questions about the derivation, unclear dataset contribution, potential typos, and suggestions for improvement). It maintains a professional and objective tone while offering specific feedback."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses positive feedback regarding the addressed concerns and the additional experiments, which aligns with the action of raising the score. There are no contradictory statements.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the novelty and reasonableness of the proposed method and the improved performance. At the same time, it raises valid concerns and suggestions for improvement, mainly focusing on the experimental setup, baselines, and further analysis. The reviewer's feedback is constructive and aims to enhance the paper's quality without contradicting the initial positive assessment of the method's potential and clarity.",
            "The review is consistent because the reviewer acknowledges the authors' response, clarifies their previous concern, and expresses a positive change in their evaluation by raising the score. The feedback is constructive and logically flows, without any contradictory statements.",
            "The review is consistent because the reviewer initially expresses concerns about the paper's reliance on parallel data and evaluation methodology, which are presented as weaknesses. Despite these weaknesses, the reviewer also acknowledges strengths such as originality and performance against baselines.  The reviewer's initial overall assessment is somewhat mixed due to the significant weakness. However, after the author's response, the reviewer explicitly states they raised their score, indicating that the author's response successfully addressed their concerns. This change is a logical progression and not a contradiction, as the reviewer's final positive assessment is built upon the initial feedback and the subsequent author response.",
            "The review is a clarification of a previous point, aiming to improve understanding and avoid misinterpretation. It does not introduce contradictions but rather refines a previous statement.",
            "The review is consistent in its assessment. It highlights the paper's strengths, such as novelty and technical soundness, while also pointing out areas for improvement, mainly in clarity and evaluation methodology. The reviewer's questions and suggestions are constructive and do not contradict the overall positive evaluation of the paper's contributions and significance.",
            "The review is consistently positive overall. While the reviewer raises several questions and points out areas for improvement in clarity and justification, the overall assessment of originality, quality, clarity, and significance is positive, aligning with the initial summary and the nature of the detailed questions which are mostly for clarification and minor improvements rather than fundamental flaws."
        ]
    },
    {
        "paper_id": "nips_2022_kK200QKfvjB",
        "paper_title": "Feature Learning in $L_2$-regularized DNNs: Attraction/Repulsion and Sparsity",
        "paper_abstract": "We study the loss surface of DNNs with $L_{2}$ regularization. We\nshow that the loss in terms of the parameters can be reformulated\ninto a loss in terms of the layerwise activations $Z_{\\ell}$ of the\ntraining set. This reformulation reveals the dynamics behind feature\nlearning: each hidden representations $Z_{\\ell}$ are optimal w.r.t.\nto an attraction/repulsion problem and interpolate between the input\nand output representations, keeping as little information from the\ninput as necessary to construct the activation of the next layer.\nFor positively homogeneous non-linearities, the loss can be further\nreformulated in terms of the covariances of the hidden representations,\nwhich takes the form of a partially convex optimization over a convex\ncone.\n\nThis second reformulation allows us to prove a sparsity result for\nhomogeneous DNNs: any local minimum of the $L_{2}$-regularized loss\ncan be achieved with at most $N(N+1)$ neurons in each hidden layer\n(where $N$ is the size of the training set). We show that this bound\nis tight by giving an example of a local minimum that requires $N^{2}/4$\nhidden neurons. But we also observe numerically that in more traditional\nsettings much less than $N^{2}$ neurons are required to reach the\nminima.",
        "review_ids": [
            "s4bgf5KBgI2",
            "AcUsOCVG5bu",
            "WYWk9_4Lhe8",
            "WtkW3FoLH0w",
            "igqho2chdu",
            "FyyRvj78aNX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Dear authors,\n\nThanks for the extensive clarifications. I checked the proofs again and am happy with the claims asserted. A couple of additional comments:\n- In Proposition 1, is it being assumed here that $\\min_{\\ell} n_{\\ell} \\geq N$? If I am understanding correctly, I think this is necessary to have $\\Phi \\circ \\Psi = \\mathrm{Id}$ (i.e. because you want $(Z_{\\ell-1}^\\sigma)^+ Z_{\\ell-1}^\\sigma = I$). Perhaps it is possible to include a more detailed argument here to remove this hypothesis, as you do when considering $\\Psi \\circ \\Phi$ in the second part of the proof?\n- I would recommend giving this proof (and perhaps others that I did not read line-by-line) a careful read for typos / outdated notation (e.g. $Y_L$), as it is fairly sparse on details and as a result hard to read -- the existence of typos make it impractical to parse.\n- Can this proof be extended to saddle points, as the paragraph header in the proof indicates the authors intended to? I see that the proof only uses the topological notion of local minimizer -- for saddles perhaps some differentiability needs to be assumed?\n\nAfter reviewing the revisions and the comments of the other referees, I am increasing my score. I think the paper presents interesting observations and correct technical arguments, although the general implications of the study are not completely clear.\n",
            " I appreciate the authors' time and effort in addressing my comments. I find the discussions useful. ",
            " Dear authors,\n\nThank you for your response to my review, and for your work in updating the submission and appendices.\n\n- I am still not quite following the proof of the claim about minima in Proposition 1. I am fixating on this point because I think the correctness of this claim would raise my appraisal of the submission, since this implies a useful correspondence between the problems' landscapes. The error you have fixed helps my understanding of the claim -- but I am still not following the claim in Line 60 of the appendix, as the term involving the $Z$s after the first equality does not match the structure of the $Z$'s in the definition of $\\mathcal{L}^r_{\\lambda}$ below line 33, where one has $Z_{\\ell} ( Z_{\\ell}^\\sigma)^+$ rather than $Z_{\\ell} ( Z_{\\ell-1}^\\sigma)^+$. Could you clarify what the argument is that gives the first inequality in line 60? I am assuming here that it is also necessary that $C \\geq 0$. If there are other typos, can you please fix these so that the claim can be appreciated and understood?\n\nI will respond to the other points you have raised after understanding this point -- thank you. ",
            " The authors study deep feedforward neural network training with $\\ell^2$\nregularization on the weight matrices. They reformulate this cost (via\nan equivalent optimization problem) in two different ways, in order to get\ninsight into the features that are learned at optimality. The first\nreformulation expresses the network weights in terms of the preactivations, via\nan orthogonal projection trick; the authors interpret the resulting problem in\nterms of the local interactions between neighboring preactivations across\nlayers of the network. The second reformulation builds off the first: it\nreplaces the preactivations with the corresponding covariance matrices, and the\nauthors interpret the resulting cost in terms of the rank constraints imposed\nby this structure when the layer widths are sufficiently large relative to the\nnumber of samples, which implies some sparsity of the optimal features. Small\nexperiments are presented to illustrate the interpretations.\n\n ## Strengths\n\n- The reformulation in terms of preactivations is insightful, and gives an\n  interesting framework for understanding global minimizers of the deep network\n  training cost.\n- The authors focus on tools that apply to deep networks, which is important --\n  many works in the theoretical literature are restricted to shallow networks.\n\n## Weaknesses\n\n- It is slightly unclear what to conclude from the authors' reformulations and\n  analysis. The authors discuss the fact that the reformulated optimization\n  problems are not computationally useful, given the undesirable scaling with\n  the number of data samples relative to training the network weights (section\n  3.3); for the second reformulation, there is no correspondence between the\n  objective landscapes of the original and reformulated problem (and see a\n  question below about the first reformulation). It could be helpful to see\n  some experiments on toy data where the scaling is less of an issue to see\n  whether or not the reformulated landscapes empirically have some similarities\n  to the original landscape, and/or provide useful insights.\n- The analysis is focused on understanding learned features in the network, but\n  it does not involve any specific structural assumptions on $Y$ and $X$. It\n  would be interesting to see if such assumptions can be made, and accordingly\n  the conclusions of the second reformulation in Section 4 could be\n  strengthened.\n\n\n## Minor Issues\n\n- line 78: is the 'plus one' in the weight matrix dimensions definition\n  misplaced? (the weights multiply the 'superscript sigma' preactivations,\n  which are the lifted ones)\n- line 112: orthogonal complement\n- Proposition 1 proof: there is a typo in the definition of the $\\Psi$ map (the\n  typo is repeated in other contexts in the proof).  The typo extends to the\n  statement of the proposition in the main paper, unless I am missing\n  something.  Also in the proof, there are missing $r$ superscripts on some\n  losses. \n- line 137 (item 2): how are these fractional powers to be interpreted for the\n  generally non-square features/input data?\n- Proposition 3: the usage of the $\\mathrm{cone}$ operator doesn't match the\n  way the notation is used above (are the braces and parentheses accidentally\n  placed in the wrong order at eqn below line 188 and similar?)\n I am not able to understand the claim in line 61 of the appendix that $\\Psi$ is\nnorm-preserving. Can you justify this? It seems like if some features are\npoorly conditioned, the norm of the image of $\\Psi$ will blow up. I am not sure\nabout the correspondence between minimizers asserted in Proposition 1 without\nthis justification.\n\nI was a bit confused about Figure 1 -- are these plots for a network trained on\nthe reformulated loss (i.e. not really a network -- just learning features), or\na standard optimization over the weights $W$? Are these figures showing the\ndifferent attractive/repulsive terms in the trained network, or the initial\nnetwork?\n\nQuadratic overparameterization may be more than is necessary for many tasks,\ne.g. as discussed in [1], or when data have low-dimensional structure [2].\nCan the requirements be relaxed (in principle?) in the latter case? What can\nbe said theoretically in this framework about networks that are less\noverparameterized than quadratic?\n\n[1] https://arxiv.org/abs/2205.10217\n[2] https://openreview.net/forum?id=O-6Pm_d_Q-\n Yes.",
            " This paper provided two reformulations of the L2-regularized DNN using the framework of representation cost. The first reformulation characterizes feature learning process in terms of attraction and repulsion, and further established an interesting connection between training a L2-penalized DNN with a partial convex optimization over a translated convex cone. The second reformulation utilizes the covariance learning and provides a mechanism for learning the sparsity effect of the L2-regularization in training of DNN with homogeneous non-linearities. Strengths:\n\n1. The two reformulations are interesting and lead to insightful understanding on the effect of L2 regularization in DNN trainings.\n\n2. The second reformulation establishes an interesting sparsity result for homogeneous DNN (Proposition 7), and the tightness of is further analyzed.\n\nWeakness:\n\nWhile I enjoy learning these insights and new understandings of the effect of L2 regularization in DNN, I would definitely find it helpful to have more discussions on their implications. Specifically,\n\n1. the first reformulation leads to the attraction-repulsion effects, which could potentially lead to better algorithmic understanding / design. As mentioned in point 3 in the bottom of pp.4, a block (of three layers) cordinate descent type of algorithm could be developed to minimize the multiplicative distances. But how would one deal with the interpolation requirement in the representation costs? While it might be natural to consider a projected gradient descent, going in this direction seems to be a waste of the established decomposition.\n\n2. Proposition 5 establishes the equivalence between training a L2-regularized DNN and a partially convex problem over a translated convex cone. What actual implication could we obtain? What actual benefit can the partial convexity offer?\n For the second reformulation, it is great to have the understanding of no later than when will we observe the plateau. Could we have any understanding of how early the plateau could occur? I think this might be interesting, especially since an early occurrence of the plateau is observed in many datasets.\n Please see the comments in the previous 2 sections.",
            " This paper studies the minima of the loss of $L_2$ regularized fully-connected DNNs. They show that the loss in terms of the parameters can be reformulated into a loss in terms of the layerwise activations of the training set, which has an attraction/repulsion problem formulation. For positively homogeneous nonlinearities, they show that the loss can be further reformulated in terms of the covariances of the hidden representations, which take the form of a partially convex optimization over a convex cone.  Strengths:\n- The two reformulations of loss of $L_2$ regularized DNNs are very novel, which may be helpful for our understanding of DNNs.\n- The second reformulation has some implications for the sparsity of the local minimum of homogeneous DNNs.\n\nWeaknesses:\n- All the theoretical results are limited to the minima of the loss of $L_2$ regularized NN, i.e. based on the assumption that the minima can be achieved. They also assume the minima can perfectly interpolate the dataset. Some typos:\n- Line 124 $\\mathcal{L}^r_{\\lambda}$, should be $\\mathcal{L}_{\\lambda}$?\n- Line 130, $\\mathcal{L}_{\\lambda}(Z_1, \\dots, Z_L)$ should be $\\mathcal{L}^r_{\\lambda}(Z_1, \\dots, Z_L)$? Yes"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses satisfaction with the clarifications, explicitly states they are \"happy with the claims asserted,\" and increases their score after reviewing the revisions. The overall assessment is positive, despite pointing out areas for improvement.",
            "The reviewer expresses appreciation for the authors' efforts and finds the discussions useful, indicating a positive reception.",
            "The review expresses a mix of acknowledgement and critique. The reviewer thanks the authors and acknowledges updates but also points out a specific issue in the proof and requests clarification. The tone is inquisitive and focused on a specific technical detail.",
            "The review presents both strengths and weaknesses of the paper, using balanced language and constructive criticism. It identifies insightful reformulations but also points out unclear conclusions and potential areas for improvement.",
            "The review expresses appreciation for the paper's insights and new understandings of L2 regularization's effects on DNNs. Phrases like 'interesting and lead to insightful understanding' and 'interesting sparsity result' indicate a positive sentiment.",
            "The review highlights the novelty and potential helpfulness of the paper's reformulations, indicating a positive assessment of the work's contributions. The reviewer explicitly mentions 'very novel' as a strength."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is balanced, acknowledging the paper's strengths (\"interesting observations and correct technical arguments\") while also pointing out weaknesses and areas for improvement (\"general implications of the study are not completely clear,\" \"hard to read,\" suggestions for more detailed arguments and typo correction).",
            "The reviewer uses appreciative language like \"I appreciate\" and \"useful,\" which conveys a supportive tone.",
            "The reviewer is critical because they are pointing out a specific error in the proof and requesting clarification. The phrase \"I am still not quite following\" and \"the term...does not match\" indicates a critical assessment of the current state of the submission.",
            "The review adopts a balanced tone by highlighting both the strengths ('insightful', 'interesting framework') and weaknesses ('slightly unclear', 'not computationally useful') of the paper. It also poses questions and suggestions for improvement in a respectful manner.",
            "The review presents both strengths and weaknesses of the paper, using a formal tone and suggesting improvements. It acknowledges the value of the reformulations while also pointing out areas where further discussion and implications would be beneficial. The reviewer uses respectful language and constructive criticism.",
            "The review presents both strengths and weaknesses of the paper. While acknowledging the novelty of the approach, it also points out limitations in the theoretical results and identifies some typos, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as the reviewer expresses satisfaction with the claims after clarifications and raises constructive questions and suggestions for improvement without contradicting their overall positive assessment. The reviewer explicitly states they are increasing their score, indicating a positive evaluation despite the identified points for improvement.",
            "The review expresses a consistently positive sentiment, appreciating the authors' effort and finding the discussions useful. There are no contradictory statements or conflicting viewpoints presented.",
            "The review is consistently focused on a single point of concern, which is the proof of a specific claim in Proposition 1. The reviewer acknowledges the authors' response but remains unconvinced about a particular detail in the proof and consistently asks for clarification on this specific issue. There are no contradictory statements or shifts in focus within the review.",
            "The review is consistent because it presents a balanced evaluation of the paper, highlighting both strengths and weaknesses without contradicting itself. The reviewer appreciates the insightful reformulations but raises valid concerns about their practical utility and the clarity of conclusions. The minor issues are specific and technical, further supporting the detailed and consistent nature of the review.",
            "The review is consistent because the reviewer clearly appreciates the theoretical contributions of the paper, highlighting the interesting reformulations and insightful understanding of L2 regularization as strengths. The weaknesses are presented as requests for further discussion and exploration of the implications and practical applications of these theoretical findings, rather than criticisms of the core work itself. There are no contradictory statements or conflicting opinions expressed in the review.",
            "The review is consistent because it highlights both the strengths (novelty of reformulations, implications for sparsity) and weaknesses (limitations of theoretical results, typos) of the paper without any contradiction. The reviewer provides a balanced assessment by pointing out positive and negative aspects."
        ]
    },
    {
        "paper_id": "iclr_2020_BkgNqkHFPr",
        "paper_title": "Enhanced Convolutional Neural Tangent Kernels",
        "paper_abstract": "Recent research shows that for training with l2 loss, convolutional neural networks (CNNs) whose width (number of channels in convolutional layers) goes to infinity, correspond to regression with respect to the CNN Gaussian Process kernel (CNN-GP) if only the last layer is trained, and correspond to regression with respect to the Convolutional Neural Tangent Kernel (CNTK) if all layers are trained. An exact algorithm to compute CNTK (Arora et al., 2019) yielded the finding that classification accuracy of CNTK on CIFAR-10 is within 6-7% of that of the corresponding CNN architecture (best figure being around 78%) which is interesting performance for a fixed kernel.\n      Here we show how to significantly enhance the performance of these kernels using two ideas. (1) Modifying the kernel using a new operation called Local Average Pooling (LAP) which preserves efficient computability of the kernel and inherits the spirit of standard data augmentation using pixel shifts. Earlier papers were unable to incorporate naive data augmentation because of the quadratic training cost of kernel regression. This idea is inspired by Global Average Pooling (GAP), which we show for CNN-GP and CNTK, GAP is equivalent to full translation data augmentation. (2) Representing the input image using a pre-processing technique proposed by Coates et al. (2011), which uses a single convolutional layer composed of random image patches.\n      On CIFAR-10 the resulting kernel, CNN-GP with LAP and horizontal flip data augmentation achieves 89% accuracy, matching the performance of AlexNet (Krizhevsky et al., 2012). Note that this is the best such result we know of for a classifier that is not a trained neural network. Similar improvements are obtained for Fashion-MNIST.",
        "review_ids": [
            "Byxqt4VztH",
            "ryltv8TTFB",
            "H1eOHZN2or",
            "S1gzyEd6FH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper shows that there is a one-to-one correspondence between pixel-shift based data augmentation and average pooling operations in CNN-NNGP/NTK based ridge regression. Interestingly, the authors show that standard average pooling + flatten can lead to a better performance than simple global average pooling. This paper further shows that using the data pre-processing step proposed in (Coates et al., 2011) can boost performance of CNN-NNGP/NTK based ridge regression by ~7% which allowed the authors to achieve classification accuracy in high 80s which is AFAIK SOTA on CIFAR-10 when not using learned representations.\n\nMy current assessment of the paper is \u201cweak accept\u201d. There are two main reasons why I am on the verge of recommending rejection of this paper: (1) I believe that the experiment evaluation is not done entirely correctly leading to inflation of the reported results (my guesstimate is by ~0.5-2%)---please see my \u201cMajor comments\u201d. If this is not fixed, I am very likely to downgrade my score. (2) While the observation of the relationship between pixel-shifts and average pooling is very nice (which is why my current score is \u201cweak accept\u201d), it seems that most of the improvement comes from application of the pre-processing step of Coates et al. (2011) (seems like a ~7% improvement!). Given the large computational cost of CNN-NNGP/NTK (authors say about 1000 GPU hours), I wonder whether a simpler algorithm like some of the newer variants of boosting combined with the Coates et al. algorithm wouldn\u2019t also perform at around 87-88% like CNN-NNGP/NTK (given the baseline 85-86% accuracy of the Coates et al. (2011) algorithm reported by the authors).\n\n\nMajor comments:\n\n- Can you please clarify why you decided to give a new name (Box Blur) to standard average pooling? Why not just use the existing name?\n\n- I believe that the way you report results in all the tables (i.e., tables 1-6) and the text based upon them is flawed. The right approach would be to select the hyperparameters \u201cc\u201d and \u201cd\u201d on a validation set, and then report the performance with these hyperparameters on the test set. While the experiments are somewhat rescued by the fact that you report results for (almost) all the possible hyperparameter settings (which allows us to see samples from the population distribution of the generalisation error), type-setting the best results in boldface and thus implying that these are valid estimates of the generalisation error is not appropriate since you are effectively selecting the best hyper-parameters on the test set! Unfortunately, I cannot accept these results to be published \u201cas-is\u201d. While re-running the experiments with hyperparameter selection on validation set is already a somewhat imperfect solution, I am not sure I can see a better way forward. However, I do understand that this could be prohibitively expensive in which case I would like to ask you to suggest an alternative solution please (of course, other reviewers are welcome to chime in as well)?!\n\n- While most of the paper is about Local Average Pooling (LAP) and the equivalence between averaging and pixel shifts, the experimental results seem to show that most of the improvement comes from the use of Coates et al.\u2019s preprocessing step. Could you please run the experiments in tables 3 and 4 with c=0 and c=32 to see what the effect of the preprocessing is without LAP?\n\n- In sect.6.3, you say \u201cOur experiment illustrates that even with a fixed last FC layer, using GAP could improve the performance of CNN, and challenges the conjecture that GAP reduces the number of parameters in the last fully-connected layer and thus avoids overfitting.\u201d I am not sure I see why fixing the last FC layer should provide more convincing evidence than training it? I do not know the conjecture to which you refer but from your description, the overfitting without GAP should occur because the FC layer has more parameters than with GAP?! If this is true, then the overfitting would happen in the last layer (due to the large number of parameters) which you have (at least partially) prevented by not training it?! Can you clarify and also report the results of this experiment with all the layers trained please?\n\n- In Appendix D, you say that you have used lambda = 10^{-5} for all configurations. How have you selected this particular value please? Do you have a sense of how far from optimal this value is for all the different configurations (or at least for NTK vs NNGP models---in my experience, the optimal setting between the two can differ quite a bit)?\n\n\nMinor comments:\n\n- In the abstract and throughout the paper, you claim that the cost of kernel regression is quadratic. AFAIK without any approximations, the cost is cubic (or O(n^{2.67}) to be more precise). Please clarify.\n\n- In par.1 on p.1, you say \u201cconvolutional neural networks (CNNs) whose width (number of channels in convolutional layers) **goes to infinity**\u201d (emphasis mine) and cite the Jacot et al. (2018) paper. AFAIK this paper only works with infinite networks but does not actually prove that **deep** networks of finite width (in each layer) converge to the NTK limit; IMHO you should cite the Allen-Zhu et al. (2018) and Du et al. (2018) papers from your references for that result. Based on p.2 (end of par.2 in sect.2), you seem to be aware of this distinction but cite Arora et al. (2019) instead of these two; I would suggest either citing Allen-Zhu et al. and Du et al. only, or citing all three as the Arora et al. paper came out later than the first versions of the other two paper which AFAIK already contained all the necessary derivations (even if the words \u201cNeural Tangent Kernel\u201d were not spelled out there).\n\n- Also in par.1 on p.1, you say that Arora et al. (2019) was the first to provide an algorithm to compute the CNTK kernel which is a bit of a stretch given that both Garriga-Alonso et al. (2019) and Novak et al. (2019) have implemented the CNTK kernel in their experiments. AFAIK the claim in (Arora et al., 2019) is that they provided first **efficient** implementation of the CNTK-GAP kernel which should be made clearer in the next revision of your paper.\n\n- On p.2, you say \u201cThese kernels correspond to neural networks where only the last layer is trained.\u201d In reality, the correspondence is not exact for finite networks because the induced kernel will not be exactly equal to the one at the limit.\n\n- Bottom of p.2, \u201cGlobal Average Pooling (GAP) is proposed\u201d -> \u201c... was proposed\u201d.\n\n- Top of p.3, \u201c..., and GAP is more robust\u201d -> \u201c..., and that GAP is more robust\u201d.\n\n- On p.3 in the \u201cPadding Schemes\u201d paragraph, do you mean to assume that the input image has only a single channel (not necessary later)?\n\n- On p.4, I am slightly confused by your definition of the \u201caugmented kernel\u201d. Specifically, it does not seem K^G (x , x\u2019) = K^G (x\u2019, x) holds in general. Can you please clarify? If there\u2019s no symmetry, I do not think it necessary to use a different name, but perhaps a clarifying note would be beneficial to the reader?!\n\n- On p.5, fig.1 is too small when printed and one needs to use the computer screen to see what is depicted; given the amount of white space around, can you please try to make the images larger (you can perhaps also only include 2 or 4 images instead of 16 which will give you additional space)?\n\n- On p.5, you say that for small \u201cc\u201d, circular padding will not create unrealistic images. Looking at fig.1b, it seems like the images are not as unrealistic as in fig.1a but human eye can still tell they are not realistic (potentially even more so with other images than the one selected for this figure). I am not sure whether there is a reason to assume this issue does not affect CNNs too?! Further, I am not convinced the motivation is correct in the first place given that the optimal \u201cc\u201d for CIFAR-10 is 12 which will presumably create clearly unrealistic images; perhaps it would be best to omit this motivation?!\n\n- On p.6, you claim \u201cAnother advantage of LAP is that it **does not incur any extra computational cost**\u201d (emphasis mine) while at the next line you say that there is a constant additional computational cost. Perhaps say that the extra computational cost is relatively small?\n\n- It might be nice to swap tables 3 and 4 so at least the results for NNGP are next to each other. Even better would be the current table 3 was closer to table 1 to achieve the same effect for NTK.\n\n- I am not sure I fully understand the description in sect.6.3: isn\u2019t the number of channels on the input irrelevant after computation of the kernel in the first layer? In other words, why have you opted to use only 2,048 patches in your experiments and not 32,000 or 256,000 as used by Recht et al. (2019)? Do you have an estimate of how different could the performance of NNGP/NTK be with the larger number of features? Do you know what is the performance of Coates et al.\u2019s algorithm with only 2,048 features? Relatedly, do you know how AlexNet would perform if its PCA data augmentation was replaced by the Coates et al.\u2019s feature extractor?",
            "This paper considers architectures that do not involve learning (up to the classification layer) and tries to improve their accuracies. They're based on CNTK and CNN-GP works. This is purely a numerical paper and its contribution is to show that despite being not learned, the obtained representations are competitive with supervised neural networks.\n\nOverall, despite the fact if I find this numerical result interesting, I found too many flaws to justify its acceptance. (fine tuning on the test set, lack of comparison with the state of the art...) \n\nPros:\n- Good numerical performances.\n\nCons:\n- Given the claim in the abstract about accuracies, it should be pointed out that:\u2028\n* in the unsupervised setting, with a kernel engineering method, you can obtain ~86% on cifar10 (cf https://arxiv.org/abs/1605.06265 )\n* in the no-data(up to a linear model) setting, it is possible to get ~82% on cifar10 with the scattering networks (cf https://arxiv.org/abs/1412.8659 )\nThose two works are also mainly empirical, and thus some accuracies of this paper should be compared to them.\n- There is a significant amount of experiments (table 1/2/3/4). While this should have been a positive aspect of the paper, I noticed that the accuracies reported here are computed from the test set. A validation set should have been used with a careful cross-validation. I'm aware this is a standard practice in deep learning, yet here it seems obvious to me that some hyper parameters have been fine-tuned on the testing set.\n- Section 4: isn't it a rephrasing of (Dao et al, 2018)? (which is cited) I think this should be clearly stated.\n- Section 5: The paper cites the Local Average Pooling as a \"new operation\", but this is clearly standard in the literature. \"Boxblurring\" has always been named average pooling in deep learning, low-pass filtering in signal processing. It was used before researchers employ a stride of 2 in convolutions. A similar pooling is also present in https://arxiv.org/abs/1605.06265  \n- I'm nicely surprised that the authors didn't encounter any significant conditioning issues. Would it be possible to show the spectrum of the kernel? This could be commented.\n- Nothing about the future release of the code is indicated.\n\nMinor:\n- I find the Figure 1 is not informative to the reader.\n\nPost-discussion:\nThe revision clarifies all my concerns and this work is likely to induce interesting discussions.",
            "Dear authors,\n\n1. Thanks for the clarification.\n2. Thanks for the clarification.\n3. So, as far as I understood, (Dao et al) consider data augmentation which are linearized. As all the layers of your architectures are (linearly) covariant with the action of translation, it implies that the action of translations on the sample $x$ is a linear action on the obtained representation. This is exactly the setting of the Section 4.1 of (Dao et al), if a subset of translations is uniformly sampled. In other words, if a group action is linear, averaging along an orbit of the group leads to a linear operator. I agree that this setting is different for the flips, yet this is a group with simply 2 elements...  Am I incorrect?\n4. Thanks for the clarification.\n5. I partially agree: I think it is still computationally tractable (maybe not on standard academic resources), however approximate methods exist. I think this would have been interesting, as you observed that directly solving the regression (which incorporates a regularization) allows to obtain good performances: it is surprising given that there is no supervision. Thanks however for the clarification.\n6. Thanks.\n7. OK.\n\nI will revise my review. Thank you very much for your rebuttal.",
            "This paper builds on recent developments of CNN-GP and CNTKs in multiple fronts obtaining significant performance boost on CIFAR-10 dataset (and some mild boost on Fashion-MNIST). One way is by usage of Local Average Pooling (LAP) layers which interpolates between Global Average Pooling (GAP) and no Pooling layer. The authors also introduce flip data augmentation by doubling the dataset. With the help of additional feature extractor, this paper obtained 89% classification accuracy on CIFAR-10 which is the best among methods not using trained neural networks. \n\nThe discussion on section 4 regarding augmented kernel and data augmentation is quite clear and revealing. It\u2019s unfortunate that the flip augmentation could not be introduced in kernel level. It would be interesting for future work to find kernel operation similar to GAP that encodes symmetries of the dataset. \n\nWhile the paper is clearly written and the results are strong, there are few criticisms I\u2019d like to address and hope the authors address. \n\nAFAIK both GAP and LAP for CNN-GP are already introduced and analyzed in [1]. It seems best results on CIFAR-10 all comes from CNN-GP (with without flip augmentation, with and without using extra feature extractor), and I think the authors should properly credit [1] for GAP/LAP in convolutional kernels. It\u2019s fair that this paper along with [2] was able to efficiently implement and scale up to  full CIFAR-10 dataset and demonstrated pooling layer\u2019s full potential for kernels corresponding to infinitely wide CNNs. Also in this regard the title could be misleading. It\u2019s strange to have paper\u2019s strongest result is based on CNN-GP while the title only mentions CNTK.\n\nAs the author\u2019s mention in the paper, Box Blur is just an average pooling operation. This is already widely use by practitioners(e.g. [3]) and I don\u2019t understand how author\u2019s claim: \u201cThis operation also suggests a new pooling layer for CNNs which we call BBlur\u201d \n\nFew question/comments:\n\nBest parameters for trained CNN\u2019s BBlur c is smaller than best c values for kernels, do authors understand the cause of discrepancy? \n\nIt would benefit the research community if authors could share code to generate the CNN-GP Kernels / CNTKs with LAP.  Also I would encourage authors to share actual numerical values of kernel matrix for other research groups to analyze and encourage reproducibility.\n\n\n[1] Novak et al., Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes, ICLR 2019\n[2] Arora et al., On Exact Computation with an Infinitely Wide Neural Net, NeurIPS 2019\n[3] Huang et al., Densely Connected Convolutional Networks, CVPR 2017\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses significant concerns about the experimental methodology and result reporting, stating that the evaluation is \"not done entirely correctly leading to inflation of the reported results\" and that the results cannot be accepted \"as-is\". This indicates a negative sentiment towards the current state of the paper.",
            "The reviewer initially expresses concerns about the paper's flaws, stating they are too numerous to justify acceptance, despite finding the numerical results interesting. The cons listed outweigh the pros, indicating a negative initial assessment. Although the post-discussion indicates a positive change, the overall review leans negative due to the initial strong criticisms.",
            "The reviewer expresses gratitude for clarifications, indicates agreement with some points, and states they will revise their review positively, suggesting satisfaction with the authors' responses.",
            "The review acknowledges the paper's strengths (clear writing, strong results) but also raises several criticisms and questions. The reviewer points out missing citations, potential misrepresentation of novelty, and suggests improvements for reproducibility. The mix of positive and negative feedback results in an overall neutral sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct and critical language, pointing out flaws in the experimental design and result reporting. Phrases like \"flawed\", \"not appropriate\", \"I cannot accept these results to be published 'as-is'\" and questions like \"Can you please clarify\" demonstrate a critical stance.",
            "The review uses strong, critical language to point out flaws. Phrases like 'too many flaws to justify its acceptance,' 'significant amount of experiments...accuracies reported here are computed from the test set,' and 'isn't it a rephrasing' demonstrate a critical tone. The reviewer also questions the novelty of certain aspects, further solidifying the critical stance. The use of 'clearly standard' and 'nicely surprised' also indicates a critical assessment.",
            "The reviewer uses phrases like \"Thanks for the clarification,\" \"I agree,\" and \"Thank you very much for your rebuttal,\" indicating a supportive and appreciative tone.",
            "The reviewer uses phrases like \"few criticisms I\u2019d like to address,\" points out potential issues with crediting prior work (\"the authors should properly credit [1] for GAP/LAP\"), questions the novelty of BBlur (\"I don\u2019t understand how author\u2019s claim\"), and suggests the title is misleading. These direct criticisms indicate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer clearly states their initial positive impression due to an interesting observation, but consistently raises concerns about the experimental methodology and the actual contribution of the paper, leading to a 'weak accept' assessment and potential downgrade. The major and minor comments further elaborate on these concerns without contradicting the overall message.",
            "The review is consistent because it initially identifies flaws leading to a negative assessment, but then explicitly states that revisions addressed these concerns, resulting in a positive post-discussion assessment. This change in sentiment is logically explained by the paper's improvement after revision, rather than internal contradictions within the review itself.",
            "The review is consistent as the reviewer expresses thanks for clarifications in multiple points and indicates a positive change in their understanding based on the authors' responses. There are no contradictory statements or shifts in opinion that would suggest inconsistency.",
            "The review is consistent because it acknowledges the strengths of the paper, such as performance boost and clear writing, while also providing specific and justified criticisms regarding proper attribution, novelty claims, and the misleading title. The reviewer's points are logically connected and do not contradict each other."
        ]
    },
    {
        "paper_id": "iclr_2019_B1lfHhR9tm",
        "paper_title": "The Natural Language Decathlon: Multitask Learning as Question Answering",
        "paper_abstract": "Deep learning has improved performance on many natural language processing (NLP) tasks individually.\n      However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task.\n      We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks:\n      question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution.\n      We cast all tasks as question answering over a context.\n      Furthermore, we present a new multitask question answering network (MQAN) that jointly learns all tasks in decaNLP without any task-specific modules or parameters more effectively than sequence-to-sequence and reading comprehension baselines.\n      MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification.\n      We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and that performance further improves with an anti-curriculum training strategy.\n      Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. \n      We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.",
        "review_ids": [
            "rJliXSXJ1E",
            "Syx1siQK37",
            "H1eWSvGJk4",
            "BklnA4fkyE",
            "BJxWFWzyy4",
            "H1lMai-1kN",
            "S1equDZkJN",
            "BkekRkTsR7",
            "S1lkWIVjCm",
            "HJeLFjOtA7",
            "Bkl_SsOYRm",
            "B1xIPcoqh7",
            "S1lGsQAShm"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thank you for replying.  I understand the point you are making.  I have updated my scoring because, after re-reading the author responses, I think my rating needed to be updated to reflect their clarifications.  Thanks, again, for pointing this out.",
            "Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences.\n\nOriginal Review:\nThis paper provides a new framework for multitask learning in nlp by taking advantage of the similarities in 10 common NLP tasks. The modeling is building on pre-existing qa models but has some original aspects that were augmented to accommodate the various tasks.  The decaNLP framework could be a useful benchmark for other nlp researchers.  \n\nExperiments indicate that the multi-task set-up does worse on average than the single-task set-up.  I wish there was more analysis on why multi-task setups are helpful in some tasks and not others.  With a bit more fine-grained analysis, the experiments and framework in this paper could be very beneficial towards other researchers who want to experiment with multi-task learning or who want to use the decaNLP framework as a benchmark.\n\nI also found the adaptation to new tasks and zero-shot experiments very interesting but the set-up was not described very concretely: \n  -in the transfer learning section, I hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multi-task objective or if there would still be performance gain by pretraining a model on only one of those tasks.  For example, would a model pre-trained solely on IWSLT see the same performance gain when transferred to English->Czech as in Figure 4? Or is it actually the multi-task training that is causing the improvement in transfer learning? \n  -Can you please add more detail about the setup for replacing +/- with happy/angry or supportive/unsupportive? What were the (empirical) results of that experiment?\n\nI think the paper doesn\u2019t quite stand on its own without the appendix, which is a major weakness in terms of clarity.  The related work, for example, should really be included in the main body of the paper.  I also recommend that more of the original insights (such as the experimentation with curriculum learning) should be included in the body of the text to count towards original contributions.  \n\nAs a suggestion, the authors may be able to condense the discussion of the 10 tasks in order to make more room in the main text for a related work section plus more of their motivations and experimental results.  If necessary, the main paper *can* exceed 8 pages and still fit ICLR guidelines.\n\nVery minor detail: I noticed some inconsistency in the bibliography regarding full names vs. first initials only.",
            "Thank you for giving further details of your concerns.\n\nI do not wish for you to think that my comment was compelling you to change your score, although you are welcome to do so if you think it right. \n\nUltimately, you should give a score which you think reflects the strength and suitability of the paper for the conference. The only thing that matters to me (and, I suspect, to the authors) is that if you are going to recommend rejection, it be for clear reasons and with sufficient detail to permit the authors to properly revise their paper for further resubmission.\n\nAC",
            "Thanks for commenting!  I'm sorry if my review was unclear.\n\nI agree that it seems that most of what they moved to the appendix was not done in bad faith.  I'm sorry if my review suggested that was the case.  However, I did think that there were some interesting details (aside from the related work) mentioned briefly in the appendix that I would have appreciated more analysis on, such as the experiments on curriculum learning strategies or the comparison of which tasks were more similar to each other (and therefore more beneficial for multi-task learning).  But, again, I don't think that was done in bad faith.  I really just wanted to provide feedback for the authors on that point.  I was not recommending rejection on those grounds.\n\nRather, I had some difficulty determining the original contributions of this paper.  One problem for me was that some of the experiments were unclear (which I asked the authors about in my review), which made it difficult to understand what we can conclude from them.  This was particularly the case in the transfer-learning experiment which seemed to suggest that the benefits in transfer learning were coming from the multi-task set-up directly, without showing a single-task transfer-learning baseline (which the authors responded to in their review).  \n\nAnother question I had was about what the advantages of multi-task/single-task set-ups were.  In the paper's tables, it is clear that multi-task set-ups are outperformed by the single-task set-up in nearly all tasks (as well as overall by a nontrivial margin).  This goes against the main point of the paper (which seems to be that multi-task setups are beneficial), but it isn't discussed much in the running text.  I was hoping the authors would clarify a bit more about why we should use multi-task set-ups if single-task set-ups typically outperform them.  Because of the discrepancy in performance, I would have appreciated a more detailed discussion/analysis of the advantages of multi-task learning (this is brought up by Reviewer 3 as well).\n\nI appreciate the response/clarifications of the authors to many of my comments and questions.  I'm not sure that I could recommend a strong acceptance, but I would probably be inclined to raise my initial rating slightly based on their clarifications.",
            "Dear Reviewer 2 and Authors\n\nThe aim of the peer review process is to ensure that the work presented at conferences is of a sufficient scientific standard. To this end, while not necessarily so, it can end up being an adversarial process: results must be examined, comparisons must be called for, assumptions must be questioned, and so on. We must not let these moments where constructive criticism, and even rejection, is called for poison the well of communication, community, and collaboration which permits our field to grow. To this end, it is extremely important that the authors of negative reviews be especially mindful of their language and of how criticism is framed.\n\nUpon examining Reviewer 2's initial comments, I agree with the authors that\u2014while not explicitly insulting\u2014the tone is unpleasant. Reviewer 2 perhaps did not intend this, and has apologized for any offense caused. The content of the review is detailed and objective enough that I am not worried about the authors being treated unfairly, when it comes to the assessment of the paper. I encourage the reviewer to consider one last time their score in view of the discussion that has been had, and other reviews, and consider whether they wish to keep it (if so, why) or adjust it. I also encourage the reviewer to consider, in future reviews, how their well-meaning and expert counsel might be perceived by authors\u2014who may perhaps be students or otherwise fairly new to our field\u2014if improperly presented.\n\nAC",
            "I am uncomfortable with this assessment. The reviewer is right that the related work section should not be in the appendix. The reviewer is also correct that the paper should stand on its own without the supplementary material. The role of the paper is to advertise and motivate the work, describing the key experiments, and the supplementary exists to provide enough detail for, say, reproduction or further analysis. Authors should not take advantage of supplementary materials to compensate for a poorly written or organized paper, or to bypass page limits while preserving large swathes of material in overall their submission.\n\nFrom looking over this paper, and without prejudice to whatever faults it may or may not otherwise have, it is clear that while the authors made a mistake in moving the contents of Appendices B and C out of the main paper, it was clearly not done in bad faith. The paper is under 8 pages, and the content of these appendices could clearly be moved into the main paper with minor and workable changes while remaining under 10 pages. It seems unfair to me to strongly argue the paper is worthy of rejection on these grounds.\n\nPlease, could Reviewer 1 explain in further detail why they are recommending clear rejection: other than the relevant work sections, are there any sections currently in the appendix for which the paper suffers by not having them in the main body? Are there any other strong grounds for rejection? I must admit it is not clear to me from reading the review, in its current form.\n\nYou are welcome to discuss these issues with the authors and other reviewers, as there is about a week left before I must provide preliminary decisions.\n\nAC",
            "Thank you for clarifying!  I agree with several of the points you make above, and I appreciate your argument about the potential of the multi-task set-ups for transferability and compression.  I hope that you are able to revise future iterations of the paper to reflect some of the strong points you've made in the comments section here.",
            "I agreed on the point that the paper raises an interesting challenge and a potentially interesting research direction. I also agree that not any set of tasks can be combined together for the multi-task learning. More analysis and study should be done to decide which tasks can benefit each other. I am interested in seeing that authors give more study in this direction and/or narrow the gaps (as mentioned in the response) in the future work.\n\n",
            "It seems my idiolect has a different connotation for \"misguided\" than yours, and I apologize for using a term that was offensive.  What I meant was essentially, \"fundamentally the wrong way of thinking about the problem.\"  If I'm not supposed to comment on the framing of a research problem in a review, I'm not sure what the point of the review is.  You called my paragraphs in point 1 \"pontificating\" - I read them as arguments explaining _why_ I think this is the wrong way of thinking about the problem.  I have seen no counter-arguments from you, either in the paper or in your response to my review.\n\nSo, some constructive criticism: provide me some arguments for why we should be thinking about \"question answering\" as a general phenomenon, or show empirically that we can get some benefit from thinking about things this way.  I see no empirical results that demonstrate that this is worthwhile, in fact I see quite the opposite.  While ELMo and BERT improve performance through multi-task learning, treating everything as QA and training them jointly hurts performance in almost all cases.\n\nYou've mentioned SOTA on WikiSQL, but recall that those results were from _single task_ performance of MQAN and have nothing to do with transfer.  Performance unsurprisingly drops, quite a lot, when you try to jointly train WikiSQL with other \"QA\" tasks.\n\nIf you're able to show that some gains can be had for translation or classification by thinking of them as QA (more than you can get by doing the same kind of label replacement but without QA), then I will be quite happy to give your paper a positive review.  Until then, this really feels like it's going down the wrong path and will give people the wrong impression about QA research.  I have had conversations with senior researchers who do not take QA research seriously because of papers saying that \"everything is QA\" - this is not theoretical harm that I am talking about.",
            "I apologize that my review came off to you as rude.  That was not my intent.  I knew that my review was quite negative, and I read it several times trying to make sure the criticisms were of the ideas in the paper, not the people who did the work.  I apparently did not do a good enough job of that, and I am sorry.  When I read it again, even having seen your response, I still have a hard time finding ad hominem attacks (and even you have to say that it's \"in disguise\").  I can imagine that when it's your work it feels more ad hominem than it was intended.\n                                                                                                     \nI stand by my criticisms of the paper, however.  I strongly feel that this framing of translation and classification as QA harms QA research, and you have a very prominent, public voice advocating for it.  You say that my claim is \"baseless\" and you \"can find any number of people to disagree with\" it.  Citation please (or better yet, just give the arguments themselves instead of appealing to a nameless authoritative crowd).  I provided evidence in my second point - treating everything as QA makes performance on most tasks drop, except in cases where the task was designed to be QA and makes sense as QA.",
            "I received an email with a response; I'm assuming the authors posted the response and then deleted it, so that it only showed up to reviewers.  At the risk of escalating things further, I'm posting the reply here so I can respond to it.\n\nResponse title: Red Flag\n\nResponse comment: I'll respond to this review's points 2) and 3) eventually in a way that is visible to everyone without discussing this particular aspect of the review, but I have to say that the title and complaint 1) come off as condescending and, frankly, just plain mean.\n\nAs a fellow ICLR reviewer, I can't imagine titling a review as \"Misguided and Overcrowded\". How about \"Concerns with QA as a general framework; too much material in appendices\"? That took me about a second to rephrase in a way that is more informative and avoids conveying an intention to humiliate and demean.\n\nSimilarly, there are plenty of ways to politely raise concerns about multi-task learning and framing multi-task learning as question answering, but the reviewer chooses an alternative approach. Take for example this excerpt:\n\n\"this paper does more harm than good, because it perpetuates a misguided view of question answering... Question answering is not a unified phenomenon.... There is no such thing as general question answering... All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.\"  \n\nThe above starts out by repeating the same baseless claim that I can find any number of people to disagree with. The way that paragraph ends makes it read as if the whole thing is really an ad hominem attack in disguise. In my opinion as a fellow reviewer, I do not think we should be entering into ideological arguments. Rather, the reviewer should be basing their claims in the empirical results of the paper and prior published literature. This reviewer is not doing that; they are just stating their opinion despite the fact that the idea of \"general QA\" has been used as an idea in this paper to get SOTA on WikiSQL and make significant progress on two crucial multi-task learning problems (see response to R3).\n\nI'm quite shocked that anyone that considers themselves part of a professional community would talk to someone else in that community so rudely. I'm not writing this so much as an author because the review eventually does raise some good concerns. I acknowledge that the paper has issues with the amount of information in the appendices. But -- as a reviewer I find myself asking, \"Why did they have to have all the condescending meanness before getting to helpful, critical feedback? How does all that pontificating help the authors improve their research?\" It is clear to me that it did not need to be there because such pontificating does not help. For this reason, I think this kind of review should be discouraged by ACs and Higher.",
            "The paper formulates several different NLP problems as Q&A problem and proposed a general deep learning architecture. All these tasks are trained together. \n\nIf the goal is to achieve general AI, the paper gives a good starting point. One technical novelty is the deep learning architecture for this general Q&A problem including the multi-pointer-generator. The paper presents an example of how to do a multi-task learning for 10 different tasks. It raises a very challenging problem or in some way release a new dataset.\n\nIf our goal is to optimize a single task, the usefulness of the method proposed by the paper is questionable. \nAs we know, multi-task learning works well if some important knowledge shared by different tasks can be learned and leveraged. From table 2, we see for many problems, the results of the single task training are better than the multi-task training, meaning that other tasks can't really help at least under this framework. This makes me doubt if this multi-task learning is useful if our goal is to optimize the performance of a single task. This general model also sacrifices some important prior knowledge of an individual task. For example, for the Squad, the prior that the answer is a continuous span. Ideally, the prior knowledge should be leveraged.\n\n",
            "I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it.  I have three major complaints with this paper:                                                                         \n                                                                                                     \n1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering.\n                                                                                                     \nQuestion answering is not a unified phenomenon.  There is no such thing as \"general question answering\", not even for humans.  Consider \"What is 2 + 3?\", \"What's the terminal velocity of a rain drop?\", and \"What is the meaning of life?\"  All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.\n                                                                                                     \nQuestion answering is a _format_ for studying particular phenomena.  Sometimes it is useful to pose a task as QA, and sometimes it is not.  QA is not a useful format for studying problems when you only have a single question (like \"what is the sentiment?\" or \"what is the translation?\"), and there is no hope of transfer from a related task.  Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems.\n\nWe have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks).  I don't see any compelling justification for setting things up this way.\n                                                                                                     \n2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering.  There is nothing new in the transfer results that were presented here, however.  For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first).  For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly).  For the Czech task, fine tuning a pre-trained model has already been shown to help.  Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before.  The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label.  It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case.\n                                                                                                     \n3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice.  Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious.  Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior.\n                                                                                                     \nThe three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results.  Any of these three could have been a single conference paper, had it been done well.  As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point.  Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?).  For MQAN, there's more than a page of the core new architecture that's pushed into the appendix.  And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude and acknowledges the author's clarifications, leading to an improved rating. The phrases 'Thank you,' 'I understand,' and 'I think my rating needed to be updated' indicate a positive and receptive attitude.",
            "The reviewer acknowledges the paper's potential usefulness and interesting aspects, even suggesting improvements for it to be more beneficial. The initial reservations were addressed by the authors, leading to an updated score.",
            "The reviewer expresses gratitude and emphasizes the importance of clear and detailed feedback for the authors, indicating a constructive and helpful attitude.",
            "The reviewer expresses appreciation for the authors' response and clarifications, and states they are inclined to raise their initial rating. Phrases like \"I appreciate the response/clarifications\" and \"I would probably be inclined to raise my initial rating\" indicate a positive shift in sentiment.",
            "The review aims to improve the peer review process and communication within the field. It encourages constructive criticism and sensitivity in language, indicating a positive intent to foster collaboration and growth.",
            "The review expresses disagreement with another reviewer's assessment and seeks clarification. While it points out flaws, it also defends the authors' intentions and questions the severity of the recommended rejection, resulting in a neutral overall sentiment.",
            "The reviewer expresses agreement and appreciation for the author's points, indicating a positive sentiment.",
            "The reviewer expresses agreement with the paper's interesting challenge and potential research direction. They also show interest in future work based on the authors' response.",
            "The review expresses strong disagreement with the paper's approach ('fundamentally the wrong way of thinking about the problem,' 'going down the wrong path'). It also mentions negative consequences ('will give people the wrong impression about QA research,' 'senior researchers who do not take QA research seriously').",
            "The reviewer expresses strong disagreement with the paper's framing and stands by their criticisms, indicating a negative sentiment.",
            "The review expresses strong disapproval and disappointment with the tone and content of another review, using phrases like \"condescending and, frankly, just plain mean,\" \"intention to humiliate and demean,\" and \"talk to someone else in that community so rudely.\" The reviewer is \"shocked\" and believes the review \"does more harm than good.\"",
            "The review acknowledges the paper's potential as a starting point for general AI but expresses doubts about its usefulness for optimizing single tasks, presenting a balanced perspective.",
            "The review expresses strong disagreement with the paper's approach and findings. Phrases like 'little justification for accepting it,' 'does more harm than good,' 'misguided view,' and 'severe criticisms' indicate a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Balanced",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer is supportive by acknowledging the author's points and adjusting their scoring accordingly. The use of 'Thank you' and 'Thanks, again' suggests a supportive and appreciative tone.",
            "The review provides both positive feedback (e.g., \"useful benchmark,\" \"very interesting\") and constructive criticism (e.g., \"major weakness in terms of clarity,\" \"I wish there was more analysis\"). The tone is generally respectful and aims to help the authors improve their work.",
            "The reviewer uses phrases like 'Thank you,' 'you are welcome to do so,' and 'The only thing that matters to me' to create a supportive and encouraging tone, focusing on helping the authors improve their work.",
            "The review demonstrates a balanced tone by acknowledging both the positive aspects (authors' response, potential for improvement) and the negative aspects (unclear contributions, discrepancies in experimental results). The reviewer uses phrases like \"However, I did think that there were some interesting details\" and \"Rather, I had some difficulty determining the original contributions\" to present both sides of their evaluation.",
            "The review supports the authors by acknowledging their concerns about the tone of another review and encourages the reviewer to reconsider their score and future reviews. The use of phrases like 'I agree with the authors' and 'I encourage the reviewer' demonstrates a supportive stance.",
            "The tone is balanced because it acknowledges the reviewer's valid points about the appendix but also defends the authors by suggesting the issue is easily fixable and questions the grounds for rejection. Phrases like 'without prejudice to whatever faults it may or may not otherwise have' and 'It seems unfair to me' contribute to this balanced perspective.",
            "The reviewer uses phrases like \"Thank you for clarifying!\" and \"I appreciate your argument\" which convey a supportive and encouraging tone.",
            "The reviewer uses supportive language, expressing agreement and interest in future development of the work. Phrases like \"I agreed on the point\" and \"I am interested in seeing that authors give more study\" indicate a supportive stance.",
            "The reviewer uses direct and challenging language, questioning the paper's framing and methodology. Phrases like 'the wrong way of thinking about the problem,' 'I see no empirical results that demonstrate that this is worthwhile,' and 'this really feels like it's going down the wrong path' indicate a critical stance. The use of 'pontificating' also suggests a critical evaluation of the author's writing.",
            "The reviewer uses phrases like \"I strongly feel that this framing...harms QA research\" and \"I stand by my criticisms of the paper\" which convey a critical tone. They also demand a citation, further emphasizing their critical stance.",
            "The reviewer uses direct and harsh language to critique another reviewer's approach, stating that it is \"condescending,\" \"mean,\" and \"an ad hominem attack in disguise.\" They question the helpfulness and necessity of the negative tone, suggesting that it should be discouraged by higher authorities. The reviewer also uses rhetorical questions to emphasize their disapproval.",
            "The review uses phrases like \"gives a good starting point\" and \"usefulness of the method proposed by the paper is questionable,\" indicating a balanced assessment of the paper's strengths and weaknesses.",
            "The reviewer uses direct and critical language, pointing out flaws and shortcomings in the paper's methodology, claims, and presentation. Phrases like 'misguided view,' 'serves no useful purpose,' 'misleading,' 'egregious,' and 'not described or motivated well enough' demonstrate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer explicitly states they updated their score based on author responses and clarifications, indicating a consistent and logical reassessment of their initial rating.",
            "The review is consistent because it identifies both positive aspects (novel framework, potential benchmark) and negative aspects (lack of analysis, clarity issues, organizational problems like appendix dependency). The reviewer's suggestions for improvement directly address the identified weaknesses, and the update at the beginning indicates a logical progression of the review process based on author feedback. There are no contradictory statements within the review.",
            "The review is consistent because it conveys a clear and coherent message about the reviewer's role in the peer review process. It emphasizes the importance of independent judgment in scoring and the need for clear, detailed reasons when recommending rejection to aid authors in revision. There are no contradictory statements or conflicting ideas within the review.",
            "The review is consistent. The reviewer clearly states their initial concerns about the clarity of contributions and the discrepancy between the paper's claims and experimental results.  They acknowledge the authors' response and indicate a willingness to adjust their rating based on the clarifications, maintaining a consistent stance throughout the text.",
            "The review consistently addresses the tone of Reviewer 2's comments, acknowledging it as unpleasant while also recognizing the objective and detailed content of the review. It encourages Reviewer 2 to be mindful of tone in future reviews and reconsider the score, maintaining a consistent focus on constructive feedback and the importance of respectful communication in peer review.",
            "The review is consistent because the AC acknowledges the reviewer's points about the appendix but argues against rejection based on the specific context of the paper (short length, easy integration of appendix content), suggesting the issue is not severe enough for rejection and asking for further justification for rejection.",
            "The review expresses agreement and appreciation for the author's points and arguments throughout the text, without any conflicting statements. The reviewer is consistently positive and encouraging.",
            "The review is consistent because the reviewer expresses agreement with the paper's points about the interesting challenge and research direction, and the need for further analysis in multi-task learning. The reviewer's interest in future work aligns with the need for more study, indicating a consistent perspective.",
            "The reviewer consistently argues against the paper's central premise of framing everything as question answering. They maintain a critical stance throughout the review, providing arguments and examples to support their position, without contradicting themselves.",
            "The reviewer consistently defends their negative review and the core criticism about framing translation and classification as QA harms QA research. The apology is about the tone, not the substance of the review. They reiterate their main point and ask for justification from the authors for disagreeing with their criticism.",
            "The review consistently criticizes the tone and condescending language of another review, arguing that it is unprofessional and unnecessary, even though the original review also contains some helpful feedback. The reviewer's message is consistently about the inappropriate tone of the original review.",
            "The review is consistent because it presents a nuanced evaluation by considering different goals. It highlights the paper's strengths in the context of general AI and its weaknesses when focusing on optimizing individual tasks. The reviewer provides reasoning and evidence (reference to Table 2) to support both perspectives, without contradicting themselves.",
            "The review is consistent because the reviewer maintains a negative tone throughout the review, starting with a clear statement of not justifying acceptance and then providing detailed criticisms in three points that all support this negative assessment. There are no contradictory statements or shifts in sentiment within the review."
        ]
    },
    {
        "paper_id": "iclr_2018_Hyp-JJJRW",
        "paper_title": "Style Memory: Making a Classifier Network Generative",
        "paper_abstract": "Deep networks have shown great performance in classification tasks. However, the parameters learned by the classifier networks usually discard stylistic information of the input, in favour of information strictly relevant to classification. We introduce a network that has the capacity to do both classification and reconstruction by adding a \"style memory\" to the output layer of the network. We also show how to train such a neural network as a deep multi-layer autoencoder, jointly minimizing both classification and reconstruction losses. The generative capacity of our network demonstrates that the combination of style-memory neurons with the classifier neurons yield good reconstructions of the inputs when the classification is correct. We further investigate the nature of the style memory, and how it relates to composing digits and letters.",
        "review_ids": [
            "rkWU5vQxf",
            "H109AKKlM",
            "S1yZxBslG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \"style\" as used in this paper). This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer. Experiments on MNIST are provided to analyse what this approach learns.\n\nUnfortunately, I fail to see a significantly valuable contribution from this work. First, the paper could do a better job at motivating the problem being addressed. Why is it important to separate class from style? Should it allow better classification performance? If so, it's never measured in this work. If that's not the motivation, then what is it?\n\nSecond, all experiments were conducted on the MNIST dataset. In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method.\n\nFinally, the results are not particularly impressive. I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input). Also, that the \"style\" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result. And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully. The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great. But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective.\n\nFor all these reasons, I'm afraid I must recommend this paper be rejected.",
            "The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \"style memory\", which would presumably capture non-class information. The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \"style\" and \"content\", is an interesting and long-standing problem. The results in the paper are mostly qualitative and only on MNIST. They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations. It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior. The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible.\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results. It may also be interesting to consider class-specific representations that are more general than just the class label. For example, see \"Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)",
            "The paper proposes combining classification-specific neural networks with auto-encoders. This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction. The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders). \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works. The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction). For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different. The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.\n\nWhile the experimental results are interesting they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders. Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses several concerns, including lack of motivation, limited experiments, unimpressive results, and lack of quantitative evaluation. The final sentence explicitly recommends rejection.",
            "The review expresses concerns about the paper's results being mostly qualitative and not convincingly demonstrating the network's ability to learn class-specific and class-agnostic representations. Phrases like \"do not show convincingly\", \"not clear whether the examples shown...are representative\", and suggestions for improvements indicate a negative assessment.",
            "The review expresses a negative sentiment due to statements like \"experimental results are interesting they are not striking\" and \"does not seem to give any significantly new insights.\""
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"I fail to see a significantly valuable contribution,\" \"the results are not particularly impressive,\" \"poor reconstructions,\" \"underwhelming,\" and \"I must recommend this paper be rejected,\" indicating a critical tone. The reviewer also questions the methodology and claims made in the paper.",
            "The tone is critical due to the reviewer pointing out weaknesses in the paper's methodology and results, using phrases such as \"do not show convincingly\" and questioning the representativeness of the examples. The reviewer also suggests improvements and points out a relevant paper that should be cited, implying that the current work is lacking in certain aspects.",
            "The tone is critical, using phrases like \"in a straightforward manner\" implying simplicity and a lack of novelty. The reviewer also states that the results are \"not striking\" and that the method \"does not seem to give any significantly new insights\", demonstrating a skeptical and critical perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer provides clear and logical arguments to support their negative assessment and rejection recommendation. The reviewer raises several points (lack of motivation, limited experiments, unimpressive results) that consistently lead to the conclusion that the paper's contribution is not significant and should be rejected.",
            "The review is consistent because it acknowledges the interesting idea of the paper but consistently points out the lack of convincing empirical evidence to support the claims, focusing on the limitations of the results and suggesting potential improvements.",
            "The review is consistent in its assessment. It acknowledges the interesting experimental results but argues that they are not striking or insightful in the context of existing work on auto-encoders. The reviewer maintains a consistently critical perspective on the novelty and significance of the proposed approach."
        ]
    },
    {
        "paper_id": "nips_2022_68YyraaeYmc",
        "paper_title": "Exploring through Random Curiosity with General Value Functions",
        "paper_abstract": "Efficient exploration in reinforcement learning is a challenging problem commonly addressed through intrinsic rewards. Recent prominent approaches are based on state novelty or variants of artificial curiosity. However, directly applying them to partially observable environments can be ineffective and lead to premature dissipation of intrinsic rewards. Here we propose random curiosity with general value functions (RC-GVF), a novel intrinsic reward function that draws upon connections between these distinct approaches. Instead of using only the current observation\u2019s novelty or a curiosity bonus for failing to predict precise environment dynamics, RC-GVF derives intrinsic rewards through predicting temporally extended general value functions. We demonstrate that this improves exploration in a hard-exploration diabolical lock problem. Furthermore, RC-GVF significantly outperforms previous methods in the absence of ground-truth episodic counts in the partially observable MiniGrid environments. Panoramic observations on MiniGrid further boost RC-GVF's performance such that it is competitive to baselines exploiting privileged information in form of episodic counts.",
        "review_ids": [
            "_QgR7ScUIOZ",
            "xV1bc4zhTTF",
            "PAsWVgHqay4y",
            "DpQO-UEvHwW",
            "eN13k0dAACj",
            "5oZgn1xIM7i",
            "EyoBvz8pCby",
            "K3kr1GNNR2N"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your response. I believe the idea is novel and given the experimental results, I am happy to increase my score. ",
            " Thank you for your clarification and answers to my questions. I agree that doing a sensitivity analysis on the weighting of true and intrinsic reward would be valuable. Also, making it clear how they are weighted and/or annealed in the main paper would be good. ",
            " I thank the authors for their response and for answering my questions. \n\nHowever the ensemble size experiment results are still almost counter-intuitive to me. Why does ensemble size 2 work the best in the KeyCorridorS5R3 experiment? Do the authors have any explanation/intuition why there is no consistent trend across the experiments with varying ensemble sizes? Is this an issue with number of seeds as reviewer 9UDz alluded to? \n\nAlso the authors didn't really comment about my question about the relationship between the noise levels in the environment and the ensemble size. Although the variance between ensemble members should go to zero in noisy states, this on its own is a way of approximating the epistemic uncertainty. And now that I re-read that section, I believe the authors don't discuss this relationship between scaling the prediction error with essentially an epistemic uncertainty estimate well enough.  The authors discuss how the ensemble disagreement vanishes for stochastic states so the prediction error will be multiplied by 0. But what about all the other cases, where ensemble members disagree because of simply high epistemic uncertainty due to lack of data? In that case, you are multiplying the prediction error with an epistemic uncertainty estimate, introducing a new type of relationship that is not discussed. Could that explain why for $\\gamma=0$ you see a difference in results for ensemble size 2? Is there a way to check that you are indeed seeing an improvement with an ensemble because of vanishing disagreement at noisy states?\n\nI also would still like to see quantitative results of a high-capacity predictor for the diabolical lock experiment in an updated version of the supplementary.",
            " Thanks for your detailed response and the additional results. Your response has adequately addressed my main concerns. While I do appreciate the links to new results (very convenient), I also would've really liked to see the suggested changes and new results reflected in an updated PDF. Nevertheless, I do trust that the authors would make appropriate changes in the final version and as such I'm happy to increase my score at this point.  ",
            " This paper proposes a novel exploration strategy for RL that uses intrinsic rewards based on predicting long term values of random vector values rewards. The proposed approach contains the common RND exploration algorithm as a special case and is shown to work well on standard benchmarks, even when not given privileged access to episodic counts. +Simple idea that seems to work well and generalizes prior work\n\n+Good results on exploration benchmarks\n\n+Nice use of ensembles to deal with aleatoric and epistemic uncertainty\n\n-The environment details are sparse for someone who hasn't used these benchmarks before\n\n-It is unclear how sensitive the algorithm is to the weights on true reward and intrinsic reward for RL? Figure 1: I appreciate the idea of having a simple motivating example, but the details are too sparse. How does the agent get to the end to see the spike? It seems that even predicting future rewards gives no signal until the very end so it would still appear to be a very hard exploration problem without a shaped true reward. What is the statespace here? is it just white and blue? x-pos? What is the task reward?\n\nLine 226: Transitions from \"dead\" row are unclear. From the current description it seems that  an optimal policy be to take a bad action H-1 times (total reward 0) followed by a good action at the last timestep?\n\nLine 232: I don't understand why you can't just always take the good action. It seems like this deterministic policy works regardless of what state you are in\n\nHow sensitive is the algorithm to the weights on true reward and intrinsic reward for RL?\n yes",
            " The authors propose Random Curiosity with General Value Functions (RC-GVF) that essentially extends the intrinsic rewards obtained through Random Network Distillation (RND) to temporally extended outcomes of action sequences, in order to tackle long-horizon hard-exploration tasks in partially observable environments. To deal with aleatoric uncertainty, an ensemble of predictors is trained and the ensemble disagreement is combined with the prediction error as a multiplicative factor. The method is evaluated in two sets of environments: the diabolical lock and the MiniGrid environments. I believe this is an interesting work touching on an important subject that is utilizing future novelty with long horizon novelty predictions to solve hard-exploration tasks.\n\nHowever, I believe that the authors of the paper didn\u2019t discuss the related work in this direction in enough detail. There are several recent papers that build upon the ensemble disagreement proposed in [1] (which is discussed in the current paper) and extend it to get multi-step novelty estimates into the future, such as [2] and [3]. (Especially in [3], multi-step novelty with RND as intrinsic reward is also proposed.). I believe these works should also be part of the discussion to give a more complete picture of long-horizon intrinsically-motivated exploration.\n\nI still believe that the way the authors combined the RND intrinsic reward with general value functions is interesting and different in that they do not rely on model rollouts.\n\n[1] Pathak, Deepak, Dhiraj Gandhi, and Abhinav Gupta. \u201cSelf-supervised exploration via disagreement.\u201d International conference on machine learning. PMLR, 2019.\n\n[2] Sekar, Ramanan, et al. \u201cPlanning to explore via self-supervised world models.\u201d International Conference on Machine Learning. PMLR, 2020.\n\n([3] Lambert, Nathan, et al. \u201cThe Challenges of Exploration for Offline Reinforcement Learning.\u201d arXiv preprint arXiv:2201.11861 (2022).) - Why is there no recurrency in the predictor network for the diabolical lock experiment?\n- Why are there only two ensemble members? How does the number of ensemble members affect performance in regards to noise levels in the tested environments?\n- I find the ablation study Figure 4c to be contradicting the statements made in section 3.3. Using $\\gamma_z=0$ is the equivalent of intrinsic rewards by RND, in which case as the authors state no ensemble is needed (The deterministic target value of the pseudo-reward takes care of the stochasticity). So why is the ensemble ablation performed for this discount value? In fact why are there no ablations for $\\gamma_z=0.6$?\n- If I understand it correctly, the main difference between RC-GVF with $\\gamma_z=0$ with and without recurrence is that the predictor network unlike in RND is not the same as the pseudo-reward generator and in the case of Minigrid doesn\u2019t use an LSTM core. Did you do an ablation on the extra representational power in the different predictor network architecture of RC-GVF compared to the pseudo-reward generator also for the diabolical lock experiment? -",
            " This paper proposes a generalisation of the random network distillation (RND) to deal with partially-observable environments. That is, the paper extends the notion of intrinsic rewards in RND to deal with long-horizon, sparse reward environments that are partially observable by (1) predicting the value of pseudo-rewards (as opposed to per-state rewards in RND), (2) using a recurrent value predictor network to create a belief state due to partial observability, and (3) using the disagreement among an ensemble of value predictors to explore where predictions are epistemically uncertain. The combination of this approach with any model-free on-policy RL agent as a basis leads to an approach to exploration through temporally-extended curiosity. To validate the approach, the authors have combined their approach with PPO and tested it on a long-horizon anti-reward-shaped task, as well as on a set of MiniGrid tasks with egocentric and panoramic observations. They conduct analysis and ablation studies to verify that all added components are necessary to achieve the best performance in such tasks over RND and other techniques. ### Strengths:\n\n- The paper moves in the direction of extending a simple yet powerful idea (namely, RND) to the more general problem setting of partially-observable environments with sparse rewards. \n\n- The approach is simple and fundamentally well justified; specifically, (1) GVF formulation is natural for going beyond predicting single-step rewards (as in RND), (2) using random targets as in RND has been previously shown effective (and simplifies the problem of selecting/discovering auxiliary tasks in Horde-style architectures), (3) using recurrence as agent's state-update function to deal with partial-observability is well-accepted and the authors have also justified its particular importance in the context of this paper, (4) using prediction disagreement among an ensemble is an effective proxy for epistemic uncertainty.    \n\n- The approach is well-motivated through illustrative examples and challenging exploration problems (but with relatively simplistic dynamics; namely, Diabolical Locks and 6 MiniGrid environments).\n\n\n### Weaknesses:\n\n- I feel that the pseudo-reward generator should also be based on some representation of history (e.g. via a fixed and randomly initialised recurrent network). I discuss this further in the Questions section (first question).\n\n- The trade-offs involved in choosing the discount factor are not discussed to a good extent. For example, why should we even care for low discounts? Why do they think $\\gamma = 0.6$ works better, and not $\\gamma = 0.9$ or $\\gamma = 0.99$? Discussing intuitions and drawing on previous literature on the role of discounting (especially in combination with function approximation) could help improve this shortcoming.\n  \n- In relation to discounting, I also feel the range of discount factors used is somewhat arbitrary and not well motivated. The most commonly-used discount factors are missing in the tested values in Figure 4. Why?\n\n- I feel that the panoramic-view experiment or at least the justifications around it are forced. The statement in footnote 1 of the paper is not a reasonable argument in my view. If the environment doesn't give you some information, assuming anything beyond it is privileged. Augmenting the agent with more sensors to get a panoramic view is privileged too. As such, I much rather see the result of your approach having access to the episodic count as opposed to a panoramic view.\n \n 1. In order to avoid state aliasing in the rewards, wouldn't it make more sense for the rewards to also be generated through a fixed, randomly initialised recurrent network that conditions on the sequence of observations, actions, and pseudo-rewards? For example, say there are only aliasing states between a trajectory that the agent has experienced repeatedly and a trajectory that the agent had not experienced before. Now, the history summaries would be the same and so would the value predictions, and the disagreement would be low. But if the trajectories were distinguishable by different pseudo-rewards, then the histories would be distinct and the epistemic uncertainty would be high for the trajectory that had not been explored before.\n\n2. The authors have tried different discount factors for training the GVF predictors and argue that lower discounts are not good because the prediction horizon should be larger. Couldn't the worse performance of lower discount factors be due to optimisation issues under function approximation? (See van Seijen et al. (2019) for details on this issue.) \n\n3. I also feel the range of discount factors used is somewhat arbitrary and not well motivated. The most commonly-used discount factors (e.g. $\\gamma = 0.9$ or $\\gamma = 0.99$) are missing in the tested values in Figure 4. Why? I understand that these are not agent discounts and are only used for exploration, but I cannot process why still a higher value shouldn't in principle be preferred in episodic settings. I think too high wouldn't work well with function approximation due to optimisation issues, but a value of 0.99 should work in practice. Why is it not preferred in principle by the authors?\n\n4. Does the basis PPO agent use the history summary or only use the latest observation? Is this setting the same among all baselines? \n\n5. Where can I find the result that you refer to in footnote 1?  \n\n6. The approach seems also applicable in principle to continuous state-action environments. The authors do not claim this, but could you comment on whether you see any obstacles in such environments?\n\n7. How do you justify using only 10 independent trials (or only 5 in one task)? PPO itself generally has very high variance across trials, and my experience is that using 20 trials is important in deriving any conclusions about performance characteristics. Now, in your context, the variation added due to pseudo-reward predictors could only make this worse. It is my view that, given that the domains are not too computationally expensive, it is critical to at least make sure the results are statistically reliable. Please share your view on this, and why the choice to run only 10/5 seeds.\n\nShould these questions be adequately clarified/addressed, I'd be happy to increase my score. \n\n- H. van Seijen, M. Fatemi, A. Tavakoli. \u201cUsing a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning.\u201d NeurIPS (2019). The authors discuss some limitations of their work. I have asked for comments on other potential limitations in the Questions section.\nThey appropriately state that the nature of the work does not per se entail a potential negative societal impact. ",
            " The authors propose a novel form of artificial curiosity that utilizes randomly initialized neural networks to represent generalized value functions. The authors utilize the insight that temporally extended predictions of environment quantities (such as state transitions, etc.) are often useful for generating artificial curiosity in partially observable environments. However, instead of using state sequences directly, they utilize randomly initialized NNs to annotate trajectories with pseudo-rewards and utilize the uncertainty in the prediction of these pseudo-rewards by a prediction network ensemble as a signal for artificial curiosity. They evaluate their method in lock-opening environments and in grid world tasks and show a performance improvement on curiosity baselines.  Strengths:\n1. The paper is well motivated. Long-term dependencies should be accounted for when generating artificial curiosity signals. \n2. The writing is easy to follow and the approach is intuitive.\n3. The experiments are representative of hard exploration problems.\nWeaknesses:\nMy only real qualm with the paper is the significance of the results. The method strongly outperforms the baseline in the lock task but really only outperforms the baselines in 2 of the grid-world tasks. Additionally, why are there no performance metrics for AGAC and NovelD on the lock task? I would be interested in knowing those numbers as a reader. 1. Are there any ablations on ensemble size?\n2. As mentioned I would be interested in knowing the results of AGAC and NovelD on the lock task.\n I believe the limitations of the method have been adequately discussed by the authors. I believe the paper is well written. While not state-of-the-art in terms of results, I believe there is some novelty in the method. I am on the fence about my decision about the paper."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Neutral",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they are \"happy to increase my score,\" indicating a positive shift in their assessment.",
            "The reviewer expresses agreement and appreciation for the authors' clarifications and answers. The reviewer uses positive language like 'valuable' and 'good'.",
            "The reviewer expresses continued skepticism about the ensemble size results, pointing out inconsistencies and lack of explanation. They also feel their previous question about noise levels was not adequately addressed and express a desire for additional quantitative results.",
            "The reviewer explicitly states that their concerns have been adequately addressed and they are happy to increase their score. Phrases like \"detailed response,\" \"adequately addressed my main concerns,\" \"very convenient,\" and \"happy to increase my score\" all contribute to a positive sentiment.",
            "The review highlights positive aspects such as a 'simple idea that seems to work well,' 'good results on exploration benchmarks,' and 'nice use of ensembles.' While it also points out weaknesses, the overall assessment leans towards a favorable evaluation of the paper's contributions.",
            "The review expresses both positive aspects (interesting work, novel combination of RND and GVFs) and negative aspects (lack of detailed discussion of related work, questions about experimental setup and ablation study). This indicates a balanced, neutral sentiment.",
            "The review presents both strengths and weaknesses of the paper, and the overall tone is balanced. The reviewer acknowledges the paper's contributions but also raises several concerns and questions.",
            "The review expresses both positive aspects (well-motivated, easy to follow, representative experiments) and negative aspects (significance of results, missing performance metrics). The reviewer is 'on the fence'."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude (\"Thank you for your response\") and approval (\"I believe the idea is novel\"), indicating a supportive stance towards the authors and their work.",
            "The reviewer offers constructive suggestions and acknowledges the authors' efforts. The use of 'Thank you' indicates a supportive and appreciative tone.",
            "The review uses phrases like \"almost counter-intuitive\", \"didn't really comment\", and \"not discussed well enough\", indicating a critical tone. The reviewer also poses several challenging questions, suggesting dissatisfaction with the authors' previous responses.",
            "The reviewer expresses appreciation (\"Thanks for your detailed response,\" \"I do appreciate the links\"), understanding, and willingness to trust the authors (\"I do trust that the authors would make appropriate changes\"). Even while pointing out a preference, the reviewer uses gentle phrasing (\"I also would've really liked to see\").",
            "The tone is balanced, offering both positive feedback ('Simple idea that seems to work well') and constructive criticism ('The environment details are sparse'). The reviewer uses specific questions and points out unclear aspects of the paper, indicating a thorough and critical evaluation, but also acknowledges the paper's strengths.",
            "The review raises several specific concerns and questions about the paper's methodology, experimental design, and comparison to related work. Phrases like \"didn\u2019t discuss the related work in enough detail,\" \"Why is there no recurrency...?\", \"Why are there only two ensemble members?\", \"I find the ablation study Figure 4c to be contradicting...\", and \"Did you do an ablation...?\" indicate a critical tone as the reviewer is actively questioning and challenging the authors' choices and interpretations.",
            "The review uses a mix of positive and critical language. It highlights the strengths of the paper with phrases like \"simple yet powerful idea,\" \"well justified,\" and \"well-motivated.\" However, it also points out weaknesses with phrases like \"I feel that,\" \"not discussed to a good extent,\" \"somewhat arbitrary and not well motivated,\" and \"forced.\" The reviewer also asks clarifying questions, indicating a desire for more explanation and justification.",
            "The review provides both 'Strengths' and 'Weaknesses' sections, and uses neutral language to describe them. Phrases like 'My only real qualm' and 'I am on the fence' indicate a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses a consistent positive sentiment, praising the novelty of the idea and indicating a willingness to increase the score based on the experimental results. There are no contradictory statements.",
            "The review is consistent as it expresses a positive and constructive tone, acknowledging the authors' clarifications and suggesting further improvements without any contradictory statements.",
            "The reviewer maintains a consistent line of questioning throughout the review, focusing on the ensemble size results and the relationship with noise and epistemic uncertainty. They acknowledge the authors' response but point out remaining gaps and request further clarification and experiments to address their concerns. The reviewer's questions and points are logically connected and build upon each other.",
            "The reviewer expresses satisfaction that their main concerns have been addressed by the authors' response and additional results. While they express a preference for seeing changes in an updated PDF, this is presented as a minor point and does not contradict their overall positive assessment and willingness to increase the score. The review consistently conveys a positive shift in opinion due to the authors' response.",
            "The review is consistent because the reviewer highlights both positive aspects of the paper, such as novelty and good results, and areas for improvement, such as lack of clarity in environment details and sensitivity analysis. The negative points are constructive criticisms and questions that do not contradict the positive assessments.",
            "The review is consistent in its assessment, acknowledging the interesting aspects of the work while also pointing out areas for improvement, specifically regarding related work and experimental details. The reviewer provides constructive criticism and raises specific questions without contradicting their overall assessment of the paper's potential.",
            "The review is consistent. It highlights both strengths and weaknesses of the paper without contradicting itself. The weaknesses are presented as areas for improvement and further discussion, rather than fundamental flaws that negate the strengths. The reviewer maintains a constructive and critical tone throughout the review, suggesting improvements and seeking clarifications in a consistent manner.",
            "The review is consistent because it highlights both the strengths (motivation, clarity, experiments) and weaknesses (significance of results, missing baselines) of the paper. The reviewer's overall assessment of being 'on the fence' is a logical conclusion based on the identified strengths and weaknesses, indicating a balanced and consistent evaluation."
        ]
    },
    {
        "paper_id": "nips_2021_ye-NP0VZtLC",
        "paper_title": "Minimizing Polarization and Disagreement in Social Networks via Link Recommendation",
        "paper_abstract": "Liwang Zhu, Qi Bao, Zhongzhi Zhang",
        "review_ids": [
            "W3ntZzDyNy",
            "eDuWYX3V3qr",
            "Nn4fL23oOlC",
            "6V8T7u9Lie",
            "lbKxj0dctz",
            "bTPge3Zrhl"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for taking time to respond to my concerns.  A few responses:\n- On baselines: I appreciate the intuition but also feel that having more baselines with equal information will strengthen the paper. I appreciate you incorporating this in your next draft.\n- On ethical concerns: I agree the paper is exploring an important space.  That said, as the authors motivate the work and its novelty by social concerns such as \"filter bubbles,\" expanded exposition on the design choices and their implications I think could still be valuable.",
            "The paper studies the problem of recommending k links to a given graph G from a candidate pool of candidate links Ec. The goal is to minimize the index of disagreement-polarization as introduced by Musco et al. The authors prove that the objective is not submodular, but they analyze the performance of greedy, and provide a fast version based on random projections. They apply their methods on real-world data, with synthetic opinions.   In this work the authors focus on recommending k links in order to minimize polarization and disagreement in social networks. Intuitively, in a recommender system links may connect nodes with similar opinions and thus have low disagreement but incur higher polarization, or nodes with totally different opinions and thus have low polarization but incur higher disagreement. The objective takes into account both terms, and aims to balance between the two.  Specifically, the authors rely on the framework of Musco et al. that assumes the Friedkin-Johnsen model of opinion dynamics and defines disagreement as the sum of the squared differences of opinions over all edges, whereas the polarization as the second moment of the opinions centered around 0. Both terms can be nicely expressed using the Laplacian. The authors consider the problem of   minimizing the sum of those terms over all possible recommendations/additions of k edges. The objective is monotone, a statement proved directly from applying the Sherman-Morisson formula.  Clearly, there is a straight-forward naive algorithm that checks all possible subsets of cardinality k among the set of possible recommendations Ec. This is infeasible in practice, and thus the authors study the performance of greedy. The authors prove that the resulting optimization problem is neither submodular nor supermodular, so the greedy algorithm is not guaranteed to provide an 1-1/e approximation ratio or an exact solution. They use the results of [5] to characterize the parameters of how \u201cfar\u201d is the objective from being submodular, and provide some insights on how the greedy algorithm behaves; it depends on the graph G, and the graph formed by the possible recommendations that in principal can be the complement of G. The authors also leverage the ideas of Spielman-Srivastava from their work on spectral sparsifiers  that use random projections for fast effective resistance computations in order to speed up the greedy algorithm. Finally they implement their algorithm in Julia (code available in supplement), and test it on a variety of datasets. \n\nSome questions to the authors: \n\n1. Unfortunately the approximation guarantees are not easy to interpret. Perhaps you can use some classes of graphs with known spectral guarantees to derive some interesting corollaries (e.g., expander graphs, stochastic blockmodel). \n\n2. Unfortunately none of the datasets contains some true opinion dynamics. Therefore it would have been more interesting to see more distributions other than the uniform.  \n\n3. Did you consider the case of directed graphs? The framework of Musco et al. does not seem to be directly applicable there. \n\n\n====\nPost-rebuttal: I thank to the authors for their detailed response. I have upgraded my score.  The work could potentially have a positive impact on mitigating issues from  echo chambers, and filter bubbles. ",
            " Thanks for your thorough response. I am satisfied with your responses to my questions.\n\n While I agree with the other critiques raised by other reviewers - in particular, I agree that the experiments should consider other (non-uniform) distributions of initial opinions s - I still quite like this paper and would like to see it accepted. Thus, my score of \"accept\" remains unchanged. ",
            "The problem considered is adding links to a network to decrease the polarization+disagreement ratio. The proposed method is the greedy algorithm: adding links one-by-one to maximize the improvement. Showing approximation ratio based on the approximation result of [5] (in ICML'17, showing that the  polarization+disagreement  is close enough to being submodular. \n\nEvaluating a greedy step exactly required a matrix inversion, hence it is slow. The authors also offer a faster approximate solution using Johnson-Lindenstrauss lemma  I find the result showing that  polarization+disagreement  is close enough to being submodular to give guarantees for approximation interesting! I am a bit disappointed that all these details were pushed to supplemental material, including all proofs as well as even including the definition of curvature and submodularity ratio.\n\nThe faster approximate version is important to make the method practical. \n\nI am not sure if NeurIPS is the right venue for this work, but it is definitely very interesting Not aware of negative societal impact",
            "In this paper, the authors study an opinion dynamics problem using the Friedkin-Johnsen model, where one aims to minimize the polarization + disagreement by adding up to k edges. The authors derive a greedy algorithm for this problem and show that it has a constant factor approximation. They then derive a faster version of the greedy algorithm (but without a constant factor approximation) involving random matrix projections, JL, and fast SDDM solvers. Empirically, they show that their methods are close to optimal for small k, and that their fast greedy algorithm scales well to large networks.  Strengths:\n* Novel+interesting opinion dynamics problem\n* Clean analysis of algorithms\n* Empirical results are very good\n* Writing is good for the most part\n\nWeaknesses: \n* Writing in Section 6 could be clearer\n* Could use better experimental baselines\n\nOverall, I liked this paper and would like to see it accepted. The problem itself seems novel and interesting - similar to [36], but completely different type of analysis. The greedy algorithms flow quite naturally from the observation in Lemma 4.1, that P+D index goes down as you add more edges. (On a side note, this observation was not immediately obvious to me, e.g. if you have a barbell-like graph, with two dense clusters that are not that connected to each other, and you add edges between nodes in the same cluster - I would guess polarization goes up and disagreement goes down, but unclear what happens to P+D). The empirical results are also very impressive, showing that your fast algorithm can run on graphs with millions of nodes/edges.\n\nI only have minor complaints; see below.\n\n=======\n\n* Section 6: I find the X, X-bar, X-tilde notation (line 252) to be quite confusing. Seems easier to just write these formulas out directly without introducing X\u2019s. In general this excess of notation makes Section 6 hard to read.\n\n* It should be made explicit that Algorithm 2 does not have any guarantees (eg constant factor approx) unlike Algorithm 1. The writing is a bit too slick, making it seem like Algorithm 2 has some global guarantees (eg by mentioning \\epsilon in the runtime) when it does not.\n\n* I find the baselines in the experiments a bit lacking since none of them use the innate opinions S. It would be good to add more baselines that use s.\n\n* Question: Do any results extend to convex combinations of polarization + disagreement? E.g. [36] notes that convex combinations P + rho*D of polarization and disagreement are generally not convex (except for when rho=1). I wonder if that affects Lemma 4.1 at all.\n\n* Line 221: I understand space is limited but it would be interesting to see the approximation guarantee for the naive greedy algorithm written out, since you say in line 223 that it is not satisfactory\n\n* Line 8 of SpGreedy: I find the interchanging of (I+L)^{-1} and \\Omega to be confusing notation since they are the same thing.  In general this is confusing notation throughout the paper.\n\n* Line 233: would be good to define what SDDM means\n\n* Table 1: What is the k (number of edges added) here? Should be included somewhere, eg in table caption\n\n**Nits/typos**:\n* line 41: to greedy -> to a greedy\n* line 161: \u201cin the sequel\u201d unsure what this means\n* line 172: Similar idea -> A similar idea\n* Supplement 1: Should say proof of 4.1, not 3.1\n* Problem 1: I assume E and E_C are disjoint?\n* Line 191: \u201cby the following\u201d -> \u201cin the following\u201d\n\n* Line 8 of supplement: capitalize \u201cwhen\u201d\n* Line 26 of supplement: L_{W + T} should be L_{W \\cup T}\n* Line 27 of supplement: how did you get the last inequality? No, the potential negative societal impact of their work is not discussed. I would add at least one example of how this could be used negatively (eg manipulation by social networks)",
            "In this paper the authors propose the task of adding links to a social network to minimize notions of disagreement and polarization across the network.  In particular, they define disagreement across neighbors in the graph and polarization by the variance in expressed opinions of the nodes.  The paper then offers two approximate algorithms for greedily adding a fixed number of edges to the graph to minimize the sum of the disagreement and polarization.  They show that their algorithms are both theoretically and empirically tractable, outperform naive baselines, and in some cases close to optimal.\n  Overall the paper does a nice job of clearly providing their problem formulation and demonstrating their ability to provide reasonably fast, approximate algorithms to solve these problems.  \n\nMy primary concern with the paper is with respect to its significance.  While I believe it is theoretically interesting, I worry that it is overly simplified of a model in ignoring any way of incorporating user preferences or a traditional recommender providing probability of the users being interested in connecting, as past work has done.  A discussion of how the research could be incorporated into more practical settings would be valuable.  That said, the theoretical result is still interesting and this may be unnecessary even if valuable.\n\nDetails:\n\nEval - initializing with uniformly random $s$ is particularly unrealistic, and I'd be curious how results are effected by other choices of $s$, e.g. correlation opinions with graph structure as in communities.\n\nNone of the baselines consider the opinions $s$?  They do show the effect of merely making the graph more dense but I'd think connecting individuals or clusters with very different opinions is critical.\n See above and below.\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses appreciation and agreement with the authors, indicating a positive overall sentiment. Phrases like \"I appreciate\" and \"I agree\" contribute to this positive assessment.",
            "The reviewer initially had some concerns but upgraded their score after the rebuttal, indicating satisfaction with the authors' responses. The statement 'I have upgraded my score' explicitly shows a positive shift in sentiment.",
            "The reviewer explicitly states they \"quite like this paper\" and \"would like to see it accepted.\" They are \"satisfied\" with the responses to their questions and their \"accept\" score remains unchanged, indicating a positive overall sentiment.",
            "The reviewer expresses interest in the submodularity result and acknowledges the importance of the faster approximate version for practicality. They also find the work 'very interesting'.",
            "The reviewer states, \"Overall, I liked this paper and would like to see it accepted.\" This clear endorsement indicates a positive sentiment.",
            "The review expresses both positive aspects ('Overall the paper does a nice job of clearly providing their problem formulation') and concerns ('My primary concern with the paper is with respect to its significance'). The overall assessment is balanced, leading to a neutral sentiment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses supportive language, acknowledging the authors' efforts and offering constructive suggestions rather than harsh criticism. The phrase \"I appreciate you incorporating this in your next draft\" is particularly supportive.",
            "The reviewer expresses appreciation for the authors' detailed response ('I thank to the authors for their detailed response') and acknowledges the potential positive impact of the work ('The work could potentially have a positive impact'). Although the reviewer raised some questions, the overall tone is encouraging and supportive.",
            "The reviewer expresses agreement with critiques from other reviewers in a mild manner, while simultaneously emphasizing their positive view of the paper and desire for its acceptance. The phrase \"I still quite like this paper\" is indicative of support, even amidst criticism.",
            "The review expresses both positive feedback ('interesting!', 'important') and constructive criticism ('I am a bit disappointed', 'I am not sure if NeurIPS is the right venue'). This mix suggests a balanced perspective.",
            "The reviewer expresses liking the paper and wanting it accepted, provides constructive criticism, and acknowledges the strengths of the work, indicating a supportive tone. Words like \"good,\" \"interesting,\" and phrases like \"I liked this paper\" contribute to this tone.",
            "The review offers both praise and critique, using phrases like 'nice job' to acknowledge strengths and 'My primary concern' to express reservations. It provides specific details and suggestions, indicating a balanced and constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses appreciation for the authors' responses and provides constructive feedback on baselines and ethical concerns. The feedback is consistent in suggesting improvements to strengthen the paper without contradicting itself.",
            "The review is consistent. The reviewer raises valid questions and concerns about the paper's limitations and potential improvements.  The post-rebuttal indicates a positive change in the reviewer's assessment after the authors' response, leading to an upgraded score. This shows a logical flow of evaluation and no self-contradiction in the reviewer's feedback.",
            "The reviewer expresses satisfaction with the authors' responses and maintains their positive stance on the paper despite acknowledging a weakness pointed out by other reviewers. The reviewer's recommendation for acceptance remains unchanged, indicating a consistent viewpoint.",
            "The review is consistent because the reviewer expresses interest in the work, appreciates both the theoretical contribution (approximation ratio based on submodularity) and the practical aspect (faster approximate version). The minor concerns about the venue and placement of details in supplemental material do not contradict the overall positive assessment.",
            "The review is consistent because the reviewer expresses a positive overall opinion and the detailed comments are minor suggestions for improvement, such as clarity, minor additions, and typos, which align with the positive overall assessment and recommendation for acceptance.",
            "The review is consistent in its critique, focusing on the simplification of the model and its potential impact on practical significance. While the reviewer initially expresses a 'primary concern' about significance, they later soften this by suggesting that discussing practical applications might be unnecessary, which introduces a minor nuance but doesn't fundamentally contradict the main point about the model's simplification and its implications for real-world relevance. The detailed points further support the concern about the model's realism."
        ]
    },
    {
        "paper_id": "iclr_2020_rJg851rYwH",
        "paper_title": "Making the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy",
        "paper_abstract": "Because learning sometimes involves sensitive data, standard machine-learning algorithms have been extended to offer strong privacy guarantees for training data. However, in practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the same model architecture that performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. Instead, we propose that model architectures and initializations are chosen and hyperparameter tuning is performed, ab initio, explicitly for privacy-preserving training. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the fundamental learning procedures or differential-privacy analysis.",
        "review_ids": [
            "Skx5Yj8pcr",
            "SJxqstEyqB",
            "B1gcmtH15B"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper presents experimental evidence that learning with privacy requires approaches that are not identical to those used when learning without privacy. These approaches include re-considering different model choices (i.e., its structure and activation functions), its initialization, and its optimization procedure. With these changes, they show that it is possible to obtain state-of-the-art results for some canonical learning tasks.\n\nStrengths:\nThis paper questions nearly every component in the training pipeline, including choices about the model structure, initialization strategies, and optimization procedures. For each component, they show that judiciously choosing the components (which go against the standard choices in the non-private learning setting) enables training higher-utility models than in previous works without sacrificing privacy. Moreover, in addition to the experimental evidence alone, most of the components considered in the paper were accompanied by reasonable justification/hypotheses for why the choices enable such improvements.\nThis paper helps push differentially private learning to a more practically-useful realm. First, the suggested changes here are easy for a practitioner to understand and easy to implement. With only these simple changes, the concrete results then show that it is possible to achieve utility close to the analogous non-private model while still maintaining reasonable utility (\\epsilon less than 3 with \\delta of 10^-5).\n\n\nWeaknesses:\nMy major concern with this paper lies in the experimental methodology. Specifically, most experiments are based on varying a single component while leaving all other components the same. While this is certainly the scientifically-valid way to demonstrate the component\u2019s influence on the entire system given the other fixed components, it doesn\u2019t convincingly demonstrate that the component has this influence across all (or at least most) reasonable configurations of the other components.\nThis can be made concrete using many experiments in the paper, but let\u2019s take the activation functions experiment of 3.2 as an example. Here, it is shown that after fixing the privacy guarantee, model structure, training procedure, and hyperparameters -- the tanh activation performs better than the ReLU activation. However, suppose instead that we fix all of these components except the hyperparameters; it may then be the case that the ReLU activation is capable of outperforming the tanh activation when its hyperparameters are chosen carefully. In other words, to validly compare the two activations and reach a convincing conclusion, they should be compared against each other in their own individually-best settings (e.g., the results induced by the optimal hyperparameters for ReLU versus the results induced by the optimal hyperparameters for tanh).\nThis is similar to the problem addressed in Avent et al.\u2019s \u201cAutomatic Discovery of Privacy\u2013Utility Pareto Fronts\u201d paper (https://arxiv.org/abs/1905.10862). \nThe specific technical details on some experiments were either difficult to find or were lacking. Given that this is fundamentally an experimental paper, having these details clearly listed somewhere for reference is important, even if relegated to an Appendix. Although this applies more broadly to most of the experiments, we can use Section 3 as an example again: the details on the experiment in 3.1 were found in the caption of Figure 1, whereas I would have expected them either in the main body or clearly listed in their own table; the details on the experiment in 3.2 specify that everything is identical between the tests of the two activation functions, however it is never specified exactly what is being altered (and by how much) to vary the \\epsilon value.\n4.2 Initialization by Weight Scaling proposes that judiciously scaling initial weights can improve model privacy/utility. This scaling is done by \u201ctransfer from one differentially private model to another\u201d, where \u201cDP-SGD can be applied to train a model with high utility, but less than ideal privacy\u201d and then extracting the relevant information from there in order to initialize a new differentially private model that will be trained with strong privacy guarantees. It is claimed that \u201cthis extraction can be done in a differentially-private manner, e.g., as in Papernot et al. (2018), although the privacy\nrisk of summary statistics that drive random initialization should be vanishing\u201d. It is unclear to me how this extraction of summary statistics should be done in such a way that doesn\u2019t consume a significant portion of the privacy budget. If there is such a way, it should be clearly stated and its effect on the privacy budget should be explicitly incorporated into this paper\u2019s results.\nMinor: The statement that \u201cSuch accuracy loss may sometimes be inevitable\u201d on page 1 should include a reference; e.g., Feldman\u2019s \u201cDoes Learning Require Memorization? A Short Tale about a Long Tail\u201d paper (https://arxiv.org/abs/1906.05271).\n\n\nOverall, this work provides good practical guidance to practitioners and researchers who wish to do differentially private machine learning. However, given the lack of theoretical novelty, the experimental methodology needs to be improved in order to significantly strengthen the results (assuming they continue to hold).\n\n\n----------------------------------------------------\n\nUpdate: Due to the authors' writing clarifications and experimental additions, in conjunction with the concrete and realistically-applicable insights from the paper, I've modified my rating to a Weak Accept.",
            "The paper methodically analyses the settings and choices used when training neural networks (specifically CNNs) via the DP-SGD algorithm and suggests changes to the standard procedures that empirically lead to higher accuracies despite the added noise. The main statement of the paper is quite simple: optimize hyperparameters for the model that you're training (DP-SGD) rather than the model it is inspired by. Yet, the findings an recommendations may be useful for practitioners.\n\nNevertheless, to be more practically relevant the paper needs some modifications:\n\nThe example models used in demonstrations are quite small (3 hidden layers 26,000 parameters, when, for example, a standard segmentation CNN model U-net can typically have 26,000,000, AlexNet has about 60,000,000 and so on). The results would be much more convincing if these or other models widely used in practice were used as running examples.\n\nIn Figure 1 the multitude of point on the plot makes it unclear whether they represent result variability per number of filters or simply reflect variability as the number of filters grows. If it is the latter, it seems appropriate to perform a cross validation analysis and report standard deviations. Especially in the MNIST plot the values for SGD and DP-SGD are so close that they may in fact be statistically indistinguishable. Hard to tell by looking at a point estimate. The same request holds for Figure 2, where the difference may be immaterial, but as the figure currently stands it is unclear.\n\nSection 3.2 reports some numbers for test accuracy but the uncertainty of these numbers with respect to the test set changes (cross validation) is not reported and the numbers are quite close to each other. Furthermore, the dataset is not described and it is unclear what was the size of the training and the test sets.\n\nConclusions relative Adam vs SGD seem to repeat what's already known or been discussed about these methods outside of the DP topic. May be worth highlighting that when one knows how to set learning rates for SGD (may be via learning rate scheduler, not discussed in the paper but practically relevant) then SGD may be as good or slightly better than Adam. However note, adaptive optimizers are often preferred for their ease of use as no tweaking and searching for an optimal learning rate is required. Would not this problem be detrimental for SGD optimization affecting the privacy budget?\n\nPlease add wall-clock time column to Table 5 to support the statement about 4 times gain.\n\nI think it's more accurate to change \" This confirms that earlier theoreticalanalysis (Talwar et al., 2014) also holds in the non-convex setting.\" to \"This suggests that earlier theoreticalanalysis (Talwar et al., 2014) also holds in the non-convex setting.\"\n\n",
            "Overall, this work empirically evaluates different techniques used in privacy learning and suggest useful methods to stabilize or improve performance.\n\nDetail comments:\n\nStrength:\nDespite the progress of privacy-preserving learning in theory, there are few works providing learning details for better training. Especially, considering the instability in perturbation-based private algorithms, e.g., most DP ones, the work could be valuable in the sense of practice.\n\nWeakness:\nAs far as empirical research, the compared techniques are too few. What if we use those less popular techniques, for example, RMSprop optimization method?\n\nThe model capacity of neural networks, especially deep networks, has some non-trivial relation to the number of filters or the number parameters. It is important to quantify such relation. A good reference might be [A]. Briefly, the generalization performance may not be monotonic against the number of parameters.\n\nThe baselines are not enough. Of course, Abadi et al.\u2019s work is outstanding in handling the privacy learning of deep networks. It has been further developed by the following researchers. For example, [B] and [C]. Does the conclusion still hold for these algorithms?\n\n[A] Neyshabur, B., Bhojanapalli, S., Mcallester, D., & Srebro, N. (2017). Exploring Generalization in Deep Learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Processing Systems 30 (pp. 5947\u20135956). \n[B] Yu, L., Liu, L., Pu, C., Gursoy, M. E., & Truex, S. (2019). Differentially Private Model Publishing for Deep Learning. Proceedings of 40th IEEE Symposium on Security and Privacy. \n[C] Phan, N., Vu, M. N., Liu, Y., Jin, R., Dou, D., Wu, X., & Thai, M. T. (2019). Heterogeneous Gaussian Mechanism: Preserving Differential Privacy in Deep Learning with Provable Robustness. Proceedings of the Twenty-Eighth International Joint Conference on Artificial "
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the work provides good practical guidance and insights, and ultimately modifies their rating to a 'Weak Accept' after clarifications and additions by the authors. Although there are criticisms regarding the experimental methodology, the overall sentiment is positive.",
            "The review identifies several areas for improvement, including model size, result clarity, missing statistical analysis, dataset descriptions, and potential inaccuracies in conclusions. The language used is critical, pointing out weaknesses and suggesting modifications.",
            "The review, while acknowledging the work's potential value, focuses heavily on its weaknesses, such as the limited number of compared techniques and baselines. The reviewer also points out the importance of quantifying the relation between model capacity and performance, suggesting a significant omission in the current work. These criticisms outweigh the initial positive sentiment."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both strengths and weaknesses of the paper. It uses formal language and provides specific examples to support its claims, resulting in a balanced assessment. The reviewer also offers constructive suggestions for improvement.",
            "The tone is critical, evident in phrases like \"to be more practically relevant the paper needs some modifications,\" \"the results would be much more convincing if...\", \"it is unclear whether...\", \"the difference may be immaterial,\" and \"seem to repeat what's already known.\"",
            "The review employs a critical tone by directly pointing out shortcomings such as \"the compared techniques are too few\" and \"the baselines are not enough.\" The use of \"What if\" questions and phrases like \"It is important to quantify such relation\" further emphasizes the critical assessment of the work's limitations."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents a balanced view by highlighting both strengths and weaknesses of the paper. The overall assessment and the update are logically consistent with the identified strengths and weaknesses. The update reflects a positive change in the reviewer's opinion based on the authors' actions, which is a reasonable progression.",
            "The review is consistent because it starts with acknowledging the paper's contribution and then provides specific, constructive criticisms aimed at improving the paper's practical relevance, clarity, and rigor. All suggestions are logically connected and focused on enhancing the paper's quality without any self-contradictory statements.",
            "The weaknesses pointed out by the reviewer are constructive suggestions for improvement and do not contradict the overall positive assessment of the work's practical value and relevance in addressing the instability of DP algorithms. The reviewer acknowledges the potential strength of the work while suggesting ways to make the empirical evaluation more comprehensive and robust."
        ]
    },
    {
        "paper_id": "iclr_2022_CLpxpXqqBV",
        "paper_title": "Learning State Representations via Retracing in Reinforcement Learning",
        "paper_abstract": "We propose learning via retracing, a novel self-supervised approach for learning the state representation (and the associated dynamics model) for reinforcement learning tasks. In addition to the predictive (reconstruction) supervision in the forward direction, we propose to include \"retraced\" transitions for representation/model learning, by enforcing the cycle-consistency constraint between the original and retraced states, hence improve upon the sample efficiency of learning. Moreover, learning via retracing explicitly propagates information about future transitions backward for inferring previous states, thus facilitates stronger representation learning for the downstream reinforcement learning tasks. We introduce Cycle-Consistency World Model (CCWM), a concrete model-based instantiation of learning via retracing. Additionally we propose a novel adaptive \"truncation\" mechanism for counteracting the negative impacts brought by \"irreversible\" transitions such that learning via retracing can be maximally effective. Through extensive empirical studies on visual-based continuous control benchmarks, we demonstrate that CCWM achieves state-of-the-art performance in terms of sample efficiency and asymptotic performance, whilst exhibiting behaviours that are indicative of stronger representation learning. ",
        "review_ids": [
            "k4_RFcesn2-",
            "r1HtDQ7TGtb",
            "bmzWD6qNhhzt",
            "KNWF1j1nc0P",
            "_OHOFELM_As",
            "zZPE4kbGMdz"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes \"learning via retracing\" as an approach to learn state representations, through matching a trajectory both in the forward and in the backward direction. This paper then introduces Cycle-Consistency World Model (CCWM) which is a model-based RL algorithm which learns through retracing. This method is sample efficient and provides good state representations for predicting the future and generalization. Furthermore, it proposes a value-based approach to identify \"irreversible\" transitions.\n Pros:\n\n- Interesting technique to retrieve more supervisory signals from the data in RL\n- Mathematically sound approach, with clear figure 2 to visually understand.\n- Extensive experiments to evaluate the idea and interesting ablations and investigations described.\n- Extensive details of algorithm and implementation + code is available.\n \n##########################################################################\n\nCons:\n\n- A few inconsistencies in the paper that make me think that there were extra parts that have been cut out to reach the page limit without reformatting:\n    - Section 5.3: Details of modifications to the environment are said to be found in Appendix A, but Appendix A describes implementation details and not the environment.\n    - Appendix C, last sentence. Model-free instantiation is shown in Figure ?? Figure doesn't exist.\n\n##########################################################################\n\nQuestions during rebuttal period:\n\n- Address/clarify cons above.\n- How do you select the predefined threshold to find the sudden changes in the value function for your adaptive scheduling?\n- I can imagine from Table 1, that the components that you change in the cheetah-run environment are related to Reward, Mass, Friction, Stiffness. I don't understand how changing the reward would affect evaluating an episode, as it doesn't change the dynamic behavior of the physics simulator and so the actual trained model shouldn't have any problem in evaluating.\n- In Section 5.4, you write that the curve for hopper-stand in Figure 3b for CCWM lacks the adaptive truncation of irreversible states. Does this mean that all plots in Figure 3b for CCWM are trained on all transitions, with no truncation? Please clarify, as I imagined that the experimental evaluation of your method included both contributions (CCWM+adaptive truncation).\n- In algorithm 1: I don't understand in the input, why in ${O^n_{t_n:t_n+K}, a^n_{t_n:t_n+K}}$ the $t$ has a subscript $n$, which should represent the n-th sample in the batch. Also, I think there is a typo and the subscript should be to $t+T$ and not $t+K$.\n- In figure 8 (from Appendix F), you explain how CCWM yields less accurate predictions for the irrecoverable states, but to me the first row (true trajectory for Walker), doesn't look like an irrecoverable trajectory.\n\n##########################################################################\n\nTypos/formatting:\n- Section 3.3, first paragraph: \"Here we propose an approaches\" -> \"Here we propose an approach\"\n- None of the citations in Appendix A are in brackets anymore.\n- Appendix A: \"outputting the means the Gaussian distributions\" -> \"outputting the means of the Gaussian distributions\"\n- Appendix D, second paragraph: \"empirical evaluation\" -> \"empirically evaluate\" Overall, I am for accepting. I like the idea and I think it's a novel approach for state representations in RL. My major concern is about the clarity of the paper, as written above. Hopefully the authors can address my concern in the rebuttal period.",
            " Thanks for your reply. I updated my score accordingly.",
            " Dear authors, \nThanks for your detailed reply. The two main concerns that I have described earlier and that remain are the following:\n- The prediction of the irreversibility of a transition is still not very clear, despite your general answer about it and the individual answers (as the issue was also pointed out by Reviewer HLp2 and also to some extend by Reviewer dZvG). When considering your example from Figure 1, all the states where the agent is anyway falling are \"irreversible\" in the sense that there is no action that allows coming back to the previous state, yet they have a quite similar value function. How does the algorithm handles such transitions?\n- The motivation for the cycle consistency is described as \"improving representation learning\", however there is nothing that can directly back this up (as compared for instance to other model-based learning losses?), except the improved generalization that you show on some benchmarks. I do not see any theoretical justification or specific visualizations/experiments that directly support the claim about the improved representation learning.",
            "The paper investigates a self-supervised approach for learning the state representation in RL tasks. In addition to the predictive (reconstruction) supervision in the forward direction, the authors include a \u201cretraced\u201d transitions for representation/model learning, by enforcing the cycle-consistency constraint between the original and retraced states. The authors claim that this facilitates stronger representation learning and improve upon the sample ef\ufb01ciency of learning. As it is not always possible to find such a cycle consistency between two states and for counteracting the negative impacts brought by the \u201cirreversible\u201d transitions, the authors add a novel adaptive \u201ctruncation\u201d mechanism. - The motivation for the cycle consistency is not really discussed. Why would enforcing cycle consistency improve representation learning?\n- In equation 7: can you explain the choice of the KL distance between two rewards? How is the 2-wassertstein distance estimated from data?\n- The \"model-free instantiation of learning via retracing\" (appendix C) is not clear. In particular, the paper states that there is a graphical illustration \"shown in Figure\" but there is no reference to any figure.\n- In Figure 3, can you clarify why some of the baselines are straight lines? Can you provide more details than this explanation: 'We report the asymptotic scores for the model-free algorithms due to the large gap in sample ef\ufb01ciency comparing to the model-based methods.\"\n- Section 3.3: The prediction of the irreversibility of a transition is not very clear. How is a \"sudden change in the value function\" a good predictor of the irreversibility of the transition? \n- In the conclusion, it is written \"We empirically show that CCWM yields improved performance over state-of-the-art model-based and model-free methods on a number of challenging continuous control benchmarks, in terms of both the sample ef\ufb01ciency and the asymptotic performance.\" However, CCWM is only compared to dreamer and yields to comparable results in most cases? (the other baselines are only given as \"asymptotic scores\" and they have a direct access to the state?)\n\nMinor comments:\n- The paper formalizes an observation space, O that is \"high-dimensional, due to either redundant information or simply because only visual inputs are available\". Can you clarify whether that is the POMDP setting? If it is not the POMDP setting, it might be worth clarifying as this is the usual denomination used in POMDP.\n- The paper is based on the open source implementation of Hafner et al. Is there an open source implementation of the modifications presented in this paper? The motivation for the contribution is not very clear and there are a few unclear elements in the formalization (e.g. sudden change in the value function for the prediction of the irreversibility) .",
            "This paper considers state representation learning problem in deep RL. It exploits the cycle-consistency supervision and develops a \u201clearning via retracing\u201d approach. Such supervision signals are generally inherent in existing data and does not need additional interaction with environment, which leads to better sample efficiency. Learning from predictive supervision from temporally forward and backward directions reveals information from both the future and past to the target state, leading to more accurate latent state inference. In particular, the paper proposes the Cycle-Consistency World Model (CCWM) along with practical considerations (e.g., adaptive truncation to remove irreversible states), under the model-based framework based on generative dynamics modeling (CCWM). Strength: The cycle-consistency constraints could provide extra supervision signal (beyond forward predictive loss) for learning state representations and transition models, without additional interaction with environment. It also improves the representation learning performance (including the zero-shot transfer capability and long-range predictions) by obtaining better latent state inference. \n\nWeakness: The concept of cycle-consistency constraints have also been considered in the PlayVirtual work under model-free setting. Therefore, more thorough discussion about the novel contribution relative to PlayVirtual should be included.\n\nComments:\n- The authors may need to discuss how frequent such cycle-consistency constraints appear in practical applications. For example, it is more common in navigation-type applications? Listing more practical application settings in this regard and discuss them would help justify how widely applicable the proposed approach is.\n- Why the continuity detection on action-value function could detect the irreversible states? It seems that there is not much discussion on this point in Section 3.3. It is crucial to clarify it.\n- VirtualPlay should also be included as a baseline in the experiment section in order to show which approach best exploits cycle-consistency.\n The paper proposes CCWM, a learning via retracing method, based on cycle-consistency constraint. It leads to better sample efficiency and final performance by learning better state representations. Need more clarification about its novel contribution relative to a recent work (VirtualPlay), which considers cycle-consistency learning in model-free RL setting.",
            "This paper proposes a self-supervised approach for learning the state representation of RL tasks. The main contribution apart from prior works is to involve additional 'retracing' trajectory samples in representation training, which are trained by minimizing the propensity between retraced samples and forward posterior samples. They also proposed an intuitive way to mitigate the irreversibility in RL dynamics. The strengths of the paper involve solid experiments and well-structured writing. However, I think some details are not illustrated clearly, so that I have the following questions:\n1. Is the representation learning part jointly trained with RL algorithms or pretrained? \n2. In the $L_{retrace}$, why retracing samples $\\check{z}$ are from the variational predictive distribution but forward samples $\\tilde{z}$ are from the variational posterior distribution? \n3. How the \u201creversed\u201d action policy $p_{\\eta} $ is learned? by $L_{retrace}$?\n4. The proposed method for dealing with \u201cirreversibility\u201d is too intuitive. I think it might fail in heavily irreversible environments. Why the proposed truncation method is not involved in your full algorithm in the appendix?\n5.  As you mentioned CCWM could be combined with any existing RL algorithms, why not try one or two to show that the new representation learning loss could actually improve the performance. I think the performance of CCWM is comparable with Dreamer given the huge additional computational complexity. \n The proposed representation learning method for RL is interesting, which involves addtional retracing samples for training. However, more explanations are needed to make it clear. I expect to see more elaborations.  "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer states \"Overall, I am for accepting. I like the idea and I think it's a novel approach for state representations in RL.\" This clearly indicates a positive sentiment.",
            "The reviewer explicitly states they updated their score positively ('updated my score accordingly') after a reply, indicating satisfaction.",
            "The review expresses concerns about the clarity of the prediction of irreversibility and the lack of direct evidence for improved representation learning, indicating a negative sentiment.",
            "The review raises several concerns about the paper's motivation, clarity, and experimental validation. It points out a lack of discussion for the cycle consistency, unclear explanations of equations and algorithms, missing figure references, and questionable justifications for design choices. The reviewer also questions the validity of the empirical results and asks for clarification on various aspects of the paper.",
            "The review highlights several strengths of the paper, such as improved sample efficiency and performance due to better state representations and the use of cycle-consistency constraints. While it also mentions weaknesses and areas for improvement, the overall tone suggests a positive assessment of the work's potential.",
            "The review acknowledges the paper's strengths (solid experiments, well-structured writing) and interesting approach but also points out areas needing clarification and raises concerns about the method's limitations and computational complexity. The overall sentiment is balanced, not strongly positive or negative."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive aspects ('Pros:') and negative aspects ('Cons:'), along with specific questions and points for clarification. The reviewer also provides suggestions for improvement ('Typos/formatting:') and concludes with a positive recommendation, indicating a balanced perspective.",
            "The reviewer's statement expresses agreement and willingness to adjust their assessment based on the author's response, which conveys a supportive attitude.",
            "The reviewer uses phrases like \"still not very clear\", \"nothing that can directly back this up\", and \"I do not see any theoretical justification\" which convey a critical tone. The reviewer is pointing out flaws and demanding more support for the authors' claims.",
            "The review employs a critical tone by directly questioning the paper's claims, asking for clarifications, and pointing out inconsistencies and weaknesses in the methodology and experimental results. Phrases like \"not really discussed\", \"is not clear\", \"can you clarify\", and questioning the empirical validation show a critical stance.",
            "The review presents both strengths and weaknesses of the paper. It uses phrases like \"Strength:\" and \"Weakness:\" to clearly delineate positive and negative aspects. It also offers specific constructive criticism and suggestions for improvement, indicating a balanced and objective evaluation.",
            "The review poses several direct questions indicating areas where the reviewer finds the paper unclear or lacking detail. Phrases like \"some details are not illustrated clearly,\" \"too intuitive,\" \"might fail,\" and \"why not try\" suggest a critical evaluation of the paper's methodology and presentation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it identifies both strengths and weaknesses of the paper. Despite pointing out inconsistencies and clarity issues in the paper itself, the reviewer appreciates the novelty and potential of the proposed approach and recommends acceptance. The different sections of the review (Pros, Cons, Questions, Typos) are standard components of a peer review and contribute to a comprehensive assessment without contradicting each other.",
            "The review expresses a logical action of updating the score based on a reply, indicating a consistent evaluation process.",
            "The review is consistent because the reviewer clearly expresses their remaining concerns and provides justifications for them based on the authors' response and the reviewer's understanding of the paper. The reviewer focuses on two specific points and logically explains why these points are still concerns.",
            "The review is consistent in its critique, focusing on the lack of clarity in motivation, methodology, and experimental details. The reviewer consistently asks for more explanation, justification, and clarification throughout the review, without contradicting themselves.",
            "The review is consistent because it consistently points out the lack of novelty compared to PlayVirtual as the main weakness, and all comments are related to this point, asking for more discussion, clarification, and experimental comparison with VirtualPlay.",
            "The review is consistent because it acknowledges the strengths of the paper, such as solid experiments and well-structured writing, while also pointing out areas for improvement and asking clarifying questions. The reviewer's concerns and questions are all aimed at improving the clarity and completeness of the paper, rather than contradicting their initial positive observations."
        ]
    },
    {
        "paper_id": "iclr_2020_HyeJmlrFvH",
        "paper_title": "Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization",
        "paper_abstract": "As the size and complexity of models and datasets grow, so does the need for communication-efficient variants of stochastic gradient descent that can be deployed on clusters to perform model fitting in parallel. Alistarh et al. (2017) describe two variants of data-parallel SGD that quantize and encode gradients to lessen communication costs. For the first variant, QSGD, they provide strong theoretical guarantees. For the second variant, which we call QSGDinf, they demonstrate impressive empirical gains for distributed training of large neural networks. Building on their work, we propose an alternative scheme for quantizing gradients and show that it yields stronger theoretical guarantees than exist for QSGD while matching the empirical performance of QSGDinf.",
        "review_ids": [
            "SJxLkbwatH",
            "r1xsybtTFB",
            "Bkxbah715H"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "In this paper, the authors propose a new gradient compression method, which is called nonuniform quantization. The algorithm is a reasonable variant of SGD with uniform quantization. The paper is well written. The experiments show good performance.\n\nHowever, there are several weakness in this paper:\n\n1. In this paper, a very important reference and baseline is missing, which is call error-feedback SGD [1]. Although the title of [1] focuses on SignSGD, it provides a general algorithm for arbitrary compressor with a error/variance bound similar to Theorem 2 in this paper, no matter the compressor is unbiased or not. Since [1] provides the SOTA results for quantized SGD, the proposed algorithm should be compared to it in the experiments.\n\n2. This paper claims to have strong theoretical guarantees. However, the theoretical analysis only works for convex functions. Note that the theoretical analysis in [1] also works for non-convex functions.\n\n3. Regardless of the convergence guarantees (which is weak considering the existing theorems in [1]). the proposed algorithm, NUQSGD, does not show improvement on the convergence, compared to the baseline QSGDinf.\n\n4. In Figure 3, the experiments only show loss vs. # of iterations, which does not show the actual training time. In Figure 4, training time is only shown for NUQSGD, which ignores the other baselines including QSGD and QSGDinf. What I really what to see is training loss (or testing accuracy) vs. training time (or communication overhead, such as number of bits), so that we can evaluate the trade-off between communication overhead and the convergence, compared to the baselines.\n\n\n\nMinor issue (I hope the authors can consider the following suggestions in a revised version. However, since the issue is minor, it doesn't affect the score):\n\n!. In Definition 1, in some cases $s$ is c constant integer, and in some other case $s$ become a function, which is very confusing and not friendly to the readers. I also hope the authors can highlight the definition of $r$ and $p$, which are essential for understanding the nonuniform quantization mechanism. \n\n\n\n\n--------------\nReference\n\n[1] Karimireddy, Sai Praneeth et al. \u201cError Feedback Fixes SignSGD and other Gradient Compression Schemes.\u201d ICML (2019).",
            "The authors propose a new scheme for quantizing gradients which are followed by the previous work QSGD [1]. They show that it yields stronger theoretical guarantees than QSGD while showing a great empirical performance. \nThe main difference between their scheme NUQSGD and QSGD is that they use nonuniform quantization (0, 1/2^{s},  \u2026., 2^{s-1}/2^{s}, 1) instead of uniform quantization (0, 1/s, \u2026, (s-1)/s,1).  Intuitively, by the way, it could reduce quantization error and variance by better matching the properties of normalized vectors.\nThe results are in 2 parts. First comparing with QSGD, they establish stronger convergence guarantees for NUQSGD, under standard assumptions. They also establish theoretical results for the variance upper bound and expected communication cost of their scheme. Second, they show strong empirical performance on deep models and a large dataset, with an efficient implementation in PyTorch.\n\nHowever, there are several issues and questions that if fixed or illustrated could be a great paper.\n\n\t1) The author claim NUQSGD achieves stronger convergence guarantees comparing with QSGD but hasn't illustrated the point in detail. On page 6, the paragraph named 'NUQSGD vs QSGD' mainly claims that variance upper bound controls the guarantee on the convergence speed by empirically showing the results of variance upper bound. It would be great to include more theoretical analysis which demonstrates the importance of variance upper bound for convergence speed guarantee.\n\t2) In the experimental part, they control the hyperparameters including batch-size, base learning rate, momentum, and weight decay to be identical with each method. This may cause tuning biases (the setting may favor one method but hurt others' performance).\n\t3) Although the paper mainly focuses on comparing with QSGD, there are several relative communication efficient training algorithms which I think are worth to compare empirically (at least one of them). For example:\n\t\ta. Deep Gradient compression [2]\n\t\tb. signSGD [3]\n\t\tc. TernGrad [4]\n\t4) In figure 4, the encoding cost is significantly increased from 4-bit to 8-bit NUQSGD. Any reason why it happens? Is it due to inefficient encoding implementation? \n\nI agree with the authors' point that it's worth to explore the interaction between NUQSGD with more complex reduction patterns like ring-based. Since the ring-based algorithm like all-reduce is more popular in practice nowadays, interacting with it would have a better practical meaning. \n\n[1] D. Alistarh, D. Grubic, J. Z. Li, R. Tomioka, and M. Vojnovic. QSGD: Communication-ef\ufb01cient SGD via gradient quantization and encoding. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017.\n\n[2] Lin Y, Han S, Mao H, Wang Y, Dally WJ. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887. 2017 Dec 5.\n\n[3] Bernstein J, Zhao J, Azizzadenesheli K, Anandkumar A. signSGD with majority vote is communication efficient and fault-tolerant. arXiv. 2018 Oct 11.\n\n[4] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. TernGrad: Ternary gradients to reduce communication in distributed deep learning. In Proc. Advances in Neural Information Processing Systems (NIPS), 2017.",
            "Brief summary of the paper:\nThis paper studies data-parallel SGD that K processors work together to minimize an objective function. Each processor computes a stochastic gradient and broadcasts to other peers. In this distributed system, there is a trade-off between the *communication cost* from sharing the stochastic gradient and the *variance* from gradient quantization. This paper is a follow-up of Alistarh et al.\u00a0(2017). It proposes a non-uniform (logarithmic) quantization scheme (NUQSGD). This paper provides theoretical analysis of the variance and communication cost of NUQSGD. Then the paper analyzes the convergence rate of NUQSGD for convex and smooth objective function. At the end, this paper empirically evaluates NUQSGD for image classification problem. \n\n\nOriginality and significance:\nThis paper follows up on the parallel SGD framework proposed by Alistarh et al.\u00a0(2017), where the authors proposed QSGD using a uniform quantization. This paper proposes NUQSGD using a non-uniform quantization method. The quantization of the stochastic gradient amplifies the stochastic variance, which influences the rate of convergence of SGD. Thus, on one hand, it is important to design a quantization method to improve the variance, for the sake of convergence rate. On the other hand, it is also important to decrease the communication cost. NUQSGD does not provide significant improvements in terms of the variance and communication cost. \n\nTheorem 2 and Theorem 3: QSGD has a variance of min {d/s^2, \\sqrt{d}/s} and NUQSGD has a variance of min{O(d/2^{-2s}), O(\\sqrt{d/2^{-2s}})}. QSGD has communication cost of \\tilde O(s(s+\\sqrt{d})) and NUQSGD has communication cost of \\tilde O(2^{2s}\\sqrt{d} ). Compared to QSGD, we can see that NUQSGD improves the dependence on s for the variance term, but it has a worse (exponential) dependence on s for the communication cost. Usually s is a small number and it serves as a hyper-parameter to be tuned. We would expect NUQSGD to improve the dependence on the dimension d, which is more significant. However, NUQSGD has the same dependence on d as QSGD in terms of both variance and communication cost. \n\nExperiments: Figure 3 compares NUQSGD with other parallel SGD algorithms and vanilla SGD. Figure 3 shows how fast the training loss decreases with respect to iterations. It would be great to add learning curves with the \u2018time\u2019 being the x-axis as well. Also, I would suggest the authors to record the time needed to proceed one iteration for each parallel algorithm to compare the communication cost. \n\nQuality and clarity:\nThis paper is well-written. \n\n\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review identifies several weaknesses in the paper, including a missing important baseline, weak theoretical guarantees, lack of improvement in convergence, and inadequate experimental comparisons. The reviewer also points out confusing notations and lack of clarity in definitions.",
            "While the review acknowledges the paper's strengths (stronger theoretical guarantees, good empirical performance), it raises several significant issues and questions that need to be addressed, suggesting a negative overall assessment. Phrases like \"there are several issues and questions that if fixed or illustrated could be a great paper\" and specific criticisms (lack of detailed illustration, potential tuning biases, missing comparisons, unexplained encoding cost increase) contribute to this negative sentiment.",
            "The review provides a balanced assessment of the paper, highlighting both its strengths (well-written) and weaknesses (limited improvement over existing methods, need for additional experiments)."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'several weakness,' 'very important reference is missing,' 'theoretical analysis only works for convex functions,' 'does not show improvement,' and 'very confusing and not friendly to the readers,' which indicate a critical tone. The reviewer also directly questions the claims made in the paper.",
            "The tone is critical due to the reviewer's direct identification of weaknesses in the paper. Specific points are challenged with questions and suggestions for improvement: \"The author claim NUQSGD achieves stronger convergence guarantees comparing with QSGD but hasn't illustrated the point in detail\", \"This may cause tuning biases\", \"Any reason why it happens? Is it due to inefficient encoding implementation?\" These direct questions and criticisms indicate a critical evaluation of the work.",
            "The review uses objective language and avoids overly positive or negative statements. It offers constructive criticism and suggestions for improvement, indicating a balanced perspective. For example, it states \"NUQSGD does not provide significant improvements\" but also acknowledges the paper is \"well-written\"."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review starts with a few positive remarks about the writing and initial impression of the algorithm. However, it quickly transitions to point out several significant weaknesses related to missing baselines, limited theoretical analysis, lack of performance improvement compared to baselines, and inadequate experimental evaluation. The reviewer consistently argues that these weaknesses are substantial and need to be addressed. The minor issue section further reinforces the constructive criticism without contradicting the main points.",
            "The review is consistent because it first summarizes the paper's contributions and then provides constructive criticism by pointing out specific weaknesses and suggesting improvements. The reviewer acknowledges the strengths of the paper while also highlighting areas that need further attention, which is a typical and consistent approach in peer reviews.",
            "The reviewer consistently points out that while the paper is a follow-up work and well-written, the proposed NUQSGD method does not offer significant improvements over QSGD in terms of variance and communication cost, especially concerning the dependence on dimension 'd'. This critical assessment is maintained throughout the review, from the originality and significance section to the analysis of theorems."
        ]
    },
    {
        "paper_id": "nips_2021_5JPPOluv-bp",
        "paper_title": "Asymptotics of the Bootstrap via Stability with Applications to Inference with Model Selection",
        "paper_abstract": "One of the most commonly used methods for forming confidence intervals is the empirical bootstrap, which is especially expedient when the limiting distribution of the estimator is unknown. However, despite its ubiquitous role in machine learning, its theoretical properties are still not well understood. Recent developments in probability have provided new tools to study the bootstrap method. However, they have been applied only to specific applications and contexts, and it is unclear whether these techniques are applicable to the understanding of the consistency of the bootstrap in machine learning pipelines. In this paper, we derive general stability conditions under which the empirical bootstrap estimator is consistent and quantify the speed of convergence. Moreover, we propose alternative ways to use the bootstrap method to build confidence intervals with coverage guarantees. Finally, we illustrate the generality and tightness of our results by examples of interest for machine learning including for two-sample kernel tests after kernel selection and the empirical risk of stacked estimators.\n",
        "review_ids": [
            "NtNCEQWMbKG",
            "I4MlIhkjRD2",
            "lRpLLIjPiP",
            "z4OEKudf5CP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the responses.\n\nIn terms of Assumption 1: my question is if Assumption 1 is necessary and if not could you provide a specific example where Assumption 1 is not needed for bootstrap consistency.\n\nIn terms of the usefulness of the master theorem: it seems that recovering only one result is somewhat limited. But I am sympathetic that the paper is already quite long and dense. Maybe it will be helpful if the readers could point out several other theorems that their results might be useful to prove in a discussion section.\n\nIn terms of Chatterjee's extension: I'm ok with your response but it also seems not a \"big\" extension per se from his result.\n\nUnfortunately, my original evaluation remains in light of the technicality and the applications that the authors show in the current draft.",
            "This paper was inspired by Chatterjee 2006's work on Lindeberg's condition. They established a new set of sufficient conditions under which some nice nonlinear statistics can be approximated using a variety of bootstrap resampling techniques. Several applications in machine learning are provided. This is a theoretically dense work that complements recent results in bootstrap and Gaussian approximations in high-dimensional statistics.  Overall I like the theoretical results the authors obtained. But the write-up and presentation can be further polished and improved. For example, there are still quite a few typos or grammatical errors in the submission. The technical proof is quite dense, which is evident from the length of the supplementary materials. A guide for readers on how to read the supplementary materials is necessary for this paper to have more impact if they hope machine learning researchers could use their tools to perform honest statistical inference.\n\nIn terms of technical content, the authors propose a condition called $\\mathbb{C}^{3}$-approximability and use a surrogate $\\mathbb{C}^{3}$-function sequence as an intermediate step to show bootstrap consistency. This is an important assumption but the authors delay the discussion of this assumption to Appendix C. I understand due to NeurIPS's page limit there has to be a trade-off on what to be included in the main text. But I am worried that the supplementary materials will eventually be buried under most readers' desk drawers. I also hope that the authors can provide one example (if they could) that violates Assumption 1 yet the bootstrap consistency still holds.\n\nThe authors advocate that the strength of their paper provides tools to show bootstrap consistency for general nonlinear functionals beyond what people have done case by case. But it would be useful if the authors could recover several established results as special cases of their master theorem.\n\nIn addition, the authors admit that their technical results are inspired by Chatterjee's work. Then what technical challenge do they need to solve? Are the proofs, even though very long, standard or requiring some innovations? These should be made clearer.\n\nSome extra comments:\n\n(1) Lines 47-49: \"Notably, this intuition has already been exploited to show that the bootstrap method is consistent in particular applications in the econometric literature, such as the construction of uniform confidence bands.\" This sentence should be accompanied by relevant citations.\n\n(2) Lines 88-89: \"We note that the limiting distribution of those statistics are in general not Gaussian [22].\" (i) \"distribution\" should be \"distributions\"; (ii) I understand that the authors try to tell readers Gaussian limiting distributions for these statistics are established under specific conditions, e.g. $U$-statistics with dominating first-order terms in Hoeffiding's decomposition. But this and later sentences seem to be quite disconnected from previous sentences without adding more contexts. For example, Xiaohui Chen's work [15] is about Gaussian limiting distribution for high-dimensional $U$-statistics.\n\n(3) I would say 70% of NeurIPS submission does not contain such a long supplementary material filled with technical lemmas and proofs. One presentation style that I found very useful for technical papers in statistics and theoretical machine learning is to draw a diagram indicating which technical lemma was used to prove Theorem X and how different technical results are connected.\n\n(4) Lines 146-147: \"Given that these results typically require stronger conditions on the statistic and many times Gaussian limits.\" What does \"many times Gaussian limits\" mean exactly?\n\n(5) Line 231: \"the test statistics is computed on\" should be \"the test statistics are computed on\"\n\n(6) A recent paper (first appeared in 2020) by Vladimir Koltchinskii (https://arxiv.org/pdf/2011.03789.pdf) considered a relatively similar setup, but he focused on Gaussian limit and root-n parametric theory for estimating nonlinear statistics. I suggest the authors also cite this paper and discuss the connection and difference between their work and Koltchinskii's. \n\n(7) No discussion is written. The authors are recommended to write at least some sentences on future works or caveats... This is also strongly encouraged by NeurIPS.\n\nGiven above, I temporarily give the authors a score of 5 but if they could convince me with their rebuttal I am willing to change my evaluation. Also, this paper seems to fit better in a journal in statistics, JMLR or a TCS-type conference. The page limit of NeurIPS may have a negative impact on this paper. Yes.",
            "The authors provide theoretical results for the bootstrap, attempting to address a common issue in machine learning: namely, that it is difficult to obtain closed-form variance estimators for many quantities of interest when machine learning is used. However, the necessary assumptions stated by the authors for the theoretical results to hold are quite strong.  - The paper is quite dense. Please consider adding some exposition to each section to aid the reader.\n- The theoretical developments build upon recent developments, and are nice. However, the assumptions are quite strong and appear unlikely to hold in many cases. Can you provide more exposition about these assumptions, why they are necessary, and in which cases they hold? Is it possible to relax the assumptions?\n- In particular, the conditions of Theorem 2 seem very strong. Can you discuss these and whether or not they can be relaxed?\n- The discussion on line 206 is important and should be given more weight.\n- The manuscript ends abruptly, and would benefit from:\n  - numerical experiments showing that the theoretical properties are achieved in finite samples\n  - a discussion of the conditions, as mentioned above\n  - a data example, showing that the procedure gives reasonable results in practice\n  - more discussion of the results in the Supplementary Material (since a lot of work went into deriving these results). Not discussed; while the paper proposes new theory, the proposal can be used widely in decision-making, which does have downstream societal impact. It would be nice to have some discussion of this point.",
            "This paper proposed a new metric for arguing the validity of bootstrap approximation, which leads to a new class of estimators that the bootstrap will be valid for recovering the asymptotic distribution. The authors provided a good theoretical justification of their findings.   1. This paper proposed to use the maximum mean discrepancy to measure the difference between two probability measures.\n2. With this metric, the authors show that the empirical bootstrap approximates the asymptotic distribution of some estimators (e.g., of soft-max types). \n3. The key idea in the proof is a clever interpolation method (similar to the Slepian's interpolation).\n\nOverall, I found this paper novel and interesting. The new findings are pretty important and could be very useful. \nI have three comments/questions that I wish the authors to clarify:\n\nQ1. [Motivation] \nThe motivation should be clarified. The authors proved a new class of estimators that bootstrap would work. \nSo the authors could start with a class of estimator and argue that it was unclear if we can apply bootstrap to this class because all the existing theories (e.g., Berry-Esseen theorem, anti-concentration from Chernozhukov et al) do not apply.\nThen the authors can argue that in this paper, they have proved that the bootstrap also applied to this case.\n\nQ2. [Rate of approximation]\nMany results for the bootstrap will include the rate of approximation (e.g., how fast the difference in the approximation goes to 0). Do the authors have ideas on the rate based on this method?\n\nQ3. [Practical examples]\nThe only (positive) example given in this paper was in Section 5, the smooth stacked ensemble estimator. \nBut the construction seems to be a bit artificial. \nCan the authors give a more practical example? such as an estimator from boosting or other approach? \n\nQ4. [Assumption H0 and H1]\nThe authors provided counterexamples for these two assumptions in appendix. \nBut I think most people are more interested in the positive example of these two assumptions. \nCan the authors provide more examples that these two assumptions hold?\nIdeally, the examples should be estimators that will be used by practitioners.\n\n I do not see any major weakness of the paper as a theory work. \nPerhaps the motivations should be clarified and more practical examples to be provided."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses concerns about the necessity of Assumption 1, the limited usefulness of the master theorem, and the extent of Chatterjee's extension. The final statement, 'Unfortunately, my original evaluation remains,' indicates a negative overall sentiment.",
            "The review expresses both positive aspects (liking the theoretical results) and negative aspects (concerns about presentation, typos, and the need for more context and clarity). The reviewer offers constructive criticism and suggests improvements, indicating a balanced perspective rather than outright rejection or strong endorsement.",
            "The review expresses concerns about the strength of the assumptions required for the theoretical results and suggests the paper is dense and needs more exposition. It also points out the abrupt ending and lack of empirical validation, indicating a critical assessment of the manuscript's current state.",
            "The reviewer states the paper is \"novel and interesting\" and that the \"new findings are pretty important and could be very useful.\" The reviewer also mentions not seeing any major weaknesses of the paper."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'somewhat limited,' 'not a 'big' extension per se,' and 'my original evaluation remains' which convey a critical perspective. The questions posed about Assumption 1 also challenge the authors' choices.",
            "The review points out several weaknesses in the paper, including typos, grammatical errors, lack of clarity in the technical proofs, and insufficient discussion of assumptions. Phrases like \"I am worried that the supplementary materials will eventually be buried under most readers' desk drawers\" and questions like \"Then what technical challenge do they need to solve?\" indicate a critical assessment of the manuscript.",
            "The review uses phrases like \"assumptions are quite strong,\" \"appear unlikely to hold in many cases,\" and \"manuscript ends abruptly,\" indicating a critical evaluation of the paper's limitations and areas for improvement.",
            "The reviewer uses phrases like \"good theoretical justification\" and expresses interest in the findings. The questions are framed as requests for clarification and further exploration rather than criticisms."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it raises several concerns and suggestions, and ultimately concludes that the original evaluation remains unchanged. The reviewer questions the necessity of Assumption 1, the limited usefulness of the master theorem, and the significance of Chatterjee's extension. These points consistently lead to the final statement that the reviewer's original evaluation stands, indicating a consistent line of reasoning and critique throughout the review.",
            "The review is consistently critical but constructive. The reviewer appreciates the theoretical contribution but points out areas for improvement in writing, presentation, clarity, and contextualization. All comments and suggestions are aligned towards enhancing the paper's readability, impact, and relevance to the NeurIPS community, without any self-contradiction.",
            "The review is consistent because it repeatedly emphasizes the same concern regarding the strong assumptions underlying the theoretical results. The reviewer consistently asks for more exposition on these assumptions, their necessity, potential relaxation, and empirical validation.  The suggestions for improvement all align with addressing the limitations imposed by these strong assumptions and enhancing the practical relevance and understanding of the theoretical contributions.",
            "The review is consistently positive about the theoretical contribution of the paper, while suggesting areas for improvement such as clarifying motivation and providing more practical examples. There are no contradictory statements or assessments in the review."
        ]
    },
    {
        "paper_id": "iclr_2021_bM4Iqfg8M2k",
        "paper_title": "Graph Information Bottleneck for Subgraph Recognition",
        "paper_abstract": "Given the input graph and its label/property, several key problems  of graph learning, such as finding interpretable subgraphs, graph denoising and graph compression,  can be  attributed to the fundamental problem of recognizing a subgraph of the original one.  This subgraph shall be as informative as possible, yet contains less redundant and noisy structure. This problem setting is closely related to the well-known information bottleneck (IB) principle, which, however, has less been studied for the irregular graph data and graph neural networks (GNNs). In this paper, we propose a framework of Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Under this framework, one can recognize the maximally informative yet compressive subgraph, named IB-subgraph.  However, the GIB objective is notoriously hard to optimize, mostly due to the intractability of the mutual information of irregular graph data and the unstable optimization process. In order to tackle these challenges, we propose:  i) a GIB objective based-on a mutual information estimator for the irregular graph data; ii) a bi-level optimization scheme to maximize the GIB objective; iii) a connectivity loss to stabilize the optimization process. We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph classification, graph interpretation and graph denoising. Extensive experiments demonstrate that the information-theoretic  IB-subgraph  enjoys superior graph properties. ",
        "review_ids": [
            "4z-ItitYkIz",
            "0dRHy969zy",
            "0J7VemrR9ze",
            "4g8t-dZMv_"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary:\n\nI think this is a nice paper that successfully used information theoretic objective functions for graph representation learning. The authors leveraged the DONSKER approximation of mutual information for a global information bottleneck loss used on the input-space instead of learned latent-space. To help stabilise optimisation, the authors also use bi-level optimisation with more iterations on the inner $I(G, G_{sub})$ as well as automatic masks learning through their $L_{con}$ loss. The authors also showed many experiments on graph classification, denoising, and interpretation tasks. \n\nReason for the Score:\n\nYet, I think the authors could improve on the related work coverage. For example, I found a paper published in last year's ICLR that seems quite related but not cited or compared in this paper. I hope the authors could spend a bit more time to add the relevant methods and possibly compare against them.\n\nPro:\n- Quite interesting information theoretic objective functions that actually work on multiple graph learning tasks.\n- Figure 1 is quite helpful for understanding the story quickly.\n- Relatively well written and easy to follow the ideas.\n\nCon / Questions:\n- In Eq.13. Given that $\\textbf{$\\textit{S}$}$ are probability values, did the authors add a non-linearity such as sigmoid after the MLP ?\n- The proposed method seems quite related to last year's ICLR paper $\\textit{InfoGraph}$ which I think the authors should probably cite and maybe compare and contrast as well. For example, your proposed Eq.11 seems quite similar to $\\textit{InfoGraph}$'s Eq.6. (https://openreview.net/pdf?id=r1lfF2NYvH)\n\n\n-----------------------------------------------------------------\nPost Rebuttal:\n\nMany thanks for the authors to update their original paper addressing my questions and concerns.\nI have now updated the score.",
            "The submission proposed to use Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Basically, it makes use of bi-level optimization to find a subgraph that well encode the information for graph classification task. The major claim is that the resulting subgraph is more robust for the learning. \n\n1). My major concern is on the novelty part. The submission is lack of novelty. First, the Graph Information Bottleneck (GIB) is used to learn a robust GNN against adverbial attack in the paper accepted in this year NeurIPS.\nGraph Information Bottleneck\nTailin Wu \u00b7 Hongyu Ren \u00b7 Pan Li \u00b7 Jure Leskovec\nNeurIPS 2020.\nThe only difference is the submission uses the GIB principal to learn a subgraph. As for the subgraph selection, much work can be found. Basically, the edge dropping or node dropping  is popularly studied recently. By simple search, we can find DropEdge in ICLR\u201920, NeuralSparse in ICML\u201920, DropNode in NeurIPS\u201920. The only difference is they are modeling general GNN, while this submission is only focusing on graph classification. As for the subgraph selection, is there any superiority of the proposed methods over NeuralSparse which uses gumbel-softmax?\n\nDropEdge: DropEdge: Towards Deep Graph Convolutional Networks on Node Classification\u00a0\nICLR\u201920.\n\nNeuralSparse: Robust Graph Representation Learning via Neural Sparsification.\nICML\u201920.\n\nDropNode: A Flexible Generative Framework for Graph-based Semi-supervised Learning\nNeurIPS\u201920.\n\n\n\n2). The submission is not well presented. Many notations are not defined before use. For example, in 4.2,\n\u03b4yi (y)\u03b4(Gsub,i), what is meaning of \u03b4? What is the meaning of G_sub, i ?\n\n\n3). The experiments are not sufficient. The list methods are most famous GNNs but not SOTAs for graph classification task. Graph neural tangent kernel (GNTK), End-to-end graph classification (DCGNN) , Convolutional network for graphs (PATCHY-SAN). Besides, the graph kernel methods, like Graphlet kernel (GK), Weisfeiler-Lehman Graph Kernels (WLGK), and Propagation kernel (PK) are not compared.\n\nDCGNN: Zhang, Muhan, Cui, Zhicheng, Neumann, Marion, & Chen, Yixin. 2018. An End-to-End Deep Learning Architecture for Graph Classification. In: The Thirty-Second AAAI Conference on Artificial Intelligence\n\n4). The results reported seems much worse than results reported in other paper. For example, in the paper below. We can easily see the best result in MUTAG is 93.28\u00b13.36, while the submission gives only 0.844 \u00b1 0.141, in Proteins, best result is 77.47\u00b14.34 while the submission gives only 0.749 \u00b1 0.051. Similar case shows in DD dataset.\n\nStructural Landmarking and Interaction Modelling: on Resolution Dilemmas in Graph Classification,\nhttps://arxiv.org/pdf/2006.15763.pdf\n\n5). Typos:\nIn 4.1, \u201ca informative representation\u201d==>\u201dan informative representation\u201d\nIn 4.3, \u201cS is 2-dimensional vector\u201d==>\u201dS is a 2-dimensional vector\u201d\n\n\n6). The authors does not report or discuss the running time complexity of the algorithm. Since the framework needs bi-level optimization, it is supposed to discuss how fast the algorithm will converge.\n",
            "# Summary \nThe paper introduces the Graph Information Bottleneck (GIB) which aims to learn the most-informative compressed representation $Z$ given graph $G$ with associated label $Y$. Further, it defines GIB-Subgraph which aims to learn the compressed representation as the subgraph $G_{sub}$ which maximizes the mutual information within the family of subgraphs ${\\cal G}_sub$ of $G$. The paper introduces bi-level optimization objective which has the following parts: \n\n(a) optimizing the mutual information loss $L_{cls}$ between the subgraph representation $G_{sub}$ and the graph label $Y$ using the backbone GNN followed by aggregation of subgraph node embeddings $X_{sub}$ and cross-entropy loss when comparing to graph labels. \n\n(b) approximates the mutual information $L_{MI}$ between the original graph and a subgraph $I(G, G_{sub})$ using statistics network $f_{\\phi}$ which uses the backbone GNN to obtain graph embeddings (using mean/sum or pooling over node embeddings) followed by MLP over concatenated embeddings of $G$ and $G_{sub}$. \n\nThe procedure retrains the graph-subgraph mutual information estimator in the inner loop for each step (eqn. 10) before updating the parameters of the backbone GNN and the subgraph selection MLP and finally updating the subgraph-label MI estimator ($L_{cls}$). In order to obtain compact subgraphs, the paper introduces a regularization term $L_{con}$ closely related to graph cut. \n\nThe papers shows empirically on downstream task of graph classification that adding the GIP objective improves classification accuracy. Further, on graph interpretation task, the authors show that the GIP objective improves the similarity of the retrieved subgraphs using domain-specific metrics. The authors also evaluate on graph denoising on the MUTAG dataset.\n\n# Recommendation \nI vote for a strong accept. This paper is well-written, makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation. \n\n# Questions to the authors \n- I would have liked to see in the supplementary material an example of the algorithm on a toy graph example (similar to case study A). \n\n- I wonder does the initialization have an influence on the final chosen subgraph nodes. Does $S$ (node-assignment) (always/almost always?) saturate  as mentioned on page 5? \n\n- What is the influence of the ${\\cal L}_{con}$ on the size of the final chosen subgraph. A table showing the size of final subgraphs (in term of output of MLP $\\theta_2$ in Figure 1) might be helpful, though this is partially addressed in Table 4. \n\n- For completeness, it would be good to provide in the supplementary material the properties of the datasets used e.g., number of graphs, mean/max/min number of nodes, edges, dimension of node features, dimension of edge features (if any), etc. \n\n- It would have been good to see plots showing the convergence of the different losses as part of the bi-level optimization iterations. \n\n- [optional] On the graph denoising experiment, it might be good to add more concrete evaluation both on larger graphs e.g. on graph families such as Power-Law, SBM as well as non-uniform edge addition. \n",
            "Authors propose to apply information bottleneck to network structured data which is represented by graphs whose nodes are assigned features. \nThe idea seems promising but the authors need to improve their manuscript considerably. In particular, the probabilistic model underlying the IB framework needs to be made clear right from the start. Which random graphs do you consider ? \n\n- \"... GCN outputs the node embeddings X from the following process:... \" what does that mean ? \n- \"...the GIB seeks for the most informative yet compressed representation Z by optimizing the following objective .. \" what is the domain of the optimization problem here ? and what do you mean precisely by \"compresse representation\" "
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer initially describes the paper as \"nice\" and notes its successful use of information theoretic objective functions. The reviewer also appreciates the helpfulness of Figure 1 and the clarity of the writing. After the rebuttal, the reviewer thanks the authors for addressing their concerns and updates the score, suggesting a positive resolution.",
            "The review expresses significant concerns about the submission's novelty, presentation, experimental setup, and results. The reviewer points out that the core idea is similar to existing work, notations are poorly defined, experiments are insufficient, and the reported results are significantly worse than those in other papers. These criticisms collectively indicate a negative sentiment.",
            "The reviewer explicitly states \"I vote for a strong accept. This paper is well-written, makes a clear theoretical contribution to the field as well as provides sufficient empirical evaluation.\"",
            "The review expresses concerns about the clarity and completeness of the manuscript, indicating a negative sentiment. Phrases like 'need to improve their manuscript considerably' and questions highlighting confusion suggest the reviewer is not satisfied with the current state of the work."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive aspects (pros) and areas for improvement (cons/questions). The reviewer uses a mix of appreciative language (\"nice paper\", \"helpful\") and constructive criticism (\"could improve\", \"should probably cite\"). The initial tone is balanced, becoming more supportive after the rebuttal.",
            "The tone is critical, as evidenced by the use of phrases like \"My major concern is on the novelty part. The submission is lack of novelty,\" \"The submission is not well presented,\" and \"The experiments are not sufficient.\" The reviewer also directly questions the superiority of the proposed method and points out discrepancies in reported results, indicating a critical evaluation of the work.",
            "The reviewer's overall tone is encouraging and constructive. The language used such as \"I would have liked to see\" and \"it would be good to\" suggests a desire to help improve the paper rather than criticize it. The final recommendation of \"strong accept\" is a strong indicator of a supportive tone.",
            "The tone is critical, using direct questions and phrases like 'need to improve their manuscript considerably' which indicates dissatisfaction and points out specific weaknesses."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer initially points out areas for improvement (related work, questions about implementation and relation to existing work) and also acknowledges the strengths of the paper. The post-rebuttal statement confirms that the reviewer's concerns were addressed, leading to a positive update in the score. There are no contradictory statements or shifts in opinion without justification.",
            "The review is consistently negative, raising concerns about novelty, presentation clarity, experimental sufficiency, result quality, typos, and lack of discussion on running time complexity. All points consistently suggest weaknesses in the submission.",
            "The review is consistent because the reviewer recommends 'strong accept' and the questions are all constructive suggestions for improvement, not criticisms that would contradict the positive recommendation. The reviewer praises the paper's writing, theoretical contribution, and empirical evaluation, which supports the strong accept recommendation.",
            "The review is consistent because the reviewer raises valid questions about the clarity and completeness of the manuscript, specifically regarding the probabilistic model, the definition of GCN output, the domain of optimization, and the meaning of 'compressed representation'. The reviewer consistently points out areas where the manuscript needs improvement and clarification without contradicting themselves."
        ]
    },
    {
        "paper_id": "nips_2021_EvhsTX6GMyM",
        "paper_title": "Conformal Prediction using Conditional Histograms",
        "paper_abstract": "This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achieving conditional coverage and optimal length if the black-box model is consistent. Numerical experiments with simulated and real data demonstrate improved performance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches.\n",
        "review_ids": [
            "u5U3iPj_NMk",
            "6AnlYA3EaUH",
            "IPyeeGZw25D",
            "IX5AXfmDULU",
            "oqz2m-Q1pl",
            "F3gJsZUVtmf",
            "WdLi3rQ1m1",
            "7M9urTnc4Pg",
            "-Pw0iD1I2A_",
            "6Q0EddXbPXi"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the clarifications; they were very helpful!",
            " Thank you for the helpful clarifications!",
            "This paper introduces a new method to construct conformal prediction intervals based on histograms of the conditional distribution of an outcome variable. Given a histogram of the conditional distribution of the outcome variable the method finds the shortest interval whose associated mass is no less than the desired coverage probability of the prediction interval. This yields intervals that automatically adapt to the skewness of the data. The intervals have provably correct marginal coverage in finite samples and correct conditional coverage and optimal length in large samples. A simulation study and numerical experiments on several benchmark data sets corroborate the theoretical results and demonstrate the advantage over several competing methods.\n  This a well-written and clearly organized paper that makes an interesting contribution to conformal prediction. The paper addresses the important problem of how to construct conformal confidence sets that have finite-sample marginal coverage and large-sample conditional coverage while being as narrow as possible. Typically, adaptivity and optimality of (conformal) confidence sets are only argued heuristically (by judiciously choosing tuning parameters, e.g. Lei and Wasserman, JRSSB 2014) and are often excluded from the theoretical analysis. The novelty of the proposed method is that adaptivity and optimality are guaranteed.\n\nTo me the strength of the paper lies in the simple and intuitive idea of the proposed method (setting aside the somewhat tedious actual implementation). I believe that the basic idea of directly minimizing the length of the prediction interval subject to a constraint on the coverage probability can be extended and adapted to related problems. Thus, the paper does not simply propose a solution to an important problem, but also provides food for thought (see below).\n\nAn obvious weakness of the paper are the strong assumptions for the theoretical analysis. I am fine with Assumptions 1-3, but I would have preferred if the authors had done without Assumption 4 and 5 on the unimodality of the true and estimated conditional density of the outcome variable. At the very least, the authors could have included empirical results from an outcome variable with non-unimodal conditional distribution.\n\nI wonder whether the authors have thought about the following extension of their approach:  Why \"only\" consider prediction intervals of shortest length and not more general prediction regions with smallest Lebesgue measure? It seems to me that opt. problem (7) can be easily modified in such a way that even non-unimodal conditional distributions of the outcome variable can be handled. Consider solving\n\n$\\mathcal{S}(x, \\pi, S^-, S^+, \\tau) :=  \\arg\\min_{M\\subseteq \\\\{1,\\ldots, m\\\\}} \\left\\\\{ |M| :  \\sum_{j \\in M} \\pi_j(x) \\geq \\tau, S^- \\subseteq M \\subseteq S^+ \\right\\\\} \\quad{}\\quad{} (*)$\n\nIf the histogram of the conditional density of the outcome variable is unimodal, then $\\mathcal{S}(x, \\pi, S^-, S^+, \\tau)$ is an interval. Otherwise it is a collection of convex sets. Therefore, (\\*) seems to be even more adaptive than the original opt. problem (7). Yes, the authors have adequately addressed the limitation of their assumptions and the lack of control of lower and upper miscoverage rates.",
            " Thanks, this explanation was quite helpful.",
            " Thank you to the authors for their helpful response. \n\n- Thank you for clarifying the differences in asymptotic oracle performance between your work and Dist-Split; I missed that oracle defined by loss function in Dist-Split is symmetric. The distinction with that work is now clear, and I understand the desire for predictive intervals rather than sets from your comments to the other reviewers.\n\n- By scalable density estimator, I simply meant a single parametric one (perhaps something like Real NVP, but I'm not that familiar) instead of learning and evaluating a large collection of quantile estimators to create the histograms. To clarify, this suggestion is only cursory---I am then unsure of an efficient solution to the optimization in (7). The main \"concern/limitation\" expressed here is that to actually realize the asymptotic results of 3, the computational complexity for inference becomes quite large. This might be unavoidable in regression while upholding the theoretical desiderata of this work. Out of curiosity, what is the runtime of the current method vs, say, CQR?\n\n- I understand that high-dimensional regression is a general challenge. This method requires computing m high-dimensional regression models, as opposed to, say, just two for CQR. It would be great if the authors could further comment on if a larger net error accumulation (in practice, obviously not under the consistency assumptions) could be a problem.\n\nAdditionally, I do agree with the authors that overall the paper is well cited/situated and is a strong and valuable contribution.",
            "This paper proposes a new nonconformity-score based on conditional density estimates P(Y|X). The main idea is to approximate the optimal prediction interval based on the conditional density estimate. The proposed non-conformity score is experimentally assessed using the conformalization technique of split conformal. The overall method provably achieves marginal coverage, performs comparably to other methods for conditional coverage, and achieves the shortest prediction intervals on all datasets.   My overall opinion of this paper is quite positive. The method makes sense and the experiments indicate a significant improvement on the efficiency/length of the prediction intervals. Thus I think the paper is a useful contribution to the ML community. In the following 'sections' I elaborate further on things that I liked about the paper, along with some suggestions for further improvements.\n\nHowever, I have one methodological question that is almost 'begging to be answered'. The optimal prediction **set** given P(Y|X) is simply to take the highest predictions until 1-\\alpha coverage is reached (Appendix F [2]). In the case of viewing it from a 'histogram' perspective, one would take the largest histogram bars until 1-\\alpha is reached. Perhaps considering the optimal prediction **interval** is more interpretable in some applications, as the authors do. However, in some applications, I think sets could also be ok? Thus, I think it makes sense to also explore if taking prediction sets helps reduce the widths. If the authors found in experiments that it does not help, could the authors briefly comment on this? Also, what if one takes the convex-hull interval of the PS to make it a PI? \n\nOn a related note, lines 43-46 do not answer the following question. P(Y|X) is not known, but after an estimate of P(Y|X) is computed, why is it discretized into bins? As far as I can tell, the P(Y|X) models that the authors use essentially have real-valued output. Why not work with the full real-valued estimate and define nested sets for them? \n\n*Writing and clarity:*\nThe introduction was clean and easy-to-follow. By the end of page 2, I already had a broad idea of what to expect from the rest of the paper. \nI have a significant concern regarding the density of notation and ideas introduced in Sections 2.2 and 2.3. In lines 165\u2013167, the authors say: \u201cIn particular, we analyze a slightly modified version of Algorithm 1 in which there is no randomization; this is theoretically more amenable and equivalent in spirit, although it may yield wider intervals in finite samples.\u201d I fully agree. In light of this, is there a reason the authors chose to describe the full randomization strategy along with the main conformal algorithm in Section 2.2 (rather than in a future section or Appendix)? In my opinion, Section 2.2 has way too much notation and many nitty-gritties of conformal inference are introduced very densely\u2026. I think this could make the current description of the method *quite* difficult to grasp for someone unfamiliar with conformal prediction. \n\n*Originality:*\nThe main idea of using P(Y|X) estimates to produce efficient conformal sets has appeared in different forms, and is not striking. In this sense, the initial prediction sets that the authors describe (Sec 1.2) are natural. However, they pose the issue of not being nested sets making it tricky to apply conformal prediction. I really liked the way the authors translated this setup into a nested problem by first fixing a single set (eq. (9)), and then considering supersets and subsets of this initial set (eq. (10)). The experimental section verifies that this is a useful description of nested sets for producing short prediction intervals. \n\n*Terminology:*\nIn the nonparametrics statistics literature, histogram classification and histogram regression have been used to refer to methods that partition the feature space X into a number of bins, and compute a single prediction for each bin [1, and many others]. To the best of my understanding, there is very little intersection between the ideas proposed by the present authors, and the histogram regression literature. Due to this, I strongly suggest changing the title of the paper and the name of the proposed method. Something as simple as \u201cConformal inference using conditional histograms\u201d avoids the confusion, and avoids overstepping on another interesting body of work. \n\nMinor comments: \n- Space permitting, it would be great if Assumptions 1\u20135 are latex enumerated.\n- It may be useful to demonstrate that the prediction interval based on the unconformalized conditional density estimate (based on Sec 1.2) does not have valid coverage. \n\nReferences:\n \n[1] https://www.jstor.org/stable/2242583\n\n[2] https://arxiv.org/pdf/1910.10562.pdf Yes",
            "This paper proposes an extension to conformal prediction that adapts to skewed data, and can achieve better conditional coverage (and provably achieves conditional coverage asymptotically). Conformal prediction in general is a methodology for constructing confidence sets that output likely response candidates $\\widehat{\\mathcal{C}}(X) \\subseteq \\mathcal{Y}$ for and input $X$, rather than a single value. The goal is to ensure that $\\widehat{\\mathcal{C}}$ covers the true response variable, $Y$, with specifiably high probability. While many prior methods provably control *marginal* coverage, *conditional* coverage is a much harder (and more practically important) goal (albeit impossible in finite samples in the general case). This paper attacks this problem by developing a novel conformalization strategy that leverages calibrated estimates of the conditional density $Y \\mid X$ to obtain approximate conditional coverage in finite samples (empirically), and asymptotic conditional coverage (theoretically).   === Strengths ===\n\n- The motivation of the paper is very strong; achieving the harder task of approximate conditional coverage is far more important than marginal coverage for most practical problems. \n\n- The writing for the most part is fairly clear, with few exceptions (see questions below).\n\n- The theoretical analysis is quite comprehensive, with more precise guarantees than several other asymptotically conditional conformal methods.\n\n- Empirically the proposed method performs well relative to baselines. \n\n=== Weaknesses ===\n\n- Using estimates of the conditional distribution have been proposed before, most relevantly perhaps in CD-Split (cited) and HPD-Split (also by Izbicki et. al., 2021). Dist-Split is compared to in this work, though not the other methods (although CD-Split is designed for multi-model targets which are assumed to not exist here, it reportedly seems to work empirically better than Dist-Split even across unimodal tasks). Due to the similarities in approaches, it would be helpful to give better intuition as to why the proposed histogram-based method works better. The comments on line 192 are also confusing in terms of what exactly they mean, as Dist-Split in Izbicki et. al., 2020 has similar guarantees (Thm. 2.5 and Corollary 2.6). More discussion on this would be helpful.\n\n- Achieving conditional coverage relies on the consistency of the histogram-based conditional estimator, which scales linearly with the number of bins (which goes to infinity). It seems that achieving conditional coverage as the number of data points grows is at odds with computational complexity, as opposed to a potentially more scalable density estimator (either way, both need to be calibrated). \n\n- The quality of the conditional estimator will likely degrade considerably in high dimensional problems. The tasks considered in this work are also fairly simple. It would be good to see results on more compelling tasks; i.e. that would illustrate the value of the proposed extension across tasks relevant to the NeurIPS community.\n\n=== Questions ===\n\n- Section 2.2 is a bit dense to follow at first in terms of motivation, and it's not entirely clear what the motivation is for constructing the nested sets based off of the initial $\\bar{t}$. Instead of conformalizing the index of the set per Eq. (12), what is the downside to conformalizing $\\tau$ directly? I.e., one would find the smallest $\\tau$ such that $(1 - \\alpha)\\frac{1 + |\\mathcal{D}^{\\mathrm{cal}}|}{|\\mathcal{D}^{\\mathrm{cal}}|}$ fraction of calibration instances satisfy $y \\in \\underset{(l, u), l \\leq u}{\\arg\\\\!\\min} \\\\{ l - u  \\colon \\sum_{j=l}^{u} \\pi_j(x) \\geq \\tau \\\\}$. It seems that this is due to the randomization strategy in Eq. (8), which should produce slightly more efficient intervals depending on how coarse the binning is?\n\n=== Minor ===\n\n- Typo, line 188 \"increased flexibility\"\n- Supplement, line 16, I believe should have $\\hat{c}$ instead of $c$ in the numerator.\n\n=== Justification for score ===\n\nIn general, this paper is a valuable contribution to the growing body of approaches for approximate conditional conformal inference. There are a couple of clarity/differentiation concerns that could be addressed in the writing, and the method is similar to several already existing approaches (e.g., extending Romano et. al., 2020 to a regression setting, including the binning strategy as compared to Izbicki et. al., 2020)---but overall the work is quite solid.\n\n=== Citations ===\n- Izbicki et. al., 2020. Flexible distribution-free conditional predictive bands using density estimators.\n- Izbicki et. al., 2021. CD-split and HPD-split: efficient conformal regions in high dimensions.\n- Romano et. al., 2020. Classification with Valid and Adaptive Coverage. I don't see any potential negative societal impacts other than those inherited by the underlying wrapped algorithm this method gives confidence intervals for. Conditional coverage is overall more desirable than marginal coverage in terms of mitigating risks like fairness. ",
            "This paper considers constructing prediction set using histogram estimation of conditional density. The results and methods are new and interesting.   Prediction set based on estimated density seems a natural and fundamental problem. The current paper provides a novel method and theory regarding this problem. Paper writing is clear and results are significant. The term \"conformal\" needs to be further explained. As to common readers, the current approach is a \"plug-in\" method since unknown density is replaced by its estimator. Authors should explain the explicit reason of using \"conformal\" to name this procedure.\n\nSecond, can $Y_i$ be multi-dimensional? \n\nThird, can $f$ be estimated using general kernel density estimator?\n\n\n",
            "This paper introduces a conformity score that aims to decrease average interval lengths and improve conditional coverage from a black-box estimate  $\\hat{P}(y \\mid x)$ for split conformal prediction. The method, conformal histogram regression (CHR), involves first binning the space of $Y$, resulting in a conditional histogram from which approximate oracle intervals can be computed. A nested sequence of these intervals is then created for a sequence of predictive miscoverage values $\\tau$, where $\\tau$ will be close to $\\alpha$ if $\\hat{P}(y \\mid x)$ is a good estimate. The value of $\\tau$ is selected through a conformity score, and the authors show their method obtains finite marginal coverage and asymptotic conditional coverage. The authors then demonstrate the method on a few examples.   Strengths: \n\nThis paper is an interesting natural extension of [Romano et al., 2020] from discrete to continuous $Y$, and also connects to the nested conformal interpretation of [Gupta et al., 2019]. Although the conformity score has been previously introduced, the construction of the nested prediction sets through histograms is novel. The asymptotic results are also stronger than previous works, where both asymptotic oracle length and conditional coverage are attained. The empirical results demonstrate superior conditional coverage and average interval lengths to other methods, especially for skewed data. The paper is well-written and enjoyable to read.\n\n#####################################################################\n\nWeaknesses/Questions:\n\nI only have a few minor points:\n\n1.) For equation (7), does treating $|u-l|$ as the length require the bins to be equally spaced? I don't think this is stated.\n\n2.) It may be good to briefly mention the negligible computational cost of CHR (which is in the appendix) in the main paper to help motivate the method. A rough example of some run-times in the experiments may also be useful for readers looking to apply the method.\n\n3.) Just a few typographical/communication points:\n- I found Section 2.2 slightly difficult to read, as the notation gets a little heavy. This may not be necessary, but the authors could consider presenting the nested intervals without randomization (e.g. after Line 119), with the randomization in the Appendix, as it is not needed in Theorem 2. This would give more room for intuitive discussions, related to my next point.\n- It may be helpful to introduce some intuition on the conformity score in equation (12) and why we need the sets to be nested for readers unfamiliar with previous work, perhaps at the start of Section 2.3.\n- Line 113: $\\epsilon$ is mentioned here before it is defined\n- Line 188: 'increased' instead of 'increase'\n\n#####################################################################\n\nOverall:\n\nThis paper is an interesting extension of previous work, and the provided asymptotic justifications of attaining oracle width and conditional coverage is useful. The method is also general and can empirically provide better average widths and conditional coverage than other methods, particularly under skewed data, making it useful in practice. \n\n#####################################################################\n\nReferences:\n\nRomano, Y., Sesia, M., \\& Candes, E. (2020). Classification with Valid and Adaptive Coverage. Advances in Neural Information Processing Systems, 33, 3581-3591.\n\nGupta, C., Kuchibhotla, A. K., \\& Ramdas, A. K. (2019). Nested conformal prediction and quantile out-of-bag ensemble methods. arXiv preprint arXiv:1910.10562. The authors have described the limitations of their method - in particular their method does not control for upper and lower miscoverage, and they provide alternative recommendations.",
            "The paper shows how to obtain *short* prediction intervals that approximately have the right conditional coverage. In order to do so, the method estimates the conditional distribution of a new label given its features, and then uses a conformal score based on the quantiles implied by such estimate to create the prediction intervals. The experiments show that the method outperforms other quantile-based conformal methods in terms of width while still approximately controlling conditional coverage. Some theoretical results that prove converge to the oracle (i.e., the shortest prediction interval) are also shown.  The paper is well written and technically sound. Given the importance of accurate uncertainty quantification methods, the paper is also very relevant.\n\nAs far as I know, this is the first paper on conformal methods that attempt to find *short* prediction *intervals*; most conformal-quantile approaches are based on intervals that have the form $(q_\\alpha,q_{1-\\alpha})$, which are only the shortest intervals if the underlying distribution is symmetric. Thus, the paper has an important and novel goal.\n\nAs the authors mention, there has however been work on how to recover short prediction *regions* (which may not be intervals). Although such regions will be smaller than the oracle intervals if the underlying distribution is **not** unimodal, the fact that the CHR yields intervals is an advantage in some settings (for instance, they are easier to report). This advantage should be emphasized.\n\nRelated to this point, the conformity score (Eq. 12) is essentially (up to the randomization need due to the discrete nature of the estimated density) the HPD-split score introduced by [20] (https://arxiv.org/abs/2007.12778). Indeed, the score is (a monotonic transformation of) the area under a region with high density. The main difference is that CHR has an additional constrain that the output regions must be intervals. Indeed, Theorem 2 is very similar to Theorem 22 of [20], but apparently, the HPD-split method does not need to satisfy Assumption 4 (unimodality) to recover the oracle region. Thus, in principle, HPD-split could give even smaller regions (as they don't need to be intervals), while still approximately controlling conditional coverage. It would be interesting to see how it compares to CHR in unimodal settings.\n\nIt also seems to me that the conformity score in (12) could actually be computed using any conditional density estimator (although Eq. 7 would have to be slightly generalized to mimick Eq. 4). That is, there is no need for the method to be restricted to the histogram-based approach (although such an approach is interesting). In other words, the CHR score is agnostic to the density estimator being used, which is an attractive advantage.\n\nMinor comment: It is not clear to me how conditional coverage was computed/estimated on real datasets. It doesn't seem to be trivial, because it is a conditional probability per se. Limitations have been properly addressed."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude and appreciation, stating that the clarifications were \"very helpful!\"",
            "The reviewer expresses gratitude with the phrase \"Thank you for the helpful clarifications!\"",
            "The reviewer states the paper is \"well-written and clearly organized\" and makes an \"interesting contribution\". They highlight the \"strength of the paper\" and believe the core idea can be extended to other problems, indicating a positive overall assessment.",
            "The reviewer uses the phrase \"quite helpful\", indicating a positive assessment of the explanation.",
            "The reviewer expresses satisfaction with the authors' response and clarification, and explicitly states that the paper is a \"strong and valuable contribution\".",
            "The reviewer appreciates the paper's contribution to the ML community and finds the method logical. They also compliment the clarity of the introduction and the authors' innovative approach to creating nested sets.",
            "The review is generally positive, highlighting the paper's strong motivation, clear writing, comprehensive theoretical analysis, and good empirical performance.",
            "The reviewer states that the results and methods are \"new and interesting,\" the problem is \"natural and fundamental,\" the paper provides a \"novel method and theory,\" the writing is \"clear,\" and the results are \"significant.\"",
            "The reviewer expresses overall positive feedback, highlighting the paper's interesting extension of previous work, strong asymptotic results, and practical utility, particularly for skewed data. They also note the paper is well-written and enjoyable to read.",
            "The review expresses overall positive sentiment, highlighting the paper's novelty, relevance, and technical soundness. Phrases like 'important and novel goal,' 'attractive advantage,' and 'well written' contribute to this positive assessment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses positive language such as \"Thank you\" and \"very helpful!\", indicating a supportive and encouraging tone.",
            "The phrase \"Thank you\" indicates a supportive and appreciative tone.",
            "The review acknowledges the paper's strengths and novelty while also pointing out a weakness regarding strong assumptions. The reviewer also suggests an extension of the approach, providing constructive criticism and maintaining a balanced perspective.",
            "The reviewer expresses gratitude (\"Thanks\") and appreciation for the explanation's helpfulness, suggesting a supportive and encouraging tone.",
            "The reviewer uses encouraging language such as \"Thank you\", \"I understand\", and \"It would be great if the authors could further comment\", indicating a supportive and constructive approach. The reviewer also acknowledges the paper's strengths and value.",
            "The reviewer offers both positive feedback and constructive criticism, creating a well-balanced review. They acknowledge the paper's strengths but also identify areas for improvement, like notation density and potential terminology confusion.",
            "The review provides both strengths and weaknesses, and asks questions to help improve the paper, indicating a balanced and constructive approach.",
            "The reviewer uses positive language like \"new and interesting,\" \"natural and fundamental,\" \"novel method,\" and \"significant,\" indicating a supportive tone. The reviewer also offers constructive criticism in the form of questions and suggestions for improvement, further reinforcing this tone.",
            "The reviewer uses supportive language like 'interesting extension,' 'useful,' and 'enjoyable to read.' They also offer constructive suggestions for improvement rather than harsh criticisms, indicating a desire to help the authors strengthen their work.",
            "The review adopts a balanced tone by acknowledging the paper's strengths (novelty, relevance, well-written) while also pointing out limitations and areas for improvement (comparison with HPD-split, applicability of conformity score with other density estimators, clarity on conditional coverage computation). The reviewer provides constructive criticism rather than outright praise or condemnation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses a single, positive sentiment without any contradictory statements or conflicting opinions. It is a short and straightforward positive feedback.",
            "The review is a short, positive statement expressing gratitude and does not contain any contradictory information.",
            "The review is consistent because it presents a balanced assessment of the paper, highlighting both its strengths (novelty, simplicity, contribution to conformal prediction) and weaknesses (strong assumptions). The reviewer offers constructive criticism and suggestions for improvement without contradicting their overall positive evaluation of the paper's contribution. The reviewer explicitly acknowledges that the authors have addressed the limitations, further supporting the consistency of the review.",
            "The review expresses a single, positive sentiment without any conflicting statements.",
            "The review is consistent because the reviewer expresses initial misunderstanding, appreciates clarifications, and raises valid concerns about computational complexity in a constructive manner. The reviewer maintains a positive overall assessment of the paper while pointing out potential limitations and asking for further details. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because despite raising points for improvement and questions, the reviewer maintains a positive overall opinion of the paper and its contribution. The criticisms are constructive and do not contradict the initial positive assessment.",
            "The review is consistent because it presents both strengths and weaknesses of the paper in a balanced way. The reviewer acknowledges the value of the contribution while also pointing out areas for improvement and further discussion. The overall tone is constructive and the justification for the score aligns with the points raised in the strengths and weaknesses sections. There are no self-contradictory statements or conflicting opinions expressed within the review.",
            "The review is consistent because it starts with positive feedback, highlighting the novelty and significance of the paper's results and methods. It then raises specific questions and suggestions for clarification and further details, such as the explanation of the term 'conformal' and the scope of the method regarding the dimensionality of Y_i and the estimator for f. These questions are constructive and do not contradict the initial positive assessment of the paper.",
            "The review is consistent because the overall positive assessment aligns with the identified strengths, which are significant (novelty, strong results, good empirical performance). The weaknesses are minor suggestions for improvement and do not contradict the positive overall assessment.",
            "The review provides a balanced and coherent assessment of the paper. It highlights the paper's strengths, such as its novelty and technical soundness, while also suggesting potential improvements and further research directions. The reviewer's points are logically connected and do not contradict each other. The critique is constructive and focused on enhancing the paper's contribution."
        ]
    },
    {
        "paper_id": "nips_2021_nlR7LzSArtK",
        "paper_title": "Exploiting a Zoo of Checkpoints for Unseen Tasks",
        "paper_abstract": "There are so many models in the literature that it is difficult for practitioners to decide which combinations are likely to be effective for a new task. This paper attempts to address this question by capturing relationships among checkpoints published on the web. We model the space of tasks as a Gaussian process. The covariance can be estimated from checkpoints and unlabeled probing data. With the Gaussian process, we can identify representative checkpoints by a maximum mutual information criterion. This objective is submodular. A greedy method identifies representatives that are likely to \"cover'' the task space. These representatives generalize to new tasks with superior performance. Empirical evidence is provided for applications from both computational linguistics as well as computer vision.\n",
        "review_ids": [
            "7XSPGTuCBs",
            "ddT4HBYS_S",
            "zV7ypOdPjwY",
            "RqoqSJCdepL",
            "93LBb02RKai",
            "zgzziX58hjV"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " The authors addressed all points. \n1. As for A-map that would be still an interesting result to be seen.\n2. it is a good outcome\n4. That is a reasonable reply, means one can say it is useful when one is on a computational budget and wants a choice by prior guess. \nall the others are satisfactory.\nThe reviewer feels it is a paper above acceptance threshold though not on the level of a strong result like score 8 or 9 . \n ",
            " Thank you for the additional clarification and for running additional experiments in a short time. Although the responses help alleviate some of my concerns, regrettably not at a level to improve my original rating. ",
            " The authors have made the responses to all the reviewers. I also read these. One the whole, The idea is indeed interesting for me but does not give me some reason of increasing the score sufficiently.",
            "When no task specific heads are available sample labels cannot be predicted and the dependency between two tasks and thus task transferability cannot me measured. The paper uses a Gaussian process to model the task space and comes up with a closed solution for the mutual information gain. Proposes a greedy selection algorithm for selecting checkpoints to maximize the information gain. Experiments with text and image datasets are performed to show that selecting checkpoint based on this criterion outperforms random selection.  Strengths:\n-\tUsing unsupervised probing to get around the problem of missing true and predicted labels and showing that kernel alignment can be achieved reasonably well at the level of feature extractors without having access to task specific heads. \n-\tGood leveraging of the analogy between sensor replacement and checkpoint selection for a more generalizable task transfer from seen tasks to unseen ones.\nWeaknesses: \n-\tImaging experiments are less compelling\n\n\nEach checkpoint is added in a greedy way to maximize the gain, where the gain is formulated as the information gained by adding the current checkpoint to the current set of checkpoints minus the information lost by removing the current checkpoint from the remaining part of the checkpoint zoo. A good checkpoint generalizable to unseen tasks is the one that is representative of the remaining part of the checkpoint zoo (the information loss minimum when removed) but at the same that surprises the current set of checkpoints the most. \n\nIn the CIFAR 100 experiments classes within seen and unseen tasks can overlap as 30 classes per task are chosen randomly. What would happen if unseen tasks do not contain any classes used in seen tasks?\n\nImaging experiments select n=100 seen tasks. How does the accuracy change with respect to n?\n\nIt is not clear why choosing too many checkpoints hurts the performance in such a significant way? Overfitting explanation is not very compelling\n\nTypos:\nFrom the abstract: \u201cThis objective is submoludar\u201d. Is this supposed to be submodular?\nAn extra \u201cthe\u201d in line 225\nIs missing in line 232.\nSenor placement lince 340\n Yes.",
            "provide a method of how to (adaptably) combine  existing task models for new unseen task with aiming to relieve practitioners'  selection difficulty.  Through analogizing sensor placement problem, the authors give the above method and conduct effective verification on language and CV tasks.  The idea of the submission is interesting and it uses the sensor placement as checkpoints to formulate the problem in hand for intuitive understanding. Concretely, using GP to model task space to capture the task relation and while using checkpoints and unlabeled probing data to estimate covariance of the GP involved, then using Max mutual information criterion to identify the representative checkpoints for new tasks. To my best knowledge, this is relatively new. the experiments also show effectiveness of the proposed method. My comments are\n1. a task corresponds to its data distribition, then what differences are between the distribution/task-oriented inductive learning and the proposed method? \n2. when a task is viewed a domain, again what differences are between the domain generalization and the proposed method? \n3. the checkpoint selection here is quite similar to the coreset selection, thus in some sense this point is not new.\n4. if the selected checkpoint diversity is needed, or can the diversity be useful?  Considering both data biases and task biases, the proposed method inherits naturally these biases to some extent.",
            "\nThe paper considers the question of how to select an ensemble of existing pretrained models to be used as feature extractors, when the task for which they are to be used is not known a priori. It seems to aim less to generalize to a given fixed target task distribution, but rather at generalizing over an expectation over target task distributions.\n\nTo achieve this, the authors propose essentially two steps:\n\nFirstly, considering the feature representation of each model from the zoo, and computing a within-model similarity matrix between the features extracted for one fixed model over a set of samples. Then to use the cosine angle between similarity matrices of different models as a similarity between models.\n\nSecondly, to select a fixed number of models (corresponding to a fixed budget) based on mutual information estimates between the set of models and the remaining models. This is intuitively appealing.  Originality: the approach seems to be novel. The reviewer finds the amount of originality good enough for NeurIPS.\n\nQuality: \n\nthere are limitations in the experiments:\n\n1 lack of comparison against existing baselines for ensemble selection, in particular A-Map seems to be within the scope of the question.\n\nThey compare only against random selection for NLP and random and top-k performance selection for CIFAR-100. At least for CIFAR computing gradients for A-MAP as another baseline should be not an issue. \n\npossibly they could also use \n\n2a evaluation using a small number of single point tasks in 5.1. While they seem to optimize without having a fixed task in mind, they evaluate on four standard tasks, for which one may suspect that some of the NLP models were implicitly optimized for and thus possibly leaking onto the 4 tasks. \n\n2b As a consequence, when evaluating against 4 fixed tasks as in 5.1, one should also show how the best single model selected for one task using a task-dependent selection algorithm as baseline performs ... on all 4 tasks (including also the three tasks for which it was not optimized).\n\nThe reviewer disagrees on the statement that backprop is expensive when discussing A-MAP. Each model was trained using backprop, thus it seems to be feasible to use backprop also after training, at least until the feature representation if the computations below were a black box.\n\n\nSignificance: In terms of setup there is one doubt: While theoretically interesting, what is the practical applicability of optimizing for an expectation over tasks and not for a given single target task ? When this can be helpful?\n\nQuestion: 5.2 - how often this experiment has been repeated?\n\n\nClarity, the paper is written such that it can be read easily.\nHowever, a few things can be improved:\n\n3.2 the interpretation of the kernel alignment in terms of cosine-angle between similarities matrices within a model can be added. This interpretation also holds without any assumptions on the tasks\n\nSec 2: generalizing to unseen tasks seems to amount to optimizing over an expectation of a statistic over tasks. This can be worked out somewhat. \n\nIs there a connection between optimizing MI over feature similarities between the chosen set and the complement set of tasks, and under certain assumptions, between the expected prediction performance over many tasks ? It might be helpful to assume that each model and its feature representation is optimal for the task it was trained for, among the set of all models used for MI optimization. \nThe reviewer would be surprised if such a link could not be discussed (whereas the submodularity does not give a clear result, this would provide some theoretical link to the set of a priori unknown target tasks).\n\n\ntypos:\n\nThis objective is submoludar. \n\nhow many tasks has to be seen\n--> have\n\n picking checkpoints amount to identifying useful \n--> amounts\n\nIn optimal senor placement [15]\n\n As stated in the previous box:\n\nWhile theoretically interesting, what is the practical applicability of optimizing for an expectation over tasks and not for a given single target task ? When this can be helpful (as opposed to optimizing for a given target task which one usually has in practice)?\nThe reviewer sees this as some level of weakness and feels that motivating this would make the paper stronger.\n\nThe discussions on submodularity yielded no conclusive result and are maybe not the most important.\n\nThe reviewer sees no issues regarding societal impact. Reusing checkpoints is positive, as it does  waste less GPU training time and energy."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Neutral",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that \"the authors addressed all points\" and that it is a \"good outcome.\" They also conclude that the paper is \"above acceptance threshold.\"",
            "The reviewer explicitly states that the responses, while helpful, are \"not at a level to improve my original rating,\" indicating a negative overall assessment.",
            "The review acknowledges the authors' responses and finds the idea interesting, but ultimately states that it doesn't warrant a significant increase in the score. This indicates a neutral sentiment, neither strongly positive nor negative.",
            "The review identifies both strengths and weaknesses of the paper, but the strengths are presented more prominently and with greater detail. The overall assessment seems favorable, suggesting a positive sentiment.",
            "The review acknowledges the interesting idea, the novelty of the approach, and the effectiveness demonstrated by the experiments. Phrases like \"interesting idea,\" \"effective verification,\" \"relatively new,\" and \"show effectiveness\" indicate a positive sentiment.",
            "The review identifies several limitations in the experiments, including the lack of comparison against existing baselines, evaluation using a small number of tasks, and doubts about the practical applicability of the approach. The reviewer also disagrees with a statement about the expense of backprop and raises questions about the experiment's repetition and the theoretical link to prediction performance."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Neutral",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"good outcome\" and \"reasonable reply,\" indicating a supportive and encouraging tone. While they mention it's not a \"strong result,\" the overall feedback is positive and constructive.",
            "The reviewer expresses gratitude (\"Thank you for the additional clarification\") but also conveys disappointment (\"regrettably not at a level to improve my original rating\"), resulting in a balanced tone that acknowledges the effort while maintaining a critical stance.",
            "The tone is neutral because the reviewer uses objective language like 'The idea is indeed interesting for me but does not give me some reason of increasing the score sufficiently.' There's no strong positive or negative emotional expression.",
            "The review provides both positive (\"Strengths:\") and negative (\"Weaknesses:\") feedback. It also asks clarifying questions and points out typos, indicating a balanced and constructive approach.",
            "The review uses a formal tone and presents both positive feedback and constructive criticism. Phrases like \"My comments are\" and the numbered list of questions indicate a balanced approach.",
            "The review uses critical language by pointing out 'limitations,' 'lack of comparison,' and 'doubts.' It also directly disagrees with the authors' statement ('The reviewer disagrees on the statement that backprop is expensive')."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its positive assessment of the paper. It starts by stating that the authors addressed all points, and then proceeds to highlight positive aspects such as 'good outcome', 'reasonable reply', and 'satisfactory' for other points. The conclusion that the paper is 'above acceptance threshold' aligns with these positive remarks, indicating a consistent positive evaluation throughout the review.",
            "The review is consistent because the reviewer appreciates the authors' efforts and finds the responses helpful, but states that these responses are not sufficient to improve their original rating. This is consistent as appreciating efforts and maintaining a rating are not contradictory.",
            "The reviewer acknowledges the author's responses and finds the idea interesting, but states that the responses do not justify increasing the score. This is a consistent viewpoint.",
            "The review is consistent in its assessment, highlighting both strengths and weaknesses of the paper. It provides constructive criticism and raises relevant questions without contradicting itself. The reviewer acknowledges the novelty and potential of the approach while also pointing out areas that need further clarification or improvement, such as the imaging experiments and the overfitting explanation. The comments and questions are logically connected to the described strengths and weaknesses.",
            "The review is consistent because it starts with a positive summary of the paper's contributions (interesting idea, effective verification, relatively new, experiments show effectiveness) and then raises valid questions and points out potential weaknesses or areas for clarification (similarity to coreset, need for diversity, relationship to existing concepts). The questions are aimed at improving understanding and further development of the method, not contradicting the initial positive assessment.",
            "The reviewer consistently points out limitations in the experimental validation (lack of baselines, small number of evaluation tasks, evaluation methodology) and questions the practical significance of optimizing for an expectation over tasks rather than a specific target task. While acknowledging originality and clarity, the core critique remains focused on these aspects throughout the review, without presenting contradictory statements."
        ]
    },
    {
        "paper_id": "iclr_2020_H1g6s0NtwS",
        "paper_title": "Learning Neural Surrogate Model for Warm-Starting Bayesian Optimization",
        "paper_abstract": "Bayesian optimization is an effective tool to optimize black-box functions and popular for hyper-parameter tuning in machine learning. Traditional Bayesian optimization methods are based on Gaussian process (GP), relying on a GP-based surrogate model for sampling points of  the function of interest.  In this work, we consider transferring knowledge from related problems to target problem by learning an initial surrogate model for warm-starting Bayesian optimization. We propose a neural network-based surrogate model to estimate the function mean value in GP.  Then we design a novel weighted Reptile algorithm with sampling strategy to learn an initial surrogate model from meta train set. The initial surrogate model is learned to be able to well adapt to new tasks. Extensive experiments show that this warm-starting technique enables us to find better minimizer or hyper-parameters than traditional GP and previous warm-starting methods.",
        "review_ids": [
            "H1exBwhaKS",
            "BylBDJDFYB",
            "SJe6YTeoKH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "POST-REBUTTAL FEEDBACK\n\nThanks for your response. \n\nThe justifications provided in the response have not convinced me to improve my score. They are at times hard to understand: For example, the authors have claimed that while their design choice is not reasonable, it is less unreasonable than the other.\n\nSUMMARY OF REVIEW\n\nThe authors have proposed the use of a neural surrogate model in place of the GP posterior mean and a weighted Reptile algorithm to meta-learn the initial weights of the neural surrogate model. This approach appears interesting. However, there seems to be multiple highly restrictive (at times impractical) assumptions in this work that are atypical of the BO setting adopted by other meta BO algorithms and not discussed, as detailed below. Justifications are required.\n\nClarifications are also needed with regards to how they exactly run their algorithm in the experiments and whether the prior/initial information from related problems/meta tasks provided to the tested algorithms is fair.\n\n\n\nDETAILED COMMENTS\n\nThe authors say that \"We still use the variance in Eq. (4) to measure uncertainty, because the estimation uncertainty should be independent in individual problems.\" This does not seem to hold true. If a meta task or train set is indeed correlated (or provides information) to the new problem, the posterior variance/uncertainty at a point depends on the observations in the meta task or train set near to this point (see, for example, \nFeurer et al. (2018)). Can the authors discuss the implications of such an assumption in their work?\n\nQ and Q_i have always been referred to as problems. In Algorithm 3, Q is suddenly referred to as meta train set. On page 4, you have said that x^*_i is the minimizer of i-th problem Q_i(x) in meta train set. Based on these information, I assume that the authors consider x^*_i as the global minimizer and that x^*_i is known in order to compute the rewards. Can the authors discuss why is this a reasonable assumption?\n\nIn their proposed weight Reptile algorithm (Algorithm 3), the authors have also assumed access to the black-box functions of the related problems or meta tasks, which is not typical of other meta BO works that only require the existing observations or datasets of the related problems/meta tasks. As a result, compared with the existing meta BO algorithms, their proposed weighted Reptile is considerably more expensive due to the need to additionally evaluate the black-box functions of the related problems or meta tasks many times during execution. Can the authors discuss the practical implications of such an assumption and how it affects the types of problems/applications that can be considered by this work?\n\nThe authors have not provided any justification for their choice of reward on page 4. If the black-box function is indeed complex and highly varying, the distance between points may not work well at all. Can the authors provide a justification and discuss the practical implications and limitations with such a choice?\n\nIsn't it more natural to consider a single Bayesian neural network instead of using a neural network for the mean and a GP for the variance?\n\nFor the experiments, it would be good to see two other variants of the proposed algorithm to understand the individual contributions of the neural surrogate model and weighted Reptile algorithm: one without neural surrogate model and the other with simply the use of neural surrogate model.\n\nCan the authors explain in greater detail how they run their algorithms (Algorithms 2 and 3) in the experiments? For example, the authors say that \"WRA-N starts with learned initial surrogate model\". I assume that WRA-N refers to Algorithm 3 based on its acronym. Isn't the learned initial surrogate model the output of WRA-N in the first place? Also, the graphs in Fig. 2 seem to show iteration 1 to 13 in NOE (Algorithm 2). However, Algorithm 3 accepts N_T = 13 and executes NOE for N_T = 13 (and not 1, 2, or 3, ...) for each problem in each epoch. How do the authors generate the plot of WRA-N for iterations 1 to 12? \nWhat seems to make more sense to me is that the authors in fact run Algorithm 2 instead of Algorithm 3 for each experiment and they initialize w in Algorithm 2 to the output of Algorithm 3. In any case, a clarification is needed here.\n\nIt is not clear to me whether the initial/prior information from related problems/meta tasks provided to WRA-N, TST-R, AND TSR-M is fair. Can the authors provide a justification?\n\nTo clarify, for each related problem/function, only N_T number of datapoints are used to train a corresponding neural network with 1 hidden layer of 15 hidden units?\n\nThe authors say that \"Since TST-R needs base models for combination, we sample 20 points from uniform distribution in (\u221210, 10) to construct base models.\" Is this sampling procedure the same as that in (Wistuba et al., 2016)?\n\nCan the authors explain the comparable performance of WRA-N and TST-R in Fig. 5? Why are the error bars missing?\n\nHow does the proposed approach compare with that of Feurer et al. (2018)?\n\n\n\nMinor issues\n\nPage 1, 3: adapt well to new tasks.\nPage 2: The author says \"depends on a GP-based surrogate model fitting function values without learnable parameters\". This is not true: The GP hyperparameters need to be learned and they adapt to new problems.\nPage 4: descent order?\nPages 4, 5: Why is there an input x to Q_i?\nAlgorithm 2: t^* should be at the superscript of x.\nEquation 7: What is N?\nPage 5: Does it make a difference in the performance when delta is set to 0?\nPage 5: well define meta-features?",
            "Learning from past experience to quickly adapt to a new task has been an important and fast-growing issue in machine learning. Such technique facilitates Bayesian optimization as well, warm-starting Bayesian optimization. Recently a few methods have been developed along this direction, from designing handcrafted meta-features to learning meta-features. The current paper takes a similar step, learning neural surrogate model from related tasks to warm-start Bayesian optimization. The main idea is to replace the mean function of GP by neural surrogate model, so that parameterized models are used for meta-training, in the framework of RETILE. An idea of weighted REPTILE is another contribution in this paper, where parameter updates are done with weighs defined by rewards.\n\n---Strength---\n\n- Learning initialization for a surrogate model to warm-start Bayesian optimization is a sound approach.\n\n---Weakness---\n\n- While mean function of GP is replaced by a neural surrogate model, the posterior variance of GP should be calculated. In other words, GP regression should be run in addition to updating the neural surrogate model. One can use the conditional neural process (instead of GP regression). Have a look at the ICML18 paper: Marta Garnelo et al. (2018), \"Conditional neural processes,\" ICML.\n\n---Comments---\n\n- You can also learn an initial mean function of GP. Any comparison?\n- There is also interesting work on meta Bayesian optimization: Zi Wang et al. (2018), \"Regret bounds for meta Bayesian optimization,\" NeurIPS.\n- Ranking loss is used to train neural surrogate models. It is not clear why minimizing ranking loss makes sense in this case. It will be different from the mean function of GP regression, so it is not clear what is the behavior of the acquisition function constructed by the neural surrogate model as the posterior variance of GP.\n",
            "Summary of the paper:\n  \nThe authors propose a meta-learning approach for BO. The method consists in using a NN as the predictive mean of the model used to guide the search in BO. This NN is initialized cleverly so that its solutions is close to the actual solution to the problem by using related optimization tasks. The proposed method is validated on several experiments.\n\nDetailed comments:\n\nThe writing of the paper needs to be improved. It has awkward sentences like \"Bayesian optimization iteratively samples new point by\".\n\nThe intro on Bayesian optimization has to be improved, it explain very poorly this technique.\n\nThe description of the MAML method is not clear. The same for the reptile algorithm.\n\nEq. (3) is wrong. It does not take into account that the mean of the GP is different from 0.\n\nIt seems the authors replace the mean of the GP predictive distribution with the output of a neural network. This is strange and not very well justified. I would have expected that they use the output of the NN as the GP prior mean, to then compute the GP posterior mean.\n\nIn the related problems the actual objective is unknown. How is that difficulty addressed?\n\nIn Section 3.1, why do not you standardize the output values to have zero mean and unit deviation instead of using the ranking?\n\nHow are the hyper-parameters of the GP tuned? It seems GP based method will fail, essentially, because the authors do not consider the posterior distribution of the GP and change the mean of the predictive distribution to be the NN output.\n\nThere are no error bars for WRA-N in the experiments. Does this means that only one realization has been carried out? If so, this is insufficient to extract any conclusion. The results can be simply obtained by chance. The authors should consider several repetitions with different random seeds or different problems.\n\nThe experiments in section 4.1 are non realistic, since the actual shape of the objective function is known beforehand.\n\nIn section 4.2 it is not clear what relation are between the train / test functions. Therefore, it is not possible to understand why the proposed approach works better.\n\nThe experiments need another baseline to compare with. Namely, the same method in which the NN is randomly initialized. This will allow to check that the meta-learning procedure is useful. Currently, it can be the case that the improvements are simply due to using a different model for optimization.\n\nSumming up, I think that:\n\n(1) This paper needs further improvement in the writing.\n\n(2) The experimental section is questionable since there are missing methods in the comparison and no error bars in the experiments.\n\nTherefore I believe that this paper is still at an early stage and not ready for publication.\n"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses multiple concerns regarding the assumptions, practical implications, and experimental setup of the proposed method. Phrases like 'highly restrictive (at times impractical) assumptions,' 'not convinced me to improve my score,' and 'hard to understand' indicate a negative sentiment.",
            "The review presents both strengths and weaknesses of the paper, along with suggestions for improvement, indicating a balanced and neutral assessment.",
            "The review expresses significant concerns about the paper's clarity, correctness, experimental design, and writing quality. Phrases like 'needs to be improved,' 'very poorly,' 'not clear,' 'wrong,' 'strange and not very well justified,' 'questionable,' and 'not ready for publication' indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct and challenging questions, pointing out inconsistencies and demanding justifications. Examples include: 'Can the authors discuss the implications of such an assumption in their work?' and 'Can the authors discuss why is this a reasonable assumption?' The reviewer also uses phrases like 'This does not seem to hold true' and directly questions the authors' choices and assumptions.",
            "The review uses a mix of positive and critical feedback, pointing out both strengths ('Learning initialization for a surrogate model to warm-start Bayesian optimization is a sound approach') and weaknesses ('It is not clear why minimizing ranking loss makes sense in this case') in a constructive manner.",
            "The tone is critical, as evidenced by the direct and pointed feedback on various aspects of the paper, including the writing ('awkward sentences'), methodology ('Eq. (3) is wrong'), experimental design ('non realistic'), and overall suitability for publication ('not ready for publication'). The reviewer identifies specific flaws and questions the validity of the approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently raises concerns and questions throughout the review, focusing on the assumptions, justifications, experimental details, and clarity of the paper. The reviewer maintains a critical stance and points out areas needing improvement without contradicting themselves.",
            "The review is consistent because it acknowledges the strength of the proposed approach while also pointing out weaknesses and suggesting improvements. The reviewer's comments are constructive and do not contradict the overall assessment of the paper's direction.",
            "The review is consistently negative, highlighting several weaknesses in the paper, including writing quality, clarity of methodology description, technical correctness, experimental design, and lack of proper baselines and error analysis. All comments contribute to the overall negative assessment and the conclusion that the paper is not ready for publication."
        ]
    },
    {
        "paper_id": "nips_2022_oPzICxVFqVM",
        "paper_title": "Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance",
        "paper_abstract": "Score-based generative models are shown to achieve remarkable empirical performances in various applications such as image generation and audio synthesis. However, a theoretical understanding of score-based diffusion models is still incomplete. Recently, Song et al. showed that the training objective of score-based generative models is equivalent to minimizing the Kullback-Leibler divergence of the generated distribution from the data distribution. In this work, we show that score-based models also minimize the Wasserstein distance between them. Specifically, we prove that the Wasserstein distance is upper bounded by the square root of the objective function up to multiplicative constants and a fixed constant offset. Our proof is based on a novel application of the theory of optimal transport, which can be of independent interest to the society. Our numerical experiments support our findings. By analyzing our upper bounds, we provide a few techniques to obtain tighter upper bounds. ",
        "review_ids": [
            "cYx5VOcTxk",
            "vS5Kh3lLYS7",
            "7NHYArjAFwc",
            "pR7NfrlRMRL",
            "E54goC9XYda",
            "iS9Zuu5tYmS",
            "LV_ddj7_wmP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for answering my questions.\n\nI understood that the present study deals with an ideal model of the continuous limit, and how to approximate this limit in a discrete manner can be another issue, which is an open question to be addressed in the next step using techniques of numerical analysis, dynamical systems, or others. Personally I feel more comfortable with the scenario \"approximating a continuous physical system with a discrete model\" than the opposite one, so this scenario makes sense to me.\n\nIn any case, understanding the behavior in some limit cases (continuous limit in this paper) will provide some useful insights and information for developing practical methods. So the present results will be interesting as a guideline for future studies in this area. I have not yet fully followed the details of the proofs, but if these are correct, they would be of sufficient interest.",
            " Thank you for your valuable comments and the revised manuscript. I would like to check the revised manuscript to understand your claims.  Hence, I require sufficient time for checking the revised manuscript. ",
            " I thank the authors for replying to my questions and updating the paper accordingly. I leave my overall score of 8 unchanged.",
            " This is a theoretical paper on score-based generative models. The main result states that the Wasserstein distance between the data distribution and the generated distribution of such models is upper bounded by the square root of their objective function up to multiplicative constants and a fixed constant. This suggests that such models tend to minimize the Wasserstein distance between the target distribution and the estimated one, and complements another recent similar result on the Kullback-Leibler divergence [31]. Another theorem connecting this result to diffusion models is also provided. The findings are supported by numerical experiments on toy data (Gaussian mixtures). ## Strength\n- The paper provides a strong contribution towards a better theoretical understanding of the dramatic empirical success demonstrated by score-based and diffusion-based generative models in recent years.\n\n## Weakness\n- The paper is hard to follow for a reader who is not deeply familiar with PDE formulations of scored-based models, diffusion models, and Wasserstein metrics.\n\nUnfortunately I do not have this background nor the time to acquire it, and hence was not able to check the details of the proofs in the paper. My assessment of the paper is based on the assumption that the proofs are all correct. - The analysis is conducted in the continuous time domain, whereas in practice, diffusion model are always trained using a discrete, finite number of time step. A comment on how this theoretical analysis translates to the discrete setting would be welcome. \n- In a similar vein, it is not clear how the various continuous-time integrals were numerically calculated in practice by the authors in the various experiments.\n- The loss function defined in eq. (10) is not very well explained or motivated.\n- Please check the bibliography, it contains many mistakes. For instance, bibliographical reference [31] appears to be incorrect (it seems to include a long list of \"editors\"). Moreover, many bibliographical references are duplicate, eg., [31] and [32],  [33] and [34], [36] and [37]. \n\nTypos:\n- L29: the upper bound => an upper bound\n- L59: neither variables J_SM or J_DSM have been properly defined at this point of the paper\n- L94: from in the above => from the above\n- L269: makes us hard => makes it hard\n The limitations of the proposed theoretical analysis are adequately addressed in Section 6.",
            " - This paper discusses properties of the PDF realized by the data generating process of score-based models (denoising diffusion models), which have received a great deal of attention recently.\nThe three main theorems are as follows\n  1. If the true density is $p_0$ and the density of generated data is $q_0$, then its Wasserstein2 distance $W_2(p_0, q_0)$ is bounded above by \n$$W_2(p_T, q_T) \\times \\text{(a constant dependent on Lipschitz constants)} + \\sqrt{\\text{(training loss)}} \\times \\text{const}.$$\nTherefore, under ideal conditions such that $p_T=q_T$, the loss function is an upper bound of the Wasserstein2 distance between the true and generated densities.\n  2. The loss function commonly used in the training of score-based model is just an approximation of the true score-matching loss. In this paper, a theorem that gives the relation betweem them is provided. This gives a theoretical backborn  that the approximate loss function used in practice can be used as a surrogate function for the true objective function which is difficult to evaluate exactly.\n  3. Perturbing around the initial value (eliminating singularities at the time origin) is a fundamental technique in practical situations. A theorem is provided to evaluate the upper bound of the Wasserstein distance between the true density and the perturbed density. This is a direct consequence of the triangular inequality of the Wasserstein distance and the above theorem.\n\n- It is then demonstrated numerically using a toy model and toy data that the upper bound given by these theorems actually valid. It is also argued that the main factors of the gap between the upper bound and the loss are related to the the Lipschitz constant of the neural network.\n\n- The proof begins with a review of the fundamentals that the Fokker-Planck equation is analogous to the equation of continuity (or the law of mass conservation) which is the fundamental relationship between density and velocity field. Based on that argument, a equation is presented that states that the time derivative of the Wasserstein2 distance is expressed by the velocity field, the proof of which is based on outside literature.\n - These theorems are interesting because they guarantee fundamental properties of the score-based generative models. (However, I did not follow all the technical details of the proofs, and am not still confident in my understanding.)\n\n- It is an interesting claim that there is a theoretical guarantee that the same algorithm minimizes KL and Wasserstein distances simultaneously, as opposed to GAN, which leads to different algorithms depending on its distance measure.\n\n- These evaluations are strictly valid only for special cases, and for more general cases, theoretical evaluation is an open question. This is discussed in Section 6.\n - The discussion in this paper is based on a continuous stochastic process, but the numerical simulations appear to be dealing with discretized processes with the step size of $t = 1$. Is my understanding correct that that this is justified by Cor 2?\n\n- (The issue that interested me most while reading the text was the behavior in higher dimensions rather than the 2-dim toy data. However, this was discussed in the appendix, albeit to a limited extent.)\n - It is argued in section 6 that more general conditions make theoretical evaluation more difficult.\n",
            " This article uses optimal transport theory to prove that score-based generative models not only minimize the Kullback-Leibler divergence but also the Wasserstein distance between the generated samples distribution and the data distribution. In addition, it establishes that the Wasserstein distance can be upper-bounded by the square root of the objective function (up to multiplicative and additive constants) for both score-based models and denoising diffusion models. \nThis paper then discusses the impact of weight decay and the number of timesteps $T$ on the upper bounds. Theoretical results are experimentally checked and illustrated on simple synthetic datasets. Beyond that, the article also discusses the robustness of score-based models to noise perturbation on the datasets using the Wasserstein distance.   - The results proved in this article bring new interpretations on the score-based models learning properties, which were, to the best of our knowledge, not known before. The results are clearly stated and discussed thanks to numerous remarks sections. \n- The article is well written and easy to follow, despite some paragraphs that could be made clearer. Previous works, both in the field of generative models and optimal transport, are referenced and well-integrated in the narration.\nNotations are clearly defined and respected, even if prior knowledge about the related works is sometimes needed. Also, note that objective functions $J_{SM}$ $J_{DSM}$ are referenced (line 59) before definitions, or that the variables with respect to which variance and expectation are taken in Eq. 15 should be precised.\n- The experiments and figures help with the comprehension of the article and provide illustrations of the theoretical statements.\n\n- The motivation of the article could be stated more clearly. Besides the theoretical result, the article could mention the benefits of minimizing the Wasserstein distance between the model and the data distributions.\n- My biggest concern is about the Experiments part: This is mostly a theoretical paper and the presented experiments look somewhat superficial or confusing. The fact that the upper bound of Eq. 19 depends on the training procedure is rather unusual. This could have been controlled before training by devising a dedicated architecture. There are numerous resources on this subject, so that controlling the network's Lipschitz constant only via weight regularization does not seem convincing in this case. Also, it would have been interesting to discuss if the upper bound of Eq. 19 could suggest new ways to select the forward SDE. In the paper, only the dependence on the network's Lipschitz constant $L_s$ is discussed.\nThe assumption $W_2(p_T, q_T)=0$ depends on the choice of the forward SDE and is only approximately true. Completely removing it from Eq. 19 does not show the balance between low Wasserstein distance at $T$ and small $I(t)$.\n- It is not very clear how the theoretical results of this article can motivate improvement in the case of score-based model training on more complex datasets in practice.\n- Some details could be added regarding the training and evaluation of the network: T=10 timesteps seems like a too small value compared to what is regularly used in the literature. The number of samples used to compute $W_2$ is also missing.\n- Some adjustments regarding the overall organisation of the paper could be made: \"The score matching and diffusion probabilistic models\" subsection in \u00a72.4 could be included in the Introduction. This could help understanding that there is not much distinction between score-based generative models and DDPMs, as suggested in the Introduction, and would help understanding Sect. 2. The sketch of the proof could follow the theorem.\n\n- l. 219 $L_s(t)$ instead of $L_s(r)$\n - The article discusses casts light on reducing the intercept $I$ by applying weight decay (since it allows to reduce the Lipschitz constant $L_s$). Considering that $I$ also depends on the SDE used through $L_f$ and $g^2$, did the authors study the effect of the choice of the SDE on the intercept value given a fixed dataset? \n- On figure 6 (Appendix E), we can notice that the denoising score matching loss does not decrease much during training. Can the authors explain why the observed gap is equal to the theoretical one and not the consequence of the neural network of the denoising score matching model underfitting the distribution? (since it is a very small network).\n There is no direct potential negative societal impact of the article proposed in this article.\n\n[Edit] - I raised my score",
            " This paper shows that score-based models minimize the Wasserstein distance between them and that the Wasserstein distance is upper bounded by the square root of the objective function up to multiplicative constants and a fixed constant offset. Strengths: This paper shows that score-based models minimize the Wasserstein distance between them and that the Wasserstein distance is upper bounded by the square root of the objective function up to multiplicative constants and a fixed constant offset.\n\nWeaknesses:\n1. The assumptions in the main theorems seem to be strong. \n2. The abstract indicates the claim such that score-based models minimize the Wasserstein distance between them. However, the main theorems cannot follow the claim.  1. The main question is whether or not the theorems in this paper lead to the finding that score-based models minimize the Wasserstein distance between them, i.e., $W_2(p_0,q_0) = \\min W(p,q) = 0$. The theorems in this paper are based on the upper bounds of $W_2$. Remark 1 indicates that Corollary 1 implies that $W_2$ converges to $0$ under some assumptions. Hence, it is not guaranteed that score-based models minimize the Wasserstein distance between them.\n2. The assumptions in the main theorems seem to be strong. For example,\n- Remark 1: the conditions $J_{SM} \\to 0$, $p_T = q_T$, and $\\lambda(t) = g(t)^2$ seem to be strong. Provide some practical examples satisfying all the conditions. \n- Theorem 2: Provide some practical examples of $p_{0t}$ satisfying (15). \n3. As a result, I cannot understand what the contribution of this paper is and why the results in this paper are significant. In particular, I cannot understand why the upper bounds of $W_2$ are needed for deep machine learning and its related areas.  1. There is a gap between the claim in the abstract and the main theorems in this paper. \n2. The assumptions in this paper seem to be strong.\n3. I do not know that numerical results in section 4 are appropriate since the condition $W_2(p_T, q_T) = 0$ is assumed. Also, I do not know why the setting in Page 6 (e.g., a 3-layer neural network, AdamW with lr = 0.001) are appropriate. The numerical results are based on factitious setting, and hence, limited results. "
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses understanding and agreement with the study's approach, stating the scenario 'makes sense to me.' They also believe the results will be 'interesting' and 'of sufficient interest,' indicating a positive assessment.",
            "The reviewer expresses gratitude and states their intention to review the manuscript thoroughly, indicating a neutral stance rather than positive or negative.",
            "The reviewer expresses gratitude ('I thank the authors') and acknowledges the authors' efforts in addressing their previous concerns ('replying to my questions and updating the paper accordingly'). Maintaining the original score of 8 also indicates satisfaction.",
            "The review presents both strengths and weaknesses of the paper, and the overall assessment is balanced.",
            "The review expresses interest in the paper's findings, highlighting the interesting theorems and their guarantee of fundamental properties of score-based generative models. The reviewer also acknowledges the paper's discussion of limitations and future research directions.",
            "The reviewer acknowledges the novelty and clarity of the results, stating they bring new interpretations and are clearly stated and discussed. The reviewer also appreciates the well-written nature of the article and the helpfulness of the experiments and figures. The final edit explicitly states the score was raised, indicating a more positive overall assessment.",
            "The reviewer expresses confusion about the paper's contribution and the significance of its results. They highlight a gap between the abstract's claim and the main theorems, criticize the strength of the assumptions, and question the validity of the numerical results. Phrases like \"I cannot understand\", \"it is not guaranteed\", \"seem to be strong\", and \"limited results\" indicate a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Neutral",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'Thank you,' 'I understood,' 'makes sense to me,' and 'will be interesting as a guideline,' which convey a supportive and encouraging tone. They also acknowledge the value of the work even while admitting they haven't fully verified all details ('I have not yet fully followed the details of the proofs, but if these are correct...').",
            "The language is polite and professional ('Thank you for your valuable comments'), but lacks strong positive or negative indicators. The statement is factual and procedural.",
            "The reviewer uses appreciative language ('I thank the authors') and acknowledges the authors' efforts positively, suggesting a supportive stance.",
            "The review points out several weaknesses and areas for improvement, including clarity issues, lack of explanation for certain elements, and errors in the bibliography. Phrases like \"hard to follow\", \"not very well explained\", and \"many mistakes\" contribute to the critical tone.",
            "The review presents both positive aspects (interesting theorems, theoretical guarantees) and limitations (special cases, difficulty of evaluation in general cases). The reviewer also expresses some uncertainty about their understanding of the technical details, indicating a balanced perspective.",
            "The review provides both positive feedback (e.g., \"well written and easy to follow\", \"results are clearly stated\") and constructive criticism (e.g., concerns about the experiments, suggestions for improvement in clarity and organization). The tone is generally respectful and objective, focusing on the strengths and weaknesses of the paper.",
            "The review adopts a critical tone by directly pointing out weaknesses in the paper's logic, assumptions, and experimental setup. The reviewer uses phrases like \"The assumptions in the main theorems seem to be strong,\" \"There is a gap between the claim in the abstract and the main theorems,\" and \"I do not know that numerical results in section 4 are appropriate\" to express their concerns and critique the paper's methodology and presentation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses a coherent and consistent understanding of the paper's focus on a continuous limit model and its potential value for future research, without any contradictory statements.",
            "The review is consistent as it expresses a logical flow of actions: acknowledging the revised manuscript and stating the need for time to review it properly. There are no contradictory statements within the review.",
            "The review is consistent because the reviewer appreciates the authors' response and updates, and maintains the same score, indicating a stable evaluation of the paper's quality.",
            "The review is consistent because the reviewer acknowledges the theoretical contribution as a strength while pointing out weaknesses related to clarity, practical details, and presentation. The reviewer's limitations in expertise are stated clearly and do not contradict the overall assessment. The weaknesses are constructive suggestions for improvement.",
            "The reviewer consistently appreciates the theoretical contributions of the paper, highlighting the importance and interestingness of the theorems. While acknowledging limitations such as the scope of validity and the toy nature of numerical experiments, and raising questions for clarification, the reviewer's overall assessment remains consistent in recognizing the value of the theoretical guarantees provided by the paper.",
            "The review presents a balanced assessment, highlighting both strengths (theoretical contributions, clarity, writing) and weaknesses (experiments, motivation, practical implications).  The reviewer provides constructive criticism and suggestions for improvement, while acknowledging the value of the work. The final edit indicating a score increase suggests an overall positive, or at least improved, evaluation despite the identified weaknesses, indicating a consistent perspective.",
            "The review consistently points out the gap between the abstract's claim and the main theorems, questioning whether the theorems actually support the claim of minimizing Wasserstein distance. It repeatedly emphasizes the strong assumptions required for the theorems and expresses concerns about the practical significance and contribution of the paper due to these issues."
        ]
    },
    {
        "paper_id": "iclr_2019_r1xFE3Rqt7",
        "paper_title": "Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling",
        "paper_abstract": "Modern deep neural networks have a large amount of weights, which make them difficult to deploy on computation constrained devices such as mobile phones. One common approach to reduce the model size and computational cost is to use low-rank factorization to approximate a weight matrix. However, performing standard low-rank factorization with a small rank can hurt the model expressiveness and significantly decrease the performance. In this work, we propose to use a mixture of multiple low-rank factorizations to model a large weight matrix, and the mixture coefficients are computed dynamically depending on its input. We demonstrate the effectiveness of the proposed approach on both language modeling and image classification tasks. Experiments show that our method not only improves the computation efficiency but also maintains (sometimes outperforms) its accuracy compared with the full-rank counterparts.",
        "review_ids": [
            "Syg3sevJJ4",
            "SygF_SIkkV",
            "B1g0zh_7hQ",
            "ryxVHego0Q",
            "SJxCVNotR7",
            "rylcGdES6m",
            "r1lHGXH0hX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "A list of previous publications is missing in the references. Those publications inspired and advanced the low-rank baseline (M. Jaderberg et al 2014) mentioned in this paper. Please consider to cite them. \n\nM. Denil, B. Shakibi, L. Dinh, M. A. Ranzato, and N. de Freitas. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems (NIPS). 2013.\n\nE. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014.\n\nV. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In International Conference on Learning Representations (ICLR), 2015.\n\nY. Ioannou, D. P. Robertson, J. Shotton, R. Cipolla, and A. Criminisi. Training cnns with low-rank filters for efficient image classification. In International Conference on Learning Representations (ICLR), 2016.\n\nY.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. In International Conference on Learning Representations (ICLR), 2016.\n\nC. Tai, T. Xiao, X. Wang, and W. E. Convolutional neural networks with low-rank regularization. In International Conference on Learning Representations (ICLR), 2016.\n\nX. Zhang, J. Zou, K. He, and J. Sun. Accelerating very deep convolutional networks for classification and detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(10):1943\u20131955, Oct 2016. \n\nWen, Wei, Cong Xu, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. \"Coordinating filters for faster deep neural networks.\" In The IEEE International Conference on Computer Vision (ICCV). 2017.",
            "Dear Authors and Reviewers,\n\nGiven the disagreements between the reviewers, I took a careful read of the key technical parts of your paper. I have a couple of questions:\n\n1) Is the proposed adaptive mixture of low-rank factorization network trained end-to-end, or trained by approximating Wh, where W is pre-trained?\n\n2) One may consider that using a linear low-rank factorization as a bottleneck layer is not changing the network architecture (in particular, depth of the network, since U(Vh) is just used to approximate Wh for lower computation). In your case, however, I would analogize it to be U*PI(h)*V*h, where the block structured PI(h)=diag[pi(h)_1,...,pi(h)_1,pi(h)_2,...,pi(h)_2,...pi(h)_K] involves a nonlinear transformation of h; from this perspective, your network architecture has actually been modified (i.e., no longer for reducing computation of the same structured network), where a linearly transformed h (i.e. V*h) is reweighed by a nonlinearly transformed h and then linearly transformed by U before sending it to the next layer. The computation saving mainly comes from the fact the unique number of elements in the block structured PI(h) is K. My questions: (1) has anyone tried similar types of architecture but does not impose PI(h) to have a block structure? (2) Is it really that fair to compare the original network with this modified network that effectively has more nonlinear layers? Could you choose deeper networks with similar computation that are trained end-to-end, and compare their performance? Without such type of comparison, I am somewhat hesitate to accept the claims. \n\n3) (As Reviewer 4 also pointed out) Error bars were added to none of the figures and tables, making it difficult to judge whether 1.x% or 2.x% improvement is statistically significant.\n\nThanks,\nAC ",
            "In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness.\nThe paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network.\nThe authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach.\nI enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well.\nThe proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn\u2019t understand very well its implications on the expressiveness of proposed method against classical low-rank approach.\n",
            "Thanks for the positive clarifications, especially appendix D on how to merge computation segments into two matrix-vector multiplications. A add-on, the equation should be able to generalize to K<d, where the mixing coefficients just need to be duplicated and tiled in a typical way.\n\nOn the speed when rank ratio is 1/2: \n1) \"low-rank ratio\" is not clearly defined. It is misleading to refer to 2d/(m+n) as \"low-rank ratio\". Conventionally, full rank is min(m, n) and a more reasonable definition of rank ratio is d/min(m, n).\n2) the dimension is 1024x1024 square in Table 6, where the FLOPs is the same but wall-clock time increases, validating my previous arguments. \n3) the dimension was 650x1500 in a comment but changes to 1024x1024 in the appendix. It may be more reasonable to pick up a dimension used in the benchmarks of CNNs/LSTMs? When converging to a conclusion \"With similar speedup, the proposed adaptive lowrank\nfactorization provides better theoretical expressiveness and accuracy/perplexity improvement in practice\", the speed should be measured for dimensions in Table 2 for example.\n\nTypo:\n\"rand-d\" in the caption of table 1.",
            "Thanks for feedbacks! I remain my first rating as no new but helpful info provided to solve the concerns, because of following reasons:\n\n-- Can you clarify what is \"weighting bottleneck\"? The computation of mixing weight is fine. The explanation regarding \"mixing weight\" and \"weighting bottleneck\" is unrelated. Concretely, in Figure 3(b), although one low-rank branch can be done using two matrix-vector multiplications, but there are K branches. The computation segments are also clearly shown in Eq. (1). The \"computation efficiency\" problem is further supported by the table kindly provided by the author(s): \n-----------------------------------------------------------------\n  Method      |      low-rank ratio; time in ms\n-----------------------------------------------------------------\n                      |    1   |  1/2  |  1/4  | 1/8 | 1/16 \n-----------------------------------------------------------------\nFull  Rank     | 10.8 | N/A  | N/A | N/A | N/A\nregular LR    | N/A  | 13.1 | 6.6  | 3.3   | 2.0\nadaptive LR  | N/A | 16.5  | 8.6  | 4.5   | 2.8\n-----------------------------------------------------------------\nIn regular LR, breaking one matrix to two matrices even increases wall-clock time from 10.8ms to 13.1ms when the ratio is 1/2, proving the disadvantage generated by splitting a bulk of computation to segments; in adaptive LR, breaking it to 2*K consistently increases wall-clock time. \nI agree the problem may be alleviated with optimization efforts on library or hardware, then it is unclear how good/worse will it be when compared with fine-grain pruning solutions (Han et al. 2015b & Han et al. 2016), which achieved a higher FLOP reduction.\n\n-- I agree that mixing coefficients are from non-linear neural networks, while the mixing step is linear once coefficients are known. The only difference is the coefficients are \"dynamic and data dependent, making it some sense of non-linear combination\".\n\n-- FLOPs in the output layer are missing when reporting computation reduction. SVD-softmax will deteriorate/compress the model capacity, making compression in LSTMs more challenging. \n",
            "The paper proposes an input-dependent low rank approximations of weight matrices in neural nets with a goal of accelerating inference. Instead of decomposing a matrix (a layer) to a low rank space like previous work did, the paper proposes to use a linear combination/mixture of multiple low rank spaces. The linear combination is dynamic and data dependent, making it some sense of non-linear combination. The paper is interesting, however,\nI doubt its significance at three aspects:\n(1) computation efficiency: the primary motivation of this paper is to accelerate inference stage; however, it might not be wise to break computation in a single low-rank space to segments in multiple low-rank spaces. In the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together. Because of this, the primary motivation in this paper wasn't successfully supported by wall-clock time;\n(2) low-rank approximation: low-rank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?). Because of this, low-rank neural nets [1][2] start from trained models, approximate it and fine-tune it, while this method trains from scratch without an approximation target. Although, we can fit the method to approximate trained matrices, then decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent). Therefore, I view this paper more in a research line of designing compact neural nets, which brings me to a concern in (3).\n(3) efficient architecture design: essentially, the paper proposes a class of compact neural nets, at each layer of which there are K \"low-rank\" branches with a gating mechanism to select those branches. However, branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. \n \nClarity:\nHow FLOPs reduction are exactly calculated? I am not convinced by FLOPs reduction in the LSTM experiments, since in LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper.\n\nImprovement:\n(1) Accuracy improvement in Table 1 is not statistically significant, but used more parameters. For example, an improvement of 93.01% over 92.92% is within an effect of training noise;\n(2) It is a little hacking to conclude that a random matrix  P_random has a small storage size because we can storage a seed for recovery. When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation;\n(3) sparse gating of low-rank branches may make this method more computation efficient.\n\n[1] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014.\n[2] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014.\n[3] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. \"Going deeper with convolutions.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9. 2015.\n[4] Huang, Gao, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. \"Multi-scale dense networks for resource efficient image classification.\" arXiv preprint arXiv:1703.09844 (2017).",
            "Some suggested improvements follow below\n\n1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected.\n2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN.\n3. In (1), the dimensions of U^k and V^k should be mentioned explicitly.\n4. The choice of \u201ck\u201d in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN?\n5. The paper addresses low rank factorization for \u201cMLP\u201d, RNN/LSTM and \u201cpointwise\u201d convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward.\n6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer?\n7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?"
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Positive",
            "Neutral",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review points out a missing citation and suggests adding relevant references. It is a constructive suggestion for improvement rather than a critique or praise.",
            "The review expresses hesitation to accept the claims made in the paper due to concerns about the comparison methodology and lack of statistical significance indicators. The reviewer also points out a concern raised by another reviewer, indicating a consensus on a weakness in the paper.",
            "The reviewer states they 'enjoyed reading the paper' and describes the paper as having a 'nice toy example' and an 'intuitive line of reasoning'. The reviewer also acknowledges the 'extensive experimental results'.",
            "The review expresses both positive acknowledgment (\"Thanks for the positive clarifications\") and critical feedback regarding unclear definitions, misleading references, inconsistencies, and typos. This mixed feedback results in an overall neutral sentiment.",
            "The reviewer states they are maintaining their initial rating due to a lack of new information addressing their concerns. They raise specific issues with the paper's claims regarding \"weighting bottleneck,\" computation efficiency, and FLOPs, indicating dissatisfaction with the authors' arguments and presentation.",
            "The reviewer expresses doubt about the significance of the paper in three aspects: computation efficiency, low-rank approximation, and efficient architecture design. They also point out issues with clarity and improvements, suggesting a critical assessment of the work.",
            "The review points out inconsistencies in the results, questions the methodology, and identifies potential limitations of the proposed approach. The language is critical and suggests areas where the paper is lacking or unclear."
        ],
        "tone": [
            "Formal",
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses formal language and avoids colloquialisms. The request is phrased politely and professionally ('Please consider to cite them').",
            "The reviewer uses questioning and challenging statements such as \"Is it really that fair to compare...\", \"I am somewhat hesitate to accept the claims\", and \"Without such type of comparison...\", indicating a critical assessment of the paper's methodology and claims.",
            "The reviewer uses positive language such as 'nicely introduces', 'good proof of concept', 'enjoyed reading', and 'intuitive line of reasoning'. While they point out a weakness, it's done constructively.",
            "The review balances appreciation for clarifications with critical points regarding clarity, consistency, and accuracy. The reviewer uses specific examples and suggestions, indicating a constructive approach. Phrases like \"It is misleading to refer to...\" and \"It may be more reasonable to pick up a dimension used in the benchmarks...\" demonstrate a critical yet balanced tone.",
            "The review employs a critical tone by directly challenging the authors' claims with phrases such as \"Can you clarify,\" \"The explanation regarding...is unrelated,\" and \"proving the disadvantage.\" The reviewer also points out missing information (FLOPs in the output layer) and questions the effectiveness of the proposed method compared to existing solutions (fine-grain pruning).",
            "The review uses phrases like 'I doubt its significance,' 'wasn't successfully supported,' 'only makes sense when matrices are known and redundant, otherwise, no approximation target exists,' 'a little hacking,' and 'I am not convinced,' indicating a critical and questioning tone. The reviewer also provides specific criticisms of the methodology, results, and comparisons with other work.",
            "The review uses phrases like \"should be corrected,\" \"should include,\" \"should be discussed,\" indicating a critical assessment of the paper's content and presentation. The reviewer also points out potential issues and limitations, highlighting areas where the paper needs improvement."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review clearly identifies a missing list of relevant publications related to the low-rank baseline and suggests adding them to the references. There are no contradictory statements or inconsistencies within the review.",
            "The review is consistent in its critique, focusing on the methodology, experimental setup, and the validity of the claims. The questions and concerns raised are logically connected and do not contradict each other.",
            "The review is consistent because the reviewer provides both positive feedback (enjoyed reading, nice introduction, good proof of concept, extensive experimental results, intuitive reasoning) and constructive criticism (lack of strong theoretical founding, difficulty understanding the theoretical result in the appendix). These points are not contradictory and represent a balanced assessment of the paper's strengths and weaknesses.",
            "The review provides specific and consistent feedback, focusing on areas for improvement such as clarity of definitions, consistency in experimental setup dimensions, and minor corrections like typos. The reviewer's points are logically connected and aim to enhance the paper's quality without internal contradictions.",
            "The review is consistent because it maintains a critical stance on the computational efficiency of the proposed method throughout the text. The reviewer consistently questions the 'weighting bottleneck' and argues that splitting computation into segments, even with low-rank approximations, can increase wall-clock time, as supported by the provided table. While acknowledging some aspects of the method, the reviewer's core concern about computational efficiency and comparison to other methods like pruning remains consistent, without any self-contradictory statements.",
            "The reviewer consistently raises concerns about the computational efficiency and novelty of the proposed method. The arguments are logically connected and build upon each other, without any self-contradiction. The reviewer questions the claimed benefits and suggests improvements based on these concerns.",
            "The review is consistent as it provides a list of specific, actionable suggestions for improvement and questions for clarification. Each point is focused on enhancing the paper's clarity, accuracy, and completeness without any contradictory statements or conflicting feedback."
        ]
    },
    {
        "paper_id": "iclr_2020_BJlXgkHYvS",
        "paper_title": "Information-Theoretic Local Minima Characterization and Regularization",
        "paper_abstract": "Recent advances in deep learning theory have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering properties of good local minima or developing regularization techniques to induce good local minima, no approach exists that can tackle both problems. We achieve these two goals successfully in a unified manner. Specifically, based on the Fisher information we propose a metric both strongly indicative of generalizability of local minima and effectively applied as a practical regularizer. We provide theoretical analysis including a generalization bound and empirically demonstrate the success of our approach in both capturing and improving the generalizability of DNNs. Experiments are performed on CIFAR-10 and CIFAR-100 for various network architectures.",
        "review_ids": [
            "H1xq1PY2tr",
            "BJx7S-pojr",
            "r1xAnokijS",
            "rygL3EBFiH",
            "Bke1Ojf_jH",
            "SkxtuOMdir",
            "H1l91QkdsS",
            "rJgVQyi9FB",
            "HJeaomh79S"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Post-rebuttal update: I have just noticed the authors modified their summary post below and claimed \"[my concerns] are all minor or resolved\". This is not true. Here is my summary of unresolved concerns written after the discussion period.\n\nThis work has been substantially improved during the rebuttal process, and some of my concerns are addressed. But there are still major issues, as raised in my [last comment]( https://openreview.net/forum?id=BJlXgkHYvS&noteId=r1xAnokijS ), that remains unanswered. Specifically,\n\n(A) the relation between this work and information theory\n\nIn the revision, the authors have make it very clear that the relation between FIA and their proposed regularized objective is very vague, relying on the crude approximation of expected Fisher information with observed Fisher information. Therefore the \"information-theoretic\" part in the title seems awkward and to some extent, misleading.\n\nAs Reviewer 1 has pointed out, it would have been better if the authors relate their theory and method to the observed FIM, instead of information theory, from the beginning. Since the observed FIM and the neural tangent kernel (NTK) share the same eigenspectrum, it would also be interesting to relate this work to the NTK.\n\n(B) the different behavior of the proposed regularization (log det(I)) and its bound that is actually implemented (log tr(I))\n\nThis is the more important issue. My concern is that the observed FIM (or the NTK) is known to have fast decaying spectrum; (Karakida et al) has shown empirically that the decay can be exponential. Thus log det(I) would be dominated by the long tail (since after taking logarithm it is the sum (or average) of an arithmetic sequence), while log tr(I) would be dominated by the first largest few values. \n\nThe authors claim that this is not an issue since they replaced the observed FIM with a subsampled, low-rank (<=10), version. It corresponds to consider a small submatrix of (the gram matrix of) the NTK. Denote this matrix as .\n(a) This does not help with the problem, since we now have no chance of recovering the smaller eigenvalues that would have dominated log(det(I)), and it is impossible that the proposed regularizer has a similar behavior to log(det(I)).\n(b) One could verify easily, using small feed-forward networks (or even simpler, computing a gram matrix using RBF kernels, since the FIM shares its eigenspectrum with NTK which is a p.d. kernel), that the new matrix  still has a fast-decaying eigenspectrum, so the behavior of  and \\tilde{I} are still significantly different, even though this cannot be established by concentration bounds as the authors argue. While FFN and modern deep architectures can have different behaviors, I believe the above evidence suggests that a numerical experiment comparing the behavior of the two bounds is a must.\n\nFollowing this argument we can see *another issue* of this work, namely the proposed generalization bound will be vacuous given the fast-decaying spectrum of the FIM, since it contains gamma=log(det(I)).\n\nReviewer 1 mentioned this work could enlighten future discussions on this subject. While I agree this paper presents interesting empirical observations (namely its final algorithm, which is vaguely connected to the proposed objective, leads to improved performance on CV tasks), I think this submission in its current form is a bit too misleading to serve this purpose well, and overall I believe it would be better to go through another round of revision.\n\n\nOriginal Review\n============================================\n\nThis paper presents a generalization bound based on Fisher information at a local optima, and proposes to optimize (an approximation to) it to get better generalization guarantees. There are issues in both parts, and I don't think it should be accepted. Specifically,\n\n1. The definition of Fisher information is incorrect (for almost every parameter). The expectation should be taken w.r.t the model distribution p(c_x|x;w), instead of the data distribution S. \n2. Assumption (1) (loss locally quadratic) is not reasonable for DNNs, since local optimas will not be unique in their neighborhoods. See e.g. Section 12.2.2, \"Information Geometry and Its Applications\".\n3. Regarding the approximation to the bound, approximating log det(I) with log trace(I) is not a good idea: adding a very small eigenvalue will lead to noticeable change in the former, but negligible change in the latter. This is particularly problematic for DNNs, since the spectrum of their Fisher information matrix varies in a wide range: see \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach\". \n\n(Edit 11.8:\n* regarding point (1), there is a quantity called observed Fisher information in e.g. Grunwald (2007) that coincide with Eq (1) in the paper, but it is a function of the dataset instead of the model parameter, and can only used to study model parameters near the *global optima* (as it is applied in Grunwald (2007)); it cannot help with choosing between different local optimas as this work claims. Additionally, the FIA criterion, which is used in this paper to devleop the generalization bound, is defined using the standard form of Fisher information (i.e. taking expectation w.r.t model distribution), see Rissanen (1996). These facts lead me to believe this is a confusion on the authors' part.\n* in point (2) I was referring to the authors' argument \" Since L(S,w) is analytic and w_0 is the *only local minimum* of L(S,w) in M(w_0)\", which is incorrect.)",
            "Thank you for the revision, which has addressed most of my comments and substantially enhanced the presentation.\n\nI am writing here some further remarks here for the next round of revision or for potential future works.\n\nFor the definition of the observed Fisher information, the authors may consider cite \" Efron & Hinkley. Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher Information. 1978\".\n\nSomewhere around 5.1.1 (in the revised draft), it has to be mentioned the volume element defined by the Fisher information (including the observed information) that is \\sqrt{\\vert{I}(\\theta)\\vert}d\\theta, and the total volume, the exponential of the last term of the FIA, is invariant to reparameterization. In the FIA, w_0 should be the maximum likelihood estimation (that means the global optimum).\n\nIn the writing, it may help to emphasize that 5.1.1 is only a rough approximation to show the relationship between the proposed \\gamma quantity and the FIA, as the approximation is rough and the quality is not guaranteed, and the theoretical guarantee is given by 5.2.  Actually, the authors may also consider approximate \\gamma based on Balasubramanian's book chapter \"MDL, Bayesian Inference and the Geometry of the Space of Probability Distributions\", where there is an explicit term of the log determinant of the observed Fisher information.\n\nRegarding the bound in section 6. It is better to have some remarks and/or numerical simulations (e.g. in the appendix) on the tightness of the bound so that the reader can have some intuitions. A potential extension is a variational bound (with free parameters).\n\nOverall, I don't think this paper in its current form has any major flaws and the contribution is valid with both a theorem and empirical results. It should be interesting to the ICLR community and could enlighten discussions. ",
            "It appears that there are still some misunderstanding (e.g. Q2 was never my question), as well as unaddressed concerns. Let me clarify.\n\n\n(A) Relation between FIA and your regularized objective\n=======================================================\n\nLet me re-state my concern: The FIA criterion is defined using the expected FIM; but at the end of Section 5.1.1, it is replaced with the observed FIM. This is only valid when the conditional distribution defined by the model, $p(y|x;w_0)$, coincides with the conditional data distribution $p_{data}(y|x)$ (or its smoothed version, in your setup). \n\nIt is true that expected and observed FIM do not always coincide, even at global minima. However it is reasonable to consider the asymptotic setup, which is also what you mentioned in the previous draft. In this case, the replacement can only be valid when $w_0$ is a global minima in the entire parameter space, not only in the neighborhood $\\mathcal{M}(w_0)$; in the finite sample case, it becomes even less clear when (or if) your replacement is valid or reasonable.\n\nIn the revised version you explicitly mentioned the replacement of $\\mathcal{I}_w$ with $\\mathcal{I}$ (which is not because of any \"asymptotic equivalence\"). It is certainly helpful for clarification, but the replacement itself is still not well-justified, outside the global optimas that could potentially be justified by an asymptotic argument; so is the *connection* of your work to FIA. \n\n\n(B) On the connection between log|I|/W and log(tr(I)/W)\n=======================================================\n\nUnfortunately I cannot follow your argument in this part. Most importantly, \n\n> By Theorem 3, the trace of the submatrix version is a ``sub-sampled'' version which is quite unlikely to have extremely large eigenvalues\n\nWhen the eigenvalues vary in a wide range, the bounds in Theorem 3 merely becomes vacuous. It does not indicate the eigenvalue of the principal submatrices are uniformly (in any sense) distributed in that large bound.\n\nFurthermore, from the first equation in Page 6 you appear to be randomly subsampling a Gram matrix (of the neural tangent kernel). In this case standard concentration results (e.g. Proposition 10, Rosasco et al, 2010) imply the eigenvalue of your submatrix converges to the *largest* eigenvalues of the Gram matrix. \n\n> We understand the reviewer\u2019s concern that large eigenvalues have a larger impact on tr(I)\n\nThe equally important part is the tail that decays exponentially. It would dominate log(det(I)) and have vanishing impact on log(tr(I)). For this reason, one would expect the behavior of the two regularizers are sharply different.\n\nFinally, I can't follow the following sentence:\n\n> As most modern network architectures are over-parameterized, we believe that in practice the difference between using tr(I) in the proposed regularizer instead of using the intractable det(I) is not critical\n\nThis is actually exacerbated by over-parameterization, as theoretical results like (Karakida et al) only work in the over-parameterized scheme.\n\n\nMinor Points\n============\n\n* It is not clear from Section 4 that you focused on global optimas. The original words are\n> Label smoothing enables us to assume a local minimum w0 (in this case, also a global minimum) of the training loss with [sum KL] = 0.\nTo me, it only appears that you are using label smoothing to make sure useful local minimas exist. Please consider revising.\n\n* I fail to see the need to mention the efficiency of MLE. Efficiency is about the asymptotic variance, not recovery of the true conditional distribution in any finite-sample case. \n\n\nReferences\n==========\n\nRosasco, Belkin and De Vito, On Learning with Integral Operators, JMLR 11 (2010)",
            "Thanks for your reply. I agree there is a discrepancy about the definition of FIM initially; however, I do not find part (II) and (III) of your response convincing for the reasons below:\n\n=============================================================\n\n(A) The role of Information Theory, and confusions in Section 5.1\n\nIn part (II) you did not address my concern, namely the FIA is defined using the expected FIM instead of the observed FIM, and it would be \"confusing, irrelavent to the rest of the paper, and cannot be used to justify the work.\" To be specific, here is one place where the introduction of FIA created confusion:\n\n1. Just above Sec 5.2, you defined $\\gamma(w_0)$ as $\\log|\\mathcal{I}(w_0)|$, which is \"undefined unless $|I(w_0)|\\ne 0$\". You then claims (Pennington and Worah, 2018) showed $I(w_0)$ is generally non-singular. However, at the second line on Page 3, Pennington and Worah stated that the matrix they studied \"is equal to the Fisher information matrix of the model distribution with respect to its parameters\". So it is clear they are studying the expected FIM $I_w$, not the observed FIM $I$ you claims.\n\n2. You might argue that $w_0$ in Sec 5.1 only refers to the global minimum, as it is defined in Sec 4.1. However, you mentioned Theorem 1 applies to \"*a* local minimum $w_0$\", i.e. *any* local minimum; yet in the bound $\\gamma(w_0)$ appeared. As $\\gamma$ is only defined in Sec 5.1, to readers it would appear the discussions about the validity of $\\gamma$ applies here, which cannot be true, as I've argued in point (1). \nBesides, Theorem 1 must apply to any local minima, since it is about model parameter selection, not hyperparameter selection. Restricting it to the global minima would cut its link to the regularization method you developed below.\n\n3. Now that $w_0$ may refer to any local minima, the statement in Sec 5.1 that \"the last term [of FIA] becomes $ln V + ln \\sqrt{|I(w_0)|}$\" becomes misleading: It is only true for the global minimum where observed and expected FIM coincide. \n\nFor this reasons I believe this section, as well as Sec 5.2, must be revised.\n\n=============================================================\n\n(B) Whether the underlying behavior of the proposed regularizer is as expected\n\nYou claim\n> Secondly, the reviewer argues that \u201cdet(I) focusing on the average eigenvalue yet tr(I) focusing on the few largest ones\u201d. This is not the case since at a local minimum tr(I) is the L1 norm of the eigenvalues of I,\n\nIndeed tr(I) is the L1 norm of the eigenvalues. But my point has been that as the eigenvalues decay very quickly, thus the average is dominated by the first few ones: consider the global optima $w_0$, where the observed FIM and the expected coincide. As empirically shown in Fig 1 in (Karakida et al, AISTATS 2019), eigenvalue of the FIM decays exponentially. In this case, log(tr(I)) will be determined by the largest few values (a small and constant number of them, to be precise), while log(det(I)) would be dominated by the large number of small values.\n\nAnother issue is that, as I've pointed out in (A), it appears that you do not have any justification about the validity of regularizer $log|\\mathcal{I}(w_0)|$, at any $w_0$ that is not a global optima; your justification in Sec 5.1 only applies to $\\log|\\mathcal{I}_w(w_0)|$. \n\nFor these reasons, I strongly believe the numerical experiment, as well as additional references justifying the use of $\\log|\\mathcal{I}|$ instead of $\\log|\\mathcal{I}_w|$, are needed.\n\n=============================================================\n\n(C)\n\nFinally, you claim there is no ambiguity in the notations chosen in Sec 5.1. In (A) I've given an example where ambiguity appears.",
            "Q7: \u201cRegarding the approximation to the bound, approximating log det(I) with log trace(I) is not a good idea.\u201d\n\nI was referring to the process of *bounding* log det(I) with log trace(I), after appropriate re-scalings, since following my argument, it may not lead to informative gradients. Of course this is a upper bound following Jensen's inequality, but this mere fact would not be a sufficient justification - otherwise we would be training variational autoencoders without using encoders at all, instead relying on the variational bound using the prior p(z). The fact that your proposed regularization works could very possibly be attributed to other factors; it might even be superior to the true regularizer (log det(I)) since they have such different behaviors, with log det(I) focusing on the average eigenvalue and log tr(I) focusing on the a few largest ones. This is particularly problematic, since, as I've mentioned in the original comment, the eigenvalues of the FIM varies in a wide range.\n\nWe will need a link between them since your theory is about the original one. If a convincing argument is to be made, I would recommend to look at toy networks where calculating log det(I) is possible, as well as toy, potentially 1D, datasets, and compare the behavior of the two regularizers.",
            "Indeed by model distribution I meant the conditional data distribution p(y|x;\\theta)p(x), not any distribution over the model parameters, which we don't have any to begin with. Still, I'm not completely certain this is a \"nonstandard notation\" - a Google search produces the following paper using the same notion: arxiv:1905.12558, arxiv:1906.07774; and in any case, I've made it clear from my first comment that I was referring to this distribution (\"the model distribution p(c_x|x;w)\", which, to be fully clear, refers to the joint distribution p(c_x|x;w)p(x)).\n\nAlso, note that in Section 5.1 you used the expected Fisher information and referred to it using the same notation, which is confusing, irrelavent to the rest of the paper, and cannot be used to justify the work. You also stated that you are approximating it in the end of Section 5.1. By \"lose the information theoretic interpretation\" I was also referring to this part, since this is the only place where \"information theory\" appeared in the entire paper. In this regard, I believe the \"information theoretic\" part in the title as well as this section should be revised / removed to clarify this.",
            "Thank you for your response. I will upload a detailed response in 1-2 days regarding all aspects in your rebuttal, but I'd like to point out your response regarding the definition of Fisher information is simply incorrect.\n\n> the Fisher information, no matter the observed *or the expected one*, has nothing to do with expectation w.r.t. the model distribution\n\nSee e.g.\n* (Karakida et al, 2018), below Eqn (1):\n\"The expectation E[\u00b7] is taken over the input-output pairs (x, y) of the joint distribution p(x, y; \u03b8).\"\n* (Gr\u00fcnwald, 2007), Eqn (4.18) (regarding the expected FIM), where there is a subscript of theta in the expectation; this is further clarified in Eq 18.48 (expected FIM, equivalent to Eq 4.18 assuming suitable differentiability), where you can see the model density appears in the integral.\n* Eq 10.10 and 10.11, \"All of Statistics: a Concise Course in Statistical Inference\", electronic version from https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf , where, again, the model density appears in the integral.\n\nWhile it may be valid to formally use the observed FIM as a model selection criteria, it will lose the information theoretic interpretation as I've argued, and would be hugely misleading in my opinion. I would also need to re-check the proof of your results.\n\nReferences:\n\nKarakida et al: Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach",
            "This paper contributes to the deep learning generalization theory, mainly from the theoretical perspective with experimental verifications. The key proposition is given by the unnumbered simple equation in the middle of page 4 (please number it), where \\mathcal{I} is the Fisher information matrix. According to the authors, this simple metric, which is the log-determinant of the Fisher information matrix, can characterize the generalization of a DNN.\n\nRemarkably, this piece of work is well written in terms of English and formulations, and complete, with a rigorous theoretical analysis (section 5.1, 5.2), practical approximations (section 5.3) and empirical verifications (section 6).\n\nOn the theoretical side, this work builds upon Rissanen's formulation of the MDL principle, which has two parts (describing data given the model as well as the model complexity). Under rough approximations, the complexity term becomes the log-determinant of the Fisher information matrix evaluated at the local (global) optimum. This simple approximation is further proved to upper-bounds the generalization error as stated in theorem 1.\n\nTo make the criterion to be practically useful, the author used the Jensen inequality so that the metric simply depends on the trace of the Fisher information matrix.\n\nThe empirical study showed the usefulness of the proposed metric which can well approximate the testing error and a regularization term (based on the trace of the Fisher information matrix) that can improve generalization on real DNN experiments.\n\nThe reviewer has the following minor comments to further improve this contribution:\n\nsection 5.1, explain the abbreviation FIA\n\nRegarding the choice of the neighborhood \\mathcal{M}(w_0), what is the reason to define the model (neighbourhood of w_0) based on the loss? Why not simply take a coordinate neighborhood?\n\nAccording to your metric, the smaller the scale of the Fisher information matrix, the better the generalization. In section 5.1, there has to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization.\n\nAs this contribution is related to the spectral properties of the Fisher information matrix, the reviewer points the authors to \"Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. Karakida et al. 2018.\" and \"Lightlike Neuromanifolds, Occam's Razor and Deep Learning. Sun and Nielsen. 2019\", which deals with asymptotic cases and have similar MDL formulations expressed in terms of the spectrum of the Fisher information matrix.\n",
            "This paper provides a metric to characterize local minima of deep network loss landscapes based on the Fisher information matrix of the model parameterized by the deep network. The authors connect the Fisher information to the curvature of the loss landscape (the loss considered is the negative loss likelihood) and obtain generalization bounds through PAC Bayes analysis. They further propose regularizing the training of deep networks using the local curvature of the loss as a regularizer. In the final experimental section of the paper, the relationship between the empirical measures and generalization is shown on a variety of networks.\n\nThis is an interesting paper, but I have a few concerns.\n\n1. The information-theoretic measure that is proposed is essentially the (log) determinant of the hessian of the loss function.  If there are degenerate eigendirections (zero eigenvalues) then the proposed measure would not be able to distinguish between minima with different numbers of degenerate directions / same number of degenerate directions but different spectral norms of the hessians. If the authors contention is that there will be no zero eigenvalues, that suggests that local minima of deep networks are all strict, isolated minima, contrary to recent work on connected solutions (See Draxler et. al. 2018, Essentially No Barriers in Neural Network Energy Landscapes, ICML 2018).\n\n2. I would like to see how the authors believe their measure deals with rescalings layer parameters in deep networks, ie the issue brought up by Dinh et. al. in \"Sharp Minima can Generalize for Deep Networks\" ICML 2017. While I can see that the log determinant is invariant, it is not clear that the proposed approximation will be invariant to rescaling of deep network layer parameters. If the parameters corresponding to the eigenvalues sampled in the approximation are rescaled, I believe the proposed measure will not be invariant.\n\n3. The experiments regarding the local minima characterization are well constructed, though some details are missing such as how the authors decided that training had converged to a local minimum. As far as regularization based on the local curvature is concerned, I would like to see some more experiments that compare the proposed technique to adagrad/adam and other techniques that purport to condition the gradient based on local curvature. It would also be interesting to see whether the regularization indeed converges to flatter minima characterized by the proposed flatness measure. Since the claim is that the regularizer gets you flatter solutions, that information is important to decide whether the proposed technique is performing as advertised.\n\nI am willing to update my score based on responses to these concerns."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses significant unresolved concerns about the paper's theoretical grounding and practical implementation. Phrases like \"major issues,\" \"misleading,\" \"vacuous,\" and the recommendation for \"another round of revision\" indicate a negative sentiment.",
            "The reviewer states that the revision has addressed most comments and substantially enhanced the presentation. They also say the paper doesn't have major flaws, the contribution is valid, and it should be interesting to the ICLR community.",
            "The reviewer expresses multiple concerns, stating they cannot follow the arguments in section (B) and highlighting misunderstandings and unaddressed concerns from previous drafts. Phrases like 'I cannot follow your argument,' 'the replacement itself is still not well-justified,' and 'I fail to see the need' indicate a negative sentiment.",
            "The reviewer expresses strong disagreement with the author's response and points out several issues with the paper, using phrases like \"I do not find part (II) and (III) of your response convincing,\" \"created confusion,\" \"misleading,\" \"must be revised,\" and \"I strongly believe the numerical experiment, as well as additional references..., are needed.\"",
            "The review expresses doubt and criticism regarding the author's approximation method. Phrases like \"not a good idea,\" \"may not lead to informative gradients,\" and \"particularly problematic\" indicate a negative sentiment.",
            "The review points out confusing notation and irrelevant content, suggesting a need for revisions and removal of certain sections. The reviewer also expresses uncertainty about the justification of the work.",
            "The reviewer directly states that the author's response regarding Fisher information is \"simply incorrect\" and expresses concern that using the observed FIM will be \"hugely misleading\". This strong disagreement and cautionary language indicate a negative sentiment.",
            "The review expresses overall positive feedback, highlighting the paper's well-written nature, rigorous theoretical analysis, practical approximations, and empirical verifications. Phrases like 'Remarkably, this piece of work is well written' and 'the empirical study showed the usefulness of the proposed metric' indicate a positive sentiment.",
            "The review expresses interest in the paper (\"This is an interesting paper\") but also raises several concerns about the proposed method and experimental validation. The reviewer is willing to update their score based on responses, indicating a neutral stance."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a critical tone by explicitly pointing out flaws in the paper's methodology and theoretical justifications. The reviewer uses phrases like \"incorrect definition,\" \"not reasonable,\" \"not a good idea,\" and \"misleading\" to express their criticism. The detailed explanations of the issues (A) and (B) further contribute to the critical tone.",
            "The reviewer uses phrases like 'Thank you for the revision,' 'it may help to emphasize,' 'It is better to have some remarks,' and 'Overall, I don't think this paper... has any major flaws.' These phrases indicate a supportive and constructive tone, aiming to help the authors improve their work rather than criticize it harshly.",
            "The review adopts a critical tone by directly challenging the validity of the authors' justifications and arguments. Phrases like 'still not well-justified,' 'merely becomes vacuous,' and 'sharply different' demonstrate a critical evaluation of the work. The reviewer also points out specific flaws in the reasoning and methodology.",
            "The review employs a critical tone, directly challenging the author's arguments and identifying flaws in their reasoning and methodology. Specific criticisms are presented with detailed explanations, demonstrating a rigorous evaluation of the work. Phrases like \"you did not address my concern,\" \"this is not the case,\" and \"it appears that you do not have any justification\" contribute to the critical tone.",
            "The tone is critical, using phrases like \"not a good idea,\" \"may not lead to informative gradients,\" \"would not be a sufficient justification,\" and suggesting the authors \"will need a link\" and \"look at toy networks\" to strengthen their argument. The reviewer challenges the validity of the approximation and requests more evidence.",
            "The reviewer uses phrases like \"confusing,\" \"irrelevant,\" and \"cannot be used to justify the work.\" They also suggest revisions and removal, indicating a critical stance.",
            "The reviewer uses phrases like \"simply incorrect\", \"hugely misleading\", and \"I would also need to re-check the proof of your results\", which express a critical evaluation of the author's work and understanding.",
            "The tone is supportive, as the reviewer acknowledges the paper's strengths and provides constructive criticism to 'further improve this contribution'. The reviewer also points the authors to related works, indicating a desire to help them strengthen their paper.",
            "The review raises specific concerns about the proposed metric, its limitations in handling degenerate eigendirections and rescalings, and the need for more comprehensive experimental comparisons. The reviewer uses phrases like \"I have a few concerns\", \"I would like to see how the authors believe\", \"some details are missing\", and \"it is not clear that\" to express their critique."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer acknowledges improvements in the revised manuscript but consistently maintains that major concerns remain unresolved, particularly regarding the misleading use of information theory and the problematic approximation of log det(I) with log tr(I). The reviewer's arguments in the post-rebuttal update are a direct continuation and elaboration of the concerns raised in the original review, demonstrating a consistent critical stance.",
            "The review is consistent because it starts by acknowledging the improvements made in the revision, then provides constructive suggestions for further improvement, and concludes with a positive overall assessment of the paper, stating that it has no major flaws and is a valid contribution. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistently critical of the paper, pointing out logical inconsistencies and lack of justification in the authors' arguments regarding the use of observed FIM instead of expected FIM, the connection between log|I|/W and log(tr(I)/W), and other minor points. The reviewer's arguments are logically connected and do not contradict each other.",
            "The review is consistent because it maintains a clear and logical line of argumentation throughout. The reviewer consistently argues against the authors' response, focusing on the discrepancy between expected and observed FIM, the validity of the regularizer, and notational ambiguities. Each point raised (A, B, C) supports the central theme of the review, which is the need for revisions in the paper due to these inconsistencies and lack of justification.",
            "The review is consistent in its critique of approximating log det(I) with log trace(I). It provides several coherent arguments against this approximation, focusing on the potential lack of informative gradients, the different behaviors of the two functions, and the need for empirical validation on toy examples. The reviewer's points logically build upon each other to support their concern about the approximation's validity in the context of the paper's theory.",
            "The review is consistent in its argumentation. The reviewer clarifies their initial point about notation, provides justification, and then points out a separate issue in the paper related to notation and interpretation of Fisher information. The reviewer's arguments are logically connected and do not contradict each other.",
            "The review is consistent because the reviewer clearly states their disagreement with the author's response regarding the definition of Fisher Information and provides multiple references to support their claim that Fisher Information is related to the expectation with respect to the model distribution. The reviewer consistently argues against the author's understanding and provides a clear line of reasoning.",
            "The review is consistently positive, praising the paper's contributions, clarity, rigor, and completeness. The reviewer highlights the strengths of the theoretical analysis, practical approximations, and empirical verifications. The comments provided are minor suggestions for improvement and do not contradict the overall positive assessment of the paper.",
            "The review is consistent as it raises valid concerns about the proposed method without contradicting itself. The reviewer points out potential limitations in the theoretical foundation and experimental validation of the paper, maintaining a critical yet constructive tone throughout the review. The concerns are logically connected and focused on improving the paper."
        ]
    },
    {
        "paper_id": "iclr_2022_p4H9QlbJvx",
        "paper_title": "Rethinking Again the Value of Network Pruning -- A Dynamical Isometry Perspective",
        "paper_abstract": "Several recent works questioned the value of inheriting weight in structured neural network pruning because they empirically found training from scratch can match or even outperform finetuning a pruned model. In this paper, we present evidences that this argument is actually \\emph{inaccurate} because of using improperly small finetuning learning rates. With larger learning rates, our results consistently suggest pruning outperforms training from scratch on multiple networks (ResNets, VGG11) and datasets (MNIST, CIFAR10, ImageNet) over most pruning ratios. To deeply understand why finetuning learning rate holds such a critical role, we examine the theoretical reason behind through the lens of \\emph{dynamical isometry}, a nice property of networks that can make the gradient signals preserve norm during propagation. Our results suggest that weight removal in pruning breaks dynamical isometry, \\emph{which fundamentally answers for the performance gap between a large finetuning LR and~a small one}. Therefore, it is necessary to recover the dynamical isometry before finetuning. In this regard, we also present a regularization-based technique to do so, which is rather simple-to-implement yet effective in dynamical isometry recovery on modern residual convolutional neural networks.",
        "review_ids": [
            "YMX00Wvatt",
            "4tkIvTULlLU",
            "bha0c__ww0O",
            "TmjV7MVWRiB",
            "qLaHLkc7gAt",
            "6sA-fgy87ZM",
            "LdBwgvSpwzf",
            "-kEDu9CDHa2"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This work takes a second look at the set of papers (Crowley et al 2018, Liu et al 2019) which claim that there is no generalization benefit offered by pruning, i.e., training a large network first and then pruning performs identically or worse than training the same pruned network architecture from scratch. This work claims that these previous works do not set learning rates during fine-tuning correctly while performing their experiments and hence these claims are invalid. The paper also offers an explanation for this phenomenon via dynamical isometery theory. 1) **Nice analysis of previous papers**: It is commendable that this paper points out issues with two previous papers (Crowley et al 2018, Liu et al 2019) who make methodological errors by not tuning the learning rate for fine-tuning, and use the results of these erroneous experiments to make general statements about the efficacy of pruning (regardless of whether these statements are true or not). However from the perspective of these previous papers, it seems they had a fixed small fine-tuning budget of 20 epochs, and did not want to exceed this as we require number of fine-tuning epochs to be much smaller than the number of training epochs. While I do think that this requirement is arbitrary, it helps understand the claims of the previous papers.\n\n2) **Unfair experimental comparison**: In Table 1, it seems that the small Resnet models (A,B) trained from scratch are trained for 120 epochs, while the best prune-finetuned models are trained for 90 epochs. However also accounting for the training of the original large Resnet-34 model, the total number of training epochs are 90+90=180 epochs. Hence it seems that the prune+finetuned models are trained overall for a larger number of epochs, which may be a confounding factor which explains their better results. I would *strongly* suggest that the experiments be run such that the number of training epochs for \"training small model from scratch\" and \"large model training with pruning and finetuning\" be equalized for a fair comparison.      \n\n3) **Unclear why hypothesis must be true**: This paper rest on the hypothesis that training a large model followed by pruning and fine-tuning offers improved generalization benefits over training a small model from scratch. In principle it should be clear that both two methods can produce models with similar generalization given sufficiently strong optimizers, as the only difference between them is the initialization of the small model. Even in practice, we often are able to train models such that the train loss is close to zero (or sufficiently small), regardless of whether we train small models from scratch or fine-tune pruned models.  \nHence given that both methods solve the same overall optimization problem (i.e, minimizing loss of the small model), I am confused regarding why one would expect pruning + fine-tuning to generalize better than training from scratch, even in principle. For example, it is clear that given infinite epochs and a suitable (well-tuned) optimizer both methods should work equally well. Is the paper's hypothesis a claim about the speed of optimization, that pruning+fine-tuning is a fast way to obtain a small model? If so, how does this change after having accounted for the training of the dense model itself (which is usually slower to train)?\n\n4) **Incorrect metric for measuring Dynamical Isometry**: In equation 1, the paper uses mean singular values as a metric for assessing dynamical isometry. However if we require that all singular values are equal to one, then the metric to use would be the sum of (squared) deviations from unity of each singular value (\\sum_i (\\Sigma_{ii} - 1)^2). In such a metric, zero corresponds to dynamical isometry being achieved. However the metric of mean singular values has no such meaning as the standard deviation can be large. I am hence confused regarding why the paper chose to use the mean JSV metric, and if there is no strong reason for doing so, I would *strongly* recommend re-running experiments with the deviations-from-unity metric.\n\n5) **Relationship between mean JSVs and convergence unclear**: Hypotheses 1-4 and the boxed explanation talk about the \"recovery\" of mean JSVs and their relationship to network convergence. I am confused regarding why the behaviour of mean JSVs during or after training is deemed meaningful. I understand that too small or too large mean JSVs must be avoided at initialization as they can cause vanishing or exploding gradients, but can increasing mean JSVs from ~1 to ~5 (as in Fig 2) be deemed meaningful? It is thus not clear why mean JSV (or any measure of dynamical isometry) can be used to measure model convergence (ref. to boxed explanation in page 7). For example, if gradient-norm regularization is used, then the model would converge and yet have small JSVs. The only measure of model convergence is the gradient norm of the loss w.r.t. weights, and other measures are meaningless unless explicitly shown otherwise.  This paper does not make a solid case for the hypothesis that \"pruning+finetuning produces models with better generalization than training small models from scratch\", as there are issues with both the experimental and the analytical studies. In the experimental setup, the comparisons are not performed fairly, while in the analytical parts use flawed metric to measure model convergence and dynamical isometry. Even fundamentally, it is unclear why the proposed hypothesis must be true in the first place. For all these reasons, I would recommend rejection.\n\nPost rebuttal update: The discussions did not change my opinion of the paper. However I appreciate the efforts taken by the authors to update the paper in light of reviewer suggestions, particularly to make the core argument more rigorous. Although I do not think the paper still succeeds at this, I am increasing my score slightly to reflect this change.",
            " I appreciate the effort taken by the authors to update the paper to incorporate reviewer comments, and I will increase my score accordingly. However I still do not feel the paper makes rigorous arguments and experiments to back its core argument regarding the effectiveness of pruning, I still cannot recommend acceptance.",
            " Thanks for experiments in Tab R3-#2 and Tab R3-#3. I found them very interesting. Based on the authors' claim, the \"value of pruning\" mainly lies in nonlinear networks, where initializing with \"pruning+strong reg\" may achieve better isometry than existing initialization methods. \n\nAs the authors later update their claim to be \"pruning will (probably) have no values\" in the linear case, the current paper arrangement becomes very confusing to me. I personally think it might be more appropriate to make \"strong reg\" the main contribution and take the linear case as a motivating observation.\n\nAlso, several new claims in Section 5 need more rigorous supports (also pointed out by other reviewers). For example, 1) more evidence on \"pruning+strong reg\" can achieve better isometry than existing initialization and pruning methods, and 2) more analysis on why \"pruning+strong reg\" achieves better isometry.\n\nWith these being said, I believe this paper has potential and is very interesting, but need further investigations on its claims. I will keep my original rating.",
            " `Weakness 1` I don't think practicality is an issue here, as we spend as much number of epochs pruning as we do training from scratch. For example, as in this paper, models are pruned for 90 epochs and the large model training itself takes 90 epochs. In any case, it is critical that the paper make the precise statement it intends to show or disprove. There is no mention of such practicality constraints in the paper as far as I can see. The hypothesis mentioned in section 2.1 is that \"Inheriting weights from a pretrained model in pruning has no value, i.e., training from scratch the small model can match (or outperform sometimes) the counterpart pruned from a big pretrained model\", which makes no statements regarding practicality.\n\nIt is also possible that the authors have misunderstood my argument. When 5 models are pruned in parallel, we would still need to compare against 180 epochs trained-from-scratch model as each such pruned model is only trained for 180 epochs each. The point is to equalize the number of optimizer iterations that these models have seen.\n\n`Weakness 2` I think the authors have misunderstood my point yet again. My point is that even if pruned models outperform models trained from scratch, under what conditions must we expect that to happen? For instance, if dynamical isometery is achieved, and one trains a model from scratch long enough, one would expect that pruned models work as well as models trained from scratch according to the ideal conditions. Again, I think one way to be clear about these is to **clearly** and **rigorously** state the hypothesis that the paper intends to prove or disprove, as claims regarding such optimizer practicality were never brought up in the original paper. Without this, one can very easily write endless follow up papers \"proving\" and \"disproving\" the same hypothesis. A clear hypothesis in your case would roughly state that \"under practical conditions such as blah blah, we expect pruning for x epochs to outperform training from scratch done for y epochs\".\n\n`Weakness 3` The argument that deviation-from-unity is not better than mean JSV is confusing. If dynamical isometery measures how close each of the spectral values are equal to one, then it is clear that deviation-from-unity measures **exactly** this phenomenon, which mean JSV misses. The authors bring up auxiliary issues that are thought to be related to the **outcomes** of dynamical isometery such as model performance and trainability, which is not the point at all. Nonetheless it is nice to see results with deviation-from-unity as well.\n\n`Weakness 4` Regarding mean JSV and convergence, my point is that there is no underlying theory connecting both concepts. The paper claims some correlations between both, but this means that high mean JSV need not imply good convergence and so on. \n\n> A good metric indeed needs more deliberation. Right now, we do not have a better alternative\n\nThere is indeed a perfectly good way to detect convergence, which I also mentioned earlier, which is to look at the loss gradients w.r.t. parameters, which must be equal to zero.",
            " Thanks for the detailed response. Unfortunately, there are some concerns still remain.\n\n1. From the empirical analysis one cannot \"define\" that the large of the mean JSV can be regarded as \"around 1\". This statement is stretching to connect to the existing DI theory. I would strongly suggest rephrasing that \"in practice networks with JSV in the range (2-5) yield good empirical performance\".\n\n2. If a network from scratch can be tuned to match or exceed the performance of a pruned network, the 'value of pruning\" becomes questionable. This also relates to the \"Rectified Argument on Value of Pruning\". It seems the main message of the paper has changed during the rebuttal phase due to the new experiments.  \n\n3. Related to the above, \"pruning has the potential to be valuable if the random initialization of scratch training cannot achieve exact isometry\" what is meant by \"exact isometry\" in this statement? \n\n4. The L2 regularization does not directly incentives JSV to be close to 1, it simply encourages the magnitude of the weights to be small. From the experiments, if JSV becomes closer to 1 with strong L2 regularization, it is merely a side-effect. Further analysis is required to understand why is such behaviour emerging.\n\n",
            "There are recent works questioning the value of inheriting weights in structured neural network pruning as it is empirically observed training from scratch can match or outperform finetuning a pruned model. This paper mainly includes three components: 1) the authors reinvestigated the problem and demonstrates that the conclusion is inaccurate because of improperly small finetuning learning rates. They show finetuning with pruned weights actually outperforms training from scratch, when larger learning rates and longer training epochs are adopted. 2)  the authors explored dynamical isometry (DI) to understand how finetuning LR affects the final performance. They show that weight pruning breaks dynamical isometry and finetuning can recover it and a larger LR can recover faster. 3) They proposed to fully recover dynamical isometry in fitler pruning before finetuning.\n The paper is well written and organized. The effect of learning rate on fine-tuning stage of pruning is not thoroughly investigated and this paper provides an in-time and thorough study. My major concerns are as follows:\n\n- Pruning as poor initialization: the concept of initialization needs to be cleared. Does the pruning in Fig 1 refer to weights pruning for both networks? It might be true if it is weights pruning as different pruning ratios correspond to the same weights dimensions with different values. However, filter pruning results in different architectures and different pruning results are not different initializations with respect to the same architecture. \n\n- The dynamic isometry might explain the easiness of optimization, however, the relationship with the generalization is not quite clear. There might be other metrics such as flatness might show a similar trend as JSV. A comparison with other metrics for generalization could be more persuasive.\n\n- The analysis with MLP-7 and MNIST might be too simple for practical usage, and this method does not generalize to more practical non-linear convolutional neural networks. The authors propose strong regularization helps, however, no explanation on why this method helps and how it connects with previous analysis. Does this method work for MLP-7 and how is it connected to DI? The paper carefully studies recent thoughts on the meaning of pruning and made some rigorous investigations. Though some issues exists and DI may not be the ultimate solution to the question, the paper provides valuable thoughts for this filed, including fair experimental comparison and new explanations. Therefore I recommend to accept.",
            "The paper argues pruning a pretrained network and finetuning is better than training a sparse network from scratch and tries to connect the fine-tuning phase properties to dynamical isometry (DI) literature. The empirical observation makes a case for inheriting the pretrained weights. # Strengths\n\n1. The empirical observation that inheriting pretrained weights is better than training a sparse network from scratch if the fine-tuning phase is carefully tuned.\n2. The empirical analysis on fine-tuning hyperparameters (LR, #epochs) might be useful to practitioners.\n\n# Weaknesses\n\n1. lack of novelty: connection to DI and pruning breaks DI is known [Lee et al 2020, analysed at initialization] and analysing for a pretrained network is straightforward and claiming it as a contribution is weak.\n\n2. The hypothesis on page 7 connecting larger LR in fine-tuning and DI is dubious and not backed by any theory or experiments. In fig.2 plots mean JSV increases and that does not mean improved DI. As DI theory suggests to have mean JSV around 1.\n\n3. The comparison against scratch does not seem fair. Usually, fine-tuning phase is much shorter compared to training from scratch but in this case fine-tuning is done for almost the same no of epochs as the scratch version. Is it possible that the scratch version could be tuned to improve the performance?\n\n4. In section 4, the L2 regularization of pruned weights is regarded as a method for DI recovery which in my opinion is unsubstantiated. There are many unsubstantiated claims (or weak statements) in the paper. ",
            "This paper challenges an existing argument that \u201cinheriting the weights of the pruned network is not necessary for fine-tuning\u201d. This paper provides empirical evidence that the previous experiments are not carried out with proper learning rates and training epochs to ensure the network convergence. \n\nThe authors conjecture that fine-tuning pruned network requires larger learning rates and longer training epochs because its dynamical isometry is broken. As a result, large learning rates (if training for shorter epochs) or long training epochs are required for the recovery of dynamic isometry. \n\nThen the authors propose OrthP to re-initialize the pruned weights to completely recover the dynamic isometry in simple MLP. With OrthP, the fine-tuning process is less sensitive to different learning rate and training epochs. \n Strength: \n\n1) This paper brings new insights for the role of the \u201cweights\u201d in the network pruning.\n2) This paper re-examines the established arguments with careful designed experiments over wide range of pruning ratios.\n3) The proposed technique is clearly motivated by empirical observations and is very effective on MLP networks under large pruning ratios.\n4) This paper is easy to follow.\n\nWeakness:\n\n1) Table 1,2 shows that fine-tuning a pruned network requires much more training time and tuning efforts to achieve a satisfying result. Although the author claims that OrthP can \u201ccomplete\u201d resolve the dynamical isometry issue, the pruned network converges still only after prolonged training epochs (90 and 900 epochs). Does that means the pruned networks weights are more difficult to optimize than random initialized ones, which contradicts the \"the value of weights\" claim?\n2) The application area of the proposed OrthP is limited to MLP (Table 6). Although the author proposed a Strong L2 Regularization for more complicated networks, this section seems to be \u201cunfinished\u201d \u2014 the connection with dynamical isometry seems a little bit weak to me, can the authors provides more elaborations? This seems to be more of practical interests.\n3) I have some confusions regarding the experiment designs and presented results (see the Questions section below). \n\nQuestions:\n\n1)  For Table 1: In \u201cOur rerun\u201d,  are \u201cScratch\u201d results trained under 120 epochs with a decayed LR schedule? For Table 2, what are the hyper-parameters for the \u201cScratch\u201d results, are they carefully tuned?\n2) Have the authors investigated the JSV of randomly initialized sparse networks? I am not sure if applying OrthP initialization or other orthogonal regularization methods on \u201cScratch\u201d can also boost its performance. I believe this would be a fairer comparison to support \"the value of weights\" claim.\n3) Table 5: Can the authors also provide the \u201cScratch\u201d results and their hyper-parameters? I am asking because the reasons stated in Weakness 1) \u2014 if the \u201cScratch\u201d result is comparable to OrthP without using prolonged epochs, then this means even when the dynamical isometry is recovered, the pruning selected weights need more training time to optimize than randomly initialized ones. Also, in Table 2, it looks like \u201cScratch\u201d results work well under large PR, which is exactly the case in Table 5.\n4) Related to 3), from Figure 2 and Table 5, it looks like OrthP takes the role of a large training epochs to recover dynamical isometry. It would be nice to see if OrthP works for non-prolonged epochs, e.g., < 90. I like this paper in that it thoroughly investigates existing arguments on the value of pruning weights. The perspective of OrthP is not that novel, but is simple and effective on MLP under large pruning ratios. \n\nMy major concern is in the experiments design and results part. I would consider raising my score if my questions can be addressed.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the paper's methodology, experimental setup, and underlying hypothesis. Phrases like \"Unfair experimental comparison\", \"Unclear why hypothesis must be true\", \"Incorrect metric\", and \"Relationship between mean JSVs and convergence unclear\" indicate critical issues. The reviewer concludes by recommending rejection, further solidifying the negative sentiment.",
            "The reviewer explicitly states they cannot recommend acceptance despite appreciating the authors' efforts. This indicates a negative overall assessment.",
            "The review points out areas of confusion and claims needing more support, indicating a neutral overall assessment.",
            "The review expresses disagreement and criticism regarding the authors' understanding and arguments, using phrases like \"I don't think practicality is an issue here,\" \"authors have misunderstood my point,\" and \"The argument that deviation-from-unity is not better than mean JSV is confusing.\"",
            "The review expresses concerns and raises questions about the paper's claims and methodology. Phrases like \"some concerns still remain,\" \"stretching to connect,\" \"value of pruning becomes questionable,\" and \"further analysis is required\" indicate a negative sentiment.",
            "The reviewer concludes with a recommendation to accept the paper, highlighting its valuable contributions and rigorous investigations, despite some existing issues.",
            "The review expresses several concerns about the paper, including a lack of novelty, dubious hypotheses, unfair comparisons, and unsubstantiated claims. The reviewer uses negative language like \"lack of novelty,\" \"dubious,\" \"not backed by any theory,\" \"does not seem fair,\" and \"unsubstantiated claims.\"",
            "The review expresses overall positive sentiment, highlighting the paper's strengths such as bringing new insights, re-examining established arguments, and proposing an effective technique. The reviewer also mentions that the paper is easy to follow and expresses liking the thorough investigation of existing arguments."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses direct and critical language to point out flaws in the paper. Phrases like \"Nice analysis of previous papers\" is contrasted with \"Unfair experimental comparison\", \"Incorrect metric\", and the repeated use of \"I am confused regarding why\" indicate a critical tone. The reviewer also uses \"strongly suggest\" and \"strongly recommend\" to emphasize the need for changes, indicating a critical stance. The final recommendation of rejection reinforces this tone.",
            "The review expresses reservations about the paper's rigor, stating that it 'does not feel the paper makes rigorous arguments and experiments.' This critical assessment contributes to the overall tone.",
            "The review uses phrases like \"very confusing to me,\" \"need more rigorous supports,\" and points out issues raised by other reviewers. While acknowledging the paper's potential, the language is critical of the current state and claims made.",
            "The tone is critical, employing direct challenges to the authors' reasoning and methodology. Phrases such as \"misunderstood my argument,\" \"misunderstood my point yet again,\" and \"confusing\" clearly indicate a critical stance. The reviewer also uses bold text to emphasize the need for clarity and rigor.",
            "The tone is critical, using phrases like \"stretching to connect,\" \"value of pruning becomes questionable,\" and directly questioning the validity of certain claims and interpretations of results. The reviewer also points out inconsistencies and areas needing further analysis.",
            "The review acknowledges the paper's strengths ('well written and organized', 'provides an in-time and thorough study', 'valuable thoughts') while also raising concerns and criticisms ('My major concerns are as follows', 'not quite clear', 'might be too simple'). The reviewer ultimately leans towards acceptance, suggesting a balanced perspective.",
            "The review uses direct and critical language to point out the weaknesses of the paper. Phrases like \"lack of novelty,\" \"dubious and not backed by any theory,\" \"does not seem fair,\" and \"unsubstantiated claims\" indicate a critical assessment. The reviewer also questions the validity of the paper's claims and methodology.",
            "The review presents both strengths and weaknesses of the paper, along with specific questions and concerns. While acknowledging the paper's contributions, the reviewer also points out limitations and areas for improvement, indicating a balanced perspective. Phrases like \"I like this paper\" and \"My major concern is\" contribute to this balanced tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently critical of the paper throughout all points (1-5) and in the post-rebuttal update. The reviewer consistently raises concerns about the methodology, experimental design, and theoretical foundation, leading to a recommendation for rejection. Even after rebuttal, the reviewer maintains a negative stance, indicating a consistent critical evaluation.",
            "The reviewer acknowledges the authors' effort and is willing to increase the score, but consistently maintains a negative stance due to the lack of rigorous arguments and experiments to support the core argument, leading to a rejection recommendation.",
            "The review is consistent because the reviewer clearly identifies a potential confusion in the paper's claims regarding the value of pruning in linear vs. nonlinear networks. The reviewer suggests a reframing of the paper's focus to 'strong reg' and requests more rigorous support for claims in Section 5. All points are logically connected and contribute to a coherent critique of the paper's clarity and the strength of its claims, without any self-contradiction.",
            "The reviewer consistently argues for more clarity and rigor in the paper, particularly regarding the hypothesis and the metrics used.  Across all weaknesses, the reviewer emphasizes the need for a clear hypothesis related to practicality and criticizes the paper's understanding or application of concepts like dynamical isometry and convergence, suggesting better alternatives or clarifications. There are no contradictions in the reviewer's feedback; each point reinforces the overall message of needing more precision and theoretical grounding in the paper.",
            "The review is consistent in its critical assessment of the paper. Each point raises a valid concern regarding the methodology, interpretation of results, and the overall value proposition of the work. The reviewer consistently questions the claims made by the authors and asks for further clarification and justification.",
            "The reviewer identifies several limitations and areas for improvement in the paper, such as the clarity of initialization concept, the unclear relationship between dynamic isometry and generalization, and the limited experimental setup. However, the reviewer also acknowledges the paper's strengths, including its timely and thorough study of learning rates in pruning, its rigorous investigations, and its valuable contributions to the field. Despite the concerns, the reviewer concludes with a recommendation to accept, indicating that the strengths outweigh the weaknesses and the paper provides valuable insights. The concerns are presented as constructive criticisms rather than reasons for rejection, making the review consistent overall.",
            "The review is consistent in its critique of the paper. It acknowledges the empirical findings as strengths but consistently points out weaknesses related to lack of novelty, unsubstantiated claims, dubious hypotheses, and methodological concerns in the experimental setup. The reviewer's arguments flow logically and do not contradict each other.",
            "The review is consistent because the reviewer provides both positive and negative feedback in a logical and coherent manner. The weaknesses and questions raised are relevant to the paper's claims and experimental design, and they do not contradict the overall assessment of the paper's contributions."
        ]
    },
    {
        "paper_id": "iclr_2021_GHCu1utcBvX",
        "paper_title": "Transferability of Compositionality",
        "paper_abstract": "Compositional generalization is the algebraic capacity to understand and produce large amount of novel combinations from known components. It is a key element of human intelligence for out-of-distribution generalization. To equip neural networks with such ability, many algorithms have been proposed to extract compositional representations from the training distribution. However, it has not been discussed whether the trained model can still extract such representations in the test distribution. In this paper, we argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, we propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines. To our best knowledge, this is the first work to focus on the transferability of compositionality, and it is orthogonal to existing efforts of learning compositional representations in training distribution. We hope this work will help to advance compositional generalization and artificial intelligence research.",
        "review_ids": [
            "7_3ehygfkjK",
            "LwA2Gc3fsA5",
            "xGHcvStyzNG",
            "kJbEvyHG9U",
            "2oUn545DV9i"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "*Summary*\n\nThis paper proposes an architecture that addresses transferability of compositionality. The proposed architecture consists of three components: a network that transforms the input X into a series of hidden representations {H_1, H_2, ... H_K}, a network that reconstructs the input X from this series of hidden representations, and a prediction network that generates a prediction from the hidden representations. The authors propose several datasets meant to address transferability of compositional generalisation, and show that their architecture significantly improves standard DNN architectures as well as humans on these datasets.\n\n*Motivation for score*\n\nCompositional generalization in neural networks is a relevant and hot topic, with still many open questions. This paper aims to contribute on this topic and proposes some interesting datasets. However, However, despite citing several papers addressing compositionality in neural networks in the related work section, I am not convinced that the authors have properly understood the questions that are asked in this domain and were able to address them properly. Below, I outline my concerns.\n\n1. Definition of compositionality\n\nI do not find the definition of compositionality that the authors propose well motivated.\n- None of the three papers cited in the introduction to motivate the work actually has the word \"compositionality\" in the paper\n- The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits\n- The author's definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself. In addition to that, whether the mapping between input and output is compositional does not depend on what is in the train and what is in the test distribution. A model can understand the compositional structure of a dataset also if it has been trained on *all* examples of the dataset, only it will be impossible to behaviourally evaluate if it has. For this reason, much previous work on compositionality in neural networks has created datasets where the training and testing data were distributionally different (as also the authors of this paper do).\nI would recommend the authors to have a look at the paper _Compositionality decomposed: how do neural networks generalise?_ (Hupkes et al.; 2020), for a detailed account of compositionality in the context of neural networks. In particular, their section on _localism_ is particularly important for the author's definition of compositionality.\n\n2. Architecture\n\nI find the proposed architecture interesting, but it is not completely clear to me how it differs from an auto-encoder setup where the encoding is larger than the input instead of smaller (it is very well possible I misunderstood). Nevertheless, it can be interesting to see if auto-encoding based architectures behave better on datasets proposed to evaluate compositionality. One thing that is not clear to me is how the number of components _K_ is determined.\n\n3. Data\n\nI appreciate the effort of the authors to design new datasets that test out-of-distribution generalisation. I do have a few comments/questions:\n- If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn't it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?\n- What is the motivation for using a new dataset, rather than one of the previously proposed datasets for out-of-distribution generalisation?\n- It is nice that the authors try to include tests from different domains, but I think that calling a dataset mapping inputs like \"januarymarch\" to (0, 2) cannot really be called \"natural language processing\"\n\nOverall, I do not believe that this paper should be accepted for the conference.\n\n_**Update after author response:** I have read the author response, but do not find that the answers really address my concerns.  I have also not really seen any improvements in the paper itself._ ",
            "Dear authors,\n\nFirst, thanks a lot for your answers.\n\nIn my original comment I wrote: \"It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. [...] I would have appreciated a thorough study of the \"compositionaly\" limitations of previous techniques.\"\n\nIs there anything you can say about this? My biggest challenge is that I find it hard to believe that representations behave significantly differently on test data yet is able to perform the downstream task with similar accuracy.\n\nAlso, I will be happy to look at a revised version of the manuscript to see if I can get a better understanding of section 4.",
            "The paper introduces a \u201ctransferability of compositionality\u201d problem and proposes an approach to alleviate it. The said problem may arise when one trains neural models to produce \u201ccompositional\u201d representations of the input. In the paper \u201ccompositional representations\u201d consist of multiple vectors which are supposed to correspond to semantically meaningful aspects of the input, for example different objects in the case of images or different parts of compound words in the case of linguistic inputs. The transferability problem arises when there is a difference between training and test distributions, namely when certain combinations of objects have different probabilities in training & testing. The proposed solution at inference time is to project object representations to the manifold of individual object representations. The manifold is estimated by saving representations of individual object representations from the training time. \n\nThe problem that the paper considers is an interesting one. There have been a lot of papers on learning object-oriented representations recently [1, 2], and an implicit assumption in all these works is that there is no statistical dependency between which objects that occur in the scenes. There is also the literature on disentangled representations that the paper extensively cites, where the independence assumption is also common. \n\nMy concerns regarding the paper are as follows: \n- Positioning with the respect to the prior work. The literature on learning object-oriented representations is not cited. The work on disentangled representation is cited, but still new setups are created from scratch. \n- Related to the previous point, the use of full supervision (in the form of labels) in the proposed tasks strikes me as a deviation from most previously used setups. Previous work aimed to learn compositional representations without supervision, often positioning their efforts as a cog-sci-style inquiry in building human-like models. The use of labels makes this look less like a cog-sci and more like a machine learning paper. Viewing the work as an ML paper, one thing that stands out is the lack of connections to any applied ML problem. \n- I think the negative results in the paper would look stronger if pretrained image- and language- processing models were used in all experiments (e.g. Contrastive Predicting Coding & BERT) \n\nThe proposed method seems appropriate for the tasks that the paper considers. The experiments appear to be technically sound. My main concerns are thus focused on the motivation of the proposed tasks themselves and the positioning with respect to their prior work. I think a great direction to improve the paper would be to add experiments without supervised learning and using 3D-rendered images with multiple objects as it is done e.g. in [1] and [2]. \n\nFew comments on writing: \n- Algorithm 1 is very confusing because sample-level steps 1-4 are mixed with dataset-level steps 5 and 6. \n- A confusing sentence in the intro: \u201cFor a test sample, we regularize each hidden representation in its training manifold, and optimize them to recover the original input\u201d\n- for colored digits experiments you might want to compare to and cite [3] \n\n- [1] \u201cMulti-Object Representation Learning with Iterative Variational Inference\u201d by Greff et al, 2020\n- [2] \u201cMONet: Unsupervised Scene Decomposition and Representation\u201d by Burgess et al, 2019\n- [3] \u201cInvariant Risk Minimization\u201d by Arjovsky et al, 2019",
            "## Summary\n\nThis paper studies \"compositionality\" and in particular the way in which it \"transfers\" on test data. They run simple baselines on three experiments (overlapped MNIST, colored MNIST and concatenated month names) and find that the baselines do not learn compositional representation. They proposed the use of an *auxiliary reconstruction network and a regularized optimization* which improves on these baselines. \n\n## Analysis\n\nThe authors frequently say that *compositionality may not transfer to test distribution* but I have a hard time understanding exactly what they mean by this. As I understand it, \"compositionality\" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. They claim theirs is the *first work for the transferability problem of compositionality* which I find really hard to believe. I would have appreciated a thorough study of the \"compositionaly\" limitations of previous techniques.\n\nI found section 4 particularly hard to understand. A lot of symbols, equations and nomenclatures seem to be used with too little introduction. As a result, I cannot vouch for the correctness of this section.\n\nGiven their claim that this is the *first work for the transferability problem of compositionality* the experiments presented on section 5 are on a new dataset and are not compared to previous work. Moreover, the proposed experiments seem relatively simple (two overlapped MNIST digits, colored MNIST digits, concatenated month names) and their baseline seem trivial (*we use a standard neural network with two sub networks, each for an output*) \n\n## Conclusion\n\nOverall, I find the claim of this paper substantial, while the experiments are relatively simple with trivial baselines and an absence of comparison to related work.\n\n## Typos\n\nI find the text difficult to read, it would benefit from a thorough revision. Ex: *This work is orthogonal to many efforts of learning compositionality in training distribution.*\n",
            "##### Impact:\n\nThe submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization (\"because all combinations have positive joint probabilities in training\"). However, I disagree that this is the case in prior work; here are some examples of prior works that correctly hold out novel combinations (of underlying components) for test time:\n- https://openreview.net/forum?id=HJz05o0qK7\n- http://papers.nips.cc/paper/8825-learning-by-abstraction-the-neural-state-machine\n- https://arxiv.org/abs/1910.09113\n- https://arxiv.org/abs/1912.09713\n- https://arxiv.org/abs/1912.12179\n\nThere are many such examples; they are too numerous to list here.\n\n##### Quality: \n\nThe algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed.\n\n  The evaluation tasks are extremely simple (overlayed MNIST digits and conjoined word token) and are, as such, far from the complexity of existing work on compositionality (which can deal with, for example, naturalistic image data; see the references above for examples of such works).\n\n##### Clarity:\nThere are many points of ill clarity / inconsistencies; for example:\n- \"The main approach for compositional generalization is to learn compositional representations\" Is this really the \"main approach\"? Compositional generalization has been studied in many contexts outside of representation learning (e.g., see https://semanticsarchive.net/Archive/jcyZDc1Y/Goldberg.Compositionality.RoutledgeHandbook.pdf)\n\n- \"We find that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions\" Why is it assumed here that there is an extraction network? The \"extraction network\" is referred to several times in the introduction and methods section prior to its introduction/explanation.\n\n- \"compositional generalization is a type of out-of-distribution (o.o.d.) transferring or generalization, which is also called domain adaptation\" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation. I think the submission could do with a better job of dealing with the distinctions between OOD generalization / domain adaptation / concept drift.\n\n- \"We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network.\" At this point, neither the \"extractor\" nor the \"auxiliary network\" are defined.\n\n- \"These networks can be some existing networks for compositionality learning\" If so, what are examples of \"existing networks for compositionality learning\"?"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states \"Overall, I do not believe that this paper should be accepted for the conference.\" and expresses concerns about the paper's understanding of compositionality, the novelty of the architecture, and the design of the datasets.",
            "The review expresses concerns and requests clarification, but also acknowledges the authors' responses and expresses willingness to review a revised version. The reviewer is neither overtly positive nor negative, but seeks further understanding.",
            "The review expresses concerns regarding the paper's positioning within existing research, the use of full supervision which deviates from previous approaches, and the lack of connection to applied ML problems. It also points out issues with the writing and suggests improvements, indicating a critical stance.",
            "The review expresses difficulty in understanding the paper's claims, particularly regarding compositionality and transferability. It criticizes the lack of comparison to previous work, the complexity of section 4, the simplicity of the experiments, and the triviality of the baselines. The reviewer also mentions the text is difficult to read and needs revision.",
            "The review expresses strong disagreement with the paper's claims, identifies several issues with clarity and motivation, and judges the evaluation tasks as too simple. The reviewer uses phrases like \"I disagree,\" \"not adequately motivated,\" \"far from the complexity,\" and \"ill clarity / inconsistencies,\" indicating a negative assessment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review contains several criticisms, such as \"I am not convinced that the authors have properly understood the questions that are asked in this domain and were able to address them properly\" and \"I do not find the definition of compositionality that the authors propose well motivated.\"",
            "The tone is balanced as it acknowledges the authors' efforts while also pointing out areas of concern and requesting further clarification. Phrases like 'thanks a lot for your answers' show appreciation, while 'I find it hard to believe' indicates skepticism. The offer to review a revised manuscript demonstrates a willingness to engage constructively.",
            "The review uses phrases like 'My concerns regarding the paper are as follows,' 'strikes me as a deviation,' 'lack of connections,' and 'Algorithm 1 is very confusing' to express specific criticisms and concerns about the paper's methodology, positioning, and clarity. The reviewer also suggests significant changes, indicating a critical evaluation.",
            "The review uses phrases like 'I have a hard time understanding,' 'difficult to evaluate,' 'hard to believe,' 'particularly hard to understand,' 'cannot vouch for the correctness,' 'trivial baselines,' and 'absence of comparison to related work,' indicating a critical assessment of the paper's content and presentation.",
            "The review uses direct and critical language. For example, \"However, I disagree that this is the case in prior work,\" \"not adequately motivated,\" \"are, as such, far from the complexity,\" and listing multiple points of \"ill clarity / inconsistencies\" demonstrates a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently expresses a negative opinion about the paper, starting from the motivation for the score and continuing through detailed points about the definition of compositionality, the architecture, and the datasets. The reviewer concludes with a clear rejection recommendation and reinforces this negative stance after considering the author response. There are no contradictions in the reviewer's arguments, and all points support the overall negative assessment.",
            "The reviewer consistently expresses their concern about the lack of comparison with other methods and the difficulty in evaluating the claims of the paper. They reiterate their previous point and ask for more clarification, without contradicting themselves.",
            "The reviewer raises concerns about the paper's positioning with respect to prior work, the use of full supervision deviating from unsupervised approaches, and suggests using pretrained models. However, the reviewer also acknowledges that the proposed method is appropriate for the tasks and the experiments are technically sound. The concerns and positive aspects are consistently presented, focusing on different aspects of the paper (motivation and positioning vs. method and experiments).",
            "The review is consistent because the reviewer's critique revolves around a central theme: the paper's claims about 'compositionality transfer' are substantial but are not adequately supported by the experiments, baselines, and comparison to prior work. This concern is consistently expressed throughout the analysis and conclusion sections, from questioning the definition of 'compositionality transfer' to criticizing the simplicity of experiments and lack of comparison to previous methods. The reviewer's arguments build upon each other to form a coherent and consistent critique.",
            "The review is consistent in its criticism of the submission across different aspects (Impact, Quality, and Clarity). The reviewer consistently points out weaknesses in the submission's claims, methodology, and clarity, without presenting contradictory statements within the review itself. The negative points raised in each section reinforce the overall critical assessment."
        ]
    },
    {
        "paper_id": "iclr_2021_FZ1oTwcXchK",
        "paper_title": "Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks",
        "paper_abstract": "Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation of the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only \u223c1/10 of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. Codes are available at https://github.com/Jackn0/snn_optimal_conversion_pipeline.",
        "review_ids": [
            "8d5tvAq6XzW",
            "Nzg6I9vupip",
            "HxhMCtNSinJ",
            "Q-qB6_aVp75",
            "oKSNjjleynB"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I appreciate the addition of the RNN study in the appendix. That topic needs to be fleshed out more -- but in another paper. For the scope of this paper, that's good enough I think. You have shown that the approach is feasible for the RNN case (which to me is the more interesting one), which should prompt follow-on work to study that in more depth.\n\nNice.",
            "## Edit on second review\n\nI apologize again for the tone of my first review, I sincerely tried to understand the paper but I could not when I first read it. A re-read the paper and finally understood it during the review. I left a comment to the authors in the discussion below and they appropriately addressed my new recommendations. With the new equation (1) the paper is hopefully more understandable now. \n\nI increase my grade from 3 to 5. The findings are quite interesting but I still believe that the paper is not well written: the equations are interesting but the explanations between the equations are often unclear. One has to understand each equation and be quite imaginative to finally identify the contributions of the paper (even for somebody only \"very slightly\" off from the research topic).\n\n## Summary\n\nThe authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model.\nThis relationship suggests a mapping between the two models which is imperfect, a loss seems to be derived to reduce this mismatch along the network training. The method is tested on CIFAR-10 and CIFAR-100, and compared with some other methods for converting ANNs to SNNs.\n\n## Critical review\n\nThis topic is potentially important since spiking neural network are gaining popularity. But this paper is clearly badly written and it is extremely hard to understand, both in the math and in the text. I don't think it would help the progress of the field to publish the article in the current form.\n\nI tried to read that carefully and got lost after equation (4), the transition to equation (5) and (6) are not clear at all. I do not understand what is an approximation, what is a definition and what is a derivation.\n\nAlso (5) seems wrong in itself, the authors are trying to approximate a rectified linear network but it suggest that the activity will be equivalent to a linear network (at least when v(T) is small) ? And magically this changes in (6), and a clip non-linearity is introduced ?\n\nThe Figure 1 seems very encouraging at first, because it suggests that there is a clear and easy mapping between accumulating the spikes and computing a relu. I did not understand where this is appearing in the math and I cannot check whether the intuition conveyed by the figure is correct or not.\n\nI was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? This is not shown.  I do not even understand if it is necessary to re-train the network to go back from the SNN to ANN or vice versa.\n\nSince I had not understood the basics of the paper, it was impossible for me to understand the later section about the conversion error. My only take is that it seems wrong at first sight: how minimizing the error in the loss would minimize the mismatch between the network activity?\n",
            "Considering the positive grades of the other two reviewers, I read the paper again more carefully. It is in fact better than I first thought! I apologize for the tone of my first review. \n\nI now understand that the value of this paper is to provide a theory that tells how one can reduce the conversion error by adding a threshold and a shift in the artificial model. The theory is fine (even though hard to read), but I wonder how much better is it in practice to the relu alone conversion from Dielh et al. 2015. If I read the performance figure and table correctly, it seems that the performance difference from a relu to SNN conversion is not extremely large and the number of time steps is not extremely larger without the threshold and the shift (all details equal otherwise). Is that correct?\n\nI find that the math is still unclear in a various points. I highly recommend adding a few clarifications. I am listing a couple of recommendations below and I will improve my grading if and once they are addressed. And even beyond the math, I still find it very unfortunate, that the paper lacks clarity in the writing. It really undermines the quality of interesting findings and it is a loss of time and energy for the reader. \n\n## recommendations for modifications:\n\n- I now understand equation (4) to (6), what made be think it was fallacious was the second part of equation (1). What made a difference for me, was to understand that $\\boldsymbol x_l'(t) = \\boldsymbol \\theta^{(l-1)}(t) $ (I may be helpful to add that in a comment). Therefore, $\\boldsymbol x_l'(t)$ is the spike train, and writing $\\boldsymbol x_l'(t) = h_j( W_l \\cdot \\boldsymbol x_{(l-1)}'(t) )$ in equation (1) is wrong because is $\\boldsymbol x_l'(t)$ is the spike train and not the accumulated spike count $\\boldsymbol a'_l$. The correct version of equation (1) is written later in equation (7), which I now understand. It would be great to correct equation (1) to make this less misleading.\n\n- I think the sentence: \"We first derive the formula of converting the source ANN to target SNN with the threshold balancing\nmechanism.\" is not clear for somebody who does not know what \"threshold balancing\" means. At least a reference to Diehl et al. 2015 in necessary here. I would also have appreciated a lame-man explanation of this term to anticipate what is going to be shown in the upcoming section. Something like: \"Following the work of Diehl et al. 2015, we describe in this section the equivalence between a SNN and an ANN with thresholded relu non-linearity.\"\n\n- There are still obvious typos in the abstract, I think the word \"filed\" is meant to be \"field\". Also it is not clear what is meant here by \"adaptive\". I recommend that authors go again carefully through the entire paper to make sure that the writing is fine, at least according to them.\n",
            "Strength: \n(1) This paper proposes a layer-wise optimization method for ANN-SNN conversion. I appreciate the theoretical analysis of how to minimize the conversion error.\n\n(2) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error. \n\n(3) It's interesting that conversion to SNN actually improves rather than damage the accuracy on ResNet.\n\nWeakness:\n(1) Although the proposed method is much more efficient, it does not show obvious performance improvement compared to existing methods.\n\n(2) In the experiment, the ResNet consistently show better performance. I hope the authors can provide more comments on this.\n\n(3) I'm not sure if I missed anything. The threshold RELU is not defined in the paper which may cause confusion.\n\n(4) I hope the authors can summarize the whole steps by formula or algorithm to help readers understand the entire process. ",
            "The authors seek a mechanism to train a spiking neural net to duplicate the function of a non-spiking one. This is desirable for energy-efficient inference, although the training process becomes challenging due to the discrete nature of the spiking process.\n\nTo achieve their goal, they described the spiking neuron non-linearity by a \"staircase\" function of the input (spiking output increases by 1 each time the input gets big enough to reach the next stair), and related that to the ReLu function used in the non-spiking neural net. They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN -> SNN). This scales with the square of the threshold voltage for spiking, divided by the simulation time. As one might expect, lower thresholds, and longer simulation times, both of which lead to potentially higher spike counts and thus lower discretization errors, lead to smaller conversion errors.\n\nUsing this, they defined their procedure for training SNN to mimic ANN as follows: they trained the ANN with their modified ReLU (which is closer to the SNN activation function but more readily differentiable), and then used the weights from that ANN in their SNN.\n\nNext, the authors evaluated their procedure on several different image categorization networks. Nice performance was obtained in all cases: better than using a normal ReLU, or other comparison activation functions, in the \"target\" ANN.\n\nOverall, this is a reasonably nice piece of work. I'd like to see this applied to recurrent neural nets: there, the dynamics of the SNN could be used more naturally, and the results might be more meaningful.\n"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses appreciation for the added content and acknowledges the feasibility of the approach, indicating a positive overall sentiment. The use of \"Nice\" further reinforces this.",
            "Despite the increased grade, the reviewer maintains a critical stance, highlighting issues with the paper's writing quality, clarity, and mathematical rigor. Phrases like \"badly written,\" \"extremely hard to understand,\" \"I tried to read that carefully and got lost,\" and \"seems wrong at first sight\" indicate a negative sentiment.",
            "The reviewer expresses a change in opinion from negative to positive after a more careful reading, stating \"It is in fact better than I first thought! I apologize for the tone of my first review.\" While there are criticisms, the overall sentiment is positive due to this change and the acknowledgement of interesting findings.",
            "The review expresses appreciation for the theoretical analysis and the reduction in simulation time. It also highlights the interesting finding that conversion to SNN improves accuracy on ResNet.",
            "The review uses phrases like \"reasonably nice piece of work\" and \"Nice performance was obtained in all cases\", indicating a positive sentiment."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses encouraging language like \"I appreciate\", \"good enough\", and \"Nice\". They also suggest future research directions, showing support for the authors' work and its potential.",
            "The review employs a critical tone by directly pointing out flaws in the paper's writing, mathematical derivations, and explanations. The reviewer uses phrases like \"not well written,\" \"transition to equation (5) and (6) are not clear at all,\" \"(5) seems wrong in itself,\" and \"I cannot check whether the intuition conveyed by the figure is correct or not,\" demonstrating a critical assessment of the paper's content and presentation.",
            "The review acknowledges the paper's value and interesting findings but also points out areas needing improvement with specific recommendations. The reviewer apologizes for a previous negative tone, indicating a desire to be constructive. The language is generally formal, but the tone shifts between supportive (offering recommendations) and critical (pointing out unclear writing and mathematical inconsistencies).",
            "The review presents both strengths and weaknesses of the paper, offering positive feedback alongside constructive criticism. Phrases like \"I appreciate\" and \"It's interesting\" indicate a positive inclination, while concerns are expressed using phrases like \"Although the proposed method...does not show obvious performance improvement\" and \"I hope the authors can provide more comments on this.\"",
            "The tone is generally supportive and encouraging, with constructive criticism (suggesting application to recurrent neural nets) rather than harsh judgment. The reviewer acknowledges the value of the work and suggests future directions, creating a supportive atmosphere."
        ],
        "consistency": [
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer appreciates the addition of the RNN study but suggests further development should be in a separate paper, keeping the current paper focused on its main scope. There are no contradictory statements or conflicting opinions within the review.",
            "The reviewer initially strongly criticized the paper as \"badly written\" and \"extremely hard to understand,\" leading to a low grade. After re-reading and clarification, the reviewer increased the grade significantly, suggesting improved understanding and appreciation of the findings. However, the reviewer still maintains the paper is \"not well written\" and \"explanations are often unclear.\" This combination of a significantly increased grade with persistent strong criticism of writing quality creates a sense of inconsistency. A paper deemed \"badly written\" might not typically warrant a grade as high as 5, even with interesting findings.",
            "The review is consistent because the reviewer acknowledges an initial misjudgment but then provides a balanced and constructive critique. They identify both strengths (theory) and weaknesses (clarity, math, practical impact) of the paper and offer specific recommendations for improvement that align with their criticisms. The reviewer's stance evolves from initial negativity to a more nuanced and constructive approach, but their core concerns about clarity and practical impact remain consistent.",
            "The review is consistent because the strengths and weaknesses are logically separated and do not contradict each other. The reviewer appreciates the theoretical contribution and efficiency of the method, while pointing out areas for improvement such as lack of performance improvement compared to existing methods and clarity in presentation. The points raised are valid and do not create any self-contradiction within the review.",
            "The review is consistent as it provides a logical and coherent summary of the paper, describing the different aspects of the work without any conflicting statements or contradictions. The reviewer clearly outlines the authors' approach, findings, and provides a positive overall assessment with a constructive suggestion for future work."
        ]
    },
    {
        "paper_id": "iclr_2022_ErX-xMSek2",
        "paper_title": "A Study on Representation Transfer for Few-Shot Learning",
        "paper_abstract": "Few-shot classification aims to learn to classify new object categories well using only a few labeled examples. Transfering feature representations from other models is a popular approach for solving few-shot classification problems.In this work we perform a systematic study of various feature representations for few-shot classification, including representations learned from MAML, supervised classification, and several common self-supervised tasks. We find that learning from more complex tasks tend to give better representations for few-shot classification, and thus we propose the use of representations learned from multiple tasks for few-shot classification. Coupled with new tricks on feature selection and voting to handle the issue of small sample size, our direct transfer learning method offers performance comparable to state-of-art on several benchmark datasets.  \n",
        "review_ids": [
            "wNedRwIqxp",
            "JEMz-45aOvw",
            "VdUtPlkb9gh",
            "yZ0ccqkU0eP",
            "L_ml8_nEnCj"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies few-shot learning from the viewpoint of transferring feature representations. The authors investigate few-shot performance with varying complexities of source tasks together with a few empirical tricks for improvements. The main finding in the paper is that transferring from more complex source tasks tend to result in better performance. Accordingly, using multiple source tasks is also found to be useful.\n\nStrengths:  \n(+) The paper is clearly motivated and well-written.  \n(+) Reported scores could be useful to server as a baseline in this direction of research.  \n\nWeaknesses:  \n(-) No technical novelty.  \n(-) Results and conclusions seem rather obvious.   My main concern of this paper is its novelty. In my opinion, it is already a kind of common sense that training with more complex data results in better feature representation. (that's why the race to develop a giant pre-learning model with big data is on)  \nUsing multiple source tasks is also a straightforward direction to go. For example, it is well-studied in the following paper.   \n  - Zamir et al., Taskonomy: Disentangling Task Transfer Learning, CVPR 2018.  \n \nI understand that the focus of this paper is different (i.e., few-shot, self-supervised tasks),  but at high level, the findings and conclusions in the paper seem rather obvious to me. Or, is this result really surprising for few-shot learning? If so, please clarify it as the reviewer doesn't understand the point.  \n \nAs for the feature selection (auxiliary class) and voting methods, I can find no novelty in them either. In my understanding, they are nothing but multi-task learning and test-time augmentation, respectively.  Overall, I find this paper presents an interesting case study of feature transfer focusing on few-shot learning, it lacks enough novel insights and technical contributions to warrant publication at ICLR. \n",
            "This paper presents an interesting study comparing the various feature representations for few-shot classification tasks (in Computer Vision) learned from the supervised classification with/without MAML, from multi-task prediction, and self-supervised tasks like rotation prediction and location prediction, and contrastive learning. \n\nThe paper also introduces two tricks \u201cirrelevant feature elimination using auxiliary classes\u201d  and \u201cvoting with auxiliary task instances\u201d which made the ImageNet pre-trained model\u2019s performance on downstream tasks of 5 way 1 shot & 5 way 5 shot close to SOTA (State Of The Art) while handling the issue of a small sample size of 5 examples per class. * Although the ideas of using the MAML algorithm to adapt the last layer of the neural network which performs very close to adapting the whole network (Raghu et al., 2019) and simple baselines perform close to SOTA from learning a good representation through a proxy task (Tian et al., 2020) already exist. The authors of the paper acknowledge it and conducted experiments on how simple these baselines (ConvNet, ResNet) can be. And which proxy tasks (rotation prediction, location prediction) are helpful.\n* In-depth analysis and experimentation with various techniques, architectures like ResNet, ConvNet, and sizes of the network help point out the limitations of those techniques and lead to the critical finding that complex tasks (combining classification, rotation prediction, and location prediction) tend to give better representations for few-shot classification. \n* The use of transfer learning with multi-task representations and novel tricks like \u201celiminating irrelevant features using auxiliary classes\u201d improved the performance on 5w1s, 5w5s, 10w1s by emphasizing the unique task-related features. The other trick is \u201cvoting with auxiliary task instances\u201d where a set of rotated or cropped copies of the input image are given with the input image may not have any impact on the learned feature representations but it helped to improve the accuracy even closer to SOTA.\n* The description of various hyperparameters and the reasoning for constraints used in the experiments helped to better understand and compare the results to other methods.\n\n#### __Concerns:__\n\nI have some queries about the studies presented in the paper.\n\n* In Section 3.2, Imagenet pre-training, It\u2019s unclear how much data is used? \n    * As miniImagenet, TieredImageNet are subsets of ImageNet, Is data of common classes removed from the pre-training data at the time of pre-training?.\n    * To what extent does the amount of data and the diversity of data used in the pre-training affect the representations and performance of the various models.\n* The authors have chosen the representation size to be 640, may I know the reason behind it? Will the performance of the various models used in the studies vary with size?\n* Although the paper provides various experiments on the Initialization based method (MAML), Self Supervised Methods, will the proposed tricks improve Metric Based Models like Prototypical Networks, Matching Networks?\n\n#### __Other suggestions:__\n It is difficult to follow section 3.2 due to the missing reference to Table 3.\n\n The idea of improving existing baselines to be almost comparable to SOTA with some techniques may not be a novel one but this paper stands out in experimenting and in-depth analysis of the various techniques and also combining them with heuristic tricks like feature selection and voting to handle the issue of small sample size. Hopefully, the authors can address my queries/concerns (mentioned above) in the rebuttal period.",
            "The authors study impact of self-supervised methods on feature representation in few-shot learning task. They analyze the way features are transferred for MAML method and 3 self-supervised methods. They introduced a method combining self-supervised representation training methods with voting system in order to improve performance. The paper is generally well written with a clear explanations and research direction.\nThe topic is relevant and of interest to the community.\n\nAfter initial description I was saddened that the analysis of feature representation of few-shot methods involved only MAML. It is in fact significant method that influenced many other, but its performance was quickly surpassed by more advanced methods which should be analyzed instead. At least some more methods should be analyzed as then instead of studying few-shot methods the paper studies MAML only (which might mean that conclusions might not generalize). The self-supervised methods studied (rotation, location and contrastive) are also very simple, and although their analysis is good, the proposed way of combining them is difficult to see as a strong technical novelty. The same goes for the voting system built on top of the proposed training scheme. The results confirm the importance of both, however the technical contribution itself seems weak.\n\nIn Table 11, only the authors method is using modified ResNetW12 -- this raises the question how significant that is? If the authors' method requires it, then other methods should be compared fairly with same backbone. Regardless of this, the results do not build a strong case for the proposed method (since only for a 5w5s on miniImageNet it outperforms existing methods). Comparable results would be impressive if the proposed method would provide some additional benefit, however none were mentioned. Interesting case study of feature representations for few-shot learning. However, it lacks depth (only MAML analyzed as few-shot method) and the proposed solution lacks technical novelty and convincing results.",
            "This paper conducts an empirical study on the usefulness of different representations for downstream few-shot learning tasks, using the transfer strategy of training a new linear layer on top of the frozen pre-trained network. They also propose to utilize auxiliary data during this downstream learning phase: instead of learning an N-way readout layer for an N-way classification task, they learn a (N+T)-way one, where T is the number of training classes, using a subset of the training dataset for this. To perform inference on the query set, though, they only consider the N classes appearing in the given task. They hypothesize that this approach prevents the linear classifier from paying attention to spurious features, and it sometimes yields a gain in performance. They also propose to combine different training objectives, including the supervised classification objective and different self-supervised ones in a multi-task manner, and propose to do so by keeping the input the same for all objectives involved, meaning that to train the supervised loss together with the rotation, the former will also be computed on rotated images. Finally, they propose a voting scheme, where the predictions for different augmentations of each input image are aggregated to form the final prediction. All experiments are conducted on mini-ImageNet and tiered-ImageNet, and gains are reported mostly on the former.\n Studying the utility of different representations for few-shot learning tasks is an interesting and important topic, due to the success and wide adoption of the transfer learning paradigm. Jointly training for different objectives is also interesting to study, as well as the proposed voting and auxiliary class downstream training methods proposed here. These have the potential to produce impactful results. The paper is also clearly written for the most part and easy to follow. However, the study is limited to simple datasets, and the proposed methods do not yield a consistent gain, especially on tiered-ImageNet. Some additional concerns and feedback below:\n\nCorrectness\n- \u201cThe paper Tian et al. (2020) show that by just pre-training \u2026 and adapting using logistic regression, they can obtain results \u2026 that are competitive with state-of-the-art algorithms\u201d - This isn\u2019t exactly true. While the approach of Tian et al indeed doesn\u2019t use meta-learning, their approach is more complicated than what this sentence makes it sound, as it involves a distillation phase aside from just standard pre-training.\n\n- \u2018... and train a logistic regression model on top of it during the adaptation stage on the query set\u2019 - this training should take place on the support set, not the query set. I assume this is just a typo but it would be great if the authors could clarify.\n\n- \u2018Transfer learning, on the other hand, \u2026 , a much simpler task model (e.g. a linear classifier) instead of having to learn feature extraction at test time\u2019. This doesn\u2019t seem correct - meta-learning approaches sometimes learn simple models too, e.g. Prototypical Networks is a form of a linear classifier, and transfer learning often involves fine-tuning the entire network at test time [1] (some examples in the context of few-shot classification: [3, 4]). The fact that just training a linear classifier on top of pre-trained features works so well for mini- and tiered- imagenet might just be a symptom of the simplicity of these datasets and resulting transfer problems, but this is not true of transfer learning more generally. \n\n- \u2018it is very difficult to figure out which feature is relevant given only a handful of examples\u2019 - is there some evidence to support this claim? Doesn\u2019t the success of simple linear classification on top of pre-trained features contradict this claim to some degree?\n\nRelationship to previous work\n- The observation that using a larger \u2018way\u2019 during training than testing has already been made in prior literature [4]. The authors there also hypothesized that this is due to harder training tasks being beneficial for representation learning. Though in this case the experiments are done with MAML instead of Prototypical Networks, this connection should still be discussed.\n\n- In addition, the effect of the number of shots used at training time on test performance on different configurations of tasks has been explored both theoretically and empirically in the past [5,6].\n\n- The approach used here is reminiscent of the \u2018input harmonization\u2019 in [7]. Can we get an additional performance boost from also incorporating the lasso architecture proposed there? This allows different tasks to have slightly different weights instead of forcing them all to use exactly the same feature extractor.\n\n- [8] (and approaches cited within) also study transfer learning with several different types of pre-trained representations, on several different data-limited downstream tasks. They explore both supervised and self-supervised representation learning approaches. It is worth discussing the connections to that work and how the goals of this work differ from previous studies.\n\nAdditional experiments\n- What happens if a more naive approach is used for multi-task training, where each task receives different inputs (e.g. the classification loss is computed on the original images instead of the rotated or cropped ones). The assumption made here is that this would perform worse, but it would be useful to actually report these results and verify this hypothesis. It seems, for instance, that [9] performed joint training between a supervised a self-supervised objective without making any modifications, and still observed improvement in performance.\n- Can location-based voting and rotation-based voting be combined? More generally, how does this compare to voting with random augmentations (e.g. those typically used for data augmentation, like color jittering, horizontal flipping, etc)? This would help understand if this voting is beneficial only when the augmentations chosen are the same ones used for training.\n- The authors hypothesize that the reason auxiliary class training helps is to alleviate the issue of learning spurious features. Is there some evidence to support this hypothesis? Perhaps looking at the gap between the support and query accuracy in both cases (with and without the auxiliary classes) would be helpful, since if the classifier is learning spurious features perhaps this would lead to overfitting on the support set and not generalizing to the query set?\n\nClarity\n- It\u2019s not entirely clear how logistic regression differs from fast adaptation of MAML, when only adapting the final layer of MAML. I\u2019m assuming the main difference is that a new random readout layer is initialized instead of re-using MAML\u2019s meta-learned classification layer. Is there also a difference in the number of steps, e.g. logistic regression takes more steps than fast adaptation? It would be good to clarify exactly what the differences are.\n\n- \u2018To further simplify the re-initialization step\u2019, citing the Reptile paper. This sentence is confusing; it\u2019s not clear what re-initialization step means and how this paper simplifies it. It seems more appropriate to mention Reptile in the context of first-order approximations for MAML.\n\n- The term \u2018domain\u2019 is used in an inappropriate way throughout the paper. I wouldn\u2019t say that the test set of mini-ImageNet comprises a different domain than the training set of mini-ImageNet. Referring to it as \u2018held-out class split\u2019, for instance, would be more appropriate.\n\n- Why not just refer to \u2018modified MAML\u2019 by its name, ANIL (from Raghu et al, as cited).\n\nReferences\n- [1] Do Better ImageNet Models Transfer Better? Kornblith et al. CVPR 2019.\n- [2] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. Triantafillou et al. ICLR 2020.\n- [3] Comparing Transfer and Meta Learning Approaches on a Unified Few-Shot Classification Benchmark. Dumoulin et al. NeurIPS 2021.\n- [4] Prototypical Networks for Few-shot Learning. Snell et al. NeurIPS 2017.\n- [5] A Theoretical Analysis of the Number of Shots in Few-shot Learning. Cao et al. ICLR 2020.\n- [6] Learning Flexible Classifiers with Shot-CONditional Episodic (SCONE) Training. Triantafillou et al. Meta-learn workshop, NeurIPS 2020.\n- [7] Multi-task Self-Supervised Visual Learning. Doersch et al. ICCV 2017.\n- [8] A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark. Zhai et al. 2020.\n- [9] Boosting Few-Shot Visual Learning with Self-Supervision. Gidaris et al. 2019.\n\n Although the paper studies an interesting topic, I have some concerns about correctness of various statements in the paper, the relationship with prior work that has explored similar questions, and the need for additional experiments and analyses to aid in better understanding where the observed performance gains come from. Finally, a weakness of this work is that the study is limited to simple datasets and a linear transfer learning adaptation technique, which makes me question whether the conclusions will generalize to other scenarios.",
            "The paper provides a study on different approaches to train a backbone network used in a transfer fashion for Few Shot Learning. \nRepresentation learned from MAML, supervised classification, and self-supervised tasks are considered here and their performance are compared. From this analysis it is confirmed that the use of multi-task trained representation (based on supervised classification and self-supervised auxilary tasks) leads to the most performing backbone.\n\nFurthermore it is proposed to improve the classifier part using two additional tricks: \n- use of auxiliary classes to help eliminate irrelevant features\n- use of voting with image transformations (rotation, location splitting)\nThis additionnal trick allows to boost performance over traditionnal simple classifier. \n\nFinal results show performance on par with various state of the art solutions. # Main review \nThe main part of the paper is dedicated to the study on the various training approaches for the transferred backbone. This study confirm the observations already published in the literature (here work of Tian et al is especially cited). Author could also have cited work of Mangla et al that also addressed this point and provide some insights on the benefits of using additional self-supervised task in the context of Few Shot Learning. This study part is quite long and some experiments could be omitted (e.g. training the backbone only on one self-supervised task) and does not really bring some new learnings with respects to previous works. In previous works it is already clear that self-supervised tasks should be considered as additional tasks. \n\nNote also that contrastive loss (from Khosla et al) has also been proposed recently in the context of Few Shot Learning by Ma et al, where 63.76% accuracy is reported on 5w1s (see table 3 of the paper)  which is rather pointing in favour of using such loss in the context of Few Shot Learning. This is contradictory to the comment made on table 4. This is not to point something to be cited, but rather to highlight the weakness of the observations and conclusions made in this part.\n\nAlthough being cited, no comparison result to Tian et al is provided in the benchmarks.\n\nSection 5 presents some really interesting tricks to improve the classifier. \nFrom table 8, a significant systematic gain is observed when using auxiliary classes. I find it as being the main contribution of this paper. Additional studies would be interesting to be performed in order to see what is the impact of the number of additional samples used as well as more detailed explanation on how additional samples are used. \n- It is not clear here if all of the 1000 additional samples are always used here  (1000 is not a multiple of 64)\n- does the 64 additional classes are always used?\n- what is the impact if using less auxiliary classes with less samples per class?\n\nIn the final comparison, a combination of the various approaches is compared to SOTA solutions. However it is not clear from where the benefits comes from in the selected combinations. Especially since we can observe that the combination is different for mini-ImageNet and tiered-ImageNet. Could we have more insights why the voting solution does not work for tiered-ImageNet?\n\nTable 11 reports comparative results with other SOTA approaches. However since backbone architecture is different and with the use of auxiliary classes and voting mechanism it is not clear where the benefit come from. Ablation studies is required to see what are the relative benefits of the varying tricks. Especially when we can observe that the settings between mini-ImageNet and tiered-ImageNet are different (location voting used only for mini-ImageNet)\n\n\n# Minor remarks\n- page 3, it is mentioned from results in Tables 1 and 2, that it is better to train on 10-ways tasks. But from those tables 5w5s and and 5w1s are still competitive for imagenet. conclusions here are not that straightforward.\n- table 3 is not cited in the paper. It should be cited in section 3.2 when discussing about MAML based training vs supervised training.\n\n\n# Additional references\n- Mangla et al, \"Charting the right manifold: Manifold mixup for few shot learning\"; WACV 2020.\n- Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, andDilip  Krishnan.Supervised  contrastive  learning.arXivpreprint arXiv:2004.11362, 2020\n- Jiawei Ma, Hanchen Xie, Guangxing Han, Shih-Fu Chang, Aram Galstyan, Wael Abd-Almageed, Partner-Assisted Learning for Few-Shot Image Classification. arxiv:2109.07607, 2021 From the contributions reported by the authors:\n- contribution 1 and 2 are not really novel and do not really provide some new insight with respect to previously published works and current trends in training a backbone for Few Shot Learning\n- contribution 3 (feature selection, voting) is quite novel and provide some interesting gains, but more experiments would be needed to assess the relative benefits of those proposals.\n\nConsidering that the focus in this paper is more put on contribution 1 and 2, a significant reorganization of the paper as well as more experimental results should be done in order to focus more on contribution 3. That's why my vote is to reject this paper."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses concerns about the paper's novelty and questions the significance of its findings, using phrases like \"no technical novelty,\" \"results and conclusions seem rather obvious,\" and \"lacks enough novel insights and technical contributions to warrant publication.\"",
            "The review expresses overall positive feedback, highlighting the paper's interesting study, in-depth analysis, and valuable experiments. The reviewer also acknowledges the paper's contribution in improving baselines to be comparable to SOTA and conducting in-depth analysis of various techniques.",
            "The review expresses concerns about the limited scope of analysis (only MAML), the simplicity of the self-supervised methods, the weak technical novelty of the proposed method, and the lack of convincing results. Phrases like 'saddened,' 'difficult to see as a strong technical novelty,' 'results do not build a strong case,' and 'lacks depth' indicate a negative sentiment.",
            "The review expresses concerns about correctness, relationship to prior work, and the need for additional experiments. The reviewer also points out limitations in the datasets and adaptation technique, questioning the generalizability of the conclusions.",
            "The reviewer recommends rejection, stating that contributions 1 and 2 are not novel and the paper needs significant reorganization and more experimental results to focus on contribution 3. Phrases like 'weakness of the observations and conclusions,' 'not clear where the benefit comes from,' and 'ablation studies is required' further contribute to the negative sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses critical language such as \"no technical novelty,\" \"results and conclusions seem rather obvious,\" \"main concern,\" and explicitly questions the paper's contribution to the field. The reviewer also directly states the paper \"lacks enough novel insights and technical contributions to warrant publication.\"",
            "The review presents both positive aspects of the paper, such as its interesting study and in-depth analysis, and concerns, such as unclear data usage and the choice of representation size. It also offers suggestions for improvement, indicating a balanced perspective.",
            "The review uses critical language to point out weaknesses in the paper, such as the limited analysis ('only MAML analyzed'), lack of technical novelty ('difficult to see as a strong technical novelty'), and questionable experimental setup ('only the authors method is using modified ResNetW12'). The reviewer directly questions the significance and generalizability of the findings.",
            "The review uses phrases like \"some concerns about correctness\", \"a weakness of this work\", and questions the generalizability of the conclusions. The detailed feedback and suggestions for improvement also indicate a critical assessment.",
            "The review uses critical language to point out weaknesses in the paper, such as 'does not really bring some new learnings,' 'weakness of the observations and conclusions,' 'it is not clear where the benefits comes from,' and 'ablation studies is required.' The reviewer also questions the novelty of certain contributions and suggests a significant reorganization of the paper."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No"
        ],
        "consistency_reason": [
            "The reviewer consistently argues that the paper lacks novelty. They support this claim by stating that the findings are 'common sense' and 'straightforward,' referencing existing work to emphasize the lack of novelty in using multiple source tasks, and also pointing out the lack of novelty in the proposed methods. The strengths mentioned are superficial and do not contradict the central argument about the lack of novelty. The final conclusion aligns with the weaknesses, recommending rejection due to lack of novelty.",
            "The review is consistent because it starts with an overall positive assessment of the paper, highlighting its interesting study, in-depth analysis, and practical tricks. While raising concerns and queries, these are framed as questions for clarification and further understanding, not as contradictions to the initial positive evaluation. The suggestions are constructive and aim to improve the paper. The reviewer acknowledges the paper's strengths in experimentation and analysis, even while noting the potential lack of complete novelty in the core idea, maintaining a balanced and consistent perspective throughout.",
            "The review is consistent in its criticism, pointing out limitations in the scope of analysis (only MAML for few-shot methods), lack of technical novelty in the proposed method and voting system, and the weakness of the results. While acknowledging the clarity of writing and topic relevance, the reviewer consistently focuses on these weaknesses throughout the text.",
            "The review is consistently critical, starting with acknowledging the interesting topic but immediately pointing out limitations and then detailing specific concerns about correctness, relationship to prior work, need for additional experiments, and clarity. The concluding paragraph reinforces the overall critical assessment.",
            "The review highlights a contradiction regarding the effectiveness of contrastive loss in Few-Shot Learning. The reviewer mentions Ma et al.'s work showing positive results with contrastive loss, which contradicts a comment made on table 4 of the reviewed paper (or the reviewer's interpretation of it), suggesting an inconsistency in the review's assessment of the field and potentially the paper's claims about contrastive loss."
        ]
    },
    {
        "paper_id": "iclr_2021_Gc4MQq-JIgj",
        "paper_title": "Reconnaissance for reinforcement learning with safety constraints",
        "paper_abstract": "Practical reinforcement learning problems are often formulated as constrained Markov decision process (CMDP) problems, in which the agent has to maximize the expected return while satisfying a set of prescribed safety constraints. In this study, we consider a situation in which the agent has access to the generative model which provides us with a next state sample for any given state-action pair, and propose a model to solve a CMDP problem by decomposing the CMDP into a pair of MDPs; \\textit{reconnaissance} MDP (R-MDP) and \\textit{planning} MDP (P-MDP). In R-MDP, we train threat function, the Q-function analogue of danger that can determine whether a given state-action pair is safe or not. In P-MDP, we train a reward-seeking policy while using a fixed threat function to determine the safeness of each action. With the help of generative model, we can efficiently train the threat function by preferentially sampling rare dangerous events. Once the threat function for a baseline policy is computed, we can solve other CMDP problems with different reward and different danger-constraint without the need to re-train the model. We also present an efficient approximation method for the threat function that can greatly reduce the difficulty of solving R-MDP. We will demonstrate the efficacy of our method over classical approaches in benchmark dataset and complex collision-free navigation tasks.",
        "review_ids": [
            "iHDiffAL3FY",
            "WLlDpCjhDTA",
            "iepV_udRuYn"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary: The paper studies problems that can be modelled as constrained markov decision processes (CMDPs). It proposes to solve the CMDP by decomposing it into a pair of MDPs; i) a reconnaissance MDP (R-MDP) that is trained with the help of a generative model, and ii) a planning MDP (P-MDP) that is trained given a threat function (i.e., previously trained for the R-MDP). The decomposition approach is tested over some classical benchmarks.\n\nStrengths:\n\ni) The motivation, organization and the overall writing of the paper are clear.\n\nWeakness:\n\ni) The paper seems to be missing out related works [1,2,3,4,\u2026] that can solve C-(PO)MDPs, which could have been used as baselines (i.e., instead of DQN). Therefore it is not clear if the following claim is true or not: \u201cAlthough our method does not guarantee to find the optimal solution of the CMDP problem, there has not been any study to date that has succeeded in solving a CMDP in dynamical environments as high-dimensional as the ones discussed in this study.\u201d. Overall, I would say this is the weakest part of the paper.\n\nii) It is not clear if the selected experimental benchmarks are challenging. Looking at Table 1 and 2, it seems either i) the selected domains do not benefit from long-term planning, and/or ii) the proposed method learns short-sighted policies. That is in Table 2 for N=15, the proposed method performs similar to MPC over 3 steps.\n\niii) The experimental results require better presentation. For example, since one of the main arguments is that the proposed methodology can solve high-dimensional problems, it would be great to note the dimensionality of the benchmarks in the beginning of section 4. Moreover, none of the figures are readable and are left mostly unexplained.\n\nReferences:\n\n[1] Reinforcement Learning for MDPs with Constraints, Geibel, ECML 2016.\n\n[2] Monte-Carlo Tree Search for Constrained MDPs, Chen et al., IJCAI 2018.\n\n[3] Column Generation Algorithms for Constrained POMDPs, Walraven and Spaan, JAIR 2018.\n\n[4] Hindsight Optimization for Hybrid State and Action MDPs, Raghavan et al. AAAI 2017.",
            "This paper attempts to propose a practical method for solving constrained MDP via decomposing the original problem into two MDPs. In the first phase, it uses a generative model to solve the MDP with the original constraint being the cost. The policy is then used to define secure actions to help solving the second MDP that maximizes the reward. While the method does not guarantee to find the optimal safe policy, the authors claim (and show in few experiments) that it can perform better than classical methods.\n\nWhile the approach seems simply and intuitive, it is not clear whether the contributions are significant for the community. The decomposition seems straightforward and the algorithmic innovation does not seem to be significant. All the difficulties of solving the thread function is being resolved by assuming the access to a generative model. In particular, the authors further consider the case where solving the R-MDP is simpler: the danger is described by known risky events and hence sampling rare events with the generative model is easier. As a result, the experiments primarily focus on navigation tasks. More thorough experiments would be appreciated. In addition, beyond MPC, is it possible to add other baselines that use a learning based approach with access to generative model? If possible, I think this will help to further clarify the benefit of the overall idea of decomposition.   ",
            "This paper proposes an approach to learning safe reinforcement learning policies using a simulator. The main idea is to consider essentially two distinct reward functions on the same MDP, one quantifying the risk/safety level of the states, and the other quantifying the usual reward. A policy is first trained to optimize safety, and then a policy is trained to optimize reward, subject to remaining in the safe region, i.e., subject to a constraint given by the Q function for the safety MDP. If the policy enters an unsafe state, it is required to follow the policy for the safety MDP; the paper includes some theoretical results demonstrating that this is an adequate way to ensure the expected safety remains above a given threshold.\n\nActually, the paper also proposes to decompose the safety reward into scores w.r.t. various events such as colliding with the various objects in the environment, and uses a linear combination of these component scores to obtain an aggregate safety score. This approach provides estimates of the safety scores in novel environments, for example where the configuration of the environment is changed, or with a different number of obstacles. Experiments suggest that the resulting approach is much computationally lighter than model-predictive control, and obtains the safest execution across the various methods considered. The reward obtained remains competitive overall.\n\nThe one downside here is the assumption of a simulator, which is pretty strong. It is true that pure sampling alone is never going to be able to provide a strong safety guarantee, and some kind of assumption is going to be necessary to obtain a strong guarantee, so I don't fault the use of some assumption. At the same time, the existence of the simulator does not make the problem trivial, as the optimization remains challenging, and the relative efficiency of the proposed approach is an argument in its favor. The approach is pretty effective overall and has some nice generalization properties. I recommend acceptance.\n\nOne question I have is whether the decomposition of the environment's threats into different factors could be used with other methods: if the dimension of the state space is sufficiently small, then maybe for example the synthesis (model checking/reachability) methods could be feasibly applied."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review identifies several weaknesses in the paper, including missing related works, unclear experimental benchmarks, and poor presentation of results. The reviewer questions the paper's claims and suggests improvements are needed.",
            "The review expresses doubts about the significance of the paper's contributions, stating that the decomposition seems straightforward and the algorithmic innovation doesn't seem significant. It also points out limitations related to the reliance on a generative model and suggests the need for more thorough experiments and baselines.",
            "The reviewer recommends acceptance and highlights the effectiveness, generalization properties, and computational efficiency of the approach. Positive phrases like 'pretty effective overall,' 'nice generalization properties,' and 'I recommend acceptance' indicate a positive sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'seems to be missing out,' 'it is not clear,' 'require better presentation,' and 'weakest part of the paper,' indicating a critical evaluation of the work.",
            "The review uses phrases like \"it is not clear whether the contributions are significant,\" \"decomposition seems straightforward,\" \"algorithmic innovation does not seem to be significant,\" and \"More thorough experiments would be appreciated,\" which indicate a critical assessment of the work.",
            "The reviewer expresses support for the paper's approach by acknowledging its limitations (simulator assumption) but ultimately arguing for its value and effectiveness. Phrases like 'I don't fault the use of some assumption' and 'the relative efficiency of the proposed approach is an argument in its favor' demonstrate a supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the strengths and weaknesses are addressing different aspects of the paper. The reviewer acknowledges the clarity of writing but points out valid weaknesses in terms of related work comparison, experimental setup, and result presentation. There are no contradictory statements within the review.",
            "The review is consistent in its assessment. It acknowledges the simplicity and intuitiveness of the approach but raises concerns about the significance of the contributions and the limited experimental validation. The reviewer consistently points out the lack of algorithmic innovation and the reliance on a generative model, which simplifies the problem. The suggestions for improvement, such as more thorough experiments and additional baselines, align with the initial concerns about the significance and validation of the proposed method.",
            "The review is consistent in its positive assessment of the paper. It acknowledges a potential limitation (the simulator assumption) but argues that it is a necessary trade-off for strong safety guarantees and does not diminish the paper's contributions. The reviewer consistently highlights the strengths of the proposed approach, such as its computational efficiency, safety performance, and generalization properties, ultimately recommending acceptance."
        ]
    },
    {
        "paper_id": "iclr_2020_BkgGJlBFPS",
        "paper_title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes",
        "paper_abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.",
        "review_ids": [
            "B1xrAKUnoH",
            "BJezGUojtr",
            "SJg6jDF6tr",
            "SJljZmHL9S"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Again, I do not see the point in calling variational approximation variational Bayes is there is no prior on the parameters. You seem to be confusing variational approximation in e.g. EM where a complex distribution is replaced by a factored approximation with variational Bayes where the posterior distribution of the *parameters* is approximated by a factored distribution. Notice that using the Bayes rules does not turn a frequentist approach into a Bayesian one (eg the so-called naive Bayes classifier is not a Bayesian approach unless you add a prior on its parameter). And thus writing \"A core subject of Bayesian inference is concerned with estimating the posterior distribution p(z|x)\" where x is the observed data and z the latent variable is clearly misunderstanding of Bayesian inference. ",
            "Summary: This work proposes an unsupervised hierarchical graph representation learning method, named BayesPool. The method learns a coarsening sequence of graphs together with the corresponding node representations. The coarsening sequence is learned using the method in Loukas (2019). The node representations are learned using an encoder-decoder structure, where the encoder encodes a graph to coarsened node representations, and the decoder decodes the node representations to a coarsened graph. The adopted objective function is analogous to VAE, except that the decoder does not aims to reconstruct an identical graph. Experiments on graph classification is performed on 5 different datasets, and competitive accuracy is achieved.\n\nConcerns: The authors claim that the leant representation in an unsupervised manner is more desirable in terms of generalization. However, they only provide very limited experimental results, which is not very convincing. Moreover, the authors also do not explain clearly on when the node representation of the coarsening sequence is needed.",
            "The authors propose in this paper a new unsupervised graph representation learning method. The method leverages recent advances in graph coarsening, mainly Loukas' method. The key idea of the method consists in using a reconstruction target that is not the classical one in an auto-encoder setting. More precisely, the encoder takes as an input the original adjacency matrix and node features but the decode only aims at reconstructing the coarse adjacency matrix (obtained via Loukas' method). \n\nThe experimental evaluation is quite thorough and shows that the method performs quite well, especially considering it is unsupervised but is compared to supervised representation methods. It would be nice to include statistical tests to assess the significance of the differences in cases were accuracies are very close one to another. A missing part would be to explore the relevance of the learned representation for other tasks (i.e. to use a multi task data set). Of course as the representation is learned in an unsupervised way, one can argue that the current evaluation is already providing an answer.\n\nOverall, I find the paper clear, but the variational bayes part could be much clearer. In fact I'm not sure why this is presented as variational bayes and not only variational. I do not see any prior distribution over parameters, for instance. I understand that the recent \"tradition\" in variational auto-encoder is to use this terminology, but as a (part time) bayesian, this is a bit annoying. ",
            "The paper proposes an unsupervised approach to learn a representation of graphs. The idea comes from an encoder-decoder architecture, which is common in related literature. The paper uses a variational Bayes approach in the learning process. Thorough experiments are provided to justify the feasibility of the method. \n\nThis paper provides an unsupervised style of learning graph representations, which may not be coupled with a specific downstream task so that it may be more useful in general; also, the experiments themselves seem to be at least comparable to the recent methods.  \n\nHowever, I vote for rejecting this submission for the following concerns. \n\n(1) I did not find too many significant differences between this paper and [Kingma & Welling, 2014] in the design of encoder-decoder architecture as well as the learning procedure (I am not an expert in this area so please correct me if I am wrong).\n\n(2) The intuition of learning the representation in an unsupervised manner is interesting and important to me, though the experiments are mostly on the classification tasks. I think it would be helpful to demonstrate the representation power of the learned representation of the graph in tackling other tasks.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses disagreement and points out misunderstandings in the paper, using phrases like \"I do not see the point,\" \"You seem to be confusing,\" and \"clearly misunderstanding.\"",
            "The review expresses concerns about the limited experimental results and lack of clarity in the explanation, indicating a negative assessment of the work's support for its claims.",
            "The review expresses a generally positive assessment of the paper, highlighting the thorough experimental evaluation and good performance of the proposed method. While it points out areas for improvement, the overall impression is favorable.",
            "The review expresses concerns about the novelty and applicability of the method, ultimately recommending rejection of the submission."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical as the reviewer directly challenges the author's understanding and use of terminology, pointing out specific errors and inconsistencies in the paper's approach to Bayesian inference. Phrases like \"I do not see the point,\" \"You seem to be confusing,\" and \"clearly misunderstanding\" indicate a critical stance.",
            "The reviewer uses words like \"concerns\", \"limited experimental results\", \"not very convincing\", and \"do not explain clearly\" which indicate a critical evaluation of the paper's methodology and presentation.",
            "The review offers both positive feedback (e.g., \"experimental evaluation is quite thorough,\" \"method performs quite well,\" \"I find the paper clear\") and constructive criticism (e.g., \"It would be nice to include statistical tests,\" \"A missing part would be to explore the relevance,\" \"variational bayes part could be much clearer\"). This balance indicates a balanced tone.",
            "The reviewer uses phrases like \"I vote for rejecting this submission\" and \"I did not find too many significant differences\" to express concerns and criticisms."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it maintains a single, clear argument throughout: the author is misusing the term 'variational Bayes' by applying it in a context without priors. The reviewer consistently explains why this is a misnomer, differentiating between variational approximation and variational Bayes, and using examples like naive Bayes to support their point. There are no contradictions or shifts in the reviewer's argument.",
            "The review is consistent because it raises valid concerns about the paper's limitations in experimental validation and clarity of explanation, without presenting contradictory statements or viewpoints. The reviewer points out areas where the paper could be improved to strengthen its claims and provide more justification for its approach.",
            "The review is consistent as it provides both positive feedback (clear paper, thorough evaluation, good performance) and constructive criticism (need for statistical tests, multi-task evaluation, clarification of variational Bayes part) without contradicting itself. The reviewer acknowledges the strengths and suggests improvements in a balanced way.",
            "The review is consistent because the reviewer acknowledges the paper's contributions and potential, but raises specific concerns about novelty and the scope of experimental validation that logically lead to the rejection recommendation. The concerns are presented as reasons for rejection, and they align with the reviewer's overall assessment."
        ]
    },
    {
        "paper_id": "iclr_2019_SJG1wjRqFQ",
        "paper_title": "Discrete Structural Planning for Generating Diverse Translations",
        "paper_abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ",
        "review_ids": [
            "ryg9daF53m",
            "HyxXDrx70m",
            "BkgXLmTIaQ",
            "ryx7VpEw3Q",
            "BkxQsCrAn7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the \"Discrete Autoencoders for Sequence Models\" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:\n* comparing with a stronger NMT architecture and larger data-set\n* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.\n* comparison to fully unsupervised latents and some other system, e.g., the system from [1] above\n\nIn the absence of these comparisons and with little novelty, the paper is a clear reject.\n\n[Revision]\n\nGreatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one.",
            "In summary tag planning gives higher BLEU scores, but discrete latents allow controlling diversity?\n\nWhile these results are interesting, it seems to me that there are too many moving parts that needs to be rigorously investigated for the paper to be publishable. I therefore keep my score.",
            "Thank you for your response.\n\nThere seems to be a fundamental misunderstanding re: teacher forcing. It is not teacher forcing that is the issue, the issue is that when training with gold part-of-speech tag sequences the model can look into the future as the tag sequence is derived directly form the target side to be predicted. This issue is not as severe for the autoencoded discrete codes since these are predicted from the source side only. If the part-of-speech tag sequence was predicted in the same way, there would be no time travel effect.\n\nRe: using a fixed number codes. Sure this makes the model slightly simpler, but it is a fundamental limitation since it cannot account for the structure of longer sentences.",
            "The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.\n\nContributions\n-----------------\nA simple method for improving structural diversity.\n\nThe use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.\n\nExperiments suggest that the method is rather effective (albeit only improving translation quality marginally)\n\nI like the proposed discrepancy score based on pairwise BLEU scores.\n\nIssues\n---------\nIt is not clear if teacher forcing was used in the \"tag planning\" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the \"tag planning\" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).\n\nIt is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.\n\nNo details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.\n\nHaving a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.\n\nMinor issues\n-----------------\nCitation for the Xavier method is missing.\n\nNotation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.\n\nThere are many grammatical errors. Please make sure to proofread!\n\n\"Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap.\" What does this mean?",
            "\nThis paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:\n\n\n\u00a71 \u00b61  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vague\u2014I\u2019m not convinced this is meaningful.\n\n\u00a71 \u00b62-3 These paragraphs also vague.\n\n\u00a71 \u00b65 Why is this approach naive? Is this a well-known method? There are no citations.\n\nFig.1 Very confusing: it looks like the target sentence, \u201cstructural tags\u201d and \u201ccoding model\u201d form a loop! This example is also confusing because the \u201cstructural tags\u201d are non-sensical\u2026 they have no relation to this example sentence! I can\u2019t tell if this is because they were made up without relation to the input sentence, or worse, that they\u2019re an actual example from the data, in which case there is something very wrong with the tagger used in the \u201cnaive\u201d experiments.\n\nSec. 2.1 What is the motivation behind the heuristics for the \u201ctwo-step process that simplifies the POS tags\u201d?\n\nSec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these \u201ccodes\" (in the form of \u201csimplified\u201d POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called \u201csoftplus\u201d in Eq. 2) are never explained.\n\nSec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isn\u2019t needed.\n\nTable 1. I\u2019m not sure what the code accuracy tells us. It\u2019s also unclear to me what is means to \u201creconstruct\u201d the \u201coriginal tag sequence\u201d from the codes, esp. given the description in Sec 2.1.\n\nTable 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading\u2026 this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.\n\nTable 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; I\u2019m not convinced that the sequences mean anything per se, but it\u2019s a bit like adding some random noise to the decoder state before generating the word sequence.\n\n5.1 \u201cInstead of letting the beam search decide the best \u2026 we use beam search to obtain three code sequences with highest scores.\u201d I\u2019m confused: what is the difference?\n\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the paper's novelty, methodology, and experimental setup. Phrases like \"little novelty\", \"clear reject\", and criticisms regarding the weak baseline, small datasets, and lack of crucial comparisons contribute to a negative sentiment.",
            "The reviewer states that there are \"too many moving parts that needs to be rigorously investigated for the paper to be publishable\" which is a negative assessment.",
            "The review expresses disagreement and points out fundamental limitations in the approach, using phrases like 'fundamental misunderstanding' and 'fundamental limitation.'",
            "The review identifies several significant issues, including a potential train/test mismatch, lack of comparison with related work, missing details about the tagset, and a major shortcoming in the coding scheme. While it acknowledges some positive aspects, the overall tone is critical due to these concerns.",
            "The review expresses strong dissatisfaction with the paper, using phrases like \"not ready for publication,\" \"poorly motivated,\" \"confusing,\" and \"not convincing.\" The reviewer also points out numerous flaws and expresses skepticism about the results."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a critical tone by pointing out flaws in the paper's approach, such as the lack of novelty (\"it is not new\"), questionable use of supervised data (\"has no clear motivation and seems to cause a number of problems\"), and inadequate comparisons (\"comparing to a weak translation baseline on small data-sets\"). The reviewer also uses direct negative statements like \"the paper is a clear reject.\"",
            "The reviewer uses phrases like \"too many moving parts that needs to be rigorously investigated for the paper to be publishable\" indicating a critical perspective on the paper's readiness for publication.",
            "The tone is critical, directly pointing out flaws and limitations in the author's approach. The reviewer uses phrases like 'fundamental misunderstanding' and 'fundamental limitation' to express their critique.",
            "The review uses phrases like \"major issue,\" \"major shortcoming,\" and \"unfortunate\" to express concerns. The reviewer also directly urges the authors to consider improvements, indicating a critical assessment of the work.",
            "The tone is critical, as evidenced by the direct and negative feedback. The reviewer uses strong language to express their concerns, such as \"very vague,\" \"confusing,\" \"misleading,\" and \"skeptical.\" The reviewer also poses many direct questions showing a lack of understanding of the paper."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer maintains a negative stance on the submitted version of the paper, despite acknowledging the authors' response on dataset size. The initial criticisms regarding novelty, methodology, and missing comparisons are not retracted in the revision. The reviewer explicitly states that the review is based on the submitted version, reinforcing the initial negative assessment.",
            "The reviewer acknowledges the interesting findings but expresses concern about the complexity and the need for more rigorous investigation, which justifies their decision to maintain their score and suggests the paper is not ready for publication yet.",
            "The review is consistent as it raises two separate concerns (misunderstanding of teacher forcing and limitation of fixed number of codes) without any contradiction between them. Both points are presented as areas for improvement in the methodology.",
            "The review is consistent because it acknowledges the contributions of the paper while also pointing out significant issues and areas for improvement. The reviewer appreciates the simplicity and potential of the method, but also raises valid concerns about experimental setup, comparison to prior work, and technical details. There are no self-contradictory statements within the review; it presents a balanced perspective with both positive and negative feedback.",
            "The review is consistently negative, expressing confusion and criticism throughout all sections of the paper. The reviewer finds issues with motivation, clarity, experimental design, and presentation, without any contradictory positive feedback."
        ]
    },
    {
        "paper_id": "iclr_2022_m716e-0clj",
        "paper_title": "Communicate Then Adapt: An Effective Decentralized Adaptive Method for Deep Training",
        "paper_abstract": "Decentralized adaptive gradient methods, in which each node averages only with its neighbors, are critical to save communication and wall-clock training time in deep learning tasks. While different in concrete recursions, existing decentralized adaptive methods share the same algorithm structure: each node scales its gradient with information of the past squared gradients (which is referred to as the adaptive step) before or while it communicates with neighbors. In this paper, we identify the limitation of such adapt-then/while-communicate structure: it will make the developed algorithms highly sensitive to heterogeneous data distributions, and hence deviate their limiting points from the stationary solution. To overcome this limitation, we propose an effective decentralized adaptive method with a communicate-then-adapt structure, in which each node conducts the adaptive step after finishing the neighborhood communications. The new method is theoretically guaranteed to approach to the stationary solution in the non-convex scenario. Experimental results on a variety of CV/NLP  tasks show that our method has a clear superiority to other existing decentralized adaptive methods.",
        "review_ids": [
            "KdE490BuDW6",
            "fxoLQHWUhzd",
            "2wBHIQ7np2s",
            "1yqpuiuAcZM",
            "RS1QF62ZjMk"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I suggest the authors check the original paper of Adam to see the difference between your $\\beta_i$ and their $\\beta_i$. \n\nIn this paper, the coefficient for $m$ (or $v$) is $(1-\\beta_i)$. On the contrary, the coefficient for in standard Adam is $\\beta_i$. \n\nThus, your argument on this hyperparameter does not hold. ",
            "This paper proposes a decentralized adaptive method for distributed deep learning, termed DAG-Adam. Convergence results are provided for smooth non-convex objectives under a bounded gradient assumption. Numerical experiments are conducted on Image Classification (CIFAR10, ImageNet-1k) and Language Modelling (fine-tuning pre-trained BERT models on SQuAD). Strengths:\nI found this paper very interesting, and have been long awaiting progress on the topic of adaptive decentralized optimization for deep learning. Traditional gossip-based methods performing consensus on the parameters cannot be naively applied to adaptive gradient methods, such as Adam, because of the non-linearity in the gradient update (e.g., dividing by the square-root of the second moment buffer), which breaks traditional proofs and significantly degrades performance in practice compared to centralized learning (e.g., see [a]). This paper is very important and contributes to an emerging literature. In particular, to realize the benefits of decentralized (gossip-based) optimization for deep learning, one must develop strategies suited for adaptive gradient methods, especially since many popular deep learning tasks seem to only work with adaptive methods, such as language models and the new swath of image-based methods relying on Vision Transformer architectures.\n\nThis paper also shows a very good familiarity with related work. Despite my current rating, I strongly support this paper, and thus will focus on the main issues I observed below so that the authors can address them, and hopefully find their paper accepted at the end of this process.\n\nWeaknesses:\n- Proposition 1 is somewhat meaningless\u2026 what is relevant here is a lower bound on the squared norm of the sub-optimality of the iterate $x^t_i$ as $t$ goes to infinity. Similarly for Proposition 2.\n- Can you clarify the following: Theorem 1 assumes that not just the gradient is bounded, but that the gradient with an additional consensus penalty is bounded?\n- In Theorem 1, the language indicates that $x^\\star$ is the optimal solution, but note that in the non-convex setting there does not necessarily exist a unique global optimizer.\n- Would be interested in seeing the statistical significance of results (across a few seeds) in Table 1 on CIFAR10.\n- Why does heterogeneity of data distributions have such a large impact on the final validation performance in Table 1? I would expect the heterogeneity to affect the convergence rate, but not the final validation performance.\n- What architecture is used for the Image Classification experiments?\n- I don\u2019t find enough evidence to support the fact that adapt-the-combine is the issue preventing convergence. It is quite bizarre to me that there should be a difference in adapt-then-combine compared to combine-then-adapt, since the two are often trivial re-parameterizations for gossip-based methods. For example, consider your case of the adapt-then-combine structure (section 2.2) equation 8. Let z^{t+1}_j = x^{t}_j - \\gamma \\frac{m^t_j}{\\sqrt{v^t_j} + \\epsilon} and let x^{t+1}_i = \\sum_{j \\in N_i} w_{ij} z^{t+1}_j, then the $z$ variable has the same fixed point iteration as equation (16) in your combine-then-adapt setting. To me it seems that the reason your proposed method seems to resolve the obvious issue of adaptive methods with linear consensus updates is that the consensus update is incorporated into the adaptive momentum buffer (i.e., tracking not just moving average of gradients, but also the consensus constraint), as opposed to swapping the order of adapt-then-combine vs combine-then-adapt. On this note, I would urge you to also take a look at the SlowMo work of [b] which incorporates a slow-moving consensus step into the momentum update of decentralized gossip-based methods.\n- There might be a problem with Figure 1 since none of the methods convergence to the optimal solution in this strongly-convex case. For logistic regression in particular there isn\u2019t an analytic solution for $x^\\star$ so I\u2019m curious how you\u2019re computing it? Moreover, depending on the conditioning, there could be several solutions in the argmax set. I would be interested in seeing the logistic loss plotted instead, and would be particularly interested in seeing the MSE of the variable z (which I mentioned in the bullet above) plotted for the adapt-the-combine methods.\n- With respect to the fixed-point iteration on page 4: the condition $f_i(x) \\neq f_j(x)$ does not necessarily guarantee $H_i \\neq H_j$ unless you make additional assumptions about how f_i and f_j differ. E.g., consider an extreme case where the distribution differences between $D_i$ and $D_j $amount to a constant shift in the expected loss.\n\n[a] Assran et al., Advances in asynchronous parallel and distributed optimization, Proceedings of the IEEE, 2020.\n\n[b] Wang et al., SlowMo: Improving communication-efficient distributed SGD with slow momentum, ICLR, 2019. In short, I find the paper very well motivated, the problem is very important, authors show a familiarity with related work, and the method is sufficiently novel.\n\nHowever, I do not find the claims in the paper to be well supported. Specifically, I don\u2019t find enough evidence to support the fact that adapt-then-combine is the issue preventing convergence of previous methods, and that the proposed method resolves this issue by using a combine-then-adapt strategy, especially since:\n- with a simple re-parameterization, the proposed method can actually be written as an adapt-then-combine strategy\n- propositions 1 and 2 showing issues with adapt-then-combine strategies provide upper bounds, but these are completely uninformative since the relevant quantities should provide a lower bound on the sub-optimality of the iterates with these strategies\nIt seems to me that the proposed method actually works because of the fact that consensus steps are incorporated into the first/second-order momentum buffers, not because the order of adapt/combine is flipped.",
            "In this paper, authors propose the novel modification of Distributed Adam Algorithm that allows heterogeneous data. In this paper authors propose to change the order of adaptation and communication in contrast with D-Adam. More precisely, First, in Section $2$, authors consider heterogeneous data as an input of D-Adam algorithm. They show that algorithm can diverge, moreover, under some additional assumptions it is proven to diverge. Another version of distributed Adam QG-DAdam uses another strategy - adapt-then-communicate. This algorithm also cannot converge to the optimum with heterogeneous input. Finally, they discussed the loss of efficiency of recent GT-DAdam in the same setup.\nThese arguments motivates authors to design the algorithm with communicate-then-adapt. This algorithm is based on the augmented gradient update that allows to interpret it as standard SGD algorithm with the same proof techniques. \n\nIn Theorem $1$, authors present the theoretical result for their algorithm that converges: with the constant learning rate the augmented gradient converges to 0; with $\\gamma = 1/\\sqrt{T}$ the global average of iterates converges to the stationary point.\n\nFinally, in the experiments authors compare their algorithm with different modifications of DAdam (that diverge) and Distributed SGD and shows that in practice their algorithm also makes sence. \n I want to thank authors for their work. The heterogeneous input is very important and I think that the adaptation of Adaptive Distributed Adam to it is an important step. \nThe only thing I am curios is the following: how do authors propose to calculate the global average and how long will it take in the end of the algorithm to calculate it? \nFurthermore, I think that this result can be extended to the case when instead of weighted update with neighbors we select every neighbor with some probability and communicate only among them. This can save a lot of communication cost and make an algorithm even faster.\n\n",
            "This paper proposes a new variant to decentralized Adam with a strategy called Communicate Then Adapt. The paper provides analysis and toy example to illustrate why a traditional Adam would fail to converge to the exact solution, and provide experiments on CV/NLP tasks to substantiate the theory. On one hand, I think the idea of DAG-Adam looks interesting, as it requires no additional computation -- just a reordering on the adapting and communication, the algorithm is guaranteed to converge to the exact solution on strongly convex problems. However, in terms of the correctness and insights, I have three main concerns:\n\n- Both proposition 1 and 2 are shown in terms of the $O(\\cdot)$, which does not really substantiate the claim \"DAdam cannot converge to $x^*$\" since 1) $a=O(\\cdot)$ means $a\\leq C\\cdot$ for some constant $C$, but $a$ can be arbitrarily small; 2) With no lower bound, it's hard to argue a $O(\\cdot)$ is tight, so it's possible that the extra term $G^2/\\epsilon^2$ is incurred by a loose analysis. I think the only way to substantiate the claim is to derive a lower bound.\n\n- The idea of DAG-Adam, different from the DSGD which directly follows from the update rule, requires an additional hyperparameter $\\nu$. This essentially seems to reduce the mixing time of the original communication matrix $W$ -- which is straightforward to verify by Equation (13). This may be unfair in the comparison to other algorithms -- it is possible that DAdam can perform equally good if the mixing matrix there is also tuned by $\\nu$. So I think more illustration is need there, especially in the experiments.\n\n- The paper provides empirical studies on multiple tasks, including image classification and BERT finetuning. However, it's not clear to me how these results interact with the main theorem. As these are non-convex, and there is no $x^*$ anymore, it's unclear whether these improvement comes from the strategy of \"Communicate Then Adapt\", or just the fact that gradient tracker is used/mixing matrix is tuned. On a side note, I recommend the authors to include std for each table, as the results are pretty close. Please refer to the main review. I'd be happy to increase my score if these concerns are properly addressed.",
            "This paper developed a new decentralized adaptive gradient descent method to address the data heterogeneity problem. The motivation is clear and the experimental results show improvement over existing methods. However, the theoretical analysis is not  solid.  Pros:\n1. The motivation is clear. It shows why existing methods do not converge to the stationary point. \n\n2. The extensive experiments can support the superiority of this new method. \n\nCons:\n\n1. The novelty is not significant. It is an extension of (Yuan et al., 2016) to the adaptive gradient. \n\n2. This method can only converge to the neighborhood of the stationary point. However, existing methods can converge to the stationary point.\n\n3. $\\beta_2$ is lower bounded. It is not very reasonable. $\\epsilon$ should be very small. Then, this lower bound is very large. \nIt conflicts with the common practice that $\\beta_2=0.1$ or $\\beta_2=0.01$. Thus, this convergence rate is problematic. \n\n4. How is $\\beta_1$? Is there any constraint for $\\beta_1$?\n\n5. Which value is used for $\\beta_2$ in the experiment?\n\n5. Which value is used for $\\epsilon$ in the experiment? Typically, $\\epsilon$ should be very small, e.g., $1e-7$. According to Theorem 1, $v$ should be smaller than $\\epsilon$. But it looks $v$ is large in the experiment. Thus, I don't think the hyperparameters in Theorem 1 are reasonable. \n  The problem studied is interesting. But the method is not novel and the theoretical analysis is not solid. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review points out a significant flaw in the authors' understanding of the Adam optimizer, stating that their argument \"does not hold.\" This critical feedback indicates a negative sentiment.",
            "While the reviewer acknowledges the paper's strengths (motivation, problem importance, familiarity with related work, novelty), they ultimately state that the claims are not well-supported by evidence. Phrases like \"I don\u2019t find enough evidence to support...\" and concerns about the validity of the proposed solution contribute to a negative overall sentiment.",
            "The reviewer explicitly thanks the authors for their work, calls the adaptation of Adaptive Distributed Adam to heterogeneous input an 'important step,' and states that the algorithm 'makes sense' in practice. The reviewer expresses curiosity rather than criticism, indicating a positive overall sentiment.",
            "The review expresses concerns about the correctness and insights of the paper, questioning the substantiation of claims, the fairness of comparisons, and the relevance of empirical results to the theoretical findings. Phrases like 'does not really substantiate the claim,' 'may be unfair,' and 'it's not clear to me' indicate a negative sentiment.",
            "The review expresses concerns about the theoretical analysis, novelty, and hyperparameter selection, ultimately concluding that the method is not novel and the theoretical analysis is not solid."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer directly challenges the authors' argument and points out a discrepancy between their understanding and the original Adam paper. Phrases like \"I suggest the authors check\" and \"your argument on this hyperparameter does not hold\" demonstrate a critical tone.",
            "The review offers constructive criticism while also expressing strong doubts about the paper's central claims. Phrases like \"somewhat meaningless,\" \"It is quite bizarre to me,\" and \"I do not find the claims in the paper to be well supported\" indicate a critical stance. The reviewer challenges the authors' reasoning and experimental design.",
            "The reviewer uses phrases like 'I want to thank authors for their work' and acknowledges the importance of the research area. The suggestions are framed as curiosities and potential extensions, not criticisms of the current work. The language is encouraging and appreciative.",
            "The review adopts a critical tone by directly pointing out flaws in the paper's reasoning, analysis, and experimental setup. Phrases such as 'I have three main concerns,' 'does not really substantiate the claim,' 'This may be unfair,' and 'it's not clear to me' demonstrate a critical evaluation of the work.",
            "The review uses phrases like \"not significant,\" \"problematic,\" \"not reasonable,\" and \"I don't think\" to express disagreement and point out flaws in the paper's methodology and theoretical underpinnings."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it points out a specific discrepancy between the authors' description and the standard Adam algorithm, and logically concludes that the authors' argument about the hyperparameter is therefore invalid.",
            "The review is consistent because it acknowledges the paper's strengths, such as addressing an important problem and showing familiarity with related work, while also clearly pointing out weaknesses, particularly regarding the lack of strong evidence to support the central claims about 'adapt-then-combine' being the main issue. The reviewer maintains a balanced perspective, appreciating the paper's potential but requiring more robust justification for its conclusions. There are no contradictory statements; the reviewer's critique is focused on specific aspects needing improvement while recognizing the paper's overall value.",
            "The review is consistent as it provides a summary of the paper, appreciates the work, and raises constructive questions and suggestions without any contradictions.",
            "The review is consistent because it acknowledges the potential of the proposed method while raising specific, valid concerns about the theoretical analysis, experimental setup, and the interpretation of results. The reviewer's points are logically connected and aim to improve the paper's rigor and clarity, without contradicting themselves.",
            "The review is consistent because it clearly outlines both the strengths (clear motivation, good experimental results) and weaknesses (lack of novelty, theoretical issues, hyperparameter concerns) of the paper in separate 'Pros' and 'Cons' sections. The final summary reinforces the identified weaknesses, particularly regarding novelty and theoretical analysis, without contradicting any earlier points. The questions raised in the 'Cons' section are all related to the theoretical soundness and experimental setup, further supporting the reviewer's overall concern about the theoretical aspects of the paper."
        ]
    },
    {
        "paper_id": "iclr_2020_Hkxp3JHtPr",
        "paper_title": "Deep Variational Semi-Supervised Novelty Detection",
        "paper_abstract": "In anomaly detection (AD), one seeks to identify whether a test sample is abnormal,  given a data set of normal samples.   A recent and promising approach to AD relies on deep generative models, such as variational autoencoders (VAEs),for unsupervised learning of the normal data distribution. In semi-supervised AD (SSAD), the data also includes a small sample of labeled anomalies. In this work,we propose two variational methods for training VAEs for SSAD. The intuitive idea in both methods is to train the encoder to \u2018separate\u2019 between latent vectors for normal and outlier data. We show that this idea can be derived from principled probabilistic formulations of the problem, and propose simple and effective algorithms.  Our methods can be applied to various data types, as we demonstrate on SSAD datasets ranging from natural images to astronomy and medicine, and can be combined with any VAE model architecture. When comparing to state-of-the-art SSAD methods that are not specific to particular data types, we obtain marked improvement in outlier detection.",
        "review_ids": [
            "H1x83bU7cS",
            "BkgL_iMqKH",
            "BJgu6JrjtS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes two variational methods for training VAEs for SSAD (Semi-supervised Anomaly Detection). Experiments on benchmarking datasets show improvements over state-of-the-art SSAD methods.\n\nIn generally, the paper is well written. But I have some concerns.\n\n1. Some of the results have not yet been obtained.\n\n2. Missing some relevant references.\nIn addition to VAEs, there is another class of deep generative models - random fields (a.k.a. energy-based models, EBMs), which have been applied to anomaly detection (AD) recently. Particularly, the unsupervised AD results on MNIST and CIFAR-10 from [2] are much better than the proposed methods (MML-VAE, DP-VAE).\nThough semi-supervised AD is interesting, good performances on unsupervised AD can be a baseline indicator of the effectiveness of the AD models. The authors should add comments and comparisons.\n\n[1] S. Zhai, Y. Cheng, W. Lu, and Z. Zhang, \u201cDeep structured energy based models for anomaly detection,\u201d ICML, 2016.\n[2] Y. Song, Z. Ou. \"Learning Neural Random Fields with Inclusive Auxiliary Generators,\" arxiv 1806.00271, 2018.\n\n3. \u201cFor all of the experiments, our methods use an ensemble of size K = 5.\u201d\nAre other methods also tested by using an ensemble?\n\n--------update after reading the response-----------\nThe updated paper has been improved to address my concerns.\n\nI partly agree with the authors that their results demonstrate the importance of the semi-supervised AD setting (a 1% fraction of labelled anomalies can improve over the state-of-the-art AD scores of deep energy based models). However, I think, the proposed methods in this paper will not be as competitive as semi-supervised deep energy based models.",
            "The papers proposes to use VAE-like approaches for semi-supervised novelty detection. Two methods are describes:\n(1) the MML-VAE fits a standard VAE to the normal samples and add a repulsive term for the outliers -- this term encourages the encoder to map the outliers far from the latent-space prior distribution.\n(2) the DP-VAE fits a VAE with a mixture of Gaussian prior on the latent space -- one mixture component for normal samples, and one mixture component for outlier samples.\n\nThe described methods are simple, natural, and appear to work relatively well -- for this simple reason, I think that the text could be accepted.\n\nThere are several things that are still not entirely clear.\n(1) without the reconstruction term, the methods are small variations of supervised methods. Consequently, I feel that the authors should try to explain much more carefully why the introduction of a reconstruction term (which could be thought as an auxiliary task) helps.\n(2) given point (1), one could think of many auxiliary task (eg. usual colorisation, or rotation prediction, etc..) Would it lead to worse results?\n(3) proportion > 10% of anomaly is relatively standard for supervised-methods + few standard tricks to work very well. Although I understand that only one small subset of anomalies is presented during training, I think that it would still be worth describing in more details the efforts that have been spent to try to make standard supervised methods work. \n",
            "This paper presents two novel VAE-based methods for the (more general) semi-supervised anomaly detection (SSAD) setting where one has also access to some labeled anomalous samples in addition to mostly normal data. The first method, Max-Min Likelihood VAE (MML-VAE), extends the standard VAE objective that maximizes the log-likelihood for normal data by an additional term that in contrast minimizes the log-likelihood for labeled anomalies. To optimize the MML objective, the paper proposes to minimize the sum of the standard (negative) ELBO for normal samples and the so-called CUBO, which is a variational upper bound on the data log-likelihood, for anomalous samples. The second method, Dual Prior VAE (DP-VAE), modifies the standard VAE by introducing a second separate prior for the anomalous data, which is also Gaussian but has different mean. The DP-VAE objective then is defined as the sum of the two respective ELBOs which is optimized over shared encoder and decoder networks (with the adjustment that the outlier ELBO only updates the encoder). The anomaly score for both models then is defined as the (negative) ELBO of a test sample. Finally, the paper presents quite extensive experimental results on the benchmarks from Ruff et al. [2], CatsVsDogs, and an application of robotic motion planning which indicate a slight advantage of the proposed methods.\n\nI am quite familiar with the recent Deep SAD paper [2] this work builds upon and very much agree that the (more general) SSAD setting is an important problem with high practical relevance for which there exists little prior work. Overall this paper is well structured/written and well placed in the literature, but I think it is not yet ready for acceptance due to the following key reasons: \n(i) I think DP-VAE, the currently better performing method, is ill-posed for SSAD since it makes the assumption that anomalies are generated from one common latent prior and thus must be similar; \n(ii) I think the worse performance of MML-VAE, which I find theoretically sound for SSAD, is mainly due to optimization issues that should be investigated; \n(iii) The experiments do not show for the bulk of experiments how much of the improvement is due to meta-algorithms (ensemble and hyperparameter selection on a validation set with some labels).\n\n(i) DP-VAE models anomalies to be generated from one common latent distribution (modeled as Gaussian here) which imposes the assumption that anomalies are similar, the so-called cluster assumption [2]. This assumption, however, generally does not hold for anomalies which are defined to be just different from the normal class but anomalies do not have to be similar to each other. Methodologically, DP-VAE is rather a semi-supervised classification method (essentially a VAE with Gaussian mixture prior having two components) which the paper itself points out is ill-posed for SSAD: \u201c... the labeled information on anomalous samples is too limited to represent the variation of anomalies ... .\u201d I suspect the slight advantage of DP-VAE might be mainly due to using meta-algorithms (ensemble, hyperparameter selection) and due to the rather structured/clustered nature of anomalies in the MNIST, F-MNIST, and CIFAR-10 benchmarks.\n\n(ii) I find MML-VAE, unfortunately the worse performing method, to be a conceptually sound approach to SSAD following the intuitive idea that normal samples should concentrate under the normal prior whereas the latent embeddings of anomalies should have low likelihood under this prior. This approach correctly does not make any assumption on the latent structure of anomalies as DP-VAE does. I believe MML-VAE in its current formulation leads to worse results mainly to optimization issues that I suspect can be resolved and should be further investigated. I guess the major issue of the MML-VAE loss is that the log-likelihood for outlier samples has steep curvature and is unbounded from below. Deep networks might easily exploit this without learning meaningful representations as the paper also hints towards. This also results in unstable optimization. I think removing the reconstruction term for outliers, as the paper suggests, also helps for this particular reason but this is rather heuristic. These optimization flaws should be investigated and the loss adjusted if needed. Maybe simple thresholding (adding an epsilon to lower bound the loss), gradient clipping, or robust reformulations of the loss could improve optimization already?\n\n(iii) To infer the statistical significance of the results and to assess the effect of meta-algorithms (ensemble, hyperparameter tuning) an ablation study as in Table 4 (at least on the effect of ensembling) should be included also for the major, more complex datasets. Which score is used for hyperparameter selection (ELBO, log-likelihood, AUC)? How would the competitors perform under similar tuning?\n\n\n####################\n*Additional Feedback*\n\n*Positive Highlights*\n1. Both proposed methods can be used with general data types and VAE network architectures (the existing Deep SAD state-of-the-art method employs restricted architectures).\n2. The paper is well placed in the literature and all major and very recent relevant work that I am aware of are included.\n3. This is an interesting use of the CUBO bound which I did not know before reading this work. This might be interesting for the general variational inference community to derive novel optimization schemes.\n4. I found the robotic motion planning application quite cool. This also suggests that negative sampling is useful beyond the AD task.\n5. I appreciate that the authors included the CatsVsDogs experiment although DADGT performs better as it demonstrates the potential of SSAD. I very much agree that employing similar self-supervised learning ideas and augmentation is a promising direction for future research.\n\n*Ideas for Improvement*\n6. Extend the semi-supervised setting to unlabeled (mostly normal), labeled normal, and labeled anomalous training data. The text currently formulates a setting with only labeled normal and labeled anomalous samples. A simple general formulation could just assign different weights to the unlabeled and labeled normal data terms.\n7. There might be an interesting connection between MML-VAE and Deep SAD in the sense that MML-VAE is a probabilistic version of the latter. The $\\chi_n$ distance of the CUBO loss has terms similar to the inverse squared norm penalty of Deep SAD.\n8. Report the range from which hyperparameters are selected.\n9. Add the recently introduced MVTec AD benchmark dataset to your experimental evaluation [1].\n10. Run experiments on the full test suite of Ruff et al. [2]. At the moment only one of three scenarios are evaluated.\n\n*Minor comments*\n11. Inconsistent notation for the expected value ($\\mathbb{E}$ vs $\\mathbf{E}$)\n12. In Section 3, the parameterization of the variational approximate $q(z | x)$ is inconsistently denoted by $\\phi$ and $\\theta$ (which beforehand parameterizes the decoder).\n13. In Section 3.2, the current formulation first says that MC produces a biased, then an unbiased estimate of the gradients.\n14. First sentence in Section 4: I would not use \u201cclassify\u201d but rather \u201cdetect\u201d etc. for anomaly/novelty detection since the task differs from classification.\n15. In Section 4.2, there should be a minus in front of the KL-divergence terms of the $ELBO_{normal}$ and $ELBO_{outlier}$ equations.\n16. In the fully unsupervised setting on CIFAR-10 (Table 5), why is the VAE performance essentially at random (~50) in comparison to CAE and Deep SVDD although they use the same network architecture?\n17. Is the CUBO indeed a strictly valid bound if one considers the non-normal data-generating distribution?\n18. Are there any results on the tightness of the CUBO?\n\n\n####################\n*References*\n[1] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad\u2013a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9592\u20139600, 2019.\n[2] L. Ruff, R. A. Vandermeulen, N. Go\u0308rnitz, A. Binder, E. Mu\u0308ller, K.-R. Mu\u0308ller, and M. Kloft. Deep semi-supervised anomaly detection. arXiv preprint arXiv:1906.02694, 2019."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review acknowledges the paper's quality (\"well written\") and improvements after updates, but also expresses concerns about missing results, references, and comparisons to other methods. The final statement suggests a lack of complete conviction in the proposed method's competitiveness.",
            "The reviewer states the methods 'appear to work relatively well' and concludes that the paper 'could be accepted'. This indicates a generally positive sentiment.",
            "The review expresses concerns about the ill-posed nature of one of the proposed methods (DP-VAE), optimization issues with the other (MML-VAE), and the lack of thorough evaluation regarding the contribution of meta-algorithms. While acknowledging the paper's strengths, the reviewer ultimately recommends rejection due to these key reasons."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive (\"well written\", \"updated paper has been improved\") and negative aspects (concerns about results, references, and comparisons). It also offers specific suggestions for improvement, indicating a constructive approach.",
            "The review acknowledges the strengths of the paper ('simple, natural, and appear to work relatively well') but also raises several points of concern and questions ('several things that are still not entirely clear'). This balanced approach indicates a neutral tone overall, providing both positive and constructive criticism.",
            "The review uses phrases like \"ill-posed,\" \"worse performance,\" and \"optimization issues.\" It directly questions the validity of assumptions made by one method and points out flaws in the experimental setup. The reviewer also uses \"not yet ready for acceptance\" which indicates a critical stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer initially raises concerns about missing results, references, and experimental details. After the authors addressed these concerns in the updated paper, the reviewer acknowledges the improvements. While the reviewer still expresses a reservation about the competitiveness of the proposed method compared to energy-based models, this is presented as a future outlook rather than a contradiction to the improvements made. The reviewer's opinion evolves logically from initial concerns to acknowledging improvements while maintaining a nuanced perspective.",
            "The review is consistent because it acknowledges the potential of the proposed methods while raising valid questions for improvement and clarification, without contradicting its initial positive assessment. The reviewer expresses a generally positive view while pointing out areas that need further explanation and comparison with existing methods, which is a constructive and consistent approach to reviewing.",
            "The review is consistent in its assessment. It acknowledges the paper's strengths, such as novelty, placement in literature, and interesting ideas, while also clearly outlining significant weaknesses related to the theoretical soundness of DP-VAE, optimization issues with MML-VAE, and insufficient experimental validation. The reviewer's critique is focused and provides specific reasons for why the paper is not yet ready for acceptance, without contradicting the acknowledged positive aspects. The suggestions for improvement further reinforce the constructive and consistent nature of the review."
        ]
    },
    {
        "paper_id": "iclr_2020_HkxlcnVFwB",
        "paper_title": "GenDICE: Generalized Offline Estimation of Stationary Values",
        "paper_abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.",
        "review_ids": [
            "BkgNEfo2ir",
            "SJxp7c5hjr",
            "r1geCyJTKB",
            "H1l3zHj6Yr",
            "Hyluc57aFB"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thank the author for the detailed response. The ablation study answers all my questions and the current paper is quite solid.  It would be great if the author can open source the code, which will definitely benefit the OPE community.",
            "Thank you for the response, the ablation study is clear and provide important guidance in empirical experiments.\n\nFor the comparison to Liu et al. normalized method, it seems that is only used in continuous setting. For tabular setting since we can constraint the summation of density function to be 1, a constraint optimization (like mirror descend or quadratic programming is enough). The normalized trick in their equation (11) (divided by $z_w$) I believe is mainly used in continuous setting.\n\nI am still wondering compared to the regularizer you use in equation (11), which one is more stable in optimization process with neural network? ",
            "Main contributions:\nThis paper generalizes the recent state-of-the-art behavior agnostic off-policy evaluation DualDice into a more general optimization framework: GenDice. Similar to DualDice, GenDice considers distribution correction over state, action pairs rather than state in Liu et al. (2018), which can handle behavior-agnostic settings. The optimization framework (in equation (9)) is novel and neat, and the practical algorithm seems more powerful than the previous DualDice. As a side product, it can also use to solve offline page rank problem.\n\nClarity:\nThis paper is well established and written. \n\nConnection of theory and experiment:\nI have a major concern for the theory 1 about the choice of regularizer $\\lambda$. For infinite samples case, the derivation of theory 1 is reasonable since both term is nonnegative. However, in practice we will have empirical gap for the divergence term, thus picking a suitable $\\lambda$ seems crucial for the experiment. I think a discussion on $\\lambda$  for average case in experiment part should be added. And compared to Liu et al. (2018) which normalized the weight of $\\tau$ in average case, which one is better in practice?\n\nOverall I think this paper is good enough to be accepted by ICLR. The optimization framework can also inspire future algorithm using different divergence.",
            "This paper proposes a new estimator to infer the stationary distribution of a Markov chain, with data from another Markov chain. The method estimates the ratio between stationary distribution of target MC and the empirical data distribution.  It is based on the observation that the ratio is a fixed point solution to certain operators. The proposed method could work in the behavior-agnostic and undiscounted case, which is unsolved by the previous method.\n\nThis paper tackles an interesting problem with an increasing number of studies in the reinforcement learning community and gives a practical algorithm with strong empirical justification, as well as theoretical justification. I think this paper should be accepted.\n\nDetailed comments:\n1) This paper provides experiment results in multiple domains, including two continuous control domains which are more complex than experiments in previous OPE methods. The paper also provides many details about the learning dynamics and ablation studies, which is very useful for the reader to understand the result of the paper.\n2) The theoretical result is as same strong as previous work DICE and DualDICE, under similar assumptions.\n3) I appreciate this paper formalizes the two difficulties of degeneration and intractability, and then explain how those are addressed in a principled way. Degeneration is important and is at least ignored in two similar works on this topic.",
            "In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks. The new proposed method utilize the variational representation of $f$-divergence, which quantifies the difference between $\\mathcal{T}\\tau$ and $\\tau p$, where $\\tau$ is the parametric density ratio between the unknown behavior policy data and the target policy. If only if $\\tau$ is the true density ratio, the loss $\\mathcal{D}_{f}(\\mathcal{T}\\tau || \\tau p) = 0$. \n\nCompared with prior work (Nachum et. al 2019), the new proposed framework can generalize the undiscounted case $\\gamma = 1$, and the derivation for the new algorithm is quite simple and easy to follow. The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases. Moreover, I have two specific questions:\n\n- The choice of $f$-divergence. Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various $f$-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice).\n\n-The authors should also have a discussion that similar idea can be generalized to more general  distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship. I think there is a concurrent submission using MMD metrics).\n \n\nOverall I think this is a good paper and I recommend for acceptance. \n\nReference Papers:\n- Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-gan: Training generative neural samplers using variational divergence minimization.\" Advances in neural information processing systems. 2016.\n- Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017).\n- Nachum, Ofir, et al. \"DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections.\" arXiv preprint arXiv:1906.04733 (2019).\n- Anonymous, \u201cBlack-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning\u201d, submitted to ICLR 2020.\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses satisfaction with the author's response and states that the paper is \"quite solid.\" The suggestion to open-source the code is framed as a benefit to the community, indicating a positive outlook.",
            "The reviewer expresses appreciation for the authors' response and finds the ablation study clear and provides important guidance. The reviewer also engages in constructive questioning, indicating a positive interest in the work.",
            "The reviewer states the paper is 'well established and written,' the optimization framework is 'novel and neat,' the algorithm 'seems more powerful,' and concludes the paper is 'good enough to be accepted by ICLR.'",
            "The reviewer states \"I think this paper should be accepted.\" and highlights multiple strengths of the paper.",
            "The reviewer states 'Overall I think this is a good paper and I recommend for acceptance.' This clearly indicates a positive sentiment."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer thanks the author, finds the paper \"quite solid,\" and suggests open-sourcing the code in a way that emphasizes the benefits to the community, indicating a supportive stance.",
            "The review starts with a positive note (\"Thank you for the response, the ablation study is clear and provide important guidance\"), but also raises a point of clarification regarding the comparison to Liu et al. and poses a question about the stability of the regularizer, indicating a balanced approach.",
            "The reviewer uses positive language like 'well established,' 'novel and neat,' 'more powerful,' and 'good enough to be accepted.' The reviewer also offers constructive criticism, indicating a desire to help improve the paper rather than simply dismissing it.",
            "The reviewer uses positive language, such as \"interesting problem\", \"practical algorithm with strong empirical justification, as well as theoretical justification\", and \"very useful for the reader to understand the result of the paper\". They also express appreciation for the paper's formalization of difficulties.",
            "The reviewer expresses a positive overall assessment and recommends acceptance. While they pose questions and suggest improvements, the tone is encouraging and constructive, indicating support for the paper's publication. Phrases like 'it would be nice to have' and 'the authors should also have a discussion' are polite suggestions rather than harsh criticisms."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and expresses satisfaction with the author's response and the paper's quality. There are no contradictory statements or conflicting opinions within the review. The reviewer uses positive language throughout, indicating a consistent positive assessment of the work.",
            "The review is consistent because the reviewer starts with a positive acknowledgement of the ablation study, then raises a specific question about the applicability of a method in different settings (continuous vs. tabular), and finally asks a comparative question about the stability of different regularization approaches. These points are logically connected and do not contradict each other, forming a coherent line of inquiry.",
            "The review is consistent because it highlights the strengths of the paper, such as novelty and clarity, while also pointing out a specific concern regarding the practical application of the theory. The reviewer suggests improvements rather than contradicting their positive initial assessment, ultimately recommending acceptance.",
            "The review is consistently positive. The reviewer starts with a strong positive statement recommending acceptance and then provides detailed comments that further support this positive assessment. All points raised in the detailed comments are positive aspects of the paper, such as strong empirical justification, theoretical justification, comprehensive experiments, and addressing important difficulties in the field. There are no contradictory statements or mixed signals in the review.",
            "The review is consistent because the reviewer praises the paper for its contributions, simplicity, and experimental results, and the suggestions for improvement are constructive and do not contradict the positive overall assessment and recommendation for acceptance."
        ]
    },
    {
        "paper_id": "nips_2022_aQySSrCbBul",
        "paper_title": "Generalization Properties of NAS under Activation and Skip Connection Search",
        "paper_abstract": "Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordingly, our numerical validation shed light on the design of computationally efficient methods for NAS. Our analysis is non-trivial due to the coupling of various architectures and activation functions under the unifying framework and has its own interest in providing the lower bound of the minimum eigenvalue of NTK in deep learning theory.",
        "review_ids": [
            "PzdA2wtITUD",
            "UIgQV4b06UG",
            "lJm1wiXZOe9",
            "LhWoq1xzo8t",
            "sf1tY8L8yjp",
            "GX16Dv7o0D",
            "A1yQaxuBJXS",
            "G7iJ-GGJ-ai",
            "vDNAK89XmqJ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your comment, my answer indeed happened concurrently with your change.\n\nThough I find the new title / abstract much better, I decided to keep my rating to 6 after some hesitations, due to limited significance.",
            " Dear authors,\n\nI am glad to know that my major concerns have been addressed at this time. I sincerely hope that your answer A2 can be placed before your studying on the minimum eigenvalue in the main paper so as to provide a good motivation for why studying the minimum eigenvalue is important and needed to understand NAS. I believe this can make the paper more coherent. Overall, I decided to increase my score to 6.",
            " Thank you to the authors for their answers to my review!\n\nWhile I still think that the title and the abstract are a little misleading and the paper orientation could be improved, I raise my rating to 6.",
            " Dear authors,\n\nI appreciate that you have provided detailed explanations to my questions. Most of my questions have been addressed. But my concerns regarding the weaknesses of this paper have not been explicitly answered. I have read your general response, but it can not directly address my concerns. I hope the authors can give more explanations on this part.",
            " Thank you to the authors for their rebuttal in answering my questions. After the rebuttal, I agree that their NTK analysis on mixed activations and skip-connection setup can be the direction to understanding the neural architecture search field. However, I am still not convinced since this theoretical approach is limited to a subset of the NAS scenario: leveraging NTK to find the architecture topology. To my understanding, the upper/lower bound of the minimum eigenvalue of NTK (Theorem 1) determines the generalization bound (Theorem 3). Therefore, their theoretical framework supports the NTK-based search method but not the general NAS approach. I believe the authors are overstating their contributions. \n\nIn conclusion, I raise my rating from reject to borderline accept.",
            " This paper provides an NTK analysis for a class of residual networks with a mixture of activation functions at different layers. The authors derive bounds on the minimum eigenvalue for both finite and infinite width settings. They also give a generalization bound for this class of network architecture. The main contribution of this work, in my opinion, is a non-trivial derivation of lower and upper bounds on the minimum and for residual networks with a mixture of activation functions. However, analysis and proof techniques are mainly taken from the well-established NTK literature, such as Huang et al., 2020, Oymak and Soltanolkotabi, 2020\u2026 Similar generalization bounds are established by Cao and Gu, 2019, and Arora et al., 2019a.\n\nThere is an interesting connection to NAS that the authors observe empirically where NAS always picks up RELU and/or Leaky RELU in the optimal search. That coincides with the fact that RELU and LeakyRELU imply larger minimum eigenvalue in the NTK sense. However, deep neural networks in practice do not generally operate in the NTK regime. \n\nIn short, I see this work purely as an NTK analysis rather than an \u201cunifying framework\u201d for NAS. I don\u2019t see how the work studies \u201coptimization and generalization of NAS\u201d or how the derived results guide NAS.\n\nIn terms of the presentation, the paper is well-written and easy to follow. One thing I would suggest is to instantiate Theorem 1 with a few special cases, such as the standard 2-layer networks with ReLU activation. Illustrating the Hermite coefficients of such cases would help the readers to compare with existing results, e.g., Oymak and Soltanolkotabi, 2020. What is the data used in the simulation of minimum eigenvalues of NTK for different activations and architectures? N/A",
            " This paper extends the Neural Tangent Kernel limit to different (and mixed) activations functions, as well as a varying proportion of skip-connections. Then this paper is able to bound the minimum eigenvalue of the neural tangent kernel matrix, and to link this value to the generalization capability of the model. This allows to guide Neural Architecture search without training.\n\nThe authors validate their methods numerically by first showing that the derived bounds match empirical results, and that using their bounds to guide NAS improves NAS performances.\n\n # Originality\n\nWhile I am not very familiar with the field, this is to my knowledge the first time that an NTK approximation was derived for various and mixed activations functions.\n\n# Quality\n\nThe paper contributions seems sounds, though the empirical validation could be more convincing: how well does the criterion rank different architectures?\n\n# Clarity\n\nThe paper is clearly written. The title and the abstract, while quite clear, appear to be a little misleading: I don't think this paper helps understanding NAS (for instance NAS convergence), rather than providing a criterion which can help guide NAS.\n\n# Significance\n\nWhile the empirical results don't show a huge improvement, being able to guide NAS across different activation functions and skip-connections proportions seems useful. The result would be much more significant if it applied to CNN and Transformers, but this is an interesting progress.\nI wish the authors had tested their approach on more challenging tasks, and with more competitive activation functions. Can this framework be used to guide NAS through choices more meaningful than choosing between a ReLu and a Sigmoid?\n\nCould the authors investigate how reliable is their criterion to guide NAS? Maybe something like how often their criterion would rank two models in the correct order? The authors clearly state that a big limitation of their work is that it only works for FC networks, and not more common CNN or Transformers.",
            " This paper provides the theoretical analysis of a fully-connected neural network with mixed activation functions in both infinite and finite width schemes. Furthermore, the authors provide analysis on both lower and upper bounds of minimum eigenvalues of NTK and generalization error bounds in SGD. Finally, the authors provide empirical experiments to support their theoretical claims on NAS problems.  Strengths\n1. Building upon Cao et al. and Nguyen et al.'s proof framework, the author provides the theoretical analysis of the minimal eigenvalue of NTK and generalization error on mixed activation neural networks.\n\nWeaknesses\n1. A considerable gap exists between theoretical analysis and the actual NAS problem. Their theoretical results only provide the minimum eigenvalue of NTK and generalization error of a fully-connected neural network with mixed activation functions. I will elaborate on this in more detail in the Question section.\n2. NAS-Bench-201 results in Table 2 are not compelling enough. Latest methods such as $\\beta$-DARTS [3] or DrNAS [4] find the optimal architecture in NAS-Bench-201.\n3. General NAS problems contain a larger degree of freedom in choosing the components of a neural network, not only just activation functions and skip connections. Similar to the first point, there is a considerable discrepancy between the analysis and practice.  1. Why does the theoretical analysis of mixed activation neural networks help the understanding of the NAS problem? NAS algorithms such as DARTS or random search WS have two phases: search and evaluation. The authors show just an analysis of the evaluation phase in the paper, which corresponds to the final architecture. In other words, the paper seems to study NTK and generalization bounds on a particular family of the neural network, which has mixed activation functions. \n2. Continuing from the Q1, how do we know architecture found from DARTS or random search RS are optimal architecture as stated in the line 147? \n3. In lines 295-296, the author mentions that DARTS results coincide with the theoretical results. However, a long line of NAS research has shown DARTS's problems ([3, 4, 5, 6, 7]), especially the overfitting. Will other latest algorithms such as $\\beta$-DARTS or DRNAS find the same results as DARTS? \n4. In Table 2, Eigen-NAS on CIFAR-10 with $k=20$ seems to converge to sub-optimal architecture with a very small standard deviation of 0.01. Since the optimal architecture achieves $94.37\\%$, can we conclude that Eigen-NAS only can find sub-optimal architecture? From 15,625 architecture candidates in NAS-Bench-201, what is the ranking of the architecture Eigen-NAS found? \n5. Can you also verify the numerical results on DARTS Search Space with Eigen-NAS? The author provides a thorough theoretical analysis of the minimum eigenvalue on NTK and generalization bounds on a neural network with mixed activation functions. However, the analysis is only for a particular family of neural networks (with mixed activation functions and selective skip-connections). Furthermore, the neural network analysis is on the final architecture, assuming that existing NAS algorithms, such as DARTS,  random search WS, or Eigen-NAS, can find the optimal architecture from their search phase. \n\nOverall, I see the authors' theoretical analysis is not closely correlated to understanding the actual NAS problem. Furthermore, the empirical performance is far lower than existing state-of-the-art in NAS literature. Therefore, I give a reject. \n\n1. Cao, Yuan, and Quanquan Gu. \"Generalization bounds of stochastic gradient descent for wide and deep neural networks.\" Advances in neural information processing systems 32 (2019).\n2. Nguyen, Quynh, Marco Mondelli, and Guido F. Montufar. \"Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep relu networks.\" International Conference on Machine Learning. PMLR, 2021.\n3. Ye, Peng, et al. \"b-DARTS: Beta-Decay Regularization for Differentiable Architecture Search.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n4. Chen, Xiangning, et al. \"Drnas: Dirichlet neural architecture search.\" arXiv preprint arXiv:2006.10355 (2020).\n5. Liang, Hanwen, et al. \"Darts+: Improved differentiable architecture search with early stopping.\" arXiv preprint arXiv:1909.06035 (2019).\n6. Dong, Xuanyi, and Yi Yang. \"Nas-bench-201: Extending the scope of reproducible neural architecture search.\" arXiv preprint arXiv:2001.00326 (2020).\n7. Yang, Antoine, Pedro M. Esperan\u00e7a, and Fabio M. Carlucci. \"NAS evaluation is frustratingly hard.\" arXiv preprint arXiv:1912.12522 (2019).",
            " This paper presents theoretical analyses on Neural Architecture Search using the recent theory of NTK. Especially, it has provided the lower and upper bounds of the minimum eigenvalue of the NTK matrix as well as the generalization bound induced by the NTK matrix for the architectures in their pre-defined search space. Finally, based on these theoretical analyses, this paper develops a training-free NAS algorithm, namely Eigen-NAS. Eigen-NAS has shown its competitive performance in training-free NAS, which in turn also supports the theoretical analyses in this paper. *Strengths*\n\n1. This paper has provided non-trivial theoretical analyses on NAS using the recent theory of NTK, which may inspire more theoretical studies on NAS in the NAS area.\n2. The Eigen-NAS inspired by their theoretical results shows competitive performance on different NAS benchmarks.\n\n*Weaknesses*\n\n1. This paper does not really study the convergence or optimization of NAS as what has been claimed in its title or abstract. Instead, it is mainly focusing on the generalization property of its restricted search space (i.e., a small search space compared with the standard NAS search space).\n2. The motivation for why the minimum eigenvalue of NTK needs to be studied and how it is explicitly related to the generalization bound (i.e., Theorem 3) have not been well clarified. As a result, the study of the minimum eigenvalue of NTK in this paper may be less related. Using the minimum eigenvalue of NTK to bound the generalization performance of DNN in Theorem 3 will make the theoretical results in this paper more coherent.\n3. This paper does not provide a clear or explicit interpretation of how the theoretical results in this paper can help us understand NAS (shown in the title). Instead, from my own perspective, this paper has mainly developed a generalization bound for NAS and then proposed a training-free NAS algorithm (i.e., Eigen-NAS) inspired by their theoretical results. And such a training-free NAS algorithm is similar to the one in [R1] using the trace norm of the NTK matrix. 1. This paper focuses on the theoretical study of NAS with only skip connection and activation function search mainly due to the non-trivial analysis of other general structures. Interestingly, both [R2] and Shu et al. [2020] have empirically shown that topology will be more important than diverse operations in NAS, and NAS with only topology search can also provide competitive performance compared with standard NAS. I suppose that these results may also serve as a good motivation to support why the theoretical study on only skip connection and activation function will still be attractive in the literature.\n2. In line 98, to my best knowledge, Du et al. [2019] and Nguyen [2021] are studying the convergence of GD instead of SGD.\n3. In the problem setting section of this paper, an explicit representation of the classification task (i.e., the binary classification task) and the possible value of $y$ (i.e., $y \\in \\\\{-1,1\\\\}$) will be more helpful for the readers to understand this section. Similarly, the \u201ccross-entropy loss\u201d in line 111 can more specific, e.g., the cross-entropy loss with a sigmoid function to represent the probability of $y=1$.\n4. In line 134, I suppose it should be \u201cwe conduct the skip connection search and activation function search independently\u201d. Besides, I am curious about why skip connection search and activation function search can not be conducted jointly.\n5. The lemma 1 and therefore the results based on Lemma 1 actually do not match the initialization applied in this paper (i.e., the Gaussian initialization with the variance of $1/m$ instead of $2/m$ in Tirer et al. [2021], Huang et al. [2020], Belfer et al. [2021].).\n6. The claim about depth $L$ in line 208 may not be true because whether the minimum eigenvalue will increase or not with larger $L$ highly depends on the activation functions as shown in Figure 3,4. Moreover, I am curious about why the minimum eigenvalue of Tanh does not increase with an increasing $L$ especially when it achieves larger $\\beta_1$ and $\\beta_2$ and hence a larger upper bound of the minimum eigenvalue than ReLU and Sigmoid? \n7. One of the comments about Table 2 is that the search cost can also be included to verify the improved efficiency of Eigen-NAS over KNAS. Another one is that the recent training-free NAS paper [R1] should also be included in the comparison or discussion. Because [R1] exploits the trace norm of the NTK matrix to indicate the performance of different architectures while this paper exploits the Frobious norm in its Eigen-NAS algorithm (line 324). They are similar.\n8. An empirical study of how the number of skip connections layers (i.e., the topology) will affect the performance of neural architectures will be more interesting and related to the NAS area, which can also further verify the impact of the skip connection on the minimum eigenvalue of NTK (Theorem 1, 2) and also the generalization performance of candidate architecture (Theorem 3).\n\n[R1] NASI: Label-and Data-agnostic Neural Architecture Search at Initialization\n\n[R2] Exploring randomly wired neural networks for image recognition This paper has provided a valuable discussion about limitations and future works."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Neutral",
            "Neutral",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the improvement in the title/abstract but maintains the original rating due to concerns about limited significance. This indicates a mixed feeling, leaning towards neutral.",
            "The reviewer expresses satisfaction that their concerns were addressed and increased the score. Phrases like 'I am glad to know' and 'I sincerely hope' indicate a positive outlook.",
            "The reviewer is raising their rating, indicating a positive shift in their evaluation despite lingering concerns.",
            "The reviewer expresses remaining concerns and indicates that their questions haven't been fully addressed, using phrases like \"concerns regarding the weaknesses\", \"have not been explicitly answered\", and \"it can not directly address my concerns\".",
            "The reviewer acknowledges the authors' efforts and raises their rating but remains unconvinced about the broad applicability of the work.",
            "The review acknowledges the paper's contribution as a \"non-trivial derivation\" but also points out limitations such as reliance on existing NTK literature and a disconnect between the theoretical analysis and practical NAS applications. The reviewer also offers constructive suggestions for improvement.",
            "The review expresses overall positive sentiment, highlighting the paper's originality and potential usefulness in guiding Neural Architecture Search (NAS). The reviewer acknowledges the paper's clear writing and the value of extending the Neural Tangent Kernel limit to different activation functions.",
            "The review expresses significant concerns about the paper's practical relevance to NAS problems and the empirical results. Phrases like 'considerable gap exists between theoretical analysis and the actual NAS problem,' 'not compelling enough,' 'considerable discrepancy between the analysis and practice,' and 'not closely correlated to understanding the actual NAS problem' indicate a negative sentiment. The final statement 'Therefore, I give a reject' confirms this.",
            "While the reviewer acknowledges the paper's strengths, the weaknesses section is extensive and detailed, highlighting several significant concerns regarding the paper's claims, motivation, theoretical coherence, and empirical validation. The reviewer questions the paper's focus, the clarity of its motivation, the validity of its theoretical results, and the completeness of its empirical evaluation. The detailed criticism suggests that the reviewer has reservations about the paper's overall quality and contribution."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude for the comment, acknowledges the improvement in the title/abstract, but also provides a reason for not increasing the rating. This shows a balanced perspective, considering both positive and negative aspects.",
            "The reviewer offers constructive suggestions ('can be placed before') and expresses a desire to help the authors improve the paper ('so as to provide a good motivation'). The overall tone is encouraging and aims to assist the authors.",
            "The reviewer expresses gratitude to the authors and explicitly states they are raising their rating, suggesting a supportive stance even while mentioning areas for improvement. The phrase 'a little misleading' is softened, showing a willingness to compromise.",
            "The tone is critical because the reviewer points out weaknesses in the paper and expresses dissatisfaction with the authors' response, using phrases like \"weaknesses of this paper\" and \"can not directly address my concerns.\"",
            "The review expresses both agreement and disagreement. It starts with appreciation (\"Thank you to the authors\") but also voices concerns (\"I am still not convinced\", \"overstating their contributions\"). The change in rating also reflects a balanced perspective.",
            "The review offers both positive and negative feedback. It acknowledges the paper's contribution while also pointing out limitations and areas for improvement. The language is generally polite and constructive, suggesting specific improvements rather than simply criticizing.",
            "The review adopts a balanced tone, acknowledging the paper's strengths (originality, clarity) while also pointing out weaknesses (empirical validation, limited scope). Phrases like 'While the empirical results don't show a huge improvement' and questions about the criterion's reliability indicate a critical yet constructive approach.",
            "The tone is critical, as evidenced by the direct and pointed questioning ('Why does the theoretical analysis of mixed activation neural networks help the understanding of the NAS problem?'), the highlighting of discrepancies and weaknesses ('A considerable gap exists...'), and the overall assessment of the paper's shortcomings. The use of phrases like 'not compelling enough' and the direct rejection recommendation further contribute to the critical tone.",
            "The review uses direct criticisms such as \"does not really study the convergence\", \"have not been well clarified\", \"less related\", \"does not provide a clear or explicit interpretation\", and \"may not be true\". These phrases indicate a critical assessment of the paper's content and presentation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer acknowledges the improvement in the title and abstract, but maintains the initial rating due to the limited significance of the work. This is consistent as the reviewer is separating the improvement in presentation (title/abstract) from the core issue of significance.",
            "The reviewer expresses satisfaction that their concerns have been addressed, suggests a minor improvement for coherence, and increases the score, indicating a consistent positive assessment.",
            "The reviewer maintains their concerns about the title, abstract, and paper orientation, but still raises the rating. This indicates a consistent evaluation where the reviewer acknowledges the authors' response and sees enough positive change to increase the score despite the remaining issues.",
            "The review is consistent because the reviewer acknowledges the authors' efforts in addressing some questions but clearly states that their main concern regarding the weaknesses of the paper remains unaddressed. The reviewer's message is clear and does not contradict itself.",
            "The reviewer acknowledges the value of the authors' work after the rebuttal and appreciates the direction of their NTK analysis. While the reviewer still points out limitations regarding the scope of the theoretical approach to a subset of NAS, this critique is consistent with the raised rating to borderline accept, indicating a nuanced evaluation rather than a contradiction.",
            "The review is consistent. It acknowledges the technical contribution of the paper in NTK analysis, specifically the derivation of bounds. However, it consistently argues that the paper overstates its connection to NAS and its practical implications for NAS. The reviewer sees the work as primarily a theoretical NTK analysis and not a practical guide for NAS, which is a consistent line of reasoning throughout the review.",
            "The review is consistent because it acknowledges the theoretical contribution of the paper (extending NTK, bounding eigenvalue) and its potential for guiding NAS, while also pointing out limitations in empirical validation (how well criterion ranks architectures, limited improvement), scope (FC networks only, not CNN/Transformers), and significance (challenging tasks, competitive activations). The reviewer consistently highlights both the strengths and weaknesses of the paper without contradicting themselves.",
            "The reviewer consistently argues that while the paper presents theoretical analysis, it is not well-connected to the practical Neural Architecture Search (NAS) problem. The weaknesses and questions raised consistently highlight the gap between the theoretical analysis and the actual NAS problem, and the lack of compelling empirical evidence to bridge this gap. The reviewer's rejection decision is a logical conclusion based on these consistent arguments about the limited practical relevance of the theoretical contribution.",
            "The review provides both strengths and weaknesses in a balanced and constructive manner. The criticisms are specific and aimed at improving the paper, without contradicting the acknowledged strengths. The reviewer's overall assessment is consistent in identifying valuable contributions while also pointing out areas for improvement and clarification."
        ]
    },
    {
        "paper_id": "iclr_2022_LcF-EEt8cCC",
        "paper_title": "Denoising Likelihood Score Matching for Conditional Score-based Data Generation",
        "paper_abstract": "Many existing conditional score-based data generation methods utilize Bayes' theorem to decompose the gradients of a log posterior density into a mixture of scores. These methods facilitate the training procedure of conditional score models, as a mixture of scores can be separately estimated using a score model and a classifier. However, our analysis indicates that the training objectives for the classifier in these methods may lead to a serious score mismatch issue, which corresponds to the situation that the estimated scores deviate from the true ones. Such an issue causes the samples to be misled by the deviated scores during the diffusion process, resulting in a degraded sampling quality. To resolve it, we theoretically formulate a novel training objective, called Denoising Likelihood Score Matching (DLSM) loss, for the classifier to match the gradients of the true log likelihood density. Our experimental evidences show that the proposed method outperforms the previous methods on both Cifar-10 and Cifar-100 benchmarks noticeably in terms of several key evaluation metrics. We thus conclude that, by adopting DLSM, the conditional scores can be accurately modeled, and the effect of the score mismatch issue is alleviated.",
        "review_ids": [
            "P-a7-gupzto",
            "qbQ9f5aMNwr",
            "GAgP65CEMT",
            "D1VbqWS9Op_",
            "1b3_o7HVmRw"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors suggest how to apply the idea of score matching to the problem of conditional generation. They introduce a new loss, which is essentially a score matching loss for the conditional distribution p(y| x). They demonstrate how to effectively calculate it without calculating p(y|x) explicitly, which seems to be the main contribution of the paper. Then the authors demonstrate how to train the model for the conditional generation in practice: it is necessary to add to the proposed loss the usual cross-entropy loss, because the loss that they came up with is noisy (this can be seen in Figure 5). They got better results compared to baselines for cifar-10 and cifar-100. Some disadvantages\n\n1) The loss is quite noisy (see Figure 5), and the authors do not explain why. \nI personally think that the reason is because they approximate p(y|x) with a mixture of Gaussians with a small variance and there is the same effect with smoothing as in case of the standard score matching. \nProbably we should start training using larger variance values and gradually decrease the variance during the training.\n\n2) I think that the motivation of the paper has too many somewhat irrelevant details. The authors compare different models, trained under different conditions, and conlude that the problem is in the loss. It seems that it was enough to say that it is difficult to approximate the gradients, so it is necessary to explicitly add them to the loss.\n\n3) Models (a) and (c) are different (in the first case it is a single model, in the second case the authors use different models for different classes). The first model minimizes cross-entropy loss, the second model optimizes the score matching function. \nIt is not entirely clear why the authors believe that the model (a) works worse than the model (c)  because of the loss? The models are also different. Maybe this is the main reason?\n\n4) In the same section with the motivation I propose to add the cross-entropy loss between p(y|x) and p(x|y), and not just the loss between their gradients (maybe the model (a) also approximates them poorly). The question immediately arises, why a good cross-entropy loss does not guarantee a good approximation of the gradients?\n\n5) Why do the authors train the classifier p(y | x, \\theta) and the score model (x, \\phi) independently, not end-to-end?\n\n6) Why is their loss noisy in practice? How much does the value of \\tau (noise variance) affect the stability of the loss and the results? Why don't the authors do the same trick with \\tau as with \\sigma (that is, they don't train several different models with different values of \\tau)?\n\n7) Experiments are done only on cifar-10/cifar-100, which is not very convincing, of course. In general, the paper is well written, it has a clear and logical structure, it is easy to read. \nIn the introduction, only clearly highlighted contributions are missing.\n\nThe proposed criteria can be useful when training models for conditional generation.\nThe authors did rather detailed discussion of different variants of the score matching procedure.\n\n============\n\nI thank the authors for their responses. The updated version of the paper, with additional information here and there, improves the readability and the overall narrative. Given the authors responses now I am convinced in the concept proposed by the authors, so I can raise my score.\n",
            "This paper presents a conditional diffusion(score-matching) model to tackle the score mismatch issue in the conditional generation scenario. This paper tests multiple alternatives in creating a conditional distribution through diffusion models, and this paper introduces a classifier assisted structure for the conditioning. 1.\nI have a question on the derivation of Eq (A1). Particularly, authors suddenly introduce $Z(\\tilde{x})$. It will be great if authors can explain how to main the equality, after the introduction of $Z$.\n\n2.\nIt seems that the scores are mismatched across conditions. For example, the score on condition A cannot be matched to the score on condition B, which is quite natural. Given that it is good to see that the authors performed the posterior score-matching by separating the score-matching by conditions. At lease, Figure 1 shows not much difference between posterior SM and suggested models. Also, Table 1 suggests not much difference (or rather favorable to (c) posterior SM than the suggested models) between the posterior SM and the suggested model.\n\nThen, what would be the gain by not just using posterior SM and by following the suggested models?\n\n3.\nI cannot find the clear definition of $L_{CE}$ which would be basically classifier, but it would be better to give a clear form of classifier with inputs and outputs.\n\n4.\nThe experiment does not report negative loglikelihood, or bpd. Why don't you report NLL?\n\n5.\nThere is no evaluation through the comparisons with the baseline models. As authors suggested that there are recent rush on the conditional modeling with diffusion approaches, why don't you find some and compare the performance with them?\n Niche paper to introduce a conditional generative model by the diffusion process",
            " Thanks for the response and I updated my score to be on the accept side.",
            "The paper points up a previously underappreciated problem in training classifiers in the context of conditional generations of diffusion-based generative models. The authors propose a novel objective for training classifiers to tackle the problems. Informally, the generation process of the diffusion-based generative models can be described by repeatedly applying an update rule with initial values: $\\tilde{x} \u2190 \\tilde{x} + \\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(\\tilde{x}) + \\sigma \\epsilon$ where $\\epsilon \\sim N(0,I)$ and the initial points are sampled from a prior distribution such as standard Normal distributions. Similarly to unconditional generations, one can perform conditional generations, e.g., $p(\\tilde{x} | y)$ by using the following update rule: $\\tilde{x} \u2190 \\tilde{x} + \\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(\\tilde{x}) + \\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(y | \\tilde{x}) + \\sigma \\epsilon$. Note that $\\nabla_{\\tilde{x}} \\log p(\\tilde{x} | y) = \\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(\\tilde{x}) + \\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(y | \\tilde{x})$. Thus, for a given pre-trained generative model, one needs to learn $\\log p_{\\textrm{model}}(y | \\tilde{x})$ (or its gradient wrt $\\tilde{x}$. Maximum likelihood training (MLE), i.e., minimizing cross-entropy loss, is commonly used. Inevitably, the qualities of the gradient $\\nabla_{\\tilde{x}} \\log p_{\\textrm{model}}(y | \\tilde{x})$ will determine the qualities of the conditional generations.\n\n\nFirst, the paper emphasizes that MLE-training of classifiers results in non-smooth gradient landscape wrt input; thus, the resulting gradients negatively affect the generation qualities. More precisely, with MLE, the learned $\\log p_{\\textrm{model}} (y|\\tilde{x})$ is high, while its gradient wrt $\\tilde{x}$ isn't necessarily close to the ground truth. In the paper, the authors refer to this phenomenon as\u00a0*a score mismatch issue.* The authors analyze the mismatch issue with toy experiments and demonstrate its negative effects on generation qualities compared to the ground truth.\n\n\nSecond, to resolve the\u00a0*score mismatch issue*, the paper proposes a new objective function, called Explicit Likelihood Score-Matching loss (ELSM), in which the mean squared errors between $\\nabla_{\\tilde{x}} \\log p_{\\textrm{model}} (y|\\tilde{x})$ and $\\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (y|\\tilde{x})$ is minimized. Here, due to the inaccessibility $\\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (y|\\tilde{x})$, the authors reduces the ELSM loss to another objective function, named\u00a0*Denoising Likelihood Score-Matching*\u00a0(DLSM), similarly to denoising score matching derivation. In DLSM, $\\nabla_{\\tilde{x}} \\log p_{\\textrm{model}} (y|\\tilde{x})$ is trained to match to $\\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (\\tilde{x}) - \\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (\\tilde{x} | x)$. Acknowledging that $\\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (\\tilde{x})$ is still unattainable, the authors propose to minimize approximate DLSM loss, where $\\nabla_{\\tilde{x}} \\log p_{\\textrm{data}} (\\tilde{x})$ is substituted by the pre-trained scores $\\nabla_{\\tilde{x}} \\log p_{\\textrm{model}} (\\tilde{x})$.\n\n\nThen, the authors demonstrate that by toy experiments, training classifiers with the approximate DLSM improves the conditional generation qualities compared to MLE-based training. Moreover, the authors mention that approximate DLSM-based training can be unstable in high-dimensional datasets and show that training classifiers by minimizing combined loss of approximate DLSM and cross-entropy can further improve the generation qualities.\n\n\nLastly, the paper demonstrates the effectiveness of the proposed methods by evaluating conditional generation qualities on CIFAR-10 and CIFAR-100 datasets. **Strengths of the paper**  \nIn general, the paper's contributions are clear, and I also consider that the results are essential for several reasons:  \n\n\nFirst, the paper highlights a previously less-recognized problem in the context of conditional generations of diffusion-based generative models. Moreover, the authors provide sufficient analysis to help readers understand the problem and potential drawbacks of commonly accepted methods. In particular, the analysis to visualize the difference between the MLE-based training and the proposed method is very interesting to read.\n\n\nSecond, the paper introduces a novel training objective to tackle the problem above, called Denoising Likelihood Score Matching (DLSM). Recently, the interest in diffusion-based generative models has increased rapidly. Consequently, the interest in controlling the generation process of such models has also increased. Regarding this, I consider that the proposed method has huge applicability and will contribute to the ML communities.\n\n\nThird, the authors have devoted themselves to discussing practical techniques for applications of the proposed method. For example, the authors discuss that the effect of combining two losses, i.e., MLE and approximate DLSM, for training classifiers. In particular, the paper discusses the potential roles of two losses on the performance of the conditional generations throughout ablation studies. Furthermore, the authors provide additional analysis about the scaling method, including how they contribute to high-precision samples.\n\n**Weaknesses of the paper**  \nHowever, the following aspects of the paper can be improved.\n\n\nFirst, clarifying three objectives seems necessary, i.e., ELSM, DLSM, and approximate DLSM. In my opinion, when the pre-trained score is plugged into the DLSM objective, it is no longer DLSM loss. However, the current version hasn't sufficiently emphasized the difference, so I found that the current version can be potentially misleading. Moreover, the provided proofs and relevant theoretical discussions are about ELSM and DLSM, not approximate. As a result, readers infer the behaviors of the approximate DLSM only by empirical results. Thus, it is important to distinguish between DLSM and approximate DLSM.\n\n\nSecond, while the experiments are well-thought and easy to follow in general, some experimental results are unclear. Moreover, additional experiments seem required. For instance, it is unclear how FID and IS are evaluated. At first glance, I understood that the FID/IS are evaluated per class (and their averaged value are reported). However, the paper describes that they are evaluated unconditionally. If the same model was used, the unconditional generators should have been the same for the baselines and the proposed method. If it wasn't, the description of the experiments needed to be updated. Another concern is that it is unclear when the combined loss is used and when it isn't during the experiments. In particular, it is unclear if \"ours\" in Table 1 corresponds to the approximate DLSM or the combined loss since the rest of the experiments have used the combined loss. I consider it important to clarify Table 1 and potentially include the results of models trained with the combined loss. Similarly, it would be interesting to see the results of CIFAR-10/100 experiments without MLE-losses.\n\n\nThird, the presentation of some backgrounds can be improved. In particular, it seems like that the current submission introduces the discrete-time Langevin dynamics, which is time-homogeneous, as a building block of the diffusion-based generative models. However, the paper follows the recent works, specifically time-inhomogeneous diffusions, which have made significant distinctions of the recent works against the time-homogeneous ones in several perspectives; for example, practicality in the context of generative models.\n\n**Minor comments**\n- I found that matching the y-axis scale in Figs 4 and A.1. will help interpret the results.\n- Optionally, I found it beneficial to run 1-dimensional experiments as done in Sec 3 but present results similar to Fig A.1. In my understanding, the classifiers trained with the approximate DLSM will underestimate the gradient (thus smoother) in comparison to the DLSM (I may be wrong). On the other hand, the MLE-based training results in unnecessarily sharper. Plotting the learned gradients and the ground truth will be helpful to explain why combining two losses makes sense.\n- In my understanding, $\\tilde{y}$ may not be necessary to derive DLSM. Could you explain why noisy $\\tilde{y}$ is necessary? In general, the paper's contributions are clear; the proposed methods are well-motivated and well-discussed. In addition, I also found that the paper has a well-organized structure so that it is clear to understand the proposed method and other practical techniques to improve training. Thus, I'm inclined to accept the paper. However, I found that several aspects of the submission can be improved. I hope that the aforementioned weak points are well addressed.",
            "The authors present a new objective, denoising likelihood score-matching (DLSM), and training mechanism for conditional score-based data generation. The new method is motivated by poor conditional score estimates observed in low-dimensional examples for the standard, Bayes-theorem-based, conditional data generation procedure of score-based models. The new objective is equivalent to directly learning to parameterise the score of p(y|x) demonstrates is high-dimensional benchmark datasets also. FID and IS improve due to this method, with the largest improvements being class-wise, and there is an overall tendency to trade off generated data recall for precision. **Strengths**\n- Sections 1 and 2 are very clear and exhibit strong understanding of the field and the most recent literature. Upon examination, the proof of (2) in A.2 appears to be correct.\n- The motivating example, in Section 3 and Figure 1, is clearly presented, describes a relevant synthetic case, and the results demonstrate enough improvement to mean the paper's flow and narrative works well. The metrics $D_P$ and $D_L$ are also valuable in this more tractable setting.\n- In Section 4, Equations (9) and (10) follow intuitively and are well explained. I have been through the proof of Theorem 1 in detail and I believe it is correct (there wasn't even a typo!). Figure 2 is also a useful/pertinent depiction of the training process.\n- I believe all of the experiments presented in Section 5 are useful, clear, and well laid out.\n\n**Weaknesses**\nThe principle weakness I can see in the paper is the lack of a breakdown of \"inaccurate likelihood scores\".  Although empirical evidence suggests the classifier is at fault, the paper only uses references for this motivation. There is no explanation as to how one might stratify these classifier score inaccuracies. As one example, does a stronger classifier provide better gradients for conditional generation? This line of research would potentially bolster the training procedure in Figure 2 by further guiding the involvement of the classifier.\n\n- The use of \"some extra information\" at the start of Section 2.4 is casual and belies the importance of the class.\n- In Section A.3, I believe \"was $p_{\\alpha}(\\mathbf{\\tilde{x}},\\mathbf{\\tilde{y}})$ not\" should be \"$p_{\\alpha}(\\mathbf{\\tilde{x}},\\mathbf{\\tilde{y}})$ was not\".\n- At the end of page 5: \"numerical results in Table 1 are\" rather than  \"numerical results in Table 1 is\"\n- The introduction of $p_{0,\\tau}(\\mathbf{x}|\\mathbf{\\tilde{y}})$ in the 5th line of the $G(\\theta)$ derivation is slightly jarring given that the zero in the subscript is not used elsewhere or explicitly described.\n\nStylistic choices:\n- Use of \\left[ and \\right] and latex would be nicer, but this is subjective.\n- Same goes for the use of \\lVert and \\rVert for norms instead of \\parallel (more for divergences). Overall, I think this is a strong paper that would make a positive contribution to the conference and the current discussion surrounding SBMs.  The paper is very well presented and explained throughout, with appropriate figures and tables, as well as accurate derivations. Class conditional generation does appear to improve based on this method, and the complementary performances trade-offs (distribution precision) are broken down and discussed. Besides one line of research that could be included in a future paper, weaknesses are minor and/or aesthetic."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they are now \"convinced in the concept proposed by the authors\" and will \"raise my score.\"",
            "The review raises several concerns about the paper, including unclear derivations, questionable gains over existing methods, lack of definition for a key term, missing evaluation metrics, and absence of comparison with baseline models. The reviewer also points out that the experimental results don't clearly favor the proposed model over a simpler approach.",
            "The reviewer explicitly states they \"updated my score to be on the accept side,\"",
            "The reviewer states that the paper's contributions are clear, the methods are well-motivated and discussed, and the paper is well-organized. The reviewer also mentions being inclined to accept the paper.",
            "The reviewer states \"Overall, I think this is a strong paper that would make a positive contribution to the conference\". The reviewer also highlights numerous strengths of the paper, including clarity, strong understanding of the field, correct proofs, and useful experiments."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Supportive",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude to the authors (\"I thank the authors for their responses\") and acknowledges the improvements made to the paper (\"improves the readability and the overall narrative\"). This indicates a supportive and encouraging tone.",
            "The reviewer uses phrases like \"I have a question,\" \"It seems that,\" \"I cannot find,\" \"Why don't you,\" and \"There is no evaluation,\" which indicate a critical and questioning tone. The reviewer also directly questions the value of the proposed model: \"Then, what would be the gain by not just using posterior SM and by following the suggested models?\"",
            "The reviewer expresses gratitude (\"Thanks for the response\") and indicates a positive shift in their evaluation, suggesting a supportive attitude.",
            "The review presents both strengths and weaknesses of the paper, providing constructive criticism and suggestions for improvement while acknowledging the paper's merits. The reviewer uses phrases like 'Strengths of the paper' and 'Weaknesses of the paper' to structure their feedback, and ends by expressing an inclination to accept the paper pending improvements.",
            "The reviewer uses encouraging language, such as \"I believe\", \"useful/pertinent\", and \"valuable\". They also explicitly state that the paper is \"very well presented and explained throughout\" and that the weaknesses are \"minor and/or aesthetic.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially raised several questions and points of concern regarding the paper's methodology, motivation, and experimental setup. However, after the authors responded and updated the paper, the reviewer explicitly states that they are now convinced by the proposed concept and will raise their score. This indicates a consistent and logical progression of the reviewer's opinion, moving from initial critique to acceptance based on the authors' revisions and clarifications. There is no contradiction in the reviewer's statements; instead, there is a clear evolution of their assessment based on new information.",
            "The review is consistent in its critique. It acknowledges the problem the paper addresses but questions the necessity and added value of the proposed method over simpler alternatives like posterior score-matching, based on the presented experimental results. The reviewer consistently asks for clarifications (Eq A1, L_CE definition), improvements in evaluation (NLL, baselines), and questions the gain of the proposed method compared to posterior SM, indicating a consistent line of questioning regarding the paper's contribution and clarity.",
            "The review is consistent because the reviewer expresses gratitude for the response and states they updated their score to 'accept' as a result of the response. Both parts of the sentence are aligned and indicate a positive change in opinion due to the authors' response.",
            "The review is consistently positive about the paper's contributions and novelty, while also providing constructive criticisms and suggestions for improvement. There are no contradictory statements or conflicting viewpoints within the review. The reviewer clearly outlines strengths and weaknesses, maintaining a balanced and helpful tone throughout.",
            "The review is consistently positive overall. While it points out weaknesses, these are framed as minor issues or suggestions for improvement, not fundamental flaws. The reviewer explicitly states that the weaknesses are minor and/or aesthetic, and emphasizes the strengths of the paper in terms of clarity, correctness, and contribution."
        ]
    },
    {
        "paper_id": "iclr_2020_S1lF8xHYwS",
        "paper_title": "Unsupervised Domain Adaptation through Self-Supervision",
        "paper_abstract": "This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously.  Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain.  The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.",
        "review_ids": [
            "rkxxcB1Rtr",
            "rklFudcVqB",
            "HkxybvFNqH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper presents a novel unsupervised domain adaptation framework for neural networks. Similarly to existing approaches, it performs adaptation by aligning representations of the source and the target domains. The main difference is that this alignment is achieved not through explicitly minimizing some distribution discrepancy (this usually leads to challenging minimax optimization problems). Instead, the authors propose to use a battery of auxiliary self-supervised learning (SSL) tasks for both domains simultaneously. Each task is meant to align the source and the target representations along a direction of variation relevant to that task. Assuming that the battery is diverse enough, optimizing the representation for all the tasks leads to matching of the distributions. \n\nPros:\n+ The paper is well-written and easy to read.\n+ I like the simplicity of the idea and the fact that it achieves competitive performance without any adversarial learning (which may be very tricky to deal with).\n+ The paper presents a reasonable procedure for hyper-parameter tuning and early stopping which seems to work well in practice.\n\nCons:\n- The paper is purely practical with no theory backing the approach. As a result, the discussion of guarantees and limitations is quite brief.\n- It\u2019s unclear how easy it is to come up with a reasonable set of SSL tasks for a particular pair of domains. It seems that it may become a serious problem when the method is applied to something other than benchmarks. Table 2 reveals that there is no consistent improvement over the existing approaches which suggests that the chosen battery of SSL tasks is not universal (as the authors themselves admit). On a related note, it\u2019s a bit disappointing that the authors mention SVHN results as a failure case but never provide a way to address the issue.\n- It would be nice to some results for the Office dataset for completeness. The authors could use a pre-trained network as a starting points just like it\u2019s done in other papers. According to the last paragraph of Section 6 this experiment should be feasible.\n\nNotes/questions:\n* Table 2, last column: The performance of DIRT-T seems to be better than that of the proposed method and yet the latter is highlighted and not the former.\n\nOverall, I think it\u2019s a good paper presenting a thought-provoking idea. In my opinion, the weakest point of the work is the lack of any (neither principled nor practical) guidance as to how to choose the set of self-supervised tasks. Despite this I feel that this submission should be accepted but at the same time I\u2019m curious to see what the authors have to say regarding the concerns I raised in my review.",
            "This paper describes an approach to domain adaptation that uses\nself-supervised losses to encourage source/target domain alignment for\nunsupervised domain adaptation. The authors propose to use four\nself-supervised tasks (variants of tasks used in the self-supervised\nrepresentation learning for object recognition literature) that are\nused with a combined loss including unlabeled source and target\ntraining samples. The authors also propose an alignment heuristic for\nguiding early stopping. Experimental results on a standard battery of\ndomain adaptation problems are given, plus some intriguing baseline\nresults for semantic segmentation.\n\nThe paper is written very well and the technical development and\nmotivations for each decision are well discussed and argued.\n\n1. The experimental evaluation is a bit limited as the object\n   recognition datasets are a bit limited. Results on Office or\n   Office-Home would be nice.\n\n2. Using location classification for semantic segmentation seems\n   intuitively to be encouraging the network to learn coarse spatial\n   priors (which should be invariant across the two domains). Have you\n   looked at how alignment is actually happening? More qualitative\n   analysis in this direction would be useful to appreciate the\n   proposed approach.\n\n3. Related to the previous point, it would be interesting to see how\n   semgmentations in the unsupervised domain gradually change and\n   improve with increasing alignment.\n\nIn summary: the ideas are simple, intuitive, and well-explained -- I\nthink the results reported would be easy to reproduce with minimal\nhead scratching. The experiments are interesting and not overstated.\n",
            "This paper introduces an unsupervised domain adaptation method that uses self-supervised tasks to bring the two different domains closer together. It runs experiments on some classic benchmarks.\n\nMy score for this paper is weakly rejected because \n\n(1) the concept of self-supervision is not first proposed by this paper. The proposed method is not novel. It introduces three simple self-supervision tasks: flip, rotation and location, and the performance is not better than previous results such as DIRT-T; \n\n(2) there are 7 benchmarks in Table2, but only 2 of 7 has result on R+L+F. In the paper, it mentioned because the result is not better, but the author should still provide them. \n\n(3) it emphasizes the contribution of encouraging more study of self-supervision for unsupervised domain adaptation. It doesn\u2019t provide any way for how to design self-supervision task or whether more tasks is better. I think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper. \n\n(4) there are some classic unsupervised domain adaption benchmarks like Office Dataset, and Bing-Caltech dataset, why not run the method on them?\n\n(5) In ICCV 2019, there is a paper \"S4L: Self-Supervised Semi-Supervised Learning\". The proposed method is almost same. I think the difference is this paper changes the setting and considers the unsupervised data as target domain and supervised data as source domain. "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states \"Overall, I think it\u2019s a good paper presenting a thought-provoking idea\" and recommends acceptance, indicating a positive sentiment.",
            "The review expresses overall positive feedback, highlighting the paper's clear writing, well-developed technical aspects, and intuitive ideas. The reviewer also mentions that the results are easy to reproduce and the experiments are interesting.",
            "The reviewer explicitly states a \"weakly rejected\" score and provides multiple reasons for the rejection, including lack of novelty, incomplete experimentation, and similarity to existing work."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive aspects (\"well-written\", \"simplicity of the idea\", \"competitive performance\", \"reasonable procedure\") and negative aspects (\"no theory backing the approach\", \"unclear how easy it is to come up with a reasonable set of SSL tasks\", \"no consistent improvement\", \"lack of any guidance as to how to choose the set of self-supervised tasks\") of the paper, resulting in a balanced tone. The reviewer also poses questions and seeks clarification, contributing to the balanced perspective.",
            "The review adopts a balanced tone, acknowledging the paper's strengths (well-written, clear motivations, reproducible results) while also pointing out limitations and suggesting improvements (limited experimental evaluation, need for more qualitative analysis). The reviewer uses constructive language and offers specific suggestions, indicating a balanced perspective.",
            "The review uses critical language to point out flaws in the paper, such as \"not novel,\" \"performance is not better,\" \"doesn't provide any way,\" and questioning the choice of benchmarks and similarity to other work. The reviewer also uses direct questions to challenge the authors' decisions."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it highlights both the strengths (well-written, simple idea, competitive performance) and weaknesses (lack of theory, unclear task selection, inconsistent improvements, missing experiments) of the paper in a balanced way. The reviewer's overall assessment and recommendation for acceptance, despite the identified weaknesses, are aligned and do not contradict each other.",
            "The review is consistently positive overall, praising the clarity and well-explained ideas of the paper, while offering constructive suggestions for improvement without contradicting the positive assessment.",
            "The review is consistent because all the points raised by the reviewer logically support the weakly rejected score. The reviewer points out issues related to novelty, experimental evaluation, and contribution, all of which are valid reasons for a weak rejection. There are no contradictory statements within the review; the reviewer consistently argues that the paper is not strong enough for a conference publication despite acknowledging its interesting aspects."
        ]
    },
    {
        "paper_id": "iclr_2019_BJxvEh0cFQ",
        "paper_title": "K for the Price of 1: Parameter-efficient Multi-task and Transfer Learning",
        "paper_abstract": "We introduce a novel method that enables parameter-efficient transfer and multi-task learning with deep neural networks. The basic approach is to learn a model patch - a small set of parameters - that will specialize to each task, instead of fine-tuning the last layer or the entire network. For instance, we show that learning a set of scales and biases is sufficient to convert a pretrained network to perform well on qualitatively different problems (e.g. converting a Single Shot MultiBox Detection (SSD) model into a 1000-class image classification model while reusing 98% of parameters of the SSD feature extractor). Similarly, we show that re-learning existing low-parameter layers (such as depth-wise convolutions) while keeping the rest of the network frozen also improves transfer-learning accuracy significantly. Our approach allows both simultaneous (multi-task) as well as sequential transfer learning. In several multi-task learning problems, despite using much fewer parameters than traditional logits-only fine-tuning, we match single-task performance. \n      ",
        "review_ids": [
            "S1eTI1PuAX",
            "BJgEyp8gAQ",
            "SJgssRVq3X",
            "BygmSOC2hm",
            "S1e7BbGqj7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thanks to the authors for their reply. I am satisfied with the current state of the paper and tend to keep my score.",
            "Several changes have been made to my comments, thanks for pointing out the mistakes. ",
            "This paper explored the means of tuning the neural network models using less parameters. The authors evaluated the case where only the batch normalisation related parameters are fine tuned, along with the last layer, would generate competitive classification results, while using very few parameters comparing with fine tuning the whole network model. However, several questions are raised concerning the experiment design and analysis:\n1. Only MobilenetV2 and InceptionV3 are evaluated as classification model, while other mainstream models such as ResNet, DenseNet are not included. Would it be very different regarding the conclusion of this paper?\n2. It seems that the only effective manner is by fine tuning the parameters of both batch normalisation related and lasts layer, while fine tuning last layer seems to be having the main impact on the final result. In Table 4, authors do not even provide the results fine tuning last layer only.\n3. The organisation of the paper and the order of illustration is a bit confusing. e.g. later sections are frequently referred in the earlier sections. Personally I would prefer a plain sequence than keep turning pages for confirmation.",
            "The authors proposed an interesting method for parameter-efficient transfer learning and multi-task learning. The authors show that in transfer learning fine-tuning the last layer plus BN layers significantly improve the performance of only fine-tuning the last layer. The results are surprisingly good and the authors also did analysis on the relationship between embedding space and biases. \n\n1. The memory benefit is obvious, it would be interesting to know the training speed compared to fine-tuning methods (both the last layer and the entire network)?\n2. It seems that DW patch has limited effects compared to S/B patch. It would be nice to have some analysis of this aspect.\n",
            "Summary: the paper introduces a new way of fine-tuning neural networks. Instead of re-training the whole model or fine-tuning the last few layers, the authors propose to fine-tune a small set of model patches that affect the network at different layers. The results show that this way of fine-tuning is superior to above mentioned typical ways either in accuracy or in the number of tuned parameters in three different settings: transfer learning, multi-task learning and domain adaptation.\n\nQuality: the introduced way of fine-tuning is interesting alternative to the typical last layer re-training. I like that the authors present an intuition behind their approach and justify it by an illustrative example. The experiments are fair, assuming the authors explain the choice of hyper-parameters during the revision.\n\nClarity: in general the paper is well-written. The discussion of multi-task and domain adaptation parts can be improved though.\n\nOriginality: the contributions are novel to my best knowledge.\n\nSignificance: high, I believe the paper may facilitate a further developments in the area.\n\nI ask the authors to address the following during the rebuttal stage:\n* explain the choice of the hyper-parameters of RMSProp (paragraph under Table 1).\n* fix Figure 3, it's impossible to read in the paper-printed version\n* explain how the average number of parameters per model in computed in Tables 4 and 5. E.g. 700K params/model in the first column of Table 4 is misleading - I suppose the shared parameters are not taken into account. The same holds for 0 in the second column, etc.\n* add a proper discussion for domain adaptation part. The simple \"The results are shown in Table 5\" is not enough. \n* consider leaving the discussion of cost-efficient model cascades out. The presented details are too condensed and do not add value to the paper.\n* explain how different resolutions are managed by the same model in the domain adaptation experiments."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses satisfaction with the paper's current state and intends to maintain their score, indicating a positive evaluation.",
            "The reviewer expresses gratitude for the feedback and acknowledges improvements made based on previous comments, indicating a positive sentiment.",
            "The review raises several questions concerning the experiment design and analysis, indicating a critical assessment of the paper's methodology and presentation. The reviewer points out limitations in the models evaluated, missing results, and organizational issues.",
            "The review acknowledges the interesting method, surprisingly good results, and valuable analysis, indicating a positive overall assessment.",
            "The review expresses positive sentiment through phrases like \"interesting alternative,\" \"I like that the authors present an intuition,\" \"experiments are fair,\" \"well-written,\" \"contributions are novel,\" and \"high, I believe the paper may facilitate a further developments in the area.\""
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Thanks to the authors\" and \"I am satisfied\" which convey a supportive and appreciative tone.",
            "The reviewer uses appreciative language (\"thanks for pointing out the mistakes\") and acknowledges the author's efforts to address previous concerns, suggesting a supportive tone.",
            "The review uses phrases like 'several questions are raised,' 'only effective manner,' 'do not even provide,' and 'a bit confusing' to express concerns and criticisms about the paper's methodology and presentation.",
            "The tone is balanced as it acknowledges the strengths of the paper ('interesting method', 'surprisingly good') while also raising specific questions and suggestions for improvement ('It would be interesting to know', 'It would be nice to have some analysis').",
            "The tone is supportive as the reviewer appreciates the paper's novelty and potential impact. They offer constructive criticism and suggestions for improvement rather than outright rejection, indicating a willingness to see the paper improved and accepted. The phrase \"I ask the authors to address the following during the rebuttal stage\" further reinforces this supportive tone, implying a desire to help the authors improve the manuscript."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses satisfaction with the paper and indicates no change in the reviewer's assessment, showing a consistent viewpoint.",
            "The review is consistent as it is a short statement acknowledging changes made based on previous feedback and expressing gratitude. There are no contradictory statements or inconsistencies within this brief review.",
            "The review is consistent in its critique, raising valid questions about the experiment design, the limited scope of models tested, the interpretation of results, and the paper's organization. The reviewer's points are logically connected and do not contradict each other, focusing on areas for improvement in the paper's methodology and presentation.",
            "The review is consistent because it starts with positive feedback, highlighting the interesting method and surprisingly good results. Then, it raises constructive questions and suggestions for further analysis, without contradicting the initial positive assessment. The reviewer's questions are aimed at improving the paper and do not negate the initial positive evaluation.",
            "The review is consistently positive and constructive. The reviewer appreciates the novelty and significance of the work, while also providing specific and actionable suggestions for improvement. There are no contradictory statements or conflicting opinions within the review."
        ]
    },
    {
        "paper_id": "iclr_2020_SJgBQaVKwH",
        "paper_title": "Effective Use of Variational Embedding Capacity in Expressive End-to-End Speech Synthesis",
        "paper_abstract": "Recent work has explored sequence-to-sequence latent variable models for expressive speech synthesis (supporting control and transfer of prosody and style), but has not presented a coherent framework for understanding the trade-offs between the competing methods. In this paper, we propose embedding capacity (the amount of information the embedding contains about the data) as a unified method of analyzing the behavior of latent variable models of speech, comparing existing heuristic (non-variational) methods to variational methods that are able to explicitly constrain capacity using an upper bound on representational mutual information. In our proposed model (Capacitron), we show that by adding conditional dependencies to the variational posterior such that it matches the form of the true posterior, the same model can be used for high-precision prosody transfer, text-agnostic style transfer, and generation of natural-sounding prior samples. For multi-speaker models, Capacitron is able to preserve target speaker identity during inter-speaker prosody transfer and when drawing samples from the latent prior. Lastly, we introduce a method for decomposing embedding capacity hierarchically across two sets of latents, allowing a portion of the latent variability to be specified and the remaining variability sampled from a learned prior. Audio examples are available on the web.",
        "review_ids": [
            "SJl9hTUijr",
            "H1xB4iw5jH",
            "BJxsoyDqYB",
            "HJgtYejpYr",
            "Hkg7mpxQ9S"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I see -- thanks for your clarification regarding [Zhang et al., 2019] and [Hsu et al, 2019]. If there is room in the revised version to make the distinctions more clear (I didn't see it in the updated PDF), I think fleshing out these subtle distinctions may be a good idea for clarity. The new equation and the latent space portions were clear and I appreciate the authors taking them into account!",
            "Dear Reviewers, thanks for your thoughtful input on this submission! \u00a0The authors have now responded to your comments. \u00a0Please be sure to go through their replies and revisions. \u00a0If you have additional feedback or questions, it would be great to get them this week while the authors still have the opportunity to respond/revise further. \u00a0Thanks!\n",
            "1. Summary: This paper proposes Capacitron, a conditional variational latent variable model for TTS which allow for controllable latent variable capacity. They optimize the Lagrangian dual of the ELBO and restrict the capacity of the rate-term through a learnable, non-negative multiplier. They demonstrate the effectiveness of their approach on a range of TTS tasks such as same-text prosody transfer and inter-text style transfer, and provide extensive analyses on their latent variable capacity (in addition to comparisons to non-variational approaches based on Tacotron).\n\n2. Decision: Weak accept. I recommend this paper for acceptance due to its strong empirical results and clear presentation of the approach/unification of existing methods.\n\n3. Supporting arguments: The extension of Capacitron to existing methods such as [Hsu et al., 2019 and Zhang et al., 2019] is simple (basically adopting the beta-VAE approach), as the conditional generative model in both the vanilla and hierarchical forms exist already. But the authors do a thorough job evaluating the strengths and weaknesses of their method through ablation studies on latent variable capacity and comparing to existing baselines in their experiments. The results are also convincing, as demonstrated through human listening tests and the samples provided in the supplement. \n\n4. Feedback:\n- The authors mention in Section 3.1 that in previous work, the variational posterior has the form q(z|x). While this is true for [Zhang et. al, 2019], I believe that [Hsu et. al, 2019] also uses the conditional generative model as described in Figure 3 -- it would be helpful to provide a more clear distinction between the two works in the exposition. From what I understood, this work\u2019s contribution is not so much the introduction of the conditional generative model but identifying the effects of controlling the rate term in the ELBO decomposition.\n- In Section 3.2, there is a lot of notation and several terms that make it difficult to parse Eq. 14 at first glance. For example, (1) R_L is never explicitly written out, (2) and is R == R_avg? It would be helpful to clean up this section so that eyeballing Eq. 14 would be easier for the reader (e.g. having all the relevant terms organized).\n\n5. Questions:\n- In the \u201cSingle speaker\u201d section of Section 4.3, you mentioned that the \u201cmodel has to divide the latent space into regions that correspond to different utterance lengths.\u201d I\u2019m curious -- is this something that was observed empirically?",
            "This paper introduces a new embedding method in expressive TTS by focusing on the capacity of hidden variables introduced in variational autoencoder. Such a method is supported by the KL divergence and lower bound theory and the paper well formulates/describes the capacity concept. In the experiments, the proposed method is compared with other conventional methods with well-designed subjective and objective metrics and shows the effectiveness in terms of the style transfer. The paper is well written in general.\n\nOne of the difficulties in this paper is reproducibility. The paper does not seem to provide source code and part of the data used in this paper also does not seem to be public data like libriTTS (I would be wrong but I could read that the multispeaker training data are not public), although I appreciate the authors' efforts to provide the implementation and evaluation details as much as possible in the appendix.\n\nAnother discussion of this paper is that it is not explicit to provide the meaning of the latent variables (this is a general issue in VAE) and I could not be fully convinced by the discussion and analysis based on the latent variable. There would be several semi-supervised studies to explicitly connect some (elements) of latent variables with actual attributes and I'd like to ask the authors to consider such direction to make the latent variable discussion in the paper more plausible.\n",
            "In this work authors present a regularized, variational autoencoder method for speech synthesis. To endow the latent space with more capacity, the authors employ a modified variational autoencoder objective, which uses a learnable Lagrange multiplier to impose a capacity limit on KL divergence between latent posterior and prior. The authors furthermore propose to decompose the latent embedding space into a two-level hierarchical representation to give generative process more control over style transfer and sample-to-sample variance. They extend earlier theoretical results providing upper bounds on the mutual information between data and its latent embedding to their hierarchical latent representation. In numerical experiments the authors evaluate their approach on a number of speech synthesis tasks involving same-text prosody transfer, inter-text style transfer, inter-speaker prosody transfer. They also analyze speech samples generated from latent samples drawn from the prior.\n\nThe paper is well-written and easy to follow, but the significance of the main contribution of the paper remains unclear to me. The authors propose to use an augmented standard VAE loss with a capacity hyperparameter and a Lagrange multiplier, but such modifications have been used before and it is not clear to me where is the novelty in there?\n\nMoreover, the authors treat the Lagrange multiplier as a learnable parameter, which brings up the question how does it effect the learning dynamics. For instance if the KL-divergence reaches its capacity, the Lagrange multiplier may be pushed down to zero, which in turn can allow the posterior to diverge unchecked to possibly a point mass distribution? The authors provide no details on how beta (the Lagrange multiplier) evolves in their experience and how it effects the stochastic nature of the model. \n\nI am not sure why the authors call non-stochastic latent variable models heuristic (instead of deterministic) methods?\n\nIn Figure 1, how are  the loss values computed for variational methods? Can we see how the error bars look for different C values and embedding dimensions?\n\nCan the authors be more clear about why for the tasks they consider, a standard (deep) VAE architecture with non-hierarchical latents does not sufficiently capture variations in the data? Can the authors quantify the differences? \n\nI am unfamiliar with prior work in this application area, but maybe the work is novel with respect to the application of the regularized VAE framework to speech synthesis. However, the application alone is in my opinion not a contribution that is significant enough for publication.\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses appreciation for the authors addressing their concerns ('I appreciate the authors taking them into account!'). While suggesting further clarification, the overall tone is positive and constructive.",
            "The language used is encouraging and appreciative, expressing gratitude for the reviewers' input and urging further engagement to improve the submission.",
            "The reviewer recommends acceptance ('Weak accept') due to strong empirical results and clear presentation.",
            "The review expresses both positive and negative feedback. It acknowledges the paper's strengths in formulation, description, experimental design, and writing quality, but also raises concerns about reproducibility and the interpretability of latent variables.",
            "The reviewer expresses uncertainty about the novelty and significance of the work, stating that the main contribution is unclear. They also raise concerns about the learnable Lagrange multiplier and its effect on learning dynamics. The final statement indicates the reviewer does not believe the application alone is significant enough for publication."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Supportive",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'thanks for your clarification,' 'I think fleshing out these subtle distinctions may be a good idea for clarity,' and 'I appreciate the authors taking them into account!' which indicate a supportive and encouraging tone.",
            "The tone is supportive, as indicated by phrases like \"thoughtful input\", \"opportunity to respond/revise further\", and \"Thanks!\" which create a collaborative and encouraging atmosphere.",
            "The reviewer uses positive language such as 'strong empirical results', 'clear presentation', 'thorough job evaluating', 'convincing results'. While offering feedback, they frame it as helpful suggestions for improvement rather than criticisms.",
            "The review balances positive feedback (e.g., \"The paper is well written in general,\" \"shows the effectiveness\") with constructive criticism (e.g., \"One of the difficulties in this paper is reproducibility,\" \"it is not explicit to provide the meaning of the latent variables\"). The reviewer also uses hedging language like \"does not seem to\" and \"I would be wrong but I could read that\" to soften the criticism.",
            "The review uses questioning and critical language such as 'the significance of the main contribution of the paper remains unclear to me,' 'it is not clear to me where is the novelty in there?', 'which brings up the question how does it effect the learning dynamics,' and expressing doubt about the contribution being significant enough for publication."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses appreciation for the authors' clarifications and improvements, while suggesting further minor improvements for clarity. There are no contradictory statements or conflicting opinions within the review.",
            "The review is a consistent message encouraging reviewers to check author responses and provide further feedback. It expresses a clear and logical request without any contradictions.",
            "The reviewer's decision to 'Weak accept' is well-supported by the positive aspects highlighted in the 'Supporting arguments' section, such as strong empirical results and clear presentation. The 'Feedback' and 'Questions' sections are constructive and do not contradict the overall positive assessment, focusing on minor improvements and clarifications rather than fundamental flaws.",
            "The review is consistent because it acknowledges the strengths of the paper (novel method, good experiments, well-written) while also pointing out areas for improvement (reproducibility, interpretability of latent variables). There is no contradiction in praising certain aspects and criticizing others; the review provides a balanced perspective.",
            "The review is consistent because the reviewer expresses a clear and consistent concern about the novelty and significance of the proposed method throughout the text. While acknowledging the paper's clarity, the reviewer repeatedly questions the contribution's originality and impact, focusing on the lack of clear novelty compared to existing methods and insufficient justification for the proposed approach."
        ]
    },
    {
        "paper_id": "iclr_2019_HJlmHoR5tQ",
        "paper_title": "Adversarial Imitation via Variational Inverse Reinforcement Learning",
        "paper_abstract": "We consider a problem of learning the reward and policy from expert examples under unknown dynamics. Our proposed method builds on the framework of generative adversarial networks and introduces the empowerment-regularized maximum-entropy inverse reinforcement learning to learn near-optimal rewards and policies. Empowerment-based regularization prevents the policy from overfitting to expert demonstrations, which advantageously leads to more generalized behaviors that result in learning near-optimal rewards. Our method simultaneously learns empowerment through variational information maximization along with the reward and policy under the adversarial learning formulation. We evaluate our approach on various high-dimensional complex control tasks. We also test our learned rewards in challenging transfer learning problems where training and testing environments are made to be different from each other in terms of dynamics or structure. The results show that our proposed method not only learns near-optimal rewards and policies that are matching expert behavior but also performs significantly better than state-of-the-art inverse reinforcement learning algorithms.",
        "review_ids": [
            "SJdOg6R3Q",
            "ryxBthEq1N",
            "SkeaSLLMyE",
            "rJgOYVPcnX",
            "SklJGpL40Q",
            "BklSaQRunm",
            "r1xjTdf_TX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary/Contribution:\nThis paper builds on the AIRL framework (Fu et al., 2017) by combining the empowerment maximization objective for optimizing both the policy and reward function. Algorithmically, the main difference is that this introduces the need to optimize a inverse model (q), an empowerment function (Phi) and alters the AIRL updates to the reward function and policy. This paper presents experiments on the original set of AIRL tasks, and shows improved performance on some tasks.\n\nPros:\n    - The approach outperform AIRL by a convincing margin on the crippled ant problem, while obtaining comparable/favorable performance on other benchmarks.\n\nCons:\n    - The justification for using the empowerment maximization framework to learn the shaping parameters is unclear. The formulation introduces a potentially confounding factor by biasing the policy optimization which clouds the experimental picture. \n\nJustification for rating:\nThis paper presents good empirical results, but without a clear identification of the source of improvement. I lean on the side of rejecting unless the authors can better eliminate any potential bias in their formulation (see question below). The justification for combining the empowerment maximization objective is also unclear while being integral to the novelty of the proposed method. \n\nQuestions I could not resolve from my reading:\n    - The \"imitation learning benchmark\" numbers in Table 2 are different from the original AIRL paper. Do the authors have an explanation as to why? Is this only due to a difference in the expert performance?\n    - Can the authors confirm that in the transfer experiments, the policy is optimized with only the transfered reward and no empowerment bonus? Otherwise, can the authors comment on whether the performance benefits could be explained by the additional bonus.\n    - In equation (12), \\Phi is optimized as an (approximate) mutual information, not a value function, so it is not clear why this term approximates the advantage (I suspect this is untrue in EAIRL as V* is recovered at optimality in the AIRL/GAN-GCL formulation). Can the authors comment?\n    - Why is w* unnormalized? Unless I am misunderstanding something, in the definition immediately above it, there is a normalization term Z(s). \n\nOther comments:\n    - \"AIRL(s, a) fails to learn rewards whereas EAIRL recovers the near optimal rewards function\" -> This characterization is strange since on some tasks AIRL(s,a) outperforms or is within one standard deviation of EAIRL (e.g. on Half Cheetah, Ant, Swimmer, Pendulum).\n    -  \"Our experimentation highlights the importance of modeling discriminator/reward functions.. as a function of both state and action\". AIRL(s) is better on both the pointmass and crippled-ant task than AIRL(s,a). Can the authors clarify?\n    - \"Our method leverages .. and therefore learns both reward and policy simultaneously\". Can the authors clarify in what sense the reward and policy is being learned simultaneously in EAIRL where it is not in AIRL?\n    - In all the tables, the authors' approach is bolded as oppose to the best numbers. I would instead prefer that the authors bold the best numbers to avoid confusion.\n\n- Typos:\n    - \"the imitation learning methods were proposed\"\n    - \"quantify an extent to which\" \n    - \"GAIL uses Generative Adversarial Networks formulation\"\n    - \"grantee\"\n    - \"no prior work has reported the practical approach\"\n    - \"but, to\"\n    - \"(see (Fu et al., 2017))\"\n",
            "Are the authors able to release code with this submission?",
            "The proposed method uses empowerment both, for reward shaping and for regularization.\nThe reward function is defined as a (properly) shaped reward function, r + \\gamma\\Phi(s') - \\Phi(s) (empowerment-based reward shaping) and the policy is optimized using a regularized TRPO update rule (empowerment-based regularization). Some of the effects _approximately_ cancel out such that the update is similar to a standard TRPO update with reward fuction r + \\gamma\\Phi(s'). However, this is a quite wild approximation (treating the inverse model as current policy) and hence the actual algorithm uses a combination of reward shaping and regularization. I think it would be much nicer to present a (slighly different) algorithm that only does regularization. This regularization can be achieved by _either_ using a different policy update rule, or (better) by having an additional objective in the reward function. Note that none of these have anything to do with reward shaping! ",
            "The authors propose empowerment-based adversarial inverse reinforcement learning (EAIRL), an extension of AIRL which uses empowerment (which quantifies the extent that an agent can influence its state, see eq. 3) as a reward-shaping potential to recover more faithful learned reward functions. \n\nEvaluation:     4/5     Experiments are more preliminary but establish the benefit of the approach.\nClarity:        4/5     Well written. Just a few typos (see below minor comments)\nSignificance:   4/5     Effective, well motivated approach. Excellent transfer learning results.\nOriginality:    3.5/5   As the empowerment subroutine is existing work, as is AIRL, combining previous work, but effectively.\n\nRating:         7/10\nConfidence:     3/5     Reviewed this paper in a little less detail than I would prefer, due to time constraints. I will review in more detail and update this and add any additional questions/comments below the minor comments below.\n\nPros:\n- Extension of AIRL which utilizes empowerment to advance the SOE in reward learning\n- Well written, related previous work well explained.\nCons:\n- Experiments more preliminary\n- Combines existing approaches, somewhat incremental\n\nMinor comments: \n- grantee (typo), barely utilized -> not fully realized?, \n\n----\n\nUpdated review:\n\nAfter reviewing the comments and the paper in more detail (whose story has evolved substantially) , I have revised my score slightly lower. While in hindsight I can see that the paper has definitely improved, the story has changed rather dramatically, and appears to be still unfolding: the paper's many new elements require further maturation, and that the utility of empowerment for reward shaping and/or regularization to evolve AIRL (i.e. the old story vs. the new story) still needs further investigation/maturation. If the paper is accepted I'm reasonably confident that the authors will be able to \"finish up\" and address these concerns. \n(typo: eq. 4 omits maximizing argument)",
            "Thanks for the revision; I agree that the quality has significantly improved and updated my review.",
            "The paper proposes a method for inverse reinforcement learning based on AIRL. It's main contribution is that the shaping function is not learned while training the discriminator, but separately as an approximation of the empowerment (maximum mutual information). This shaping term aims to learn disentangled rewards without being restricted to learning state-only reward functions, which is a major restriction of AIRL.\n\nThe main weakness of the paper is, that it does not justify or motivate the main deviations compared to AIRL. The new objective for updating the policy is especially problematic because it does no longer correspond to the RL objective but includes an additional term that biases the policy towards actions that increase its empowerment. Although both terms of the update can be derived independently from an IRL and Empowerment perspective respectively, optimizing the sum was not derived from a common problem formulation. By combining these objectives, the learned reward function may lead to policies that fail to match the expert demonstration without such bias. This does not imply that the approach is not sound per se, however, simply presenting such update without any discussion is insufficient--especially given that it constitutes the main novelty of the approach. I think the paper would be much stronger if the update was derived from an empowerment-regularized IRL formulation. And even then, the implications of such bias/regularization would need to be properly discussed and evaluated, in particular with respect to the trade-off lambda, which--again--is hardly mentioned in the submission. I'm also not sure if the story of the paper works out; when we simply want to use empowerment as shaping term, why not use two separate policies for computing the empowerment and reward function respectively. Is the bias in the policy update maybe more important than the shaping term in the discriminator update for learning disentangled rewards?\n\nKeeping these issues aside, I actually like the paper. It tackles the main drawback of AIRL and the idea seems quite nice. Having a reward function that does not actively induce actions that can be explained by empowerment, may not always be appropriate, but often enough it may be a sensible approach to get more generalizable reward functions. The paper is also well written with few typos. The parts that are discussed are clear and the experimental results seem fine as well (although more experiments on the reward transfer would be nice).\n\nMinor notes:\nI think there is a sign error in the policy update\nTypo in the theorem, grantee should be guarantee\n\nQuestion:\nPlease confirm that the reward transfer was learned with a standard RL formulation. Does the learned policy change, when we use the empowerment objective as well?\n\n\n\nUpdate (22.11)\nI think that the revised version is much better than the original submission because it now correctly attributes the improved generalization to an inductive bias in the policy update.  However, the submission still seems borderline to me. \n\n- The proposed method uses the empowerment both for regularization as well as for reward shaping, but it is not clear whether the latter improves generalization. If the reward shaping was not necessary, it would be cleaner to use empowerment only for regularization. If the reward shaping is beneficial, this should be shown in an ablative experiment.\n\n- The benefit of using empowerment (whether for reward shaping or for regularization) should be discussed. Empowerment for generalization is currently hardly motivated.\n\n- The derivation could be a bit more rigorous.\n\nAs the presentation is now much more sound, I slightly increased my rating.",
            "Thanks for the new derivation. I think it sheds some more light on the policy bias, although I think that setting the inverse model equal to the current policy is going too far and it does not really make sense to talk about \"maximizing the entropy of q(.)\" given that q is a variational distribution that is fixed during the policy improvement. \nHowever, treating the last equation of Appendix B as a rough approximation of the actual objective that is maximized by the policy updates and further assuming that lambda_I=0.99 is close enough to 1, we can see that the policy roughly optimizes \"reward + next empowerment\". I wonder whether, we could show similar generalization benefits by directly optimizing this objective, e.g. the discriminator could be computed as exp(r+\\gamma*\\Phi(s'))/(exp(r+\\gamma*\\Phi(s'))+\\pi) and a standard TRPO/PPO update could be used. According to the derivations in Appendix B, this should roughly correspond to the same algorithm. Let's say we can get similar results (potentially replacing \\gamma by a hyper-parameter and using higher entropy regularization), such algorithm could be derived in a principled way--from and empowerment-regularized MaxEnt-IRL formulation.\nIn the submitted version, there is a huge discrepancy between the text (generalization is achieved by using empowerment as potential for reward (un)shaping) and the actual algorithm (generalization is achieved by 1% of reward shaping and 99% of policy biasing). These are two completely different approaches; the former does not affect the learned policy (at least in theory) whereas the latter approach relates to regularization and has the potential to lead to much better generalization by preventing overfitting the demonstrations. As these are different approaches, deriving the algorithm from a reward shaping (better: \"advantage unshaping\") perspective can not be fully sound--which ultimately manifests in the form of a modified policy update rule which is not properly derived. From a reward shaping perspective, the policy for computing the empowerment should not be related at all to the policy that maximizes the reward.\nI think there is not much missing to turn the submission into a nice paper (if my suggested variant would work out of the box, it might even be possible to revise the submission), however, in the current state the submission is in my opinion not sufficiently sound and almost dangerous, because it gives a wrong impression about the way generalization is achieved."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the clarity of the justification for the proposed method, potential bias in the formulation, and inconsistencies in the experimental results. The reviewer leans towards rejection unless the authors address these issues. Phrases like \"unclear identification of the source of improvement\", \"potential bias\", and \"justification for combining the empowerment maximization objective is also unclear\" indicate a negative sentiment.",
            "The review poses a question without expressing any positive or negative feelings.",
            "The review expresses concerns about the method's approximation and suggests a potentially better approach, indicating a critical perspective.",
            "The review expresses overall positive feedback, highlighting the effectiveness and motivation of the approach, well-written content, and excellent transfer learning results. Even with the revised score, the reviewer expresses confidence that the authors can address remaining concerns.",
            "The reviewer explicitly states that the quality has 'significantly improved' and that they 'agree' with the revision, indicating a positive sentiment.",
            "The review expresses both positive and negative feedback. It acknowledges the paper's potential and originality while also pointing out significant weaknesses and areas for improvement. The reviewer appreciates the idea but questions the justification and derivation of the method.",
            "The review expresses concerns about the soundness of the approach, stating that the submission is \"not sufficiently sound and almost dangerous, because it gives a wrong impression about the way generalization is achieved.\" The reviewer also points out discrepancies and potential issues with the derivation."
        ],
        "tone": [
            "Critical",
            "Neutral",
            "Critical",
            "Balanced",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, as evidenced by the numerous questions raised regarding the methodology, experimental setup, and claims made in the paper. The reviewer points out inconsistencies and requests clarifications, indicating a skeptical and critical assessment of the work. Specific examples include questions about the differences in benchmark numbers, the optimization process, and the normalization of parameters.",
            "The question is straightforward and lacks any specific emotional or stylistic cues that would indicate a particular tone.",
            "The reviewer uses phrases like \"quite wild approximation\" and \"I think it would be much nicer\" to express their reservations and suggest improvements, indicating a critical tone.",
            "The review provides both positive and negative feedback, using phrases like \"Effective, well motivated approach\" and \"Excellent transfer learning results\" to praise the work, while also pointing out limitations such as \"Experiments more preliminary\" and \"Combines existing approaches, somewhat incremental.\"",
            "The reviewer expresses gratitude ('Thanks for the revision') and acknowledges the improvement in quality, which suggests a supportive and encouraging tone.",
            "The reviewer uses phrases like \"main weakness,\" \"especially problematic,\" \"insufficient,\" and \"not sure if the story of the paper works out.\" They also directly suggest improvements and point out missing justifications, indicating a critical evaluation.",
            "The tone is critical, using phrases like \"going too far,\" \"does not really make sense,\" \"huge discrepancy,\" \"not fully sound,\" and \"wrong impression.\" The reviewer directly challenges the validity of the approach and expresses concerns about the conclusions drawn from the research. However, the reviewer also provides constructive feedback and suggests potential improvements, indicating a desire to help the authors improve their work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No"
        ],
        "consistency_reason": [
            "The review consistently points out the empirical strengths of the paper (improved performance) while simultaneously criticizing the lack of clear justification and potential confounding factors in the proposed method. The reviewer's concerns about the theoretical grounding and clarity are consistently expressed throughout the review, from the \"Cons\" section to the \"Justification for rating\" and \"Other comments\". There are no contradictory statements or shifts in the reviewer's overall assessment.",
            "The review is a single question and does not contain any contradictory statements.",
            "The review consistently argues that the combination of reward shaping and regularization in the proposed method is not ideal and suggests focusing on regularization alone for clarity and potentially better presentation. The reviewer suggests alternative approaches that focus solely on regularization, highlighting the distinction between regularization and reward shaping.",
            "The reviewer provides a clear explanation for the change in their score and sentiment in the updated review, stating that it is based on a more detailed review and a better understanding of the paper's evolution and remaining issues. The updated review logically follows from the initial review and the reviewer's process of further investigation. There are no internal contradictions within either the initial or updated review, and the shift in assessment is well-justified.",
            "The review expresses a positive change in opinion due to the revision and states they updated their review accordingly. There are no contradictory statements.",
            "The review consistently points out the lack of justification and motivation for the proposed method's deviations from AIRL, especially regarding the objective function and the use of empowerment, despite acknowledging the potential and positive aspects of the paper.",
            "The review identifies a contradiction between the paper's description of its method as reward shaping using empowerment and the reviewer's interpretation of the actual algorithm as policy biasing. The reviewer argues that these are fundamentally different approaches, making the paper's explanation inconsistent with the implemented method."
        ]
    },
    {
        "paper_id": "nips_2021_Xa9Ba6NsJ6",
        "paper_title": "On the Value of Interaction and Function Approximation in Imitation Learning",
        "paper_abstract": "Nived Rajaraman, Yanjun Han, Lin Yang, Jingbo Liu, Jiantao Jiao, Kannan Ramchandran",
        "review_ids": [
            "ZxFxJU5wpPO",
            "R2aLoRDf13R",
            "kV1xSjew7-F",
            "Af07l0DflfM",
            "ff-nKEh6V-c",
            "vF02vGnnRCF",
            "6ONE0KUu69d"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " -",
            "The paper examines statistical guarantees of imitation learning under various regimes -- interactive setting, non-interactive setting, and non-interactive setting with known dynamics. It further looks at the linear setting and derives bounds for non-interactive settings with/without known dynamics. The main contributions are:\n1. Interactive: To show that \u201creduction-based\u201d online algorithms (DAGGER / AGGREVATE, DAEQUIL) have matching upper and lower bounds\n2. Non-interactive: All such algorithms have O(H^2) bounds\n3. Non-interactive + Linear + Parameter sharing: Recover O(H) bound\n4. Non-interactive + Known dynamics + Linear: Introduces a new algorithm, confidence set linear classification, that achieves O(H^3/2) \n  The paper addresses an important, yet under examined, problem -- what is the best one can expect from any imitation learning algorithm under various settings? The contributions in examining the linear setting are particularly important, one hopes functional analysis (RKHS) soon follows. \n\nI very much enjoyed reading the paper. It made central ideas accessible to the reader. I much appreciated the effort put in the introduction to give the reader a broad view of the key results. \n\nThe two main strengths of the paper are:\n1. Technically sound [4/5]: While some of the proofs get a bit hairy, there is adequate hand-holding and it is clear that the authors wield technical theorems in an adept manner\n2. Significance of claims (if true) [4/5]: In particular, the linear analysis potentially paves the way for a full RKHS analysis and that would be truly consequential. \n\nHowever, there are two main shortcomings that keep the paper from being a strong addition to the body of imitation learning knowledge: \n1. Contented contributions [2/5]: There appears to be contributions that are either incorrect or misinterpreted, and others that are previously established and refined here. Clarifying / removing these would sharpen the contributions this paper has to offer.\n2. Clarity of exposition [3/5]: For practitioners in IL (which is still a large majority), this paper is hard to read and glean insights from. Insights are often lost in the paper and a concern is that the consequences of the theory presented in this paper may be lost on its readers. \\\n\nI will now expand on the two shortcomings\n\n## Contented contributions\nBefore going into individual contributions, I would like to flag that there is a recent paper that appear to make a subset of the claims presented here that the authors do not cite:\n\nSwamy et al. \u201cOf Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap\u201d (ICML 2021) https://arxiv.org/abs/2103.03236 \n\nFor instance, Swamy et al. also have a notion of \\mu-recoverability (Def 3) and express bounds as a function of \\mu. Hence, for overlapping claims (some of which I will attempt to call out), it would be useful to clarify the differences between them. I will henceforth refer to this paper as Swamy et al.\n\n1. Interactive setting (matching upper and lower bounds) [Informal Theorem 1, Formal theorem 2,3]: \nThe upper bound is similar in form to Swamy et al. (as well as Dagger), i.e. \\mu H \\epsilon, where \\epsilon is the per-step loss (I would imagine that would be of the order of |S| / N). The lower bound appears to be sharper than Swamy et al. (H \\epsilon), and perhaps this is worth calling out.\n\nThe devices used in the upper bound proof also follow from reduction of imitation learning to online learning (learner provides a policy, receives a loss). Hence, it is unclear what this theorem adds to what is already established.\n\n2. Non-interactive setting (Informal Theorem 1, Formal Theorem 2) The O(H^2) is a well known result and the proof uses an MDP very similar to the one used in \u201cEfficient Reductions for Imitaiton Learning\u201d  (http://proceedings.mlr.press/v9/ross10a/ross10aSupple.pdf), notably Figure 1. \n\n3.  L.105, the authors mention \u201cthis is the first result to establish a clear separation in statistical minimax rate of suboptimality..\u201d. This claim is not substantiated, and at the very least requires a clear comparison to Swamy et al. which does not make such a claim but conveys results of the same kind. \n\n4. Non-interactive setting + Linear + Parameter sharing (Informal Theorem 4, Formal Theorem 5): The claim that parameter sharing results in a reduction from O(H^2) to O(H) appears to be incorrect. Notably in Lemma 1, \\gamma can be very high which would make the bound in Lemma 1 H\\gamma ~= H, i.e. trivial. \n \nTaking a step back, it\u2019s totally common to share parameters in behavior cloning (i.e. have the same weights for all timestep). The update in (4) would add up the errors over timestep and be exactly the supervised learning loss. \\gamma can be high, and in fact be masking a H term, i.e. gamma = H \\epsilon. \n\n5. Non-interactive + Linear + Known dynamics (Informal Theorem 5, Formal Theorem 6) \n\nSwamy et al. show that in this setting it is possible to get O(H \\epsilon) though one needs a \u201cplanner\u201d (ala MaxEnt IRL Ziebart et al.) to search over the sequence of actions. I was surprised to see no such discussion in this theorem although Mimic-MD clearly requires such a planner in step 6. Moreover, while O(H^3/2) seems to be a clear improvement over BC, O(H), it\u2019s still not the same order as the bound O(H). Clarifying the significance of this result w.r.t to Swamy et al. would be useful. \n\n6. Informal theorem 6 (formal theorem 7): This indeed appears to be the novel contribution. I struggled to follow the proof entirely but trust that the authors got it correct. \n\n\n## Clarity of exposition\nI\u2019ll go over the paper chronologically and identify areas where the writing was unclear / there were typos.\n\nSection 1: Introduction\n\n* Should have been at most 2 pages. There was no need to have so many definitions up front. Certainly no need to even talk about linear results. Instead, I would have structured it to talk about the 3 regimes of IL, talk about the key results in each regime and have forward pointers to theorem in the paper. There is no need to state things twice. \n* I would massively compress Def 4 onwards till the end. The only thing interesting there is the O(H) from parameter sharing and O(H^3/2) for known dynamics. \n* I would keep the parts that  convey insight (L.59-66, L.73-79,L.155-165).\n* L86. - 90: Unclear why reduction based approach fails to distinguish between power of learners. The authors claim they circumvent reduction based guarantees, but they don\u2019t in their proofs (reduce it to FTRL). \n* L.131: Classic supervised learning in IL does indeed use multi-class classification algorithm (for discrete actions > 2). \n\nSection 2/3\n\n* Move the definition here\n* Theorem 1: Explicitly mention realizability assumption here. \n* Theorem 3: Probably want the figure from appendix to be moved up here. \n* L307: should be \\theta^* without the _t subscript\n* L310-313: Unclear what is meant by separable and Markovian here. The point is unclear as well. \n* L316: The reader is still left trying to understand why, by mere rephrasing of the problem, the bound goes from O(H^2) to O(H). It feels like a smoke and mirror trick. The intuition is that to get such a reduction, surely we are paying elsewhere for added complexity (reasoning in the space of trajectories). That needs to be made clearer.\n\nSection 4\n\n\nL333: OPT: \\tilde(J)_r(\\pi^*)\nAlso one needs to explain how to solve OPT in practice and that it is hard.\n\nAlso, what if the reward function is only a function of state and not state action. Does this entire section fall apart? (Presumably no, but unclear)\n\nEq (5) explain E^c. \nL340: Simple empirical estimate -- unclear from what distribution. Please explain mathematically\n\nAlg 1, Line 6. \\tilde(J)_r(\\pi^*)\n\nAppendix:\nL565 tr_i should be \\pi_i\n\nL604: Wrong theorem (state the active setting)\n Yes",
            " I thank the authors for taking the time to provide such a thoughtful response! It was a joy to read and ponder.\n\n4. I\u2019ll start with my fundamental misunderstanding in the parameter sharing discussion: I forgot that we were in the realizable setting (at least temporarily forgot!) Once that is mentioned, it\u2019s clear to see that epsilon can be driven down by just getting more data. So all things being equal, if you get H times more data, you get an O(H) reduction. Anyways, the clear phrasing in the rebuttal was very helpful in course correcting and perhaps it can make it back to the paper in some form. \n\n1. Understood, the contribution is to show the dependence of epsilon on N, which none of the papers I referred to even attempt. I agree this is an important contribution\n\n2. Looking at the proof again, I see the cleverness of the author\u2019s MDP vs Ross and Bagnell MDP. I do think there is a notion of the bad state (s2 in RossMDP and b in the authorMDP). But while RossMDP has just 2 good states, the author introduces several good states, not all of which are visited. And since they are not visited, the learner can make a mistake and end up in the bad state. I think the proof is enlightening!\n\n3. Yeah I see that Swamy et al do not prove any sample complexity bound, so it was unfair of me to say that they establish mini-max rate. I agree with the authors that their contributions here are novel.\n\n5. The rebuttal point here was very helpful. Yes, one may get deceived into minimizing TV with their planner only to suffer worse sample complexity bounds H^2. MIMIC-MD indeed achieves better (still need to build intuition why). In fact, it achieves the minimax optimal rate so we can\u2019t do better. Either ways, I agree this is an important contribution.\n\n6. I loved this phrasing! In very lay terms, there is the Ross reduction is not tight enough to back out the sharpest bounds on the Ross algorithm. Perhaps the authors can add this phrasing in the appendix.\n\n7, 8. Thanks for the clarifications!\n\nBased on my renewed understanding, I am more than happy to change my score to 7. I thank the authors once again for patiently enlightening this reviewer. ",
            " Thank you for the clarifications, and my score remains unchanged at a 7 :).",
            " The authors study statistical guarantees for the Imitation Learning problem, for many settings: considering or not the possibility of interacting with the environment, using the \\mu-recoverability assumption, and assuming linear function approximation. Furthermore, they introduce a new setting, called confidence set linear classification.  ## Originality: \nThe main novelty consists in the analysis proposed by the authors, which allows to determine theoretical properties, improving some pre-existent results. Moreover, they introduce and study a new setting.\n\n## Quality:\nThe applied methodology seems to be sound, and the work is complete, analysing a spectrum of interesting cases.\n\n## Clarity: \nWhile the paper is clearly well written, the structure of the whole work can be improved. The authors tried to summarize in the introduction the main results provided, by means of informal theorems and definitions. I would suggest to keep a more traditional informal introduction, in which formulas and statement are few, and the main ideas are explained by words. Consequently, I would suggest to add a preliminaries section, where both the notation part and the definition could fit more.\n\n## Significance:\nThe work offers important statistical guarantees for different settings, advancing the state of the art. The obtained results are likely to be useful for other researchers to build on them. No potential negative impact is present.",
            "They study imitation learning in several variations, in particular with interaction (i.e., the expert can be queried during training) and without. And under some interesting additional assumptions, such as mu-recoverability (i.e., if the imitator visits suboptimal states, this has small impact of the overall value, by being able to go back to optimal easily).\n\nThey provide several theoretical results, \n- negative ones -- i.e., lower bounds on the suboptimality that can be achieved in terms of difference of imitator versus optimal expert value incurred -- as well as \n- positive ones -- algorithms that achieve linear instead of quadratic suboptimality, in particular their main contribution thm4. This works by reduction to supervised learning paradigm sample bounds.\n\nAs a means to this, they also have a little taxonomy on versions of IL.\n\nOne thing that's hard to judge for me is novelty. I feel this is novel, but I'm not sure how much overlap there is with DAgger and all the subsequent work in this direction, not an expert on this.  Overall this is an interesting paper, addressing common IL settings, providing non-trivial theoretical contributions. \n\nThe reduction-based approach to thm4 looks powerful (though reduction, I guess, are not new for this but originate from DAgger etc.). \n\nThe little IL taxonomy (Def1, 2, ...) is interesting in itself.\n\nSome of the proofs look non-trivial.\n\nQuality-wise, I have not checked details, but the theorems and proofs seem to be stated with care.\n\nStyle of writing is OK-good. Small limitations include:\n- I'm surprised that one of the most common settings -- no interaction, but being able to sample from the dynamics -- is not discussed. However, I guess Def6 is a close enough approximation to this for the sake of this paper.\n- How limiting is the linear expert setting? Linear would be pretty limiting, but I guess it's only linear in the features which would be quite general again.\n- There are no experiments but that's OK given there is enough conceptual and theoretical contribution.\n- I have not read the full paper. Some writing may be premature -- e.g. l98 \"regard. idea is crucial\" -- but this seems neglectable.",
            "This paper investigates statistical guarantees for imitiation learning (IL) under the $\\mu$-recoverability assumption of [27]. The authors prove  bounds on the sub-optimality of any algorithm which does not interact with the MDP and uses an offline dataset. Under linear function approximation, the authors prove bounds on the suboptimality of behaviour cloning.  Overall I found this paper to be quite good, for which I am recommending an accept.\n\nI really appreciated the informal theorems presented before the real theorems, as they help in the clarity of the paper.\n\nI think the proofs in the Appendix could be clarified and extended to make it easier for more readers to follow, as currently it seems to be written for an audience that is already quite familiar with that type of theoretical work.\n\nMore details below.\n\n## Questions / Comments\n1.  What exactly does \"0-1 loss\" mean in line 84?\n1. In line 92, why does $\\hat{\\pi}$ denote the learner, if the policy is continuously changing? Relatedly, in line 93 does $J(\\hat{\\pi})$ represent the policy at convergence? Same question on line 103. The phrasing used in informal theorem 3 seems better for this.\n1. Does the negative result of informal theorem 2 not hold for $\\mu < 1$? Why not?\n1.  The term \"statistical minimax rate\" is used quite a lot in the paper but never explicitly defined. It would be good to define for clarity.\n1.   In line 149, \"it is known from [22] that...\", is this also under the assumption of deterministic transitions, like in the current paper?\n1.  In line 168, if $h^{*}$ is defined as an $\\arg\\max$, shouldn't you use \"$=$\" instead of \"$\\sim$\"?\n1.  Please define $\\mathbb{S}^{d-1}$ in line 188.\n1.  In line 193, the term $d^{3/2} / N$ seems to suggest that there is a tradeoff between expressivity (large $d$) and vacuosity of bound (small $d$), since $d \\ll |\\mathcal{S}|$ likely incurs larger $\\mathfrak{R}$. Is this the case?\n1.  In line 230, shouldn't it be $\\hat{\\pi}^1,\\cdots,\\hat{\\pi}^T$?\n1.  In line 237, isn't the second term of the _empirical online-learning regret_ just zero?\n1.  The statement in line 245: \"By scaling each reward...\" seems a little strange. Presumably $\\mu$ has relevance under the assumption that the rewards are in $[0, 1]$ (as stated in the introduction). But just scaling the rewards, while not changing optimal policies, _does_ change the meaning and tightness of suboptimality bounds (e.g. I can make the bounds arbitrarily small by scaling the rewards). I guess this is for the lower bound, so it's ok? On this note, is $\\mu$-recoverability necessary just so you satisfy the the existing theorem's requirements (from [22])?\n1.  In Theorem 3, it seems the lower bound would be of more use if it depended on $|\\mathcal{A}|$ instead of $H$.\n1.  In line 322, I think you need to add \"with probability $1-\\delta$\" to the end of the sentence.\n1. In Section 4 you should specify from the beginning that you are considering the case when $P$ is known (this is only revealed once you reach the Algorithm).\n1.  In Algorithm 1, it would help clarify things if you highlight with a different color what is different from the original MIMIC-MD.\n1. In equation (8), what is $-\\mathcal{K}$?\n1.  In line 564 of the Appendix, it says \"samples the policy $tr_i$\", do you mean policy or trajectory?\n1.  In line 565 it says \"rolling out $tr_i$, should this be $\\pi_i$?\n1.  In line 568, what does it mean to be rolled out and conditioned on $\\hat{\\pi}^i$?\n\n\n## Minor comments\n1.  What do you mean by \"nicely\" in line 29?\n1.  What do you mean by \"game AI\" in line 37?\n1.  Throughout the paper the authors use numbered citations as nouns (e.g. \"[25] study BC from a theoretical...\"). You should use something like \"Ross & Bagnell [25] study BC from a theoretical...\". This is especially problematic when you start sentences/paragraphs like this, as you do in line 320.\n1.  In line 81, in the expectation subscript, use $a'$ (instead of $a$) to avoid confusion, as $a$ already has a different meaning.\n1.  Line 87, the sentence \"the size of expert dataset / number of MDP interactions, N in the no-interaction / active settings.\" reads weirdly, consider rephrasing.\n1.  Line 89, should be \"propose **a** policy\".\n1.  In line 98 a sentence starts improperly (\"idea is crucial...\")\n1.  In line 130 please state what ERM stands for.\n1.  Line 146, should read \"provied **a** dataset $D$\".\n1.  Line 164, should be \"output\" instead of \"otput\".\n1.  Section 1.1 (Related work) seems out of place where it is, as it interrupts the narrative flow. Consider putting at the end of the paper.\n1. Line 235, should be \"a measurable function **of** the first\".\n1.  Line 282 should be \"upper bound\" (singular, not plural).\n1.  Line 311, what do you mean by \"By contradiction\"?\n1.  In the paper use \\restatable so that the theorem statements are reprinted in the Appendix. Yes."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review is generally positive as it acknowledges the authors' efforts to address previous concerns and highlights the improvements made in the revised manuscript. The statement \"The authors have addressed most of my concerns\" indicates a positive assessment.",
            "The review expresses several significant concerns about the paper's contributions and clarity. The reviewer points out potential inaccuracies, misinterpretations, and previously established results, questioning the novelty and significance of some claims. The reviewer also criticizes the paper's clarity and exposition, suggesting it is difficult for practitioners to understand and glean insights from. These criticisms indicate a negative sentiment towards the paper's overall quality and impact.",
            "The reviewer expresses gratitude and appreciation for the authors' response, using phrases like \"thoughtful response,\" \"joy to read,\" \"clear phrasing,\" \"enlightening,\" \"helpful,\" \"loved this phrasing,\" and \"patiently enlightening.\" The reviewer also increases their score, indicating a positive shift in their evaluation.",
            "The reviewer explicitly states their score remains unchanged at a 7 with a smiley face, indicating satisfaction and a positive sentiment.",
            "The review expresses positive opinions about the paper's originality, quality, and significance. Phrases like 'improving some pre-existent results,' 'methodology seems to be sound,' 'work is complete,' 'important statistical guarantees,' and 'advancing the state of the art' indicate a positive overall assessment.",
            "The reviewer states \"Overall this is an interesting paper, addressing common IL settings, providing non-trivial theoretical contributions.\" and highlights positive aspects such as the reduction-based approach and the IL taxonomy. While acknowledging limitations, the overall assessment is favorable.",
            "The reviewer states, \"Overall I found this paper to be quite good, for which I am recommending an accept.\""
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Supportive",
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is supportive, as indicated by the acknowledgement of the authors' efforts and the overall positive assessment of the revised manuscript. The phrase \"The authors have addressed most of my concerns\" suggests a willingness to help the authors improve their work.",
            "The tone is critical due to the reviewer's direct and detailed criticisms of the paper's content and presentation. Phrases like 'incorrect or misinterpreted,' 'previously established,' 'hard to read,' 'unclear,' and 'feels like a smoke and mirror trick' demonstrate a critical evaluation of the paper's strengths and weaknesses. The reviewer also uses specific examples to illustrate their points, further highlighting the critical nature of the review.",
            "The reviewer uses encouraging and appreciative language, thanking the authors multiple times and praising the clarity and helpfulness of their explanations. Phrases like \"I loved this phrasing!\" and \"I am more than happy to change my score\" demonstrate a supportive and positive tone.",
            "The reviewer expresses gratitude ('Thank you for the clarifications') and uses a smiley face, suggesting a supportive and friendly tone.",
            "The review is generally encouraging and constructive. While it offers suggestions for improvement, the overall tone is supportive of the authors' work, using phrases like 'I would suggest' instead of direct criticism.",
            "The review provides both positive and negative feedback. It praises the paper's novelty and theoretical contributions while also pointing out limitations such as the lack of discussion on a common setting and questions about the linear expert setting. The reviewer maintains a neutral tone while expressing these points.",
            "The review offers both praise (\"I really appreciated the informal theorems presented before the real theorems\") and constructive criticism (multiple questions and suggestions for improvement). This balance indicates a balanced tone. The reviewer also maintains a professional and respectful demeanor throughout the review."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is empty, containing no statements that could contradict each other. Therefore, it is considered consistent by default as there are no conflicting points.",
            "The review is consistent in its assessment. It starts with acknowledging the paper's effort and accessibility, then highlights both strengths (technical soundness, significance) and weaknesses (contented contributions, clarity). The detailed comments in the 'shortcomings' section consistently support the identified weaknesses, providing specific examples and justifications for the criticisms. The reviewer maintains a critical but logically structured perspective throughout the review, without contradicting themselves.",
            "The reviewer demonstrates a consistent thought process by acknowledging their initial misunderstandings, appreciating the authors' rebuttal, and revising their opinion based on the clarifications provided. The reviewer explicitly states a change in score from an implied lower score to 7, indicating a positive shift in their evaluation due to the authors' response. This demonstrates a logical and consistent progression of understanding and assessment.",
            "The reviewer maintains their score after the clarifications, indicating a consistent evaluation.",
            "The review is consistently positive, praising the originality, quality, and significance of the work. The suggestions for improvement are constructive and focus on the presentation structure, without contradicting the overall positive assessment.",
            "The review presents a balanced assessment, highlighting both the strengths (theoretical contributions, novelty, interesting taxonomy, rigorous proofs) and minor limitations/questions (novelty uncertainty, missing setting, linear expert setting). The reviewer's uncertainties are clearly stated and do not contradict the overall positive evaluation of the paper's contributions. The limitations are presented as points for consideration rather than major flaws.",
            "The review is consistent because it starts with an overall positive assessment and recommendation for acceptance, while also providing constructive criticism and specific questions for improvement. The detailed questions and suggestions are aimed at improving the clarity and rigor of the paper, which is consistent with the initial positive evaluation."
        ]
    },
    {
        "paper_id": "iclr_2019_H1g4k309F7",
        "paper_title": "Wasserstein Barycenter Model Ensembling",
        "paper_abstract": "In this paper we propose to perform model ensembling in a multiclass or a multilabel learning setting using Wasserstein (W.) barycenters. Optimal transport metrics, such as the Wasserstein distance, allow incorporating semantic side information such as word embeddings. Using W. barycenters to find the consensus between models allows us to balance confidence and semantics in finding the agreement between the models. We show applications of Wasserstein ensembling in attribute-based classification, multilabel learning and image captioning generation. These results show that the W. ensembling is a viable alternative to the basic geometric or arithmetic mean ensembling.",
        "review_ids": [
            "rkgY3_xa2X",
            "H1xL7X6i3X",
            "HklQY2Q53X"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper has a simple message. When predicting families (weight vectors) of labels, it makes sense to use an ensemble of predictors and average them using a Wasserstein barycenter, where the ground metric is defined using some a priori knowledge on the labels, here usually distances between word embeddings or more elaborate metrics (or kernels K, as described in p.8). Such barycenters can be easily computed using an algorithm proposed by Benamou et al. 18. When these histograms are not normalized (e.g. their count vectors do not sum to the same quantity) then, as shown by Frogner, Zhang et al, an alternative penalized formulation of OT can be studied, solved numerically with a modified Sinkhorn algorithm, which also leads to a simple W barycenter algorithm as shown by Chizat et al.\n\nThe paper starts with a lot of reminders, shows some simple theoretical/stability results on barycenters, underlines the role of the regularization parameter, and then spends a few pages showing that this idea does, indeed, work well to carry out ensemble of multi-tag classifiers.\n\nThe paper is very simple from a methodological point of view. Experimental results are convincing, although sometimes poorly presented. Figure are presented in a sloppy way, and a more clear discussion on what K should be used would be welcome, beyond what's proposed in p.8. For these reasons I am positive this result should be published, but I'd expect an additional clarification effort from the authors to reach a publishable draft.\n\nminor comment:\n- in remark 1 you mention that as epsilon->0 the solution of Benamou et al. converges to a geometric mean. I would have thought that, on the contrary, the algorithm would have converged to the solution of the true (marginal-regularized) W barycenter. Hence the result your propose is a bit counter-intuitive, could you please develop on that in a future version? Is this valid only because \\lambda here is finite? on the contrary, what would happen when eps -> infty then, and K = ones?\n\n- GW for generalized Wasserstein is poor naming. GW usually stands for Gromov-Wasserstein (see Memoli's work).\n\n- \\lambda and \\lambda_l somewhat clash...",
            "The paper proposes a framework based on Wasserstein barycenter to ensemble learning models for a multiclass or a multilabel learning problem. The paper has theoretically shown that the model ensembling using Wasserstein barycenters preserves accuracy, and has a higher entropy than the individual models. Experimental results in the context of attribute-based classification, multilabel learning, and image captioning generation have shown the effectiveness of Wasserstein-based ensembling in comparison to geometric or arithmetic mean ensembling.\n\nThe paper is well-written and the experiments demonstrate comparable results. However, the idea of Wasserstein barycenter based ensembling comes at the cost of time complexity since computation of Wasserstein barycenter is more costly than geometric or arithmetic mean. An ensemble is designed to provide lower test error, but also estimate the uncertainty given by the predictions from different models. However, it is not clear how Wasserstein barycenter based ensembling can provide such uncertainty estimate. \n\nCan the authors comment on the time-complexity of the proposed framework in comparison with its baseline methods? Moreover, is it possible to evaluate the uncertainty of predictions with the proposed framework?\n\nIn the context of multilabel learning, Frogner et. al. (2015, https://arxiv.org/abs/1506.05439) suggested using Wasserstein distance as a loss function. In the model, they also leverage the side information from word embedding of tag labels. Is the proposed ensembling framework comparable with theirs?\n\nIn short, this paper can provide a useful addition to the literature on model ensembling.  Though the proposed framework does improve the performance of predictions in several applications, I am still not fully convinced on time-complexity introduced when computing Wasserstein barycenters.",
            "Paper overview: Model ensembling techniques aim at improving machine learning model prediction results by i) executing several different algorithms on the same task and ii) solving the discrepancies in the responses of all the algorithms, for each task. Some common methods are voting and averaging (arithmetic or geometric average) on the results provided by the different algorithms. \nSince averaging amounts to computing barycenters with different distance functions, this paper proposes to use the Wassertein barycenter instead of the L2 barycenter (arithmetic average) or the extended KL barycenter (geometric mean). \n\nRemarks, typos and experiences that would be interesting to add: \n     1) Please define the acronyms before using them, for instance DNN (in first page, 4th line), KL (also first page), NLP, etc. \n    2) In practice, when ensembling different methods, the geometric and arithmetic mean are not computed with equal weights ($\\lambda_l$ in Definition 1). Instead, these weights are computed as the optimal values for a given small dev-set. It would be interesting to see how well does the method compare to these optimal weighted averages, and also if it improves is we also compute the optimal $\\lambda_l$ for the Wasserstein barycenter. \n    3) How computationally expensive are these methods? \n    4) So the output of the ensembling method is a point in the word embedding space, but we know that not all points in this space have an associated word, thus, how are the words chosen?\n    5) The image captioning example of Fig.4 is very interesting (although the original image should be added to understand better the different results), can you show also some negative examples? That is to say, when is the Wassertein method is failing but not the other methods.\n\n\nPoints in favor: \n     1)Better results: The proposed model is not only theoretically interesting, but it also improves the arithmetic and geometric mean baselines.\n    2) Interesting theoretical and practical properties: semantic accuracy, diversity and robustness (see Proposition 1). \n\nPoints against: The paper is not easy to read. Ensembling methods are normally applied to the output of a classifier or a regression method, so it is not evident to understand why the 'underlying geometry' is in the word embedding space (page 2 after the Definition 1). I think this is explained in the second paragraph of the paper, but that paragraph is really not clear. I assume that is makes sense to use the word-embedding space for the image caption generation or other ML tasks where the output is a word, but I am not sure how this is used in other cases. \n\nConclusion: The paper proposes a new method for model assembling by rethinking other popular methods such as the arithmetic and geometric average. It also shows that it improves the current methods. Therefore, I think it presents enough novelties to be accepted in the conference."
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses a mix of positive and negative feedback. It acknowledges the paper's convincing experimental results and potential for publication, but also points out simplicity in methodology, issues with presentation, and areas needing clarification.",
            "The review acknowledges the paper's strengths (well-written, comparable results, useful addition to the literature) but also raises concerns about time complexity and uncertainty estimation. The reviewer isn't strongly positive or negative.",
            "The reviewer states that the paper 'presents enough novelties to be accepted in the conference' and highlights 'better results' and 'interesting theoretical and practical properties' as points in favor."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review adopts a balanced approach, offering both praise (\"\\[E]xperimental results are convincing\", \"positive this result should be published\") and criticism (\"\\[V]ery simple from a methodological point of view\", \"Figure are presented in a sloppy way\"). It also provides constructive suggestions for improvement, indicating a balanced perspective.",
            "The review presents both positive aspects (e.g., \"well-written,\" \"useful addition\") and critical questions/concerns (e.g., \"time complexity,\" \"not fully convinced\"). The language is polite and constructive throughout.",
            "The review provides both positive and negative feedback, pointing out strengths ('Better results', 'Interesting theoretical and practical properties') and weaknesses ('The paper is not easy to read'). The reviewer also offers suggestions for improvement, indicating a constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the paper's merits (simple message, convincing experimental results, positive publication recommendation) while also pointing out areas for improvement (poor presentation of figures, need for clarification on K, methodological simplicity). The reviewer's criticisms are constructive and aimed at improving the paper for publication, not rejecting it. The overall tone is positive but suggests revisions are needed, which is a consistent and common stance in peer reviews.",
            "The review is consistent because it acknowledges the paper's strengths (well-written, comparable results, useful addition) while consistently raising concerns about the time complexity of the proposed method and the lack of clarity on uncertainty estimation. The reviewer's concerns are reiterated throughout the review, maintaining a consistent critical perspective.",
            "The reviewer identifies points for improvement and weaknesses regarding clarity, but also acknowledges the paper's novelty, improved performance compared to baselines, and interesting theoretical and practical properties. The conclusion to accept the paper is consistent with this overall assessment, as the positive aspects and novelty are deemed sufficient for acceptance despite the identified shortcomings. The remarks are constructive suggestions for improvement rather than fundamental flaws that would warrant rejection."
        ]
    },
    {
        "paper_id": "iclr_2022_NyJ2KIN8P17",
        "paper_title": "Neural Program Synthesis with Query",
        "paper_abstract": "Aiming to find a program satisfying the user intent given input-output examples, program synthesis has attracted increasing interest in the area of machine learning. Despite the promising performance of existing methods, most of their success comes from the privileged information of well-designed input-output examples. However, providing such input-output examples is unrealistic because it requires the users to have the ability to describe the underlying program with a few input-output examples under the training distribution. In this work, we propose a query-based framework that trains a query neural network to generate informative input-output examples automatically and interactively from a large query space. The quality of the query depends on the amount of the mutual information between the query and the corresponding program, which can guide the optimization of the query framework. To estimate the mutual information more accurately, we introduce the functional space (F-space) which models the relevance between the input-output examples and the programs in a differentiable way. We evaluate the effectiveness and generalization of the proposed query-based framework on the Karel task and the list processing task. Experimental results show that the query-based framework can generate informative input-output examples which achieve\nand even outperform well-designed input-output examples.",
        "review_ids": [
            "kmnfLi3hFX",
            "MRgwZlhRRKS",
            "N9FxYCrUt9O",
            "SBMyfiTC9Ck",
            "OZJIKa4Dx5c",
            "CybYCpN_DgS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " ## on 1\n\nI think the most solid move here is to treat crash just as any other form of information and do synthesis over crash anyways. Imagine I come up to a synthesizer and says, please come up with a function that:\n\n- 8 -> 3\n- 2 -> 1\n- 1 -> 0\n- 1/2 -> -1\n- -3 -> crash\n\nthen maybe a reasonable answer would be, that is the log(x) function, which you clearly hinted to me that it is undefined over negative numbers. so really, there is mutual information between the crash outcome and what the program is, and there is no problem treating it like any information as you would. in fact, the question \"does it crash on negative numbers?\" is very informative first question one could ask no?\n\non the implementation side, for the very very short-term, you can do something like... I'm assuming your synthesizer is of the form \"generate and check\" which consisting of a generative model that, conditioned on some spec, propose a number of programs, you then explicitly execute each proposed program to see if they match the input-output pairs in the specification. you can perhaps ignore the crash information for generation (i.e. don't condition your NN program generator on the crash information -- this way you don't have to re-train anything) but during the checking step make sure the proposed program correctly crash on the same query that crashed the oracle program.\n\nhowever, a more solid approach would really be to re-train everything with crash as a special value, maybe your query network would learn to query for crashes, which would be pretty interesting in its own right. I've always had a hunch the \"valid domain\" for a function would pop up somewhere as being important thing to be communicated, as almost no computer programs are total, so that'll be kinda cool.\n\n## on 2\nlet me read the supervised contrastive learning paper again to get a better sense and get back to you. I think I understand what you are saying and I tentatively agree that, it is the right objective if you do it for long enough, but it's just the quality of samples will degrade (exponentially so as each query prunes a fixed fraction of valid programs) rather quickly",
            " \"The F-space formalism is a nice way of thinking about the space of programs and functional equivalence, although it is not practical (as it requires enumerating all possible inputs to the programs) and the definition seems unlikely to be novel in the literature.\"\n\nI definitely think F-space is just version space, and would probably be best explained as such. That being said, such space probably _could_ be modeled and will be practical if you train a large enough NN, especially under the infinite-data regime of synthesis, where given a DSL you can generate / train programs for as long as you want (in the limit, you would have memorized the whole program space). So given recent advances such as codex showing it can somewhat learn a complicated space like python, for Karel I strongly believe if you just train a big enough NN it might be able to flush out the entire version space, i.e. {programs consistent with any set of examples} and be able to perform set operations on these version spaces as we have in symbolic approaches.\n\ncurious to see what you think of it though aha",
            "This paper considers an active diagnostic setup for program synthesis as follows: There is an oracle function/program p* belonging to a space of functions, and at inference time, one would like to uncover p*. To do so, a querying algorithm iteratively/adaptively query the oracle with inputs x1, x2, ... xk, receiving outcomes y1, y2, ... yk in return. A synthesizer (or any inverse model capable mapping observations to hypothesis, doesn't really matter) then proposes a guess p' based on the collected query-observation pairs (called examples) {(x1,y1)...(xk,yk)}. One \"wins the game\" if p* and p` behave the same way (functional equivalent) on all inputs, using as few queries (i.e. small k) as possible.\n\nTo do this, the paper introduce a learned F-space, which both functions f and sets of examples e can map into. While functions map (using a learned embedding) to single points in F-space (each point in F-space represent a unique function), examples map (using yet another learned embedding) to distributions in F-space (each example set represent a _distribution of_ functions that are consistent with it). The querying network is trained to generate the next observation x conditioned on the existing observations {(x1,y1)...(xk,yk)} in such a way that maximally reduce the uncertainty of the F-space once the new example, (x, p*(x)) is augmented to the existing observations.\n\nResults are convincing to me. ## what I like about this paper\n\nI really liked the F-space formulation, it is very clever and intuitive to map single functions p to points, and sets of examples e to distribution of points, it is a \"by definition\" representation that captures the relationships between examples and programs in a learned embedding. the operations over this space is also well-justified, with algebraic manipulations of the example embedding such as union and intersections very cleanly explained.\n\nAlgorithm 1. is cleanly written, I was able to follow it well other than line 11 and 12 (see question below)\n\nI personally liked the choice of functionally equivalent proxy for evaluation. \"if the predicted program satisfies all 100 examples, then it is functional equivalent to the ground-truth program\". It is a very practical choice and and also a fairly thorough one.\n\nThe results seem convincing to me, implementing a query generator for the space of Karel and list-manipulation is very impressive, as these are fairly high-dimension spaces, and much work must have been put into making it all work out.\n\nOverall this is a solid work that (appears to me) is written by a group who understands the landscapes of neuro-guided program synthesis well.\n\n## what this paper needs work on\nThis paper would be much stronger if the following points can be addressed.\n\n### the exposition\nwhile the technical aspect of the paper (F-space, its learning objective, evaluation) are sound, the problem statement can be cleaner. the current paper does not draw a clean distinction between \"the problem statement\" and \"the proposed algorithm\". as a result, the paper reads like a long todo list of \"we need to do this, then did that, oh by the way we need to make this differentiable because neural networks\" and as a reader I'm left confused as to what the actual problem is that we're solving. for instance, in definition 2.3, it tells me what a querying algorithm is doing, but it doesn't tell me at all _why_ we need to query or _what_ are we querying for.\n\nto be constructive, I would add a \"problem statement\" subsection before \"problem formulation\" that formally define what exactly is the problem we're solving. specifically, one might borrow the terminologies of active diagnostics / bayesian optimization, that of the \"acquisition function\": given a set of possible query points X, the acquisition function a(x) scores all possible query points x \\in X, and the querying algorithm picks the x with the highest score. in this particular work, the acquisition function is that of mutual information I([[e]], p), which is to say, given a set of (potentially empty) observed examples so-far {(x1,y1)...(xt-1,yt-1)}, we wish to have the query network pick a next query point x_t such that the acquisition function of mutual information I([[e + (xt, yt)]], p) is maximized. _only then_ can one jump into the explanation of F-space and related techniques of solving this querying problem. \n\ncurrently, the relationship to mutual information (the objective that the query network is optimizing for) is only mentioned in equation (7), almost as an after-thought. this is incredibly confusing because I am left clueless what problem the paper is actually solving while reading the answers to this mysterious problem (think hitchhiker's guide, where the answer is 42 but the question is unstated).\n\nSince we use the same acquisition function both in training the synthesizer and at inference time to query, the synthesizer receives the examples from the same distribution, thus, we do not run into the problem of out-of-distribution examples at inference time. The paper definitely makes an attempt to make this claim, but it is unclear in the current version. I believe once the problem statement is cleanly defined, this contribution can be stated cleaner as well.\n\n### other baselines\nCurrent approach only compares against a weak random baseline, while there is merits to it (one can argue random examples is the standard for synthesis benchmarks), the paper can be much stronger with these additional baselines:\n\n1) query by committee. this is a standard approach in active diagnostics where one keeps a sampled distribution of valid hypothesis, then select the query point that maximizes disagreement. specifically, given current set of examples e = {(x1,y1)...(xk,yk)}, we can sample the a program synthesizer to obtain multiple programs that are consistent with e: p1, p2, ... pn, then, use 100 randomly sampled inputs (same as your testing procedure) to discover if there is an input x that maximizes disagreement between these programs. Terminate early of the synthesizer can only propose functionally equivalent programs over the 100 sampled inputs. See https://dl.acm.org/doi/pdf/10.1145/130385.130417 for a similar set up, albeit over a simpler domain.\n\n2) end-to-end RL. one can train a query proposer jointly with a synthesizer in an end-to-end communication setting, where one jointly optimizes (over both the query NN and the synthesizer NN) the probability of the synthesizer recovering the ground-truth program p, given the observation sampled by the querying NN (x, p(x))\nSee https://arxiv.org/pdf/2006.00418.pdf for an end-to-end communication game set up, albeit over a simpler domain. I believe the same gumbel trick would work here as well.\n\nI would expect a replication of 1), as it is by far the most standard approach in active diagnostics, and is incredibly easy to implement and used widely in practice. 2) might be more difficult to replicate, I am okay not seeing it. I believe 1) and 2) serve as strong, yet standard baselines that this work should compare itself to, and I am fairly confident the proposed method of this work would out-perform both 1) and 2), seeing the proposed approach demonstratively out-performs 1) and 2) will make me much more confident in this work.\n\n### related works\n\nThere are very relevant prior works that studies the relationships between optimally querying examples for program synthesis from an information theoretic perspective:\n[1] https://arxiv.org/pdf/1704.06131.pdf\n[2] https://arxiv.org/pdf/1711.03243.pdf\n\nSpecifically, these works formally justify the strategy of greedily picking the next query (i.e. picking x_t that greedily \"maximize the mutual information between the input-output examples [[e]] and the corresponding program p\", same as this work) as more than a heuristic, but is in fact (1-1/e) as good as the globally optimal solution that picks (globally) a set of k examples that maximize the mutual information between [[e]] and p. The proof involves showing that adding examples is monotonic and sub-modular in reducing the set of satisfiable programs.\n\nFurther, [1] studies the exact same problem as this work: How do you optimally query a set of observations for program synthesis? And also propose a similar querying NN that selects which next point to add, however, unlike this work, they assume the space of query points can be enumerated. Thus, one can view this work as solving a much needed algorithmic improvement to [1], by making a querying NN that can _generate_ new query points. Explaining the relationship between this work and [1,2] would help contextualizing this work better.\n\n## questions:\n\n- line 11 and 12 of algorithm 1 is bit unclear to me. The decoder makes a choice for the next query point x_t, but since the rest of the loss requires one to query the oracle p(x_t), the gradient cannot \"go through\" this sample step. typically this is learned using policy gradient or some RL objective, because presumably we cannot teacher force the correct x_t, because it amounts to sampling, from the space of all possible query points X, the best one that helps the identification of p. how is this done? (edit: ah gumbel trick. very nice!)\n\n- how do you obtain negative samples? i.e. \" we construct positive pairs {([[e]], p_i)} and negative pairs {([[e]], p_j )|i != j}. \" how does this work? presumably you want to sample p_j in such a way that p_i and p_j are functionally equivalent over [[e]] yet distinct over some other query points (i.e. exactly the x your query network should produce). however sampling this p_j that is consistent to p_i sofar over [[e]] yet still distinct to some unseen x is a challenge of its own, so you adopted to just sample a \"random p_j\" if I understand correctly. is this the case? if so it probably should be justified, at least with a few words why this particular form of contrastive learning is a good choice here.\n\n- is the synthesizer trained completely de-coupled from the latent representations of the querying NN? I'd imagine the embedding [[e]] be quite helpful, you just have to decode from it to obtain the correct program. but it is good either way, I would personally find the paper stronger if the synthesizer is completely decoupled, i.e. does not rely on [[e]] in any way just to prove a point that one could use any \"back-end\" synthesizer they want.\n\nminor:\n\n- in abstract, what does the phrase \"actual distribution\" mean? the distribution that end-user would use to communicate their intent? a distribution of input that would allow the program to be uniquely identified from all the space of programs unambiguously? or is it just \"the training distribution\", which is what I believe it to be after reading the intro.\n\n\n\n Overall I liked the paper, as it solves 2 key challenges of program synthesis:\n1. The training distribution of the synthesizer does not match the testing distribution\n2. One would like to recover the target program with as few examples as possible\n\nIt solves both problem by \"mocking\" the active diagnostic process of (2) in training time, having a trained querying network to propose the most informative queries. To build a querying network, the proposed method embed both programs and examples in a joint F-space, and use contrastive learning to train the querying NN to propose query points that maximally distinguish the target program p from distractor programs p'. The construction of F-space is both intuitive and algebraically elegant.\n\nResults give good evidence of the proposed method as well.\n\nThis paper would be stronger if the 3 points mentioned: exposition, other baselines, and related works can be addressed, I am willing to raise the score accordingly.\n\nThe author addressed the 3 points mentioned, so I am raising the score as I said I would. Although there is no score of 7, I meant a 7 instead of an 8 as the baseline (QBC) is still fairly close to the proposed method, and is derived in much less time (a week).",
            " I asked for a baseline of QBC, I have received it. I will increase my score from 6 to 7. I do not mind that the performance of QBC-crash-aware (a very simple baseline that you implemented in a week, as opposed to however long it took you to do your original approach) is dangerously good when compared against your approach, I respect the honesty of the authors showing us the results. thank you.\n\nThe question remain is how would you improve the current approach so that it may perform significantly better than QBC? I feel the answer lies in the negative sampling step of question (2), which I do not think you fully answered. Given a set of examples, we want to sample negative programs that are not the target program, but nonetheless consistent with the examples so-far. Failure to do this will cause the query network to not be maximally informative _conditioned on_ the examples given so-far. I'd be interested to see how you will manage to do that, because sampling negative programs consistent with a set of examples is a very difficult problem, yet one that must be solved for your approach to truly make sense.\n\nw.r.t. this work, I feel a good \"demo\" figure of sorts showing a concrete example (like figure13) would be a good idea. Imagine figure1 but make it very concrete, under the context of Karel the robot. Most other reviewers missed what I think is the central premise of this work, which is a method to ensure the train/test distribution in a synthesis system to be same, and yet still in a way not alien to end-users, and instead went off nitpicking the implementation of the F-space, which, since it is all neural anyways, it doesn't really matter.\n\nFinally, I do invite the authors to build an analogy of synthesis and communication, as there are many under explored problems in that space. Specifically, one can think of communicating under consistency of random input-output as communication over literal semantics (sat / unsat), and communication under consistency _and_ choice of input-output as pragmatics (meanings + context), and the end-to-end RL communication to be consistency+pragmatics+convention, i.e. the speaker and listener collude upon a shared convention knowable to the pair alone, and unintelligible to any other agents. The following is a cute paper that you might find interesting https://arxiv.org/pdf/2107.00077.pdf .\n\nbest of lucks in your research endeavors, this area is very good.",
            "The authors propose a framework to generate unit tests by generating candidate inputs for which labels are queried from an oracle. The idea is to have a vector space where each point represents a function (different programs that are functionally equivalent map to the same point) and a unit test is represented as a set of points that correspond to functions that would pass that unit test. \n\nThey project a unit test onto a normal distribution on the function space where the probability of the distribution is whether a point is the underlying function to be synthesized given the unit test. \n\nThey merge multiple unit tests using an attention mechanism into a single point in the function space and then train the unit test generator such that the mutual information between the probability distribution on the merged unit test and the probability distribution on the programs is maximized using InfoNCE loss.\n\nA synthesizer network is used to synthesize the program from generated unit tests. They test on Karel and list processing task. The problem of generating appropriate unit tests in an interactive manner and has the potential to be a key ingredient in future programming workflows. However I think the paper as written needs improvement before it can be accepted. In particular, I think the following points need attention :-\n\n(1) They say that the Product of normal distributions is a normal distribution but that's not necessarily the case in general - for example see https://math.stackexchange.com/questions/101062/is-the-product-of-two-gaussian-random-variables-also-a-gaussian/101120#101120\n\n(2) Sanity checks :-\n  (a) I would expect the merged unit test distribution to be lower entropy than the distribution of the individual unit tests (as more unit tests mean the functions satisfying them are fewer). This is something that should be examined.\n  (b) They don't test whether the program they synthesize in the end has a high probability under the merged unit test normal distribution vs a  program that doesn't satisfy those unit tests\n\n(3) Interactive unit test generation has been looked at in the past and it's unclear the different ways in which their setup differs. It's also unclear what assumptions they are making that guide their design choices (along with theoretical or empirical validation for those assumptions). Some illustrating examples would help develop their intuition here.\n\n(4) They should compare against more baseline methods. For example they mention Padhi et al. which present multiple queries to the user and let the user pick which one to label. This they claim puts additional burden on the user. But they could still compare against Padhi et al. in a way that's fair to them by selecting for example a query at random for the user to label out of the ones that Padhi et al. suggest.\n\n(5) The empirical results with the baselines they compare against aren't very compelling. For example they are much worse in the Searching for Semantics metric for list processing task compared to the baseline methods and only very marginally better in the searching for exact match metric. They also don't give error bars for their results. The problem is interesting but the paper suffers from some incorrect claims, insufficient motivation and justification, and underwhelming empirical results. ",
            "In neural program synthesis from input-output examples, the goal is to generate a program which, when executed on the inputs, produces the corresponding outouts. Typically, the set of input-output examples is taken as given by the program synthesis method. In this work, the authors propose to instead have the program synthesizer interact with an oracle to determine the set of input-output examples. The synthesizer proposes a potential input for the program, the oracle produces the output, and the process repeats. Under this paradigm, the program synthesizer can query the oracle for the inputs which would be most helpful in determining the correct program. This direction has been explored in prior work as \"interactive program synthesis\", but largely with constraint-based methods.\n\nThe paper represents each input-output example pair as a normal distribution (with a mean and variance parameter). A set of input-output example pairs is also represented as a normal distribution, with its parameters a weighted sum of each pair's parameters. A neural decoder module takes the mean/variance of this combined distribution as input to produce a new input; we can query the oracle with the input to get the output, resulting in a new input-output example pair.\n Strengths: \n- The interactive framing of the problem is useful to explore. For program synthesis from input-output examples, there often is an implicit oracle for the unknown program that the user can use, even if it is not cheap to compute. If the user has to decide the input-output examples without guidance, then the set may not end up being very informative at distinguishing between possible programs. By having the synthesizer query the user for inputs interactively, it can obtain the specification most useful for generating the right program.\n- The F-space formalism is a nice way of thinking about the space of programs and functional equivalence, although it is not practical (as it requires enumerating all possible inputs to the programs) and the definition seems unlikely to be novel in the literature.\n\nWeaknesses:\n- The modeling of F-space and sets of input-output examples as normal distributions seemed inappropriate. In particular, equation 3 states that we want to model the intersection of two sets by intersecting the distributions (taking a product of the pdfs?) but then equation 4 goes onto take a weighted sum of the mean and log of the variance parameters. Equations 4 and 5 cite Ren & Leskovec 2020, but that paper took weighted sums of Beta distribution parameters, not normal distribution parameters.\n- The paper does not provide all details necessary to reproduce the results (see suggestions for more details).\n- The paper doesn't describe what happens for invalid inputs to programs (i.e. inputs which cause the programs to crash). For example, in Karel, it is not allowed to take a marker from a cell containing no markers.\n\nSuggestions:\n- Provide greater details necessarily for reproducibility of the paper. Various details that were necessary to obtain the results are obscure in the appendix and not described elsewhere. Examples:\n  - Query decoder for Karel: if it is similar to the query encoder, how does it generate a grid world state as the output?\n  - Query decoder for list processing: since input examples \n  - \"additional batch normalization is added to keep the generation more stable\": how was this done?\n  - \"we introduce a latent code like InfoGAN\": this can be described more precisely, including with equations\n  - \"we add the reciprocal of the KL divergence [...] this loss does not have a significant impact on training most of the time and makes the \ntraining process unstable\": If the loss was not useful, then was it still kept?\n  - \"At the beginning of the training, the query network generates one query only. As the training goes on, the number of query times increases until it achieves the query time limit.\": How was the number of generated queries increased over time? Why is the curriculum necessary?\n  - \"to guarantee that every query can be recognized by the program simulator, we design the output layer of the query network elaborately.\": it is not clear what was done in the output layer to guarantee this.\n- Add a prior to F-space and use it to generate the best queries. The set of all possible programs is very large or infinite, but the set of useful programs that the user might want is smaller. It should be possible to learn the prior distribution (or assume one like Occam's razor) and use it to inform the next query. The paper has a useful motivation and explores interesting idea, but the aspects of the execution and the missing details in the work make it difficult to recommend acceptance."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses agreement with the approach and suggests improvements. Phrases like \"most solid move\", \"reasonable answer\", \"very informative\", and \"more solid approach\" indicate a positive sentiment.",
            "The review expresses both positive and negative aspects. It acknowledges the F-space formalism as a 'nice way of thinking' but also points out its impracticality and potential lack of novelty. The reviewer also presents a potential future application of the concept.",
            "The reviewer uses positive language such as \"clever\", \"intuitive\", \"well-justified\", \"impressive\", and \"solid work\". They also explicitly state that they \"liked the paper\" and are \"willing to raise the score\".",
            "The reviewer expresses appreciation for the authors' honesty and is increasing their score. They also offer constructive feedback and encouragement, indicating a positive overall sentiment.",
            "The reviewer identifies several weaknesses in the paper including incorrect claims, insufficient motivation, underwhelming empirical results, and lack of comparison to existing methods. The concluding sentence summarizes these concerns, indicating a negative overall assessment.",
            "The review expresses concerns about the paper's methodology, modeling choices, lack of reproducibility details, and handling of invalid inputs. While acknowledging the value of the interactive framing, the reviewer ultimately finds the execution and missing details problematic, making it difficult to recommend acceptance."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Balanced",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer offers constructive criticism and suggestions, using phrases like \"I think\", \"maybe\", \"you can\", and \"however\" to soften their suggestions and show support for the authors' work. The reviewer is actively trying to understand the work and offers solutions to potential problems. The phrase 'that'll be kinda cool' also indicates enthusiasm and support.",
            "The tone is balanced as it presents both positive aspects ('nice way of thinking') and criticisms ('not practical', 'unlikely to be novel'). The reviewer also expresses a personal opinion in an informal manner ('curious to see what you think of it though aha').",
            "The review provides both positive feedback and constructive criticism, acknowledging the strengths of the paper while also pointing out areas for improvement. This balanced approach indicates a neutral tone.",
            "The reviewer uses phrases like \"I respect the honesty of the authors,\" \"thank you,\" \"best of lucks in your research endeavors,\" and \"this area is very good.\" They also offer constructive suggestions and invite further exploration of related ideas, creating a supportive and encouraging tone.",
            "The review adopts a critical tone by directly pointing out flaws and shortcomings in the paper. The reviewer uses phrases like \"incorrect claims,\" \"insufficient motivation and justification,\" and \"underwhelming empirical results\" to express their concerns. The numbered list format further emphasizes the specific areas where the paper needs improvement.",
            "The review points out several weaknesses and shortcomings in the paper, using phrases like 'inappropriate,' 'does not provide all details,' and 'difficult to recommend acceptance.' The reviewer also asks pointed questions about unclear aspects of the implementation and suggests improvements, indicating a critical assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently argues for the importance of treating crashes as information in program synthesis. It suggests practical approaches for incorporating crash information, starting with a short-term fix and moving towards a more comprehensive long-term solution. The reviewer also raises a separate but relevant point about the potential challenges of sample quality in supervised contrastive learning, which does not contradict the main argument about crash information.",
            "The review starts with a mixed assessment, acknowledging the niceness of the F-space formalism while pointing out its impracticality and lack of novelty. However, it then pivots to discuss how Neural Networks could potentially make such spaces practical, especially in the context of program synthesis and large datasets. The reviewer is not contradicting themselves but rather presenting a nuanced view: acknowledging current limitations while suggesting a potential future direction that could overcome these limitations. The reviewer's argument is that while F-space as currently defined might be impractical, advancements in NN could change this, making the concept relevant and applicable.",
            "The review is consistent in its assessment. It starts by highlighting the strengths of the paper, then points out areas for improvement, and finally concludes with an overall positive evaluation, especially after the authors addressed the reviewer's concerns. The reviewer's initial positive impression is maintained and strengthened as the review progresses, without any self-contradictory statements.",
            "The review is consistent because the reviewer provides a clear and logical flow of feedback. They start by acknowledging the authors' efforts in addressing previous concerns, then raise a key question about improving the approach, suggest a concrete visualization for better understanding, and finally offer a broader perspective by drawing an analogy to communication. The reviewer's points are all related to improving the paper and are not contradictory.",
            "The review is consistent in its critique. While acknowledging the potential of the research area, the reviewer consistently points out weaknesses in the paper, including theoretical inaccuracies, lack of validation, insufficient motivation, missing comparisons, and underwhelming empirical results. The reviewer's feedback is focused on areas for improvement and justifies the need for revision before acceptance.",
            "The review is consistent because it acknowledges the potential strengths of the paper's idea (interactive framing, F-space concept) while also clearly pointing out significant weaknesses in the implementation and presentation (modeling choices, lack of reproducibility details, and handling of invalid inputs). The suggestions provided are directly related to addressing the identified weaknesses, aiming to improve the paper's quality and reproducibility. There are no contradictory statements or conflicting viewpoints expressed within the review."
        ]
    },
    {
        "paper_id": "nips_2021_i2bTx7ZWFfI",
        "paper_title": "Bandit Learning with Delayed Impact of Actions",
        "paper_abstract": "Wei Tang, Chien-Ju Ho, Yang Liu",
        "review_ids": [
            "xOscAunMYEo",
            "iaPmzIbzNur",
            "dLuX9EB0Emf",
            "LQeO5wfxV2",
            "UmjG3fAhhie"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors formulate and solve the problem of resource allocation to K groups under the bandit setting where the impact of actions perseveres over time and affects the corresponding reward function. They key idea is to realize that deploying the policy for a certain period of time helps us estimate its utility and therefore mixing this with updating the policy helps us to learn the optimal one.  A regret bound of Otilde(KT^2/3) is shown with a matching lower bound. Experiments are shown on synthetic datasets comparing it with EXP3, DUCB and SWUCB to show the performance improvement in terms of cumulative regret.   Overall, the paper is well-written and solves an important problem for the community.  In particular, it is concerned with resource allocation in delayed settings which are both challenging as well as critically important for healthcare, crime and financial domains. \n\nPros:\n\n(A) The problem is well-motivated by a crime example of deploying police officers to districts to reduce the rate of incidents. Also, applying standard algorithms is shown to lead to linear regret. These are typical in most of these settings but it is good to be formalized in Lemma 3.1\n(B) Similar to EXP3, we see the importance weighting being utilized to redefine the reward function of the observed arms so that the expected reward is maintained. \n(C) First the case where the impact functions are limited to the current selection are tackled and this is similar to UCB with the reweighted weights from above. The observation that history-dependent can then solved by utilizing the previous freeze and gather evidence and update the policy cycle is really nice and leads to an optimal regret. This is both theoretically and experimentally validated.\n\nCons/Questions:\n\n(1) The setting was a bit unclear to me from the delay perspective. So, the reward of say loan applications to a certain group will result in a certain reward but usually there is a delay in observing such rewards. Is it the case that we need both to have a better model? We have impact of the previous decisions affecting the current rewards and also the current rewards and not observed till some future date? \n(2) The issue with importance weighting, as we know, is that it can lead to high variance. How does your approach tackle the case where the policy has low probability for some of the arms? Could the usual tricks of having a biased estimator but lower variance help with this as we see with EXP3 -> EXP3-IX?\n(3) The regret is compared to a static policy and it would have been nicer if the regret performance was over the optimal dynamic policy.\n\nAfter rebuttal:\n\nThanks for the responses. I will keep my score unchanged as it clarifies the questions I had and is aligned with the other reviews as well.\n Yes",
            "The paper considers a bandit problem where actions have a discounted impact on future rewards. The authors propose an algorithm that achieves an $O(KT^\\frac{2}{3})$ regret bound, assuming that the reward function is Lipschitz and resorting to a discretization of the action space and a waiting time that enables the rewards to stabilize (the last actions are the one that will impact the rewards the most). They provide a matching lower bound.  ## Originality \nDelayed rewards and rested bandits have already been studied, but the originality of this paper lies in the fact that the actions chosen at a certain time will impact their future rewards decreasingly (their rewards can decrease even at a time step when they are not pulled).\nOne of my concerns however is that it is the probability of choosing an arm that impacts its evolution. I do not understand in which practical case a probability of choosing an arm is key, rather than its effective choice. I would like the authors to clarify this point.\nAlso, I wonder what would happen, should the impact function structure depend on the arm (for example if the discount factor depended on the chosen arm)? This would maybe be a more realistic setting. \n\n## Quality\nThe upper bound of order of $T^{2/3}$ seems satisfactory and is supported by a matching lower bound.\nI am a little uncertain about the use of discretization. How should $\\epsilon$ be chosen? Is $\\epsilon$ a function of the Lipschitz constant? If so, this is an important limitation that should be discussed.\n\n\n## Clarity\nSome parts could be explained in more detail. The intuition behind the lower bound could be explained a little bit in the main paper for example, since it is a little expeditious as is.\nI think the paper would really benefit from a discussion of the choice of the length of the approaching stage. How is the learner supposed to set $\\rho$? How does it affect $L$?\n\n## Typos\n- I think $\\delta$ should be $\\gamma$, l118 and l172. \n- \"our model has its limitation\" l350\n\n## Edit\nI thank the authors for taking the time to write clear answers. The only drawback of the paper that I see is that the fact that probabilities of actions impact their evolution is justified by very specific applications. Let me be clear, these use cases make sense to me but I think they should really be justified more in detail in the final version.  The way the authors discuss the limitations of their work is satisfactory to me.",
            "This paper investigates a bandit variant with the changing rewards. The rewards of actions depend on history information, which is very interesting and practical. The idea of combining epsilon-cover (to discretize the action space) and CMAB to design a learning algorithm is very novel. They provide an instance-independent regret upper bound and also a matching regret lower bound, which means their proposed learning algorithm has an order optimal regret guarantee.\n  The paper is quite organized and written. It has several motivating applications included, which contributes to understanding the learning problem. I have some questions.\n\nI am wondering for some special problem instances, is  it possible to have instance-dependent regret bound (takes the O(1/Delta) form) and a matching regret lower bound? \n\nRegarding the content just above Section~5, I still cannot understand why EXP3 would fail. Can authors clarify more?\n\nRegarding the learning algorithm, is there any computational issue to find epsilon covers? If yes, is it possible to have advanced algorithm that does not have any computational issue?\n I do not see too much discussion about the negative impact of their proposed setting. However, I think the framework proposed has a positive impact on society such as how to dispatch police resources to reduce the crime rate.",
            "This paper generalizes the stochastic bandit model by redefining the action space as a sequence of resource allocations, with a time discounted action-dependent bias on future resource allocations. The authors provide an example application based on their model, and propose an algorithm with proved near optimal regret upper bound. A lower bound based on the proposed model is provided.  The paper is well written and easy to read, with many details fully explained. I also think that this idea is novel especially the time discounted action impact. Existing works focus more on long-term impact by reward history, for example current action is influenced by a feedback function on reward history, such as https://arxiv.org/pdf/1802.05693.pdf, while this paper considers long-term influence by an impact function on action history, which differs from previous works by new non-trivial problem formulation and policy design.\nI have one question: since the value of \\epsilon and s_a affect the regret, and they are required as inputs, I am wondering how to tune these values to make the regret under control, or if only we make sure that \\epsilon and s_a are in the orders as stated in Theorem 5.1, the we can guarantee a good regret regradless of the coefficients before the order? How can I get some clue when tuning the parameters? I think the authors have adequately addressed parts of limitation and potential negative societal impact of their work. There could be some other limitations, such as a more general format of function f(), but that could also be a future work. My suggestion would be that the authors could consider adding more insights of why the current function f() look like that, and how to generalize it, so as to cover more applications.\nAnother tiny concern is that, in Line 118 & 172, I think it should be \\gamma instead of \\delta.",
            "In this paper, the authors formulate and study a multi-armed bandit problem with delayed impact\nof actions, in which the actions are taken in the past affect the rewards in the future. Under this\nsetting, the authors design an algorithm that utilizes the dynamics introduced by the delayed\nimpacts of past actions to look for the optimal meta arm that maximizes the collected utilities\nover time. The algorithm is shown to incur regret of at most $\\Tilde{O}(KT^{2/3})$, which\nmatches the theoretical lower bound for this problem.  Strengths:\nThe paper is well written and structured. The model is novel in the current bandit's literature with\ndelayed impacts due to its consideration of long-term impacts. The paper provides good intuition\nas to the applications of MABs with long-delayed impacts (the example in Section 2 well\nillustrates the model set-up and the roles of the impact function). The theoretical results for both\naction-dependent and history-dependent bandits are well explained. In particular, Algorithm 2/3\nuses a simple idea that uses a short approaching stage to achieve the convergence of $f(t)$,\na longer estimation stage to estimate the reward of meta arm $p$, and a UCB-type algorithm to\nfind a good meta arm. The regret upper bound also matches the theoretical lower bound, which\ndemonstrates the optimality of the proposed algorithm.\n\nQuestions:\n- How would one determine the choice of $\\rho$ in Algorithm 2? Empirically, the authors\nargue that the choice of $\\rho$ does not influence the cumulative regret too much. But if\n$\\rho$ gets too small it seems that the theoretical upper bound worsens. Is there\nanything that prevents us from always selecting a large $\\rho$? Weaknesses:\nOne limitation of the current paper is that most of the analysis on action-dependent and\nhistory-dependent bandits is done using the impact function in Equation (2). Although the\nauthors remark in Section 5.2 that similar analysis can be applied to impact functions that\nconverge when one keeps selecting the same meta arm, it is not clear how one should select\nthe monotone function $g(.)$ for a general impact function. Since this restriction does not really\napply to general functions, it might be better to state it as a formal assumption.\nIn the regret definition, the authors choose the optimal fixed policy instead of the stronger\noptimal dynamic policy as the benchmark. In Appendix H, the authors justify their choice by\narguing that for history-dependent bandits, the optimal strategy in the limit is well defined and\ncan be characterized as the best-in-hindsight policy. From the intuitive explanations, it is unclear\nto me why this holds and might need a more rigorous treatment.\n\nMinor comments:\n- On pages 3 and 4, \u201c$\\delta = 0$\u201d is a typo and should be $\\gamma = 0$.\n- In Section 4, it says that $p_k \\in \\{\\epsilon, 2\\epsilon, \\dots, 1\\}$. Can $p_k$ take the\nvalue of 0 as well?\n- It might be good to compare the performance of the algorithm of different $rho$ values\nagainst the benchmark algorithms. From Figure 3d alone, it is difficult to tell that the\nalgorithm is not sensitive to different values of $\\rho$."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states that the paper is \"well-written and solves an important problem for the community.\" The reviewer also finds the approach to be \"really nice and leads to an optimal regret.\" The reviewer concludes by saying they will keep their score unchanged, indicating satisfaction with the paper.",
            "The review expresses both positive aspects (satisfactory upper bound, originality) and negative aspects (concerns about practical application, clarity issues, limitations of discretization). The overall sentiment is therefore neutral.",
            "The reviewer uses positive language such as \"very interesting and practical\", \"very novel\", \"order optimal regret guarantee\", \"quite organized and written\", and \"positive impact on society\".",
            "The reviewer expresses positive opinions such as \"The paper is well written and easy to read\", \"this idea is novel\", and \"authors have adequately addressed parts of limitation\". The overall assessment is favorable, indicating a positive sentiment.",
            "The review highlights several strengths of the paper, including its well-written structure, novel model, good intuition, well-explained theoretical results, and optimal algorithm. The reviewer also notes that the regret upper bound matches the theoretical lower bound, which demonstrates the optimality of the proposed algorithm."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses appreciation for the work, highlighting its strengths and importance. Phrases like \"well-motivated,\" \"good to be formalized,\" \"really nice,\" and \"optimal regret\" indicate a supportive tone. The reviewer also acknowledges the authors' responses to their questions and states that they will keep their score unchanged, indicating satisfaction.",
            "The review presents both strengths and weaknesses of the paper in a constructive manner. It uses phrases like \"One of my concerns however,\" \"I am a little uncertain,\" and \"I think the paper would really benefit\" which suggest a balanced and critical evaluation.",
            "The reviewer expresses enthusiasm for the paper's contributions and offers constructive questions to improve the work. The reviewer highlights the paper's strengths and potential positive impact.",
            "The reviewer uses encouraging language like \"The paper is well written\", \"I also think that this idea is novel\", and offers constructive suggestions for improvement such as \"My suggestion would be that the authors could consider adding more insights\". The reviewer also acknowledges the authors' efforts by stating \"authors have adequately addressed parts of limitation\". The tone is generally positive and aims to help the authors improve their work.",
            "The review provides both positive feedback and constructive criticism, addressing strengths and weaknesses in a professional manner. It includes specific questions and minor comments for improvement, while also acknowledging the paper's contributions."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it starts with a positive summary, lists pros, raises questions for clarification (not strong criticisms), and after rebuttal, the reviewer is satisfied and maintains a positive stance, indicated by keeping the score unchanged and a final 'Yes' verdict. The questions are constructive and do not contradict the overall positive assessment of the paper's contribution and quality.",
            "The review is consistent. The reviewer raises valid concerns and questions throughout the review, focusing on clarity, justification, and practical relevance. The reviewer acknowledges the strengths of the paper while pointing out areas for improvement. The 'Edit' section shows a positive response to the authors' answers and a refinement of the initial concern, but the core point about the need for better justification of the probability aspect remains consistent, indicating a coherent and non-contradictory assessment.",
            "The review is consistent because it expresses an overall positive view of the paper, highlighting its novelty, practicality, theoretical contributions, and clarity. While the reviewer raises questions and seeks clarifications, these are framed as constructive feedback and do not contradict the initial positive assessment. The reviewer is seeking further understanding and suggesting potential improvements, rather than pointing out inconsistencies or flaws in the paper or their own evaluation.",
            "The review is consistent as it provides positive feedback on the paper's novelty, clarity, and contributions, while also raising specific questions and suggestions for improvement. The reviewer's concerns and suggestions are constructive and do not contradict the overall positive assessment of the paper's merits.",
            "The review presents a balanced assessment by highlighting both the strengths and weaknesses of the paper. It acknowledges the novelty and clarity of the work while also raising valid questions and pointing out areas for improvement. The reviewer's comments are constructive and do not contradict each other, leading to a consistent overall evaluation."
        ]
    },
    {
        "paper_id": "nips_2022_K2QGzyLwpYG",
        "paper_title": "Data-Efficient Structured Pruning via Submodular Optimization",
        "paper_abstract": "Structured pruning is an effective approach for compressing large pre-trained neural networks without significantly affecting their performance. However, most current structured pruning methods do not provide any performance guarantees, and often require fine-tuning, which makes them inapplicable in the limited-data regime.We propose a principled data-efficient structured pruning method based on submodular optimization. In particular, for a given layer, we select neurons/channels to prune and corresponding new weights for the next layer, that minimize the change in the next layer's input induced by pruning. We show that this selection problem is a weakly submodular maximization problem, thus it can be provably approximated using an efficient greedy algorithm. Our method is guaranteed to have an exponentially decreasing error between the original model and the pruned model outputs w.r.t the pruned size, under reasonable assumptions. It is also one of the few methods in the literature that uses only a limited-number of training data and no labels. Our experimental results demonstrate that our method outperforms state-of-the-art methods in the limited-data regime. ",
        "review_ids": [
            "6OVfhkaReUM",
            "ZIewf1LoOlY",
            "03JakGViR7",
            "hNvFdOJVlM",
            "hSqN-Vyj3E6",
            "hI64rsShb4j"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I agree that the focus of the paper isn't on speed, but it's always nice to get a sense of the time scales involved.",
            " I read the author response and find it properly addresses my concerns. I recommend the author to include some important part in to the next version. I raise my score by 1 and recommend an acceptance for this submission.",
            " The authors propose a layerwise pruning method that formulates the problem of eliminating neurons as a weakly submodular optimization problem for which the well-known greedy algorithm gives an approximation guarantee.  They illustrate the practical performance of three different variants of their strategy when extended to prune the entire network on a variety of tasks. Originality: The idea of formulating the pruning problem to take advantage of weak submodularity is novel to me.  Although it does build somewhat crucially on existing work.\n\nQuality:  The technical and experimental results seem to be well-executed to the best of my assessment.  One can always add more competitors and try on a wider variety of architectures, but I found the selected experiments to be illustrative and compelling.\n\nClarity:  The exposition was clear overall though I found Figure 1 tough to read even after significant zooming.\n\nSignificance:  While it may be more expensive than some other approaches, the cost of this procedure is typically only born once.  So, I can see this as being a very useful tool in practice and may spur pruning research in novel directions. It would be nice to see some discussion of the practical computational cost among the various methods in the main text.  While I don't see the cost of this method as overly limiting, it is nice to get a sense of how taxing the methods really are. \n\nAlso, some thoughts about why the approach seems to do much better in some regimes while only comparable in others? Some limitations were discussed, though I don't see the focus on the data limited regime as a limitation of the methodology necessarily.  Or am I missing something important?",
            " This paper propose a network pruning approach via sub modular optimization. The proposed method uses a greedy fashion to layer-wisely select neuron that improves the performance most. This paper shows that the returned solution given by the greedy algorithm is able to well approximate the optimal solution (of a NP hard problem). To reduce the cost, a computation-saving approach is also proposed. Empirical results shows that the approach is able to achieve good performance when the available training data at pruning is small. Overall I find this paper quite interesting, especially its performance when there is limited number of training data at pruning. The theoretical guarantee is also stronger and does not require stronger assumptions. However, I do have some concerns:\n\n1. It seems the proposed method in general is very similar to Ye at al.. Both methods are greedy forward selection and the different seems subtle. Could you give more discussion in terms of methodology?\n\n2. Following up Q1, it seems that an main improvement over Ye at al is to reduce the computation cost. However, [1] also propose a new technical to reduce the cost and I was wondering how does your method compares with [1]?\n\n3. What would max_{|S|\\le k} F(S) looks like? I understand that it is NP hard but it would be interesting to show how this quantities looks like.\n\n4. Can we empirically verify the \\gamma_{U,k} as this is an important quantity?\n\n[1] Greedy Optimization Provably Wins the Lottery: Logarithmic Number of Winning Tickets is Enough\n See above. Yes",
            " The paper proposes a neural network node pruning method and shows that the objective is essentially a form of weakly-submodular function optimization. Therefore, the pruning of a single layer can be solved using the greedy algorithm with a theoretical guarantee. The paper also shows that using a limited number of data, the proposed method is able to achieve the best performance compared to baselines. Strengths:\n1. The paper draws a very intriguing and solid connection between neural network pruning and submodular optimization. More specifically, the weakly-submodular optimization factor is closely related to the activation matrix.\n\n2. The paper studies the problem in a comprehensive manner, including pruning of regular regions of neurons, strategies of pruning multiple layers, and speed-up tricks for the submodular optimization.\n\n3. The proposed method empirically achieves the best performance compared to baselines on some network structures under a limited number of samples.\n\nWeaknesses:\n1. The proposed approach has relatively high computational complexity. It seems that scaling would be a problem for scenarios such as larger network structures or utilizing a large number of data samples for pruning.\n\n2. Given the limitation of the complexity, I guess that the empirical experiments could be hardly extended to more complex datasets (e.g., ones with larger image dimensions) or larger network structures.\n\n3. The proposed framework only works for one layer. For pruning of multiple layers the paper proposes some heuristics.\n 1. Can authors estimate the weakly-submodular factors gamma in the experiments and report the corresponding guarantees? Can authors also discuss high tight are the estimates of such factors provided in Proposition 3/4?\n\n2. Can authors provide wall clock running time of the optimization algorithm on the experiments and discuss the limitations of the complexity?\n\n3. For multiple layer pruning, it seems that the changes of one layer could affect the pruning of other layers. Can the authors provide some insights about the empirical behaviors of convergence of the proposed methods? Yes.",
            " Goal: effective and efficient structured pruning of a pre-trained NN-network if only small amount of unlabeled training data is available\n\nContributions: \n1) The authors propose a new technique (called \"principled data-efficient structured pruning\") that alters the existing  \"reweighting\" method [Mariet and Sra, 2015]. Unlike [Mariet and Sra,2015]:\n     * submodular optimization ... they  formulate the subset selection problem of  structured pruning as a weakly submodular maximization problem  and solve it approximately by greedy search\n     * extended pruning: pruning of regular regions of neurons (e.g., channels) and three strategies for pruning of multliple layers \n     * limited number of training data (cca. 1% of the original training data) with no labels and one-shot pruning (without fine-tuning)\n2) Theoretical justification of the method and its performance (in the Supplementary material).\n3) Experimental evaluation of a solid scope with promising results.\n Strengths (significance and quality):\n1) The problem of effective structured pruning of a pre-trained NN-network in the presented limited-data regime is important. Most of the existing structured pruning techniques require greater amount of training data and fine-tuning to work well. The proposed method (or even its particular parts) appears to be a valuable contribution with this respect.   \n2) The experimental evaluation of a solid scope offers promising (outstanding and stable) results.\n3) Theoretical performance guarantee.\n4) The authors try to be fair in their comparisons with concurrent techniques (e.g. the application of reweighting, various parameter settings...) and analyze also the weaknesses of their method.\n\nClarity:\n1) The submission is relatively clearly written and easy to read (except the theoretical parts and the figures). If there was enough place, I would prefer to move more experimental results from the supplementary into the main paper.  The figures (e.g. Figure 1) are small and less comprehensible. It would maybe help to scale the graphs differently, to highlight the variants of the proposed method or to change the used colors. \n\nNovelty:\n1) The novelty of the contribution is slightly limited. The proposed method is based on previous work by [Mariet and Sra,2015]. The original method is altered using the principles of submodular optimization to be advantageous in the new context of  limited-data. \n2) The related work is cited and addressed adequately.  1. In the abstract,  you write that \"current structured pruning methods do not provide any performance guarantees\". Later on, e.g., on lines 75-78,  you mention one such method. You should add something like \"most of\" into the abstract to be fair. \n\n2. To confirm the novelty of your method, it would be nice to analyze in a more concrete way, what the  [Mariet and Sra,2015] method uses instead of submodular optimization and greedy search. Did you compare your approach to  [Mariet and Sra,2015]  exactly? OK: The authors try to analyze both strengths and limitations of the paper in the experimental part."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses a desire for additional information ('it's always nice to get a sense of the time scales involved') without explicitly praising or criticizing the paper's existing content.",
            "The reviewer explicitly states that the author response 'properly addresses my concerns,' raises the score, and recommends acceptance, indicating a positive sentiment.",
            "The review expresses overall positive feedback, highlighting the novelty of the approach, well-executed results, clear exposition, and potential usefulness. Phrases like \"novel to me,\" \"well-executed,\" \"illustrative and compelling,\" and \"very useful tool\" indicate a positive sentiment.",
            "The reviewer states the paper is \"quite interesting\" and highlights the strong theoretical guarantee and performance with limited training data. Though concerns are raised, the overall impression is positive.",
            "The review highlights several strengths of the paper, including its intriguing connection between neural network pruning and submodular optimization, comprehensive study of the problem, and empirical performance. While weaknesses are mentioned, the overall tone suggests a positive evaluation.",
            "The review expresses an overall positive assessment of the paper, highlighting its contributions, strengths, and promising results. Phrases like 'valuable contribution,' 'promising results,' and 'theoretical performance guarantee' indicate a positive sentiment."
        ],
        "tone": [
            "Neutral",
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is polite and suggestive. The phrase 'it's always nice' indicates a preference rather than a demand or critique.",
            "The reviewer's language is encouraging and helpful, as they recommend inclusion of 'some important part' and endorse acceptance. The phrase 'I raise my score by 1' further emphasizes their support.",
            "The tone is supportive, offering constructive suggestions for improvement while praising the core contributions. Phrases like \"nice to see,\" \"I can see this as being a very useful tool,\" and the overall encouraging language point towards a supportive tone.",
            "The review acknowledges the paper's strengths (interesting, strong theoretical guarantee, good performance) but also raises concerns and questions, indicating a balanced perspective. The use of 'However, I do have some concerns:' explicitly signals this balance.",
            "The review presents both strengths and weaknesses of the paper, using objective language to describe each aspect. It also includes specific questions for the authors to address, indicating a constructive and balanced approach.",
            "The review provides both positive feedback ('valuable contribution', 'promising results') and constructive criticism (clarity of figures, limited novelty). This balance indicates a balanced tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer acknowledges that the paper's focus is not speed, but still expresses a desire to understand the time scales involved. This is not a contradiction, as understanding time scales can be relevant even when speed is not the primary focus.",
            "The reviewer states that their concerns are addressed, raises the score, and recommends acceptance. The suggestion for the next version is a minor point and does not contradict the overall positive assessment.",
            "The review maintains a consistent positive tone, highlighting the strengths of the paper in originality, quality, clarity, and significance while offering constructive suggestions for improvement without contradicting its overall positive assessment.",
            "The review is consistent because the reviewer expresses initial interest and appreciation for the paper's strengths (performance with limited data, theoretical guarantees) and then raises specific, constructive questions. These questions are aimed at clarifying aspects of the methodology, comparing it to existing work, and suggesting further empirical validation, all of which are standard and consistent elements of a constructive peer review. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because the strengths and weaknesses are logically separated and balanced. The reviewer acknowledges the paper's contributions in theory and empirical performance under specific conditions (limited data), while also pointing out valid limitations regarding computational complexity, scalability, and the single-layer focus. The questions raised by the reviewer directly address the identified weaknesses, further reinforcing the consistency of the review.",
            "The review provides a balanced assessment, highlighting both the strengths (importance of the problem, promising results, theoretical justification, fair comparisons) and weaknesses (limited novelty, clarity of figures, minor inconsistency in the abstract) of the paper. The reviewer's points are logically connected and contribute to a coherent overall message, without contradicting themselves. The critique about novelty is softened by acknowledging the value in the specific context of limited data."
        ]
    },
    {
        "paper_id": "iclr_2021_Uf_WNt41tUA",
        "paper_title": "CorDial: Coarse-to-fine Abstractive Dialogue Summarization with Controllable Granularity",
        "paper_abstract": "Dialogue summarization is challenging due to its multi-speaker standpoints, casual spoken language, and limited labeled data. In this paper, we propose CorDial, aiming to improve the abstractive dialogue summarization quality and at the same time enable granularity controllability. We propose 1) a coarse-to-fine generation strategy that generates a summary draft followed by a final summary in an autoregressive way. The summary draft, which provides weakly-supervised signals, is composed of pseudo-labeled interrogative pronoun categories and noisy key phrases extracted with a constituency parser. 2) A simple strategy to control the granularity of the final summary. CorDial can predict and control the number of summary sentences for a given dialogue by predicting and highlighting different text spans from the source text. Our model achieves state-of-the-art performance on the largest dialogue summarization corpus SAMSum. We conduct comprehensive error analysis and show competitive human evaluation results to annotated summaries.",
        "review_ids": [
            "6S5mrLlUSyx",
            "ACZNsV6ul1p",
            "QOnlVerBmC1",
            "lvs6FUDF8lj"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "\n<Summary>\n\nThis paper addresses the problem of abstractive dialogue summarization. Its key idea is to label an interrogative pronoun category and extract key phrases from each dialogue turn as weak guide for dialogue summarization. It also proposes a length-controllable generation method for final summary. The proposed approach is evaluated on the SAMSum as one of the largest abstractive dialogue summarization benchmarks, on which it shows competitive performance over recent models. \n\n<Strengths> \n\n1. It proposes a two-step coarse-to-fine approach for abstractive dialogue summarization; it first extracts category labels and key phrases from each dialogue turn, and then generates final summaries by controlling granularity. This idea itself could be novel.\n\n2. It shows strong performance over other recent methods on the recently released SAMSum dataset. \n\n3. It tests the proposed approach with four recent pre-trained language models including DialoGPT, UniLM, PEGASUS and BART.\n\n<Weakness>\n\n1. This paper proposes a novel coarse-to-fine approach for abstractive dialogue summarization but its implementation is largely ad-hoc and engineering intensive and thus bears little technical novelty.\n\n(1) The \u201ccoarse\u201d part aims at generating drafts using interrogative pronoun category prediction and key phrase  extraction. \nThese two are largely based on existing techniques (e.g. Ratner et al 2019 and Kitaev & Klien 2018) and some heuristics (e.g. thresholding for key phrases detection). \n\n(2) The \u201cfine\u201d part aims at generating target summary with controllability of granularity. \nIts implementation is also based on a series of engineering heuristics (e.g. dialogue splitting by ROUGE score, binary classification for cutpoint detection). \n\n(3) In summary, it is hard to find methodological novelty in the proposed method.  Given that ICLR is a top premier ML venue, it could be a significant weakness to be a publishable work. \n\n2. Experimental results are rather weak. \n\n(1) Although SAMSum dataset may be one of the best benchmarks for the target task, experiments on only a single dataset is limited to show the generality and effectiveness of the proposed method.\nGiven that the proposed method is ad-hoc, I suspect much additional endeavor may be required to apply to another dataset. \n\n(2) I am not sure whether the comparison in Table 1 is fair enough. Since the proposed approach relies on the additional components for draft construction, it could require more other types of training data or learned modules that other method may not need. This should be clarified in the draft.\n\n<Conclusion>\n\nMy initial decision is \u2018reject\u2019 mainly due to lack of technical novelty. Limited experiments could be another issue to be improved. ",
            "The paper proposes CorDial for abstractive dialogue summarization. CorDial extends BART by generating an intermediate \"summary draft\" which provides weakly-supervised signals and controling the length of the final summary. Results show significant improvements over competitive summarization models such as PEGASUS and BART in multiple different metrics.\n\nSome comments:\n\n1. The paper emphasizes that dialogue summarization is challenging due to its multi-speaker standpoints, casual language, and limited data. Although the use of the proposed summary draft would help solve the first challenge, it is hard to see any correlation between the other problems mentioned and the proposed solutions in the paper. This is especially the case for the controlling of the summary length. Why is this useful specifically for dialogue summarization?\n\n2. The \"summary draft\" is one kind of a content plan, which is widely used in text generation, including text summarization [1]. The technique of extracting key phrase is similar to how content selection is done in [2]. Please compare the proposed solution to other kinds of content planning.\n\n3. To extract key phrases, the method identifies the longest common sub-sequence (LCS) parameterized by a threshold, however how this threshold is set and used is not discussed in the paper. This is important information in order to understand how these key phrases would look like. For example, in Figure 1, how is \"s just one of many boring days at work\" extracted when the LCS is only \"at work\" for turn 2?\n\n[1] https://www.aclweb.org/anthology/C18-1101.pdf\n\n[2] https://arxiv.org/pdf/1808.10792.pdf",
            "This paper proposes CorDial, a new method for dialog summarization. CorDial firstly constructs a coarse draft by generating intent and key phrases for every dialog turn, and splits the dialog into chunks by inserting special boundary tokens; then the segmented original dialog and the constructed draft are feed as input to generate the final summary. CorDial employs BART-xsum as its backbone model, which is a pre-trained language model finetuned on XSUM summary dataset. Experiment result on SAMsum dataset shows CorDial achieves SotA performance under both automatic evaluation metric and human evaluations.\n\nOverall, the paper presents an interesting, practical recipe for dialog summarization. It requires few additional annotation besides the summary, and the human evaluation results looks promising. However, the method proposed here somewhat lacks in novelty, and some part of the paper is not clearly written. Thus I give this paper a weak reject rating.\n\nComments:\n1. In 2.2, how are the intents annotated? Is it a purely automatic process based on keywords matching? Or the keywords are merely cues for human annotators?\n2. In 2.3, the algorithm for finding the cutting points is an incremental one. It can't account for the similarity between last chunk and the last sentence in the summary, since the cutting point of second to last chunk already determines the boundary of the last chunk.\n3. How are the output generated exactly? The last sentence in 2.4 states each sentence is generated separately. Does it mean each output sentence have different input? How is it different from standard auto-regressive token-by-token generation?\n4. 2.4 should also explicitly refer to Figure. 1 for clarity.\n5. CorDial uses the BART-xsum as initialization, which is trained on XSUM dataset. Are other baselines also gone through the same XSUM training?\n6. What is the model size of all the models in the experiments? It would be better to have some descriptions on model architectures in the experiment section.\n\n",
            "This paper is well written and investigates dialogue summarization that has not received much attention. It proposes a new model called CORDIAL which can generate a summary draft followed by a final summary.  It achieves comparable or better results in term of both automatic evaluation metrics, e.g. compress ratio, rouge score, and human evaluations (Consistent and Informative) about the quality of generated summaries in different settings. \n\nHowever, there are still several disadvantages of this paper:\n(1) It can generate a summary draft but its quality is almost not presented in the paper except the ablation study of table 1. Even the results within table 1 are still about the quality of the final summary. The paper slightly overclaims its usefulness in the draft summary generation. More results or analysis about draft summary should be presented.\n(2) The human evaluation has only 30 examples and the scale is too small. Also, does the score is -1, 0, 1, or other scale? Do you use majority vote or mean plus standard deviation to get results in the table? Why gold in table 3 is so low in Consistent?\n(3) The method looks applicable to the generable summarization task. More results of this are also interesting. \n\nAlthough these drawbacks, its quality is good overall.\n"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review recommends rejection due to a lack of technical novelty and limited experiments, indicating a negative overall assessment.",
            "The review acknowledges the paper's contributions but also raises several concerns about the clarity and justification of the proposed method. It points out missing explanations and requests comparisons to existing work, indicating a need for improvement rather than outright rejection or praise.",
            "The review expresses concerns about novelty and clarity, leading to a 'weak reject' rating. While acknowledging the practical aspects and promising results, the negative aspects outweigh the positives, resulting in an overall negative sentiment.",
            "The review starts with a positive statement, highlighting the paper's good writing, the importance of the topic, and the model's performance. It concludes by stating the paper's quality is good overall, despite some drawbacks."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like 'largely ad-hoc and engineering intensive,' 'little technical novelty,' 'experimental results are rather weak,' and 'I suspect much additional endeavor may be required,' indicating a critical and skeptical tone towards the paper's contributions.",
            "The review uses questioning and critical language such as \"it is hard to see any correlation\", \"Why is this useful specifically for dialogue summarization?\", \"how is this threshold set and used is not discussed\", and \"how is 's just one of many boring days at work' extracted\". These phrases indicate a critical evaluation of the paper's arguments and methodology.",
            "The review uses phrases like 'lacks in novelty' and 'not clearly written,' and poses several pointed questions about the methodology. The 'weak reject' rating further reinforces the critical tone.",
            "The review uses a formal tone with objective observations and constructive criticism. Phrases like 'However, there are still several disadvantages' and specific questions about methodology contribute to the balanced and critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the conclusion to reject is based on the identified weaknesses, particularly the lack of technical novelty in the implementation, which is a major point of criticism in the 'Weakness' section. The reviewer acknowledges the strengths but considers the lack of technical novelty and limited experiments as more significant issues, leading to the rejection decision.",
            "The review is consistent in its critical assessment of the paper. Each comment raises valid questions and points out areas for improvement or clarification, without contradicting any other comment. The reviewer consistently questions the paper's claims and methodology, focusing on the relevance of the proposed solution to the stated problems, the novelty compared to existing methods, and the clarity of technical details.",
            "The review identifies both positive aspects (interesting, practical, promising results) and negative aspects (lack of novelty, clarity issues, methodological questions). The weak reject rating is consistent with the identified weaknesses, which outweigh the positives in the reviewer's judgment.",
            "The review starts with positive feedback, then points out several disadvantages of the paper, and concludes with an overall positive assessment despite the drawbacks. This structure is balanced and does not present contradictory statements. The reviewer acknowledges both strengths and weaknesses in a logical flow."
        ]
    },
    {
        "paper_id": "iclr_2022_EhwEUb2ynIa",
        "paper_title": "How to Adapt Your Large-Scale Vision-and-Language Model",
        "paper_abstract": "Pre-training large-scale vision and language models (e.g. CLIP) has shown promising results in representation and transfer learning. We investigate the question of how to efficiently adapt these models to downstream tasks. For image classification, linear probes have been the standard for ease of use and efficiency, while for language, other approaches like prompt tuning have emerged. We analyze several fine-tuning methods across a diverse set of image classification tasks across two spectra investigating the amount and similarity of downstream data to that of pretraining one. We find that just tuning LayerNorm parameters is a surprisingly effective baseline across the board. We further demonstrate a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods to improve their performance and benchmark them on few-shot adaption and distribution shift tasks. Finally, we provide an empirical analysis and recommend general recipes for efficient transfer learning of vision and language models. Website at https://sites.google.com/view/adapt-large-scale-models",
        "review_ids": [
            "VeEMKGCZ_Pu",
            "fl9uIW9QTM5",
            "kSGCGEQQFP",
            "dWoR-7KWBiD"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The goal of the paper is to investigate how to efficiently adapt large-scale pre-trained vision-language models (e.g. CLIP) to downstream tasks. Their paper is based on the observation that while the vision community dominantly uses linear probe as the standard protocol, other approaches such as prompt learning are utilized in language. They compare and analyze several fine-tuning methods -- linear probe, prompt tuning, adapter and compacter networks -- across 12 downstream classification tasks. Focusing on LayerNorm, they further demonstrate combining LayerNorm tuning with existing fine-tuning methods to improve performance. \n Strengths\n* I like the motivation of the paper -- the vision community broadly uses linear probe as the standard protocol for transfer learning, yet the prominence of prompt learning in NLP clearly shows that there can be better methods to transfer knowledge of large-scale pre-trained models in the vision domain. \n\nWeaknesses\n* In Figure 3-4, it seems that LayerNorm performs better with an extremely small margin, which could easily be reversed with initialization or hyperparameter search for each fine-tuning method. This makes me question why the paper is particularly focused on analyzing LayerNorm in the second half of the paper. Also, combining LayerNorm provides minimal improvement. \n* Instead of averaging and phrasing as \"low data\" vs. \"high data\" in the paper, it would be more accurate to plot the actual 1-512 shots on the graph. Averaging it makes it hard to grasp the correct trend of each fine-tuning method over the dataset scale. \n* Furthermore, it seems extremely arbitrary to divide the distribution of downstream data as \"high and low similarity\" with an arbitrary threshold of 55%. \n Overall, this paper seems to have limited contributions for adapting pre-trained vision-language models to downstream tasks. The depth of the analyses is limited and does not provide interesting and novel findings to the community. ",
            "The paper investigates a range of techniques for adapting CLIP to different tasks. They find that only tuning the LayerNorm is effective and further combining it with other adapting approaches delivers better performance. \n \nPro:\n\nIt is good to know that LayerNorm tuning is quite effective for CLIP and combining it with other approaches gives even better performance.\n\n\nCon:\n\n1. Limited novelty.\n\nThe methods used in this paper are existing methods and the authors do a simple combination and benchmark them with CLIP on various datasets.\n\nThe conclusion I could draw is 1) layer norm tuning works well and 2) one can combine it with other methods (essentially have more parameters to tune) to achieve better results. I am not sure if these conclusions are that exciting and sufficient for an accepted paper.\n\n\n2. Missing full-model fine-tuning.\n\nThe authors argue full-model fine-tuning as \"inefficient\" and exclude it from comparison. However, I think full-model fine-tuning results are important to include.\n\na. It is important to see where the upper bound is and results of full-model fine-tuning would put the current results in perspective. \n\nb. The authors need to be more clear about what \"inefficient\" means. Does it mean fine-tuning time cost? If it refers to the fine-tuning time cost, it is affected by many factors and is not just dependent on parameter count. And under the setting in this paper, I am not sure how \"inefficient\" full fine-tuning is, compared to the shown approaches. \n   For example, for prompt tuning, even if most parameters are not trainable, gradients still need to be calculated for back-propagation to the input. Thus, the forward/backward computational cost should be the same for prompt tuning and full model fine-tuning. This is similar to layer-norm tuning (depending on the location of the first layer norm layer). \n   The efficiency of prompt tuning over full-model fine-tuning mostly comes from 1) less optimizer overhead and 2) less communication cost of parameters during multi-GPU training. However, CLIP is not a significantly large model by today's standards (the smallest CLIP is ResNet 50) and on the datasets we are adapting to, we often do not need many GPUs to fine-tune the model. I have to suspect that the bottleneck is not on either the optimizer overhead or communication cost when we fine-tune CLIP on those small datasets (please correct me if my assumption is wrong). Thus I am curious to see the actual speedup prompt tuning holds over full fine-tuning.\n\n\n The paper presents an empirical study of how to fine-tune CLIP for downstream tasks. \n\nMy main concerns are: 1) from my subjective view, I am not sure the conclusions are significant enough; 2) full fine-tuning results are missing.\n\nI recommend adding the full fine-tuning results for a full picture and being more specific about the reason to exclude full fine-tuning. It would be good to provide a compelling reason to favor these methods over full fine-tuning in practice.",
            "This paper has extensively studied how to adapt the large-scale pre-trained vision-language model CLIP for downstream tasks. Several fine-tuning methods are analyzed across a diverse set of image classification tasks along two spectra. Further, a simple yet effective strategy that combines LayerNorm-tuning with general fine-tuning methods is proposed to improve their performance and benchmark them on few-shot classification tasks.  Strengths:\n\n1. The LayerNorm-tuning for adapting CLIP to downstream tasks is shown to be effective.\n\n2. A simple yet effective scheme that combines LayerNorm-tuning with other fine-tuning methods is proposed to obtain competitive performance across the board.\n\n3. A thorough comparison of different adaptation methods is provided in four scenarios across two spectra. \n\nWeaknesses:\n\n1. The paper title \u201cHow to Adapt Your Large-Scale Vision-and-Language Model\u201d is a bit of over-claimed. It should be toned down to \u201cHow to Adapt CLIP to Downstream Image Classification Tasks\u201d.\n\n2. The technical novelty of this paper seems very limited. The three methods for fine-tuning new parameters have been extensively studied in existing works. When coming to adapting CLIP, there should be something different. That is, novel fine-tuning methods are needed in adapting CLIP. \n\n3. The experimental evaluation is far from sufficient. (1) For \u201cLayerNorm-Tuning\u201d, the parameters of all LayerNorm layers are set to be learnable. I wonder if the results could be better when only the last K LayerNorm layers are updated (e.g., K=5). (2) For \u201cFull Model Fine-tuning\u201d, it should also be evaluated in the experiments by only updating the parameters of the last layers of the model. (3) I wonder if the proposed method is effective in other downstream tasks other than image classification. \n This paper had been well written if entitled \u201cHow to Adapt CLIP to Downstream Image Classification Tasks\u201d. However, considering the insufficient experimental evaluation and limited technical novelty, I could only give it a score of 5. ",
            "This paper proposed a new method (LayerNorm tunning) for finetuning a pre-trained vision-and-language model. While simple, it is shown this method works competitively with other methods (e.g., prompt tuning) in various settings. For the low data & high similarity setting (Fig 3), it shows the best performance across the methods. The authors further claimed LN could be used to boost the performance of general finetuning methods like linear probe or prompt tuning. However, this seems not useful as the performance typically drops when compared to the LN baseline. In addition, the proposed methods are only evaluated in a single model, it is not clear whether they are generalizable. - Strengths\n    - The proposed LN tuning is simple and seems quite strong compared to other more complex methods.\n\n- Weaknesses:\n    - While the authors claim that the proposed method is useful for vision and language models, only a single model (CLIP) is used for these studies. Though the method design is almost model-agnostic, it is not clear whether this approach generalizes to other models.\n    - The tasks studied in this work are all image classification tasks, there are no tasks that are normally recognized as vision-and-language tasks, such as text-image retrieval or image captioning [1]. If the authors' intention is to use this method for image classification tasks, it would be better to change the title a bit as \"How to adapt your large-scale pre-trained models for visual representations learning\", instead of the current confusing one of \"vision-and-language model\".\n    - In the high-data regime, it seems also feasible to finetune all the model parameters, how does this result compare to the considered baselines in this work?\n    - The best performance is often achieved by first finetuning the LN parameters, then using another approach like Adapter for a 2nd stage finetuning. This two-stage finetuning process could be cumbersome.\n    - After LN finetuning, it seems that additionally finetune with many of the other methods actually hurts performance. For example, under low data & high similarity setting, with only LN finetuning, the performance is 83.74 (see Fig. 3, top-left), while the performance after further finetuned with other methods generally falls (see Fig. 4, top-left), only Compactor shows a minimal gain with a performance of 84.07. This conclusion seems also applicable to other settings in these two figures. This seems to suggest that we should probably only use LN.\n\n- Miscs:\n    - Some of the references are not linked, e.g., the 3rd paragraph in Sec 4.5, \"Table 4\" is not hyper-linked\n    - \"Normal\" setting in Table 1 is not clearly explained. I made an educated guess as \"simultaneously tuning them\" when evaluating the work.\n\n[1] Chen et al. Microsoft coco captions: Data collection and evaluation server.\u00a0*arXiv 2015* The paper proposed a useful technique LN tunning for fine-tuning the pre-training CLIP model for downstream image classification tasks. However, the generalizability of this approach is not demonstrated, and the proposed recipe of combing LN tunning with other fine-tuning methods seems also not useful as it typically shows even lower performance than only using LN tuning. Overall, this paper does not provide enough support for it to be published at ICLR."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's limited contributions, questioning the significance of LayerNorm analysis and the arbitrary data division. The overall assessment suggests a lack of novel findings and depth in the analyses.",
            "The review expresses concerns about the paper's novelty and missing full-model fine-tuning results, questioning the significance of the conclusions. Phrases like 'limited novelty,' 'not sure if these conclusions are that exciting,' and 'main concerns' indicate a negative sentiment.",
            "The review expresses concerns about the paper's technical novelty and experimental evaluation, leading to a score of 5. Phrases like 'limited technical novelty' and 'insufficient experimental evaluation' indicate a negative sentiment.",
            "The review expresses concerns about the generalizability of the proposed method, the lack of evaluation on actual vision-and-language tasks, and the limited usefulness of combining the proposed method with other fine-tuning approaches. The reviewer concludes that the paper \"does not provide enough support for it to be published at ICLR.\""
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"extremely small margin,\" \"makes me question,\" \"extremely arbitrary,\" and \"limited contributions,\" indicating a critical perspective. It directly points out weaknesses and expresses skepticism about the paper's value.",
            "The reviewer directly questions the novelty and significance of the findings, using phrases like \"I am not sure if these conclusions are that exciting and sufficient for an accepted paper.\" They also challenge the authors' reasoning for excluding full-model fine-tuning, stating, \"The authors need to be more clear about what 'inefficient' means\" and \"I have to suspect that the bottleneck is not on either the optimizer overhead or communication cost.\" These direct challenges indicate a critical tone.",
            "The review points out weaknesses in the paper's title, technical novelty, and experimental evaluation using direct criticisms and suggestions for improvement. Examples include 'The paper title is a bit of over-claimed' and 'The technical novelty of this paper seems very limited.'",
            "The review points out several weaknesses in the paper, such as the limited scope of evaluation, lack of generalizability, and questionable usefulness of the proposed method in conjunction with other methods. Phrases like \"not clear whether they are generalizable,\" \"confusing one,\" \"cumbersome,\" and \"not useful\" indicate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the weaknesses raised directly support the overall negative assessment of the paper's contribution. While the reviewer acknowledges the motivation as a strength, the criticisms of methodology and analysis lead to the conclusion that the paper has limited contributions. There are no contradictory statements within the review.",
            "The reviewer consistently argues that while the findings on LayerNorm tuning are valuable, the paper lacks novelty and, more importantly, misses the crucial comparison with full fine-tuning. The reviewer's concerns and recommendations are aligned, focusing on the need for a more comprehensive study that includes full fine-tuning to better contextualize the presented methods and justify their advantages.",
            "The review is consistent because the strengths and weaknesses are logically connected and support the final score. The reviewer acknowledges the effectiveness of the proposed method (strengths) but points out limitations in novelty and experimental evaluation (weaknesses), which justifies the score of 5.",
            "The review consistently highlights the strength of the proposed method (LN tuning) in terms of simplicity and performance for the specific case of CLIP and image classification tasks. However, it consistently points out the limitations regarding generalizability to other models and tasks, and the lack of benefit (or even negative impact) of combining LN tuning with other methods. The summary aligns with these points, concluding that the paper is not ready for ICLR publication due to these limitations."
        ]
    },
    {
        "paper_id": "iclr_2020_BJeKwTNFvB",
        "paper_title": "Physics-as-Inverse-Graphics: Unsupervised Physical Parameter Estimation from Video",
        "paper_abstract": "We propose a model that is able to perform physical parameter estimation of systems from video, where the differential equations governing the scene dynamics are known, but labeled states or objects are not available. Existing physical scene understanding methods require either object state supervision, or do not integrate with differentiable physics to learn interpretable system parameters and states. We address this problem through a \\textit{physics-as-inverse-graphics} approach that brings together vision-as-inverse-graphics and differentiable physics engines, where objects and explicit state and velocity representations are discovered by the model. This framework allows us to perform long term extrapolative video prediction, as well as vision-based model-predictive control. Our approach significantly outperforms related unsupervised methods in long-term future frame prediction of systems with interacting objects (such as ball-spring or 3-body gravitational systems), due to its ability to build dynamics into the model as an inductive bias. We further show the value of this tight vision-physics integration by demonstrating data-efficient learning of vision-actuated model-based control for a pendulum system. We also show that the controller's interpretability provides unique capabilities in goal-driven control and physical reasoning for zero-data adaptation.",
        "review_ids": [
            "HkgvJIR9iS",
            "rkxP3xuEYB",
            "ryeX_6kAtH",
            "S1xGzyBJcH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thanks for your response and for the clarifications.\n\nWere you able to run any experiments with larger numbers of objects than 2 or 3? As I mentioned in my review, I did not think it was essential to test on Physics 101; however, I do think it is important to test on similar numbers of objects as has been done in past work.\n\n1. The way mass, gravity, and spring constants are defined in the paper are not really \"object-specific\"---they are the same value for all objects and thus are object-agnostic. I was curious about values that would be different for each object (e.g., one object has a mass of 1 and another has a mass of 2), and what changes would be required to allow such values to be inferred.\n\n2. Thanks for adding some discussion about this. That's a good point about DDPAE having memory, though I still wonder if the IN would fare much better with memory since its structure more accurately matches the computation of a physics engine than DDPAE does (so I would expect the IN to be able to better model the dynamics than DDPAE in general---which seems to indeed be true from your results).\n\n3. Thanks for including these results! It's good that it seems to at least be somewhat robust to misspecification of the number of slots, though it's a bit unfortunate that if there are too many slots it cannot ignore the missing slots. As a topic for future work, perhaps some sort of attention mechanism could help here.",
            "The paper proposes to integrate model-based physical simulation and data-driven (deep) learning. In a nutshell, one deep network predicts the state variables of the physics simulation (such as objet location, shape and velocity) from an image. A second network does the inverse task, to render images given the state variables (and a background image). In this way, one can go from a video frame to a physical system state, modify the state with physics simulation, and then go back from the modified state to a video frame. Together with a differentiable physics engine, through which one can back-propagate, this makes it possible to use the un-annotated video itself as supervision. At the same time, the two neural networks can be seen as an auto-encoder, in which the latent state is explicitly constrained to correspond to the desired physical state variables.\n\nThe topic of the paper is hot: a proper integration of physical models with data-driven deep learning is, arguably, one of the big short- to mid-term themes of machine learning research. The way it is done in the present paper intuitively makes sense. The approach is fairly obvious at the conceptual level; but in the details poses a number of technical challenges especially for the decoder, which are nicely analysed and resolved.\n\nSome minor design choices are not well justified and at first sight appear a bit l'art pour l'art. While it is a sensible, pragmatic choice to first predict object masks, then extract their location ands and velocities in a second step; I do not quite see why one would have to do the latter with neural networks. it would seem that once the masks have been found, their location can be chosen as something like the (perfectly differentiable) mask-weighted centroid and does not need a multi-layer network; and similarly that deriving velocity from locations in adjacent frames can be hard-coded and does not need a 3-layer network.\n\nThe experiments are still at an early \"toy\" level, with synthetic videos where high-contrast, homogeneous objects move in front of a uniform or blurry background. The baselines are sensible and ablation studies are done with care. Still, it would have been nice to also run the method on some real video. To my understanding, this would be easily possible at least for future frame prediction, all one has to do is either annotate the objects in the target frame or measure success by comparing the predicted and true frames at the image level. It is also not clear whether the videos were synthesised with the same physics engine also use inside the system - which would be slightly questionable, in the sense that the learnable pipeline is then a-priori matched to the biases in the data.\n\nOne comment on the presentation: while the paper is generally well-written and easy to follow, the wording could at times be more careful. There is a slight tendency to identify the particular (simple) physical systems of the paper with physics as a whole. E.g., not all physics simulation must have objects - for instance, fluid dynamics or radiative transfer do not have individual objects, but are nevertheless relevant in the context of visual data. Similarly, even for defined objects, position and velocity are not always a sufficient state, for instance objects might deform, or have different elastic properties when colliding.\n\nOverall, I find the work interesting and well-executed. It is a natural step to take towards the important goal of integrated data-driven and physical models, including the associated theme of self-supervision via physical constraints. On the negative side the paper does make a slightly rushed and unfinished impression by not showing any, even qualitative, experiments on real video. Most people - rightly - use simple toy-like datasets for development and analysis. But showing only those gives me the impression that the paper was written too early, just to be the first and to make the deadline. Or that moving to real video poses a much greater challenge than expected - but then this should be stated and discussed.\n",
            "This paper presents an approach for unsupervised estimation of physical parameters from video, using the physics as inverse graphics approach. The approach uses a feedforward encoder for localising object positions from which a velocity is estimated. These are fed as inputs to an (action-conditioned) physics simulator which generates future predictions of object positions. This simulator has knowledge of the system dynamics apriori, needing estimation of a few physical parameters such as gravity and spring constants. The outputs of this simulator is fed to a co-ordinate consistent decoder, a neural network that uses a Spatial Transformer to render the corresponding output image. The whole system is trained end-to-end on videos of dynamical systems, in an unsupervised manner. Results on two and three body interaction settings and an MNIST digit motion dataset show promising performance. The system is able to recover the underlying physical parameters accurately while also making consistent long-term ex predictions. Additionally, the model is used for visual MPC on a simulated cartpole task where it outperforms state of the art model-based and model-free RL baselines.\n\nThis paper is well written and clearly motivated, albeit a bit incremental in its approach. Many of the building blocks have been explored in prior work, with the major component being the co-ordinate consistent decoder. The experiments are visually simplistic and it is not obvious if the system will scale to more complex settings. A few comments:\n1. A major limitation of the approach is the assumption that the equations governing the system are known. This makes it harder to generalise the system to novel tasks and more complicated settings with contact and object interactions. A potential way to overcome this could be to use ideas from prior work such as Interaction Networks where the dynamics are modelled as unary and binary interactions. While such dynamics models are black-box and not as interpretable compared to the current approach, they can easily generalise to novel tasks. Additionally, using positions and velocities as the latent state representation together with IN style transition models can be a sensible middle ground.\n2. It would be great if there is an additional ablation experiment where the known equations of motion are replaced with a black-box neural network (while still retaining the position and velocity representation). This can quantify the effect of known dynamics and make the contributions of the paper (with regards to the decoder) more clear.\n3. An alternative way of generating consistent object positions from the encoder is to compute a mask-weighted average of the image co-ordinates. This can be a nice way of adding additional structure to the networks that can regularise training.\n4. The approach uses a 3-layer MLP for generating velocity estimates \u2014 could this not be done via finite differencing? (e.g. higher-order backward differencing)\n5. Both the content and mask vectors are learnable but fixed for the entire task \u2014 i.e. it is not a function of the input image. This makes the approach not applicable to novel objects or even objects with minor color changes. \n6. It is not clear how the translations, rotations and scale parameters for the Spatial Transformer are estimated. I presume that the positions and orientations predicted by either the encoder or physics simulator are directly used. This needs to be clarified in the main paper. \n7. The paper mentions that the background masks are known when localising the objects via the encoder. If this is the case, the localisation problem becomes somewhat trivial. This should be clarified.\n8. There needs to be a clear discussion on the limitations of the current approach \u2014 it does not scale to novel objects, needs to know the number of objects apriori and has not been shown to estimate object properties such as mass, friction etc. \n\nOverall, the approach presents promising initial results towards an unsupervised method for modelling dynamical systems from video. There are several limitations that need to be made explicit and some additional experiments on more complicated systems and a few ablation studies can significantly improve the strength of the paper. I would suggest a borderline accept.\n\nTypos:\n1. Section 4.1, Setup: 5 values of (K, t_pred, t_ext) are given, need only 4\n2. \u201cK\u201d is not introduced till the results section\n\nAdditional citation:\nThe following paper on learning physically consistent position and velocity representations for dynamical systems (and for use in visual MPC settings) should be cited:\nJonschkowski, Rico, et al. \"Pves: Position-velocity encoders for unsupervised learning of structured state representations.\"\u00a0arXiv preprint arXiv:1705.09805\u00a0(2017).",
            "This paper presents a method for jointly making physical predictions and inferring latent physical parameters (e.g. gravity) in an unsupervised manner from video. Specifically, the proposed architecture consists of an object-centric encoder which estimates dynamic properties of each object in the scene (e.g. position), a differentiable physics engine, and a decoder. \n\nI enjoyed reading this paper and think that it is a valuable contribution to the literature on physical reasoning, and thus lean towards acceptance. It elegantly combines several ideas that have been recently been investigated in the literature, though not yet put together. The experiments are well done and encompass not just prediction and inference but also control. The results look very impressive compared to existing models as well. However, my main critique would be that the evaluation domains are somewhat simplistic. If experiments in slightly more complex domains could be performed then I would be willing to increase my score to a full accept.\n\nIn the present paper, the scenes consist of only two or three objects, which is quite small compared to other papers in the literature which have evaluated on scenes with 6 or more objects. Similarly, given that the paper says it was inspired by the Physics 101 dataset, it is a bit disappointing that the proposed model was not evaluated on Physics 101 which would provide a more ecologically valid test of the model. At a minimum, I would like to see experiments with at least 6 objects if not more. Beyond that, including experiments on Physics 101 would change this from a good paper to an excellent paper.\n\nI had a number of additional questions and comments:\n\nI wondered how the proposed method would far at inferring object-specific latent parameters which cannot be inferred from images alone, such as friction or density. It seems like the velocity encoder could try to make these predictions as well, but it is not clear to me how well this would work with only a few frames.\n\nSomewhat relatedly, it seems like a limitation of the method is that it would not work if the objects were not visually distinct (i.e. if the balls were all the same color)---in other words, it cannot track objects over time but must learn a fixed mapping of visual property (color) to slot. This is fine to leave for future work, but merits discussion. In particular, I suspect this is also related to why the IN results are so poor; due to small errors in the IN the objects end up out of the frame, and then because the model has no memory component, it just forgets about them. If the model had access to a memory that could track objects that leave the scene, then I expect the IN would fare much better. Similarly, I expect that even the model with the perfect simulator would fail in scenes in which objects can be fully occluded. I would appreciate if some discussion of these limitations could be added to the paper.\n\nI am curious how well the model would perform if the number of object slots were not correct (i.e. if N is less then that actual number of objects, or more than the number of objects). It would be great if to include some experiments on this in the appendix.\n\nThe related work should probably mention the recent COBRA architecture [1], which also uses unsupervised scene decomposition combined with model-based RL.\n\nCan you clarify whether the interaction net baseline is pretrained, or trained end-to-end with the encoder and decoder?\n\nWhat are the errorbars over in Figure 5? Are they multiple seeds? If not, then I would like to see the figure updated with results from multiple training runs in order to properly assess variance.\n\nPage 4: can you give more details on what a \u201cfixed background mask\u201d is?\nPage 6: what is K? Is this supposed to be L (the number of frames used for velocity estimation?)\nPage 6: when describing the values of (K, T_pred, T_ext), why are there 5 different settings? \nThe paper states earlier in the paragraph that there are only 4 different systems so I am a bit confused what these settings correspond to.\n\n[1] Watters, N., Matthey, L., Bosnjak, M., Burgess, C. P., & Lerchner, A. (2019). COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration. arXiv preprint arXiv:1905.09275."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses gratitude for the author's response and clarifications. It also acknowledges the inclusion of new results and discussion, indicating a positive reception to the revisions.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges the importance and intuitive sense of the work, but also points out limitations in the design choices, experimental setup, and presentation. The overall assessment is mixed, indicating a neutral sentiment.",
            "The review acknowledges the paper's strengths (well-written, clear motivation, promising initial results) but also points out several limitations and areas for improvement. The final recommendation of \"borderline accept\" suggests a neutral overall assessment.",
            "The reviewer explicitly states they 'enjoyed reading this paper' and 'lean towards acceptance,' indicating a positive overall sentiment. They also highlight the paper's 'valuable contribution' and 'impressive' results."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Thanks for your response,\" \"That's a good point,\" and \"Thanks for including these results!\" which demonstrate a supportive and encouraging tone. They also offer suggestions for future work, indicating a desire to help improve the paper.",
            "The review provides both positive and negative feedback, using phrases like \"nicely analysed and resolved\" (positive) and \"Some minor design choices are not well justified\" (negative). It also suggests improvements and points out areas of concern, maintaining a balanced perspective throughout.",
            "The review provides both positive feedback (e.g., \"well written and clearly motivated\", \"promising initial results\") and constructive criticism (e.g., \"a bit incremental\", \"experiments are visually simplistic\", multiple points highlighting limitations). The reviewer offers specific suggestions for improvement, indicating a balanced and helpful approach.",
            "The review presents both positive aspects (enjoyment of reading, valuable contribution, impressive results) and constructive criticism (simplistic evaluation domains, questions about latent parameter inference, limitations regarding object tracking). The reviewer provides specific suggestions for improvement, indicating a balanced and helpful tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer expresses appreciation for the authors' responses and clarifications, and their feedback points are logically connected and build upon previous concerns. The reviewer acknowledges the improvements made by the authors and provides further constructive suggestions and questions without contradicting themselves or their previous statements. The review maintains a positive and constructive tone throughout while still pointing out areas for potential improvement or further investigation.",
            "The review is consistent in its assessment, pointing out both the strengths and weaknesses of the paper without any self-contradiction. It acknowledges the novelty and importance of the topic while also highlighting areas for improvement, such as the experimental validation and some design choices. The reviewer maintains a balanced perspective throughout the text.",
            "The review is consistent because it starts with an overall positive assessment while also pointing out limitations and areas for improvement. The criticisms are constructive and aimed at strengthening the paper, and the final recommendation of 'borderline accept' aligns with the mixed but ultimately positive evaluation.",
            "The review is consistent because it starts with a positive assessment, leaning towards acceptance due to the paper's valuable contribution and elegant combination of ideas. The reviewer then raises a main critique about the simplicity of the evaluation domains, suggesting improvements like more complex scenes and evaluation on Physics 101. All subsequent questions and comments are in line with this critique, aiming to improve the paper's evaluation and address potential limitations. The reviewer's feedback is constructive and focused on strengthening the paper within the context of their initial positive assessment and identified area for improvement."
        ]
    },
    {
        "paper_id": "iclr_2020_HJlRFlHFPS",
        "paper_title": "Unsupervised Distillation of Syntactic Information from Contextualized Word Representations",
        "paper_abstract": "Contextualized word representations, such as ELMo and BERT, were shown to perform well on a various of semantic and structural (syntactic) task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in few-shot parsing setting.",
        "review_ids": [
            "SyekktO3jB",
            "H1lgdg3siH",
            "HyxJmJuUjB",
            "rJxPTT-IYH",
            "SygRa5LTKS",
            "SklmVR-fqH",
            "Byey-Ljz5H"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thank you for responding. After a second *very close* reading of the updated paper and the authors' reply, I maintain that this paper is far from ICLR standard and will keep my score at 1.\n\n1. The authors propose to distill syntactic knowledge from the contextualized representation. However, the authors do not formalize the notion of \"syntactic knowledge,\" and it is unclear what exactly they hope to disentangle from these representations.\n2. The authors approached this task by generating \"syntactically similar\" (also vaguely defined) sentence pairs using heuristics on BERT representations (Section 3.1). Given that this process is heuristic-drive, one would expect an analysis of the hyperparameters selected to achieve the goal. However, the authors performed no such analysis, and the hyperparameters of this generation process (k=6, top-30, ...) appear to be selected at random.\n3. The authors fail to convince me that their method has any practical utility (Section 4.3):\n    a. Lack of a standard baseline. The authors do not address my concerns regarding the delexicalized parser baseline in the updated paper.\n    b. Unfair comparisons. In Figure 4, the author's proposed method \"Syntax\" uses BERT during training (for generating sentence pairs). It is unfair that their baselines only have access to ELMo embeddings. A standard fine-tuned BERT should be the minimum comparison.\n    c. Lack of standard datasets + automatically generated golden labels. The authors did not perform experiments on standard parsing datasets, and they fail to describe their parsing corpus in detail. According to Section 4 [Corpus], they used off-the-shelf spaCy parser to generate golden labels for 1M Wikipedia sentences. This practice is non-existent in literature. If weak-supervision (spaCy's output) is taken as golden labels, the authors should at least provide detailed statistics on their data, as well human-evaluation of the quality of those labels.",
            "I am satisfied with the responses to my review and the others. I am raising my rating to 8: Accept.",
            "I appreciate the detailed response to my review. I think the revised paper is improved in terms of background and motivation, and more complete experiments. It's also good to see the quantitative clustering purity results. \nWhile I don't think getting very high parsing results is a must for this work, I agree with reviewer 1 that comparing with a POS-based baseline is in order. \nI hope to see the paper accepted and at this point will keep my current evaluation. ",
            "Summary:\n=========\nThis paper aims to disentangle semantics and syntax in contextualized word representations. The main idea is to learn a transformation of the contexualized representations that will make two word representations to be more similar if they appear in the same syntactic context, but less similar if they appear in different syntactic contexts. The efficacy of this transformation is evaluated through cluster analysis, showing that words better organize in syntactic clusters after the transformation, and through low-resource dependency parsing. \n\nThe paper presents a simple approach to transform word representations and expose their syntactic information. The experiments are mostly convincing. I would like to see better motivation, more engagement with a wider range of related work, and more thorough quantitative evaluations. Another important question to address is also what kind of semantic/syntactic types of information are targeted, and how to handle the tradeoff between them, for instance for different purposes. \n\n\nMain comments:\n==============\n1. Motivation: I found the motivation for the problem understudied a bit lacking. The main motivation seems to be to disentangle semantic and syntactic information. But why should we care about that? Beyond reference to disentangling in computer vision, some more motivation would be good. The few-shot parsing is a good such motivation, although the results are a bit disappointing (see more on this below). Another possible motivation is potential applications of disentanglement in language generation. There is a line of work on style transfer also in language generation, and it seems plausible that the methodology could be applied to such tasks. \n2. The present work is well-differentiated from work on extracting syntactic information from word representations via supervised ways, as the current work does so in an unsupervised way. I don't quite get the terminological differentiation between \"mapping\" and \"extracting\" in the introduction, but the idea is clear. \n3. Have you considered alternative representations of word pairs besides the different of their transformations f(x)-f(y)? \n4. I found it interesting that the word representation from BERT is the concatenation of layer 16 with the mean of all the other layers. This is motivated by Hewitt and Manning's findings, and [5] found similar results. However, the different between layer 16 and others is not that large as to warrant emphasizing it so much. Perhaps a scalar mix with fine-tuning may work better, as in [5], or another method. Have you tried other word representations? I also wonder whether it makes sense to use different layers for different parts of the triplet loss, depending on whether to emphasize syntactic vs. semantic similarity. \n5. The introduction lays out connections to some related work, but leaves several relevant pieces missing. See examples below. \n6. The results in 3.3 are limited but useful. The comparison with a PCA-ed and reduced representation is well thought of, because of the risk with low-resource and high dimensionality. That said, I found the gap between the proposed syntax model and the ELMo-reduced disappointingly small. Even in the LAS, it seems like the difference is very small, ~0.5, although it's hard to tell from the figure. Providing the actual numbers and a measure of statistical significance would be helpful here. \n7. Some care should be taken to define what kind of semantics is targeted here. In several cases this is \"lexical semantics\", but then we have \"meaning\" in parentheses sometimes (end of intro). Obviously, there's much more to semantics and meaning that the lexical semantics, so a short discussion of how the work views other, say compositional semantics, would be good. \n\n\nOther comments:\n===============\n1. The introduction seeks a representation that will ignore the similarity between \"syrup\" in (2) and (4). I wonder if \"ignoring\" is too strong. One may not want to lose all lexical semantic information. Moreover, the proposed triplet loss does not guarantee that information is ignored (and justly so, in my opinion). \n2. In the example, \"maple\" and \"neural\" are said to be syntactically similar, although \"maple syrup\" is a noun compound while \"neural networks\" is an adjective-noun. Shouldn't they be treated differently then? Unless the notion of syntax is more narrow and just looks at unlabeled dependency arcs. \n3. Some experimental choices are left unexplained, such as k=6 (section 2.1) or mapping to 27 dims (section 2.3); these two seem potentially important. \n4. Section 2.3: do you also back-prop back into the BERT/ELMo model weights? \n5. The dataset statistics in section 3 do not match those in section 2.2. Please clarify. \n6. The qualitative cluster analysis via t-SNE (3.1) is compelling. It could be made stronger by reporting quantitative clustering statistics such as cluster purity before and after transformation. \n7. In the examples showin in 3.1, it would be good to give also the nearest neighbor before the transformation for comparison. \n8. The quantitative results in 3.2 convey the point convincingly. It's good to see also the lexical match measure going down. The random baseline is also a good sanity check to have. It would be good to provide full results with BERT, at least in the appendix and at least for section 3.2, maybe also for 3.3.\n9. More related work: \n+ Work that injects syntactic information into word representations in a supervised way, such as [1,2]\n+ Work that shows that word embeddings contain different kinds of information (syntactic/semantic), and propose simle linear transformations to uncover them. \n+ Engaging with the literature on style transfer in language generation would be good, as mentioned above for motivation, but also to situate this work w.r.t to related style transfer work. \n+ Another line of work that may be mentioned is the variety of papers trying to extract syntactic information from contextualized word representations, such as constructing trees from attention weights. There were a few such papers in BlackboxNLP 2018 and 2019. \n\nTypos, phrasing, formatting, etc.:\n============================\n- Abstract: a various of semantic... task -> various semantic... tasks; use metric-learning approach -> use a metric-learning approach; in few-shot parsing setting -> in a few-shot parsing setting\n- Wilcox et al. does not have a year\n- Introduction: few-shots parsing -> few-shot parsing\n- Method: extract vectors -> extracts vectors; Operativly -> Operatively \n- Section 3: should encourages -> should encourage; a few-shots settings -> a few-shot setting\n- 3.2: -- was not rendered properly\n- 3.3: matrix that reduce -> reduces \n\n\nReferences\n==========\n[1] Levy and Goldberg. 2014. Dependency-Based Word Embeddings\n[2] Bansal et al. 2014. Tailoring Continuous Word Representations for Dependency Parsing\n[3] Artetxe et al. 2018. Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation\n[4] Tenney et al. 2019. BERT Rediscovers the Classical NLP Pipeline\n[5] Liu et al. 2019. Linguistic Knowledge and Transferability of Contextual Representations",
            "The authors state a clear hypothesis: it is possible to extract syntactic information from contextualized word vectors in an unsupervised manner. The method of creating syntactically equivalent (but semantically different) sentences is indeed interesting on its own. Experiments do support the main hypothesis -- the distilled embeddings are stronger in syntactic tasks than the default contextualized vectors. The authors provide the code for ease of reproducibility which is nice.\n\nThere is a short literature review, but I am wondering if something similar was done for static word embeddings. I understand that they are obsolete these days, but on the other hand, they are better researched, so were there any attempts to disentangle syntax and semantics in the classical static word vectors?\n\nOverall, I have no major concerns with the paper.",
            "CONTRIBUTIONS:\nTopic: Disentangling syntax from semantics in contextualized word representations\nC1. A method for generating \u2018structurally equivalent\u2019 sentences is proposed, based only on the assumption that maintaining function words, and replacing one content word of a source sentence with another to produce a new grammatical sentence, yields a target sentence that is equivalent to the source sentence. \nC2. The \u2018structural relation\u2019 between two words in a sentence is modeled as the difference between their vector embeddings.\nC3a. The structural relation between a pair of content words in one sentence is assumed to be the same as that between the corresponding pair in an equivalent sentence. \nC3b. The structural relation between any pair of content words in one sentence is assumed to be different from the structural relation between any pair of content words in an inequivalent sentence. \nC4. Given a selected word in a source sentence, to generate an alternative \u2018corresponding\u2019 content word for an equivalent target sentence, BERT is used to predict the source word when it is masked, given the remaining words in the source sentence. The alternative corresponding word is randomly selected from among the top (30) candidates predicted by BERT. Given a source sentence, the set of target sentences formed by cumulatively replacing content words one at a time in randomly selected positions defines an \u2018equivalence set\u2019 in which words in different sentences with the same left-to-right index are corresponding words. (To promote the formation of grammatical target sentences, a word is only replaced by another word with the same POS.) A pre-defined set of equivalence sets is used for training.\nC5. A metric learning paradigm with triplet loss is used to find a function f for mapping ELMo or BERT word embeddings to a new vector space of \u2018transformed word representations\u2019. Implementing C2 and C3a, given the indices i and i\u2019 of two content words, the triplet loss rewards closeness of the difference D between the transformed embeddings of the pair of words with these indices in sentence S and the corresponding difference D\u2019 for an equivalent sentence S\u2019. Implementing C3b, the triplet loss penalizes closeness between D and D\u201d, where D\u201d is the difference between transformed word embeddings of a pair of content words in a sentence S\u201d that is inequivalent to S. (Eq. 4).\nC6. (Implementing C5.) To form a mini-batch for minimizing the triplet loss, a set of (500) sentences S is selected, and for each a pair of indices of content words is chosen. Training will use the difference in the transformed embeddings of the words in S with these indices: call this D, and call the set of these (500) D vectors B. For each sentence S in B, a \u2018positive pair\u2019 (D, D\u2019) is generated, where D\u2019 is the corresponding difference for S\u2019, a selected sentence in the equivalence set of S. Closeness of D and D\u2019 is rewarded by the triplet loss, implementing C3a. To implement C3b, a \u2018negative pair\u2019 (D, D\u201d), for which closeness is penalized by the loss, is formed as follows. D\u201d is the closest vector in B to D that is derived from a sentence S\u201d that is not equivalent to S. \nC7. 2-D t-SNE plots (seem to) show that relative to the original ELMo embeddings, the transformed embeddings cluster better by POS (Fig. 3). (No quantitative measure of this is provided, and the two plots are not easy to distinguish.)\nC8. Pairs of closest ELMo vectors share syntactic (dependency parse) properties to a greater degree after transformation than before (Table 1). To check that this goes beyond merely POS-based closeness, the syntactic relations that least determine POS are examined separately, and the result remains. Furthermore, the proportion of pairs of closest vectors that are embeddings of the same word (in different contexts) drops from 77.6% to 27.4%, showing that the transformation reduces the influence of lexical-semantic similarity. Similar results hold for BERT embeddings, but to a lesser degree, so the paper focusses on ELMo. \nC9. Few-shot parsing. Two dependency parsers are trained, one on ELMo embeddings, the other on their transformations (under the proposed method). In the small-data regime (less than 200 training examples), the transformed embeddings yield higher parser performance, even when the encoding size of the ELMo embeddings is reduced (from 2048 to 75) to match that of the transformed embeddings by either PCA or a learned linear mapping. (Fig. 4) \nRATING: Weak accept\nREASONS FOR RATING (SUMMARY). Using deep learning to create an encoding of syntactic structure with minimal supervision is an important goal and the paper proposes a clever way of doing this. The only \u2018supervision\u2019 here comes from (i) the function/content-word distinction (C1 above): two grammatical sentences are structurally equivalent if [but not only if] one can be derived from the other by replacing one content word with another; and (ii) filtering candidate replacement words to match the POS of the replaced word. BERT\u2019s ability to guess a masked word is put to good use in providing suitable content word substitutions. The experimental results are rather convincing.\nREVIEW (beyond the summary above)\nC1. This assumption is famously not deemed to be true in linguistics, where the structural difference between \u2018control\u2019 and \u2018raising\u2019 verbs is basic Ling 101 material: see https://en.wikipedia.org/wiki/Control_(linguistics)#Control_vs._raising. This particular structural contrast illustrates how verbs can differ in their argument structure, without there being function words to signal the difference. So substituting *verbs* in particular may be non-ideal for the purposes of this work. Even the third example given by the authors in Sec. 3.1 illustrates a related  point, where function words do signal the contrast:  while the meaning of \u2018let\u2019 and \u2018allow\u2019 may be very similar, their argument structures differ, so that replacing \u2018lets\u2019 with \u2018allows\u2019 in the first sentence, or the reverse in the second sentence, produces ungrammatical results: \n*their first project is software that *allows* players connect the company \u2019s controller to their device\n*the city offers a route-finding website that *lets* users to map personalized bike routes\nTherefore, contrary to the paper, relative to linguistic syntactic structure, it is not a good result that \u2018lets\u2019 in the original version of the first sentence is the closest neighbor in transformed embedding space to \u2018allows\u2019 in the second. Rather, it is probably meaning, not structure, that makes \u2018let\u2019 and \u2018allow\u2019 similar.\nIt would improve the paper to make note of this general concern with C1 and to provide a response.\nOn another point, an important premise of the proposed method (C2 above) is that differences in vector space embeddings encode relations; this has been used by a number of previous authors since the famous Mikolov, Yih & Zweig NAACL2013, and that work should be cited and discussed.",
            "Summary\nThe authors proposed to disentangle syntactic information and semantic information from pre-trained contextualized word representations.\n\nThey use BERT to generate groups of sentences that are structurally similar (have the same POS tag for each word) but semantically different. Then they use a metric-learning approach to learn a linear transformation that encourages sentences from the same group to have closer distance. Specifically, they defined a triplet loss (Eq4) and uses negative sampling.\n\nThey use 150,000 sentences from Wikipedia to train the transformation. POS tags are obtained from spaCy. To evaluate the learned representations, they provided a tSNE visualization of the original and transformed representations (groups by dependency label); evaluate whether the nearest neighbor shares the same syntactic role; low-resource parsing.\n\nReasons of rejection:\n1. I don't agree with the authors' argument, \"we aim to extract the structural information encoded in the network in an unsupervised manner, without pre-supposing an existing syntactic annotation scheme\".  First, what do you mean by structural information without a clear definition? Also, in the method, the authors construct a dataset where each group of the sentence share similar syntactic structures (having the same POS tag). It seems there that the structural information just means POS tags.\n\n2. The author failed to convince me that the learned representation is more powerful than just combining POS tags with the original representations. Since POS tags are assumed to be available during training. I think a reasonable baseline in all experiments would be the performance based on POS tags. For example, in Figure 3, although the original EMLo representation does not correlates with the dependency label very much, the POS tags may do. In Figure 4, the authors should compare with delexicalized dependency parsing, which performs pretty well in los-resource setting.\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they maintain the paper is \"far from ICLR standard\" and will keep their score low. The review also contains critical points regarding the methodology, lack of clarity, and unfair comparisons.",
            "The reviewer explicitly states they are 'satisfied' and are 'raising' their rating to 'Accept'.",
            "The reviewer expresses appreciation for the changes made to the paper and acknowledges the improvements in background, motivation, and experiments. They also state their hope for the paper's acceptance.",
            "The review expresses both positive and negative feedback. It acknowledges the paper's interesting approach and convincing experiments, but also points out areas needing improvement, such as motivation, related work engagement, and quantitative evaluations.",
            "The reviewer states \"Experiments do support the main hypothesis\" and \"Overall, I have no major concerns with the paper.\"",
            "The review expresses a generally positive sentiment, highlighting the cleverness and importance of the proposed method, while also acknowledging convincing experimental results. The 'weak accept' rating further reinforces this positive outlook.",
            "The review expresses disagreement with the authors' arguments and points out flaws in their methodology and evaluation. The reviewer uses phrases like \"reasons of rejection,\" \"I don't agree,\" and \"failed to convince me,\" indicating a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses strong negative language like \"vaguely defined\", \"heuristic-drive\", \"appear to be selected at random\", \"fail to convince me\", \"unfair comparisons\", \"non-existent in literature\". The reviewer also directly points out flaws in the experimental setup and methodology.",
            "The reviewer expresses satisfaction and uses positive language like 'satisfied' and 'Accept', indicating a supportive tone.",
            "The reviewer uses phrases like \"I appreciate,\" \"improved,\" \"good to see,\" and \"I hope to see the paper accepted,\" which indicate a supportive and encouraging tone.",
            "The review provides constructive criticism while acknowledging the paper's strengths. It uses phrases like \"mostly convincing,\" \"well-differentiated,\" and \"interesting\" alongside suggestions for improvement, indicating a balanced perspective.",
            "The reviewer uses positive language such as \"indeed interesting\", \"nice\", and expresses no major concerns, indicating a supportive tone. They also offer a constructive suggestion for improvement, further reinforcing this tone.",
            "The tone is balanced, offering both praise and constructive criticism. While acknowledging the importance and cleverness of the approach ('clever way of doing this', 'experimental results are rather convincing'), the reviewer also points out a significant flaw in the core assumption (C1) and suggests improvements, such as citing relevant prior work. This mix of positive and negative feedback indicates a balanced perspective.",
            "The tone is critical, using phrases like \"I don't agree,\" \"failed to convince me,\" and directly pointing out weaknesses in the authors' arguments and methodology. The reviewer questions the definition of structural information and suggests better baselines for evaluation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently expresses negative opinions about the paper's clarity, methodology, and experimental setup. All points raised (lack of definition, heuristic approach, unfair comparisons, non-standard datasets) support the reviewer's overall negative assessment and the decision to maintain a score of 1. There are no contradictory statements or shifts in opinion within the review.",
            "The reviewer explicitly states satisfaction with the responses and consequently raises the rating to 'Accept'. This is a consistent progression from satisfaction to a positive recommendation.",
            "The review is consistent because the reviewer expresses positive feedback on the improvements made in the revised paper, agrees with another reviewer on a suggestion for further improvement, and maintains a positive overall evaluation by hoping for acceptance and keeping their current evaluation.",
            "The review maintains a consistent critical but constructive tone throughout. It identifies weaknesses and suggests concrete improvements in areas like motivation, related work, experimental details, and clarity, while also acknowledging the strengths of the paper, such as the simplicity of the approach and some convincing experimental results. There are no self-contradictions in the feedback; the reviewer consistently pushes for more rigor, clarity, and context.",
            "The review is consistent because it acknowledges the strengths of the paper (clear hypothesis, interesting method, experimental support, code availability) and raises a minor question about related work without contradicting the overall positive assessment. The reviewer concludes with 'no major concerns', indicating a consistent positive stance.",
            "The review acknowledges the paper's strengths, such as the clever approach and convincing experimental results, which are summarized in the 'REASONS FOR RATING (SUMMARY)' section and support an 'accept' rating. Simultaneously, the review points out a significant weakness in the core assumption (C1) and suggests a missing citation in the 'REVIEW (beyond the summary above)' section. This critical feedback justifies the 'weak' qualifier in 'Weak accept', making the overall review consistent. The reviewer is not contradicting themselves but rather providing a nuanced evaluation that balances positive aspects with identified flaws, leading to a weak acceptance recommendation.",
            "The reviewer consistently argues against the paper's claims and methodology. Point 1 questions the definition of 'structural information' and highlights the reliance on POS tags, suggesting the method is not as unsupervised as claimed. Point 2 builds on this by questioning the added value of the learned representation over simply using POS tags, advocating for POS tag based baselines. Both points consistently criticize the paper's conceptual clarity and empirical validation in relation to the use of POS tags."
        ]
    },
    {
        "paper_id": "iclr_2018_HkCvZXbC-",
        "paper_title": "3C-GAN: AN CONDITION-CONTEXT-COMPOSITE GENERATIVE ADVERSARIAL NETWORKS FOR GENERATING IMAGES SEPARATELY",
        "paper_abstract": "We present 3C-GAN: a novel multiple generators structures, that contains one conditional generator that generates a semantic part of an image conditional on its input label, and one context generator generates the rest of an image. Compared to original GAN model, this model has multiple generators and gives control over what its generators should generate. Unlike previous multi-generator models use a subsequent generation process, that one layer is generated given the previous layer, our model uses a process of generating different part of the images together. This way the model contains fewer parameters and the generation speed is faster. Speci\ufb01cally, the model leverages the label information to separate the object from the image correctly. Since the model conditional on the label information does not restrict to generate other parts of an image, we proposed a cost function that encourages the model to generate only the succinct part of an image in terms of label discrimination. We also found an exclusive prior on the mask of the model help separate the object. The experiments on MNIST, SVHN, and CelebA datasets show 3C-GAN can generate different objects with different generators simultaneously, according to the labels given to each generator.",
        "review_ids": [
            "By9jZQukf",
            "ByfJW0vlf",
            "SygpIWqlM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary: This paper studied the conditional image generation with two-stream generative adversarial networks. More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label. During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation. An auxiliary \u201clabel difference cost\u201d was further introduced to encourage class information captured by the foreground generator. Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline.\n\n== Novelty/Significance ==\nControllable image generation is an important task in representation learning and computer vision. I also like the unsupervised learning through gating function and label difference cost. However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs).\n\n== Detailed comments ==\nThe proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region. Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability. For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet. \n\nGiven the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable. However, I didn\u2019t see such detailed analysis as in the other papers on controllable image generation.\n\nIn Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp. It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better). Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned. Is it possible to use all the binary attributes in the dataset.\n\nAlso, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks).\n\nOverall, I believe the paper is interesting but not ready for publication. I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation. Hopefully, the suggested studies will improve the quality of the paper in the future submission.\n\n== Presentation ==\nThe paper is readable but not well polished. \n\n-- In Figure 1, the \u201cG1\u201d on the right should be \u201cG2\u201d;\n-- Section 2.2.1, \u201cX_f\u201d should be \u201cx_f\u201d;\n-- the motivation of having \u201cz_v\u201d should be introduced earlier;\n-- Section 2.2.4, please use either \u201calpha\u201d or \u201c\\alpha\u201d but not both;\n-- Section 3.3, the dataset information is incorrect: \u201c20599 images\u201d should be \u201c202599 images\u201d;\n\nMissing reference:\n-- Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017.\n-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.\n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.\n",
            "[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner. In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents. To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations. After generating both parts in parallel, they are combined using alpha blending to compose the final image. This generated image is then sent to the discriminator. The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA. The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately.\n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part. Corresponding to these two parts are two generators. To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images. As usual, the label information was added as the input of generator and discriminator as well.\n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2. Based on this, outputs of Gc should not change much when flipping the input labels.\n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other. Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2.\n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem. However, to my opinion, completing it without any supervision is challenging and meaningless. In this paper, the authors proposed a conditional way to generate images compositionally. It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017).\n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model. However, there are several weaknesses existed:\n\n1. There is no quantitative evaluation and comparisons. From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance. The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples. To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance. Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation. Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images. This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier.\n\n2. The authors should try more complicated datasets, like CIFAR-10. Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs. It is easy to train since its low resolution, but also means a lot since it a relative complicated scene. I would suggest the authors also run the experiments on CIFAR-10.\n\n3. The authors did not perform any ablation study. Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models. As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum. I strongly suggest the authors add some ablation studies. The authors should at least compare with one-layer conditional GAN. \n\n4. The proposed model merely showed two-layer generation results. There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6). The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration.\n\n5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10. I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017).\n\n6. I did not understand very well the label difference loss in (5). Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image. G2 takes all the duty  can still obtain a lower L_ld. From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information.\n\n7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations.\n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN. It generates images in a layer-wise manner. To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties. In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA. However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study. Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted.",
            "\n- Paper summary\n\nThe paper proposes a label-conditional GAN generator architecture and a GAN training objective for the image modeling task. The proposed GAN generator consists of two components where one focuses on generating foreground while the other focuses on generating background. The GAN training objective function utilizing 3 conditional classifier. It is shown that through combining the generator architecture and the GAN training objective function, one can learn a foreground--background decomposed generative model in an unsupervised manner. The paper shows results on the MNIST, SVHN, and Celebrity Faces datasets.\n\n- Poor experimental validation\n\nWhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised manner, it is clear how this capability can help practical applications, especially no such examples are shown in the paper. The paper also fails to provide any quantitative evaluation of the proposed method. For example, the paper will be more interesting if inception scores were shown for various challenging datasets.  In additional, there is no ablation study analyzing impacts of each design choices. As a result, the paper carries very little scientific value."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's novelty, applicability, and experimental validation. Phrases like \"novelty in this paper is quite limited,\" \"I am not convinced if the two-stream generation pipeline can work well on more challenging datasets,\" and \"not ready for publication\" indicate a negative sentiment.",
            "The review identifies several weaknesses, including lack of quantitative evaluation, ablation studies, and experiments on more complex datasets. The reviewer concludes that the paper \"still needs more works to make it solid and comprehensive before being accepted.\"",
            "The review expresses negative sentiment due to phrases like \"Poor experimental validation\", \"fails to provide any quantitative evaluation\", \"carries very little scientific value\". These phrases indicate the reviewer's dissatisfaction with the paper's methodology and results."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review provides specific criticisms about the paper's limitations, lack of detailed analysis, and experimental setup. Phrases like \"not very sharp,\" \"insufficient for foreground and background separation,\" and suggestions for improvement indicate a critical tone.",
            "The tone is critical due to the direct identification of weaknesses and suggestions for improvement, using phrases like \"there are several weaknesses existed,\" \"I strongly suggest,\" and \"the paper missed.\"",
            "The tone is critical. The reviewer uses phrases such as \"Poor experimental validation\", \"fails to provide any quantitative evaluation\", \"no ablation study\", and \"carries very little scientific value\" which are direct criticisms of the paper's weaknesses."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer acknowledges some positive aspects of the paper, such as the interesting topic and the unsupervised learning approach. However, the reviewer consistently points out limitations in novelty, generalizability, experimental validation, and presentation throughout the review. The detailed comments and suggestions for improvement logically lead to the overall conclusion that the paper is 'interesting but not ready for publication'. The reviewer's criticisms are aligned with the final assessment, indicating a consistent evaluation.",
            "The review is consistent because the reviewer acknowledges the interesting idea and approach of the paper in the Strengths and Summary sections, but also points out significant weaknesses, primarily the lack of quantitative evaluation, comparisons, and ablation studies. The summary accurately reflects both the positive aspects (interesting idea) and the negative aspects (lack of evaluation), leading to a balanced and justified conclusion that the paper needs more work before acceptance. There is no contradiction in the reviewer's assessment.",
            "The review is consistent because the criticisms about lack of practical application examples, quantitative evaluation, and ablation study all logically support the conclusion of poor experimental validation and low scientific value."
        ]
    },
    {
        "paper_id": "iclr_2020_Syejj0NYvr",
        "paper_title": "Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness",
        "paper_abstract": "We propose a simple approach for adversarial training. The proposed approach utilizes an adversarial interpolation scheme for generating adversarial images and accompanying adversarial labels, which are then used in place of the original data for model training. The proposed approach is intuitive to understand, simple to implement and achieves state-of-the-art performance. We evaluate the proposed approach on a number of datasets including CIFAR10, CIFAR100 and SVHN. Extensive empirical results compared with several state-of-the-art methods against different attacks verify the effectiveness of the proposed approach. ",
        "review_ids": [
            "HJxv7evfYr",
            "SylrVLzjsr",
            "S1esl_BpcH",
            "SylNE5kstr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Contribution: This paper proposed an adversarial interpolation approach for generating adversarial samples and then training robust deep nets by leveraging the generated adversarial samples. However, I have the following concerns:\n\n1. In adversarial interpolation training, why \\tilde{y}_i' is set to 1/(C-1)(1-y_{n-i+1})? \n\n2. This work lack of interpretation of why the proposed method is more effective than PGD adversarial training.\n\n3. How about training deep nets with replicas of the training data but replace the true labels with random labels? I want to see such a result.\n\n4. Can the authors provide the black-box attack results also? I want to see the performance of PGD adversarially trained deep nets on the adversarial images crafted by attacking Adv-Interp trained deep nets, and vice versa.\n\n5. Can the authors provide the visualization of a few adversarial interpolated images?\n\n6. The authors should compare with the existing efforts that using interpolation to improve adversarial robustness. Below are some Related works on using interpolation in deep nets to improve their robustness\n\n1). Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher. Deep Neural Nets with Interpolating Function as Output Activation, NeurIPS, 2018\n\n2). Bao Wang, Alex T. Lin, Zuoqiang Shi, Wei Zhu, Penghang Yin, Andrea L. Bertozzi, Stanley J. Osher. Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization, arXiv:1809.08516, 2018\n\n3). B. Wang, S. Osher. Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning, arXiv:1907.06800, 2019\n\n7. Moreover, the following paper provides a theoretical interpretation of adversarial vulnerability of deep nets, and proposed a nice ensemble of neural SDEs to improve deep nets' robustness.\n\n1). Bao Wang, Binjie Yuan, Zuoqiang Shi, Stanley J. Osher. ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies, arXiv:1811.10745, NeurIPS, 2019\n\n\nPlease address the previously mentioned concerns in rebuttal, and this paper can be acceptable if all my concerns are addressed.",
            "I have read your reply, and most of my questions are addressed. Accordingly, I raise my rating to weak accept. However, please also add some discussion in your revised paper about when interpolation will be helpful. As pointed out by the public comment, the proposed method is not very robust to feature attack.",
            "The paper proposes adversarial interpolation training, which perturbs the images and labels simultaneously. The perturbed image $\\tilde x$ is around $x$ and interpolated towards another image $x'$ while the corresponding $\\tilde y = (1-\\epsilon_y)y +\\epsilon_y\\frac{1-y'}{C-1}$ is near $y$ but away from $y'$. The distance of interpolating images is L2 distance in the feature space and that for labels is L2 distance in the label space. The paper provides an interpretation of the proposed approach from the perspective of robust and non-robust features. Thorough experiments on different types of attacks and different datasets are performed. Although the results are impressive, I still have some concerns on the method itself:\n\n1. The method seems like a combination of manifold mixup [1] and adversarial training. The interpolation in the feature space is not a new idea and has been explored in Manifold Mixup [1]. The method resembles manifold mixup if we focus on $x$ because $\\tilde x$ and $\\tilde y$ both retain the original image and target $(x, y)$. The \"adversarial\" interpolation part is from $(x', y')$ in the sense that $\\tilde y$ is away from $y'$.\n\n2. The paper lacks a theoretical explanation, which makes it less convincing how it works so well.\n\n3. I noticed several papers with similar ideas, e.g. [2,3,4]. Could you please discuss the connections with them? I also suggest adding related work on semi-supervised learning in the paper (see [4] for examples). It would be better to compare with Manifold Mixup [1], UAT [4] in the experiments.\n\n\nMinor:\n\nPage 5, \" further break the correlation between $\\delta$ and $y'$\", what is $\\delta$ here? I did not find the definition above the sentence. The notation is directly used without any explanation in advance.\n\n\nReferences\n[1] Manifold Mixup: Better Representations by Interpolating Hidden States, ICML 2019\n[2] MixUp as Directional Adversarial Training\n[3] On Adversarial Mixup Resynthesis, NeurIPS 2019\n[4] Are Labels Required for Improving Adversarial Robustness?, NeurIPS 2019\n",
            "This is an interesting work proposing a new robust training method using the adversarial example generated from adversarial interpolation. The experimental results seem surprisingly promising. The ablation studies show that both image and label interpolating help the robustness improvement. \n\nI think it is important to provide a running time comparison between the proposed method and SOTA robust training method such as Madry's. Since the feature extractor is implemented by excluding the last layer for the logits, the backprop goes through almost the entire model. It seems that the proposed interpolating method has a similar amount of computation as PGD, so the training should take similar time as Madry's if it can converge quickly. \n\nAlso, there are too many works on robustness defense that have been proven ineffective (consider the works by Carlini). Since this is a new way of robust training and there is no certified guarantee, I would be very conservative and suggest the authors refer the checklist in [1] to evaluate the effectiveness of the defense more thoroughly to convince the readers that it really works. Especially, a robustness evaluation under adaptive attacks is necessary. In other words, if the attacker knows the strategy used by the defender, it may be possible to break the model. PGD and CW are non-adaptive since no defender information is provided. \n\n[1] Carlini, Nicholas, et al. \"On evaluating adversarial robustness.\" arXiv preprint arXiv:1902.06705 (2019)."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review expresses several concerns about the paper's methodology, lack of interpretation, missing experimental results, and insufficient comparison with existing work. The reviewer uses phrases like 'lack of interpretation,' 'I want to see such a result,' and 'the authors should compare,' indicating a critical stance.",
            "The reviewer states that most of their questions were addressed and raises their rating to 'weak accept,' indicating a positive shift in their assessment.",
            "The review expresses concerns about the novelty and theoretical grounding of the proposed method, stating it resembles existing techniques and lacks a convincing explanation for its effectiveness. The reviewer also points out missing definitions and suggests additional comparisons with related works, indicating reservations about the paper's current state.",
            "The review contains both positive feedback (promising experimental results, effectiveness of ablation studies) and constructive criticism (need for runtime comparison, concerns about effectiveness without certified guarantee, suggestion for thorough evaluation)."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical, as evidenced by the numerous questions and requests for additional information and experiments. The reviewer directly points out weaknesses in the paper, such as the lack of interpretation and missing comparisons, using a tone that suggests the paper needs significant improvement before being acceptable.",
            "The reviewer uses phrases such as 'most of my questions are addressed' and 'I raise my rating to weak accept', which indicates a supportive tone. Additionally, the suggestion to 'please also add some discussion' is phrased politely and constructively.",
            "The review uses phrases like \"The method seems like a combination...\", \"The paper lacks a theoretical explanation, which makes it less convincing...\", and \"I still have some concerns on the method itself.\" These phrases indicate a critical assessment of the paper's contributions and justifications.",
            "The review starts with positive comments (\"interesting work\", \"surprisingly promising\") but then transitions to critical points and suggestions (\"I think it is important to provide...\", \"there are too many works...\", \"I would be very conservative and suggest...\"). This mix of positive and negative feedback indicates a balanced tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it raises several valid concerns and requests for clarification, experiments, and comparisons to strengthen the paper. The reviewer's conclusion is logically aligned with the raised concerns, stating conditional acceptance upon addressing them. There are no contradictory statements or conflicting viewpoints within the review; all points aim to improve the paper's quality and clarity.",
            "The reviewer acknowledges that most of their questions are addressed and raises the rating accordingly. The reviewer then provides further suggestions for improvement and points out a weakness, which are consistent with a weak accept rating and aimed at further improving the paper.",
            "The review is consistent as it acknowledges the strengths of the paper (impressive results) while raising valid concerns about novelty, theoretical explanation, and related work. The reviewer's points are logically connected and aim to improve the paper's depth and completeness without contradicting themselves.",
            "The review is consistent because it acknowledges the potential of the work while also pointing out areas for improvement and further validation. The reviewer starts with positive feedback, highlighting the interesting nature of the work and promising experimental results. Then, the reviewer raises valid concerns about running time comparison and robustness evaluation against adaptive attacks, suggesting concrete steps for improvement such as comparing with SOTA methods and referring to a checklist for thorough evaluation. The reviewer's points are logically connected and contribute to a constructive critique without any self-contradiction."
        ]
    },
    {
        "paper_id": "iclr_2022_g5odb-gVVZY",
        "paper_title": "Multilevel physics informed neural networks (MPINNs)",
        "paper_abstract": "In this paper we introduce multilevel physics informed neural networks (MPINNs). Inspired by classical multigrid methods for the solution of linear systems arising from the discretization of PDEs, our MPINNs are based on the classical correction scheme, which represents the solution as the sum of a fine and a coarse term that are optimized in an alternate way. We show that the proposed approach allows to reproduce in the neural network training the classical acceleration effect observed for classical multigrid methods, thus providing a PINN that shows improved performance compared to the state-of-the-art. Thanks to the support of the coarse model, MPINNs provide indeed a faster and improved decrease of the approximation error in the case both of elliptic and nonlinear equations.\n",
        "review_ids": [
            "xKhfT6Qskki",
            "JhgksPiQIp",
            "7iTLLHB4QF8",
            "K-A1LliBGsY",
            "gCoe9kW5Fgj",
            "DfCfhaSizsh",
            "890mE_rmELb"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for responding to my review. Although I would like to emphasize that the method is interesting and could have potential, and I appreciate the updates you made to the paper, the current setup and experiments are still not convincing to me and I would like to stick with my decision of rejection. I believe the changes required are major enough that I cannot justify acceptance. But for a future resubmission, here are my comments:\n\nMedian and IQR:\n- Thanks for updating the paper to include these. I noticed though that often times the IQR is larger than the median? As an example in Table 2. It seems like the results are wildly varying for all the networks across different initializations. It would be very helpful to include a visualization to understand how much the IQR is compared to the median, without having to look at the numbers carefully.\n\nOperation count vs RMSE:\n- It seems like in the answer to another reviewer, you acknowledged that the RMSE difference is not major enough, and that the focus should be operation count. In that case, you should include a detailed description of how operation count was defined, and include it in all of your tables of results. Also, since these are run on the same computer, would it be possible to compare real-time as well? That would give a much more direct comparison.\n\nHyperparameter tuning:\n- Since it is known that NN performance is extremely dependent on hyperparameters, these should be carefully tuned (using grid-search, random-search, bayesopt) for each different architecture/training method. Specifically, I would have liked to see weight decay parameters and learning rate tuned. I don't think hand tuning is sufficient in this case unfortunately, especially when comparing training iterations. Learning rate + weight decay could have a huge influence on training iterations.",
            " Thanks for addressing my concerns, but your response raised a few more questions.\n\n* You are saying that the MPINNs is able to provide the result in less iterations and therefore with less operations. This statement is questionable, since the MPINN requires more function evaluations. It would be better to compare the performance/convergence in terms of the number of function evaluations (NFE). Also, can you comment on the number of floating point operations per iterations for the MPINN and the standard PINN.\n\n* Do I understand you correctly that you optimize the learning rate for MPINNs on the model using h=50, H=25? Then you use the same learning rate for training PINNs and for models of varying sizes?  I feel that you need to provide more detailed experiments and evaluate both models over a range of learning rates to show that one model is less sensitive to the particular choice of the learning rate. At this point your statement that MPINNs are less sensitive may or may not be valid. It is hard to tell based on the current experiments. \n\n* I feel that there should be at least one set of experiments for which all models are fine-tuned, i.e., do a careful grid search. It is somewhat strange to fix the learning rate, and use the same learning rate for all models, even if you goal is to show that your model requires less iterations. Accuracy is important and currently it looks like that the PINNs are more accurate if fine-tuned. Please convince me otherwise by showing that the MPINN can match the accuracy of a fine-tuned PINN. ",
            "This paper proposes multilevel physics informed neural networks (MPINNs). When compared to standard PINNs, MPINN uses additional networks for different levels with fine and coarse terms in the view of two-levels. By reclusive definition, this can be generalized to any multi-levels. The method is motivated via classical multigrid method, and is empirically studied with 3 simple ODEs/PDEs.     The method of MPINNs makes sense and seems to be novel. Although it is motivated via the classical multigrid method with the operators P and R, the final method is a very simple alternate optimization with two sub-networks (for two levels) where one sub-networks minimizes the loss for \u2018fine\u2019 points, and another minimizes the loss for \u2018coarse\u2019 points. Because the solution of PDEs can contain fast/fine components as well as coarse/slow components, the proposed method makes sense and I would expect that this works well for such PDEs. In this regard, I really like the paper and recommend acceptance subject to very minor revisions for several concerns that I explain below. \n\nWhile the idea and method are great, the demonstration of the method can be improved a lot. The present experimental results are somewhat inconclusive because of the experimental settings and the definitions of the plotted values.  For the experimental setting, the authors mention that the learning rates are experimentally tuned, and the best-decreasing strategy is adopted for each network. This can be problematic without presenting more details, as the learning rates can be the reason for the reported superior performance of MPINNs. I would recommend reporting the results with various learning rates or fixing the learning rates a priori by directly using the learning rates of previous work of PINNs. In the current form, I cannot tell if we are overfitting the data and problems with the tuning of learning rates of MPINNs. At least, the authors should report the learning rates for each case.\n\nThe idea is very related to extended physics informed neural networks (XPINNs), which was theoretically studied by Hu et al [1]:\n\n[1] Hu et al. When Do Extended Physics-Informed Neural Networks (XPINNs) Improve Generalization?\n\nThe previous work [1] has theoretically and empirically shown that XPINNs can improve standard PINNs by using sub-networks, each for different subdomains with different complexities of the sub-solutions. This is very related to MPINNs. Instead of decomposing the domain space, MPINNs are decomposing the solution components for fine and coarse parts. Therefore, the authors should cite the previous work [1] to mention this relationship. Moreover, unlike the previous work on XPINNs [1], the present paper does not provide any theoretical results, which should be also mentioned as a limitation of MPINNs, when compared to XPINNs, at this moment. In sum, the relationship and the relative limitation should be made clear.\n\nFor the definitions of the plotted values, the authors should use a metric that can be comparable for all methods. Instead of these loss and objective used for training, the authors can plot the value of errors for the solution of PDEs, by using a set of new unseen data points. \n\nWhat are the precise definitions of RMSE and loss used in the tables and figures?\n\nI am not entirely convinced by the results where the performance degrades as we increase h and H. Do you observe the same phenomena with BFGS instead of ADAM? It is possible that this can be caused by bad training practices by the authors. I would suggest plotting the training loss over epoch for different h and H as well to see what is going on there.\n\nThe paper makes several random claims without any evidence or support. I recommend deleting those claims from the paper. For example, the authors say that it shows improved performance compared to the state-of-the-art, but no state-of-the-art performance is cited or compared against in this paper. After Table 1, it reads \u201cWe observe that MPINNs clearly outperform classical PINNs.\u201d, but it is not clear at all because of the uncertain learning rates and the stopping criteria. The following claim is also not supported by any evidence in this paper and it is an opinion of the authors instead of a finding in a scientific paper: \u201cIt is clear that structuring the parameters in a clever way is more beneficial than just augmenting their number, to gain both computational time and expressivity.\u201d\n\nIn table 2, F is not used and the following sentence should be deleted in its caption: \u201dF\u201d means a failure in all the runs. \n\nIn table 4, is \u201c0.04.6e-2\u201d typo? On page 5, \u201cthe and of\u201d should be replaced by \u201cthe end of\u201d. \n\n While there are several concerns and weak evidence to support the main claims, the idea is very novel and significant. I infer from the method and its idea that the additional evidence to support the main claims will come by sooner or later, either by this paper or other authors. There are so many incremental papers without any originality that have been published just because of large-scale experiments for demonstrations. In contrast, this paper has weak demonstrations but a good original idea. Whereas both types of papers have their own merits, I think that the latter type contributes to our community, science, and academia in a longer-term. Accordingly, I recommend this paper for acceptance, subject to minor revisions. ",
            " My concern is all addressed. Thank you! ",
            "The authors adopt ideas from multigrid methods in solving PDEs to Physics Informed Neural Networks (PINNs). After giving a standard introduction to PINNs and Multigrid methods, they describe their approach.\n\nThey decompose the PDE solution as a sum of a coarse and a fine term, and train independent PINNs to learn each term. The coarse PINN is chosen to have smaller capacity than the fine PINN, and is also (potentially) trained on fewer datapoints on the domain. Training proceeds by alternating training between the two PINNs: $\\nu_1$ epochs on the fine PINN, followed by $\\nu_2$ epochs on the coarse PINN, and repeat.\n\nThe authors test their method on a linear non-homogeneous 1-D elliptic equation (trained with Adam or BFGS), a 2-D nonlinear equation, and 1-D Burger's equation. They compare a standard PINN vs a larger PINN vs their method (MPINN). Multigrid training for PINNs could be potentially interesting. In particular, I agree with the motivation of the authors that the particular structure of MPINNs could make training easier by imposing a kind of inductive bias. That being said, the results in this paper are too preliminary to be accepted.\n\n1-D Linear Elliptic equation:\n- It seems like the two PINNs you use only differ by capacity of the network. I thought the main motivation for Multigrid was that the PINN with less capacity would be trained on fewer points?\n- The equation is chosen so that you get two different modes in the solution. It would be interesting to vary $\\alpha$ to see whether MPINN training helps as the difference in frequency between the modes increases. I feel like the main motivation for this toy problem could be the different modes. It would be interesting to see it for more difficult parameter settings rather than only $\\alpha = 3$ or $5$.\n- Why the distinction between Adam for the 1-layer networks and BFGS for the 4-layer networks? Since BFGS uses n^2 memory, it seems Adam would be more appropriate for larger networks.\n- How were the learning rates for the different problems tuned? I am not entirely convinced that the difference in performance is not from using different learning rates / hyperparameters for the different problems.\n- You mention that MPINNs are more robust, but I am not sure I agree with the definition of robustness. It seems you are defining a budget of training epochs, and if the method does not converge in time, it is considered not robust. This would just mean the method is \"slower\", but not necessarily \"less robust\".\n- It seems you are reporting mean + std of the RMSE. Can you report median + IQR instead?\n\n2-D Nonlinear equation:\n- Could you give some intuition on what the equation represents and what the solution looks like?\n- 400 points vs 484 points does not seem like much of a difference between \"fine\" and \"coarse\". How did you choose those numbers?\n- Results do seem less convincing in this case.\n\nBurger's equation:\n- What optimizer did you use?\n- It seems you are picking different networks for each PDE. Here you have 3 hidden layers. How did you choose these sizes? The framework is potentially interesting, but the experiments in the paper do not convince me. It would be helpful to see more difficult problems, as well as multilevel problems with n > 2. Furthermore, there are a lot of details on hyperparameter tuning that is missing, leaving the question open whether the stated performance improvements are due to the hyperparameters or the MPINN idea.",
            "This paper proposes a new physics informed neural network (PINN) that is motivated by multigrid methods. The idea of the proposed multilevel PINN is to write the solution for a given problem as sum of two terms, where the first term models fine-scale structures and the second term models coarse-scale structures. This approach yields models that converge faster and reduce the approximation error as compared to standard PINNs. The performance is demonstrated on both 1D and 2D problems. Using multigrid methods as inspiration for training and designing PINNs is neat. Splitting the solution into two terms is interesting, because this scheme presents the model with an easy and a hard task (learning fine-scale structure is typically more challenging than learning coarse-scale structures). The proposed scheme can also be viewed as some form of data augmentation and it would be interesting to discuss this aspect in more detail. Further, it would be interesting to discuss whether there is some connection between the proposed multilevel learning scheme and Curriculum Learning.\n\nOverall, the presentation of the materials is very clear and the paper reads nicely. However, a major shortcoming of the paper are experiments. \n\nI would like to see the following questions addressed:\n\n* How do you construct $z_h$ and $z_H$? How do you determine whether $z_H$ is not expressive enough? Is this an automatic process or does this require manual tuning? How is the performance affected if $z_H$ is varied from less coarse to more coarse?\n\n* How does the proposed multilevel PINN compare to other state-of-the art PINNs or to classical scientific computing methods for the problems under consideration? For papers that have mainly an empirical flavor it is typically good practice to show some benchmark studies. Burger's Equation is often considered as a standard problem, hence it should not be too difficult to provide some baselines for this problem. \n\n* In Table 2 and 3, the difference between the proposed model and the ablation models appear to be marginal in terms of the RMSE. For example, is there a practical difference between 1.0e-3 and 1.2e-3 for the problems under consideration?\n\n* Why does PINN h and h+H start to fail if the number of parameters are increased? Do you regularize these models?\n\n* How did you chose the tuning parameters for the different models? I played around with the provided notebook \"MPINN_2D\" for a few minutes and I was able to improve the performance of the standard PINN quite a bit by slightly increasing the learning rate (2e-3), introducing a small amount of weight decay (5e-3) and switching off the learning rate scheduler. The PINN_15 achieves now an RMSE of 0.092 as compared to 0.14 by the proposed multilevel PINN. I am pretty sure that the performance can be further improved with some more careful fine-tuning of the hyperparamters. Tuning models are crucial and this needs to be addressed.\n\n* The term 'robustness' typically means something different in the ML community. I am not exactly sure what you mean by `increased robustness of MPINNs'. This paper introduces a new multilevel PINN for learning solutions of PDEs. The presented ideas are novel and of interest to the scientific machine learning community. One shortcoming of this paper is that the experiments appear to be too preliminary, i.e., no benchmarks are provided and the models are not carefully fine-tuned. This makes it difficult to judge the advantage of the proposed method as compared to other state-of-the-art PINNs. At this point I feel that the paper is slightly below the acceptance threshold, but I am willing to change my score if the authors can address my questions above. ",
            "The manuscript entitled \u201cMULTILEVEL PHYSICS INFORMED NEURAL NETWORKS\n(MPINNS)\u201d introduces multilevel physics informed neural networks (MPINNs), which is inspired by classical multigrid methods for the solution of linear systems arising from the discretization of PDEs. The authors showed MPINNs provide a faster and smaller approximation error in the case both of elliptic and nonlinear equations and a more robust asymptotic behavior.\n This paper is not well written in English. But since this MPINNs show interesting point that it can run faster and provide smaller approximation error for some PDE equations than using original PINNs, I recommend this paper to be published after polishing the language and also addressing all my below comments.\n\nMy comments are as follows:\n\n1.\tIn the third line on page 5, the formula for R and P has extra \u2018)\u2019 at the end. On the same page after the Equation (6c) line, it should be \u2018at the end of each cycle\u2019 instead of \u2018at the and of each cycle\u2019. In this paper, there are too many \u2018really\u2019 and \u2018then\u2019.\n2.\tIn this paper, all the experiments are done with h=2*H. Why do you choose 2 here? How does this number matter for the MPINNs training? Some explanations about how to choose this number should be added.\n3.\tOn page 6, the authors state \u201cIndeed, when the number of parameters increase the peculiar structure of the MPINN allows to make the training process easier and as a result the network reaches a better approximation.\u201d The authors should explain why MPINN works better instead of just stating the observation.\n4.\tOn page 7, on the last two lines the authors say \u201cIn 2D problems the effect of the size of the training set on the cost is far more important than in the 1D case, therefore we train the fine network on a grid of 484 points and the coarse one on a coarser grid of 400 points\u201d. What is the rule of thumb to choose the grid size? \n5.\tOn page 8, the Figure 3 uses h=10 case while in Table 3 it starts with h=30. Since Figure 3 and Table 3 are talking about the same experiment \u201c2D NONLINEAR EQUATION\u201d, why do you choose different h for table and plot?\n This paper proposed the multilevel physics informed neural networks (MPINNs), which is inspired by classical multigrid methods for the solution of linear systems arising from the discretization of PDEs. It showed some interesting results, but those results are lacking of explanation in depth. The authors are mostly just stating the observations, which is not enough to meet the standards of a good paper. \n\nIn summary, I would not recommend its publication in ICLR until all my comments are addressed.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer maintains their rejection decision due to concerns about the experimental setup and results, indicating a negative overall assessment. Phrases like \"not convincing to me\" and \"stick with my decision of rejection\" explicitly convey this negative sentiment.",
            "The review expresses concerns and questions about the author's response, using phrases like 'questionable statement,' 'need to provide more detailed experiments,' 'may or may not be valid,' and 'convince me otherwise.' This indicates a critical and skeptical sentiment.",
            "The reviewer states \"I really like the paper and recommend acceptance subject to very minor revisions\". They also mention the idea is \"very novel and significant\" and that the paper has \"a good original idea\". Despite criticisms, the overall sentiment is positive, leaning towards acceptance.",
            "The reviewer expresses that their concerns have been addressed and uses the phrase \"Thank you!\" indicating satisfaction.",
            "The reviewer states that the results are \"too preliminary to be accepted\" and that the experiments \"do not convince me.\" They also express skepticism about the performance improvements being due to the method rather than hyperparameter tuning.",
            "The review identifies a 'major shortcoming' in the paper's experiments, stating they are 'too preliminary' and lack benchmarks and careful fine-tuning. The reviewer expresses difficulty in judging the advantage of the proposed method. The conclusion is that the paper is 'slightly below the acceptance threshold'.",
            "The reviewer identifies several issues, including poor writing ('not well written in English'), lack of explanation ('lacking of explanation in depth'), and insufficient justification for choices made in the experiments. The final recommendation is not to publish until the comments are addressed, indicating a negative overall assessment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Supportive",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a critical tone by pointing out specific weaknesses in the paper, such as the large IQR compared to the median, the lack of detailed operation count description, and the insufficient hyperparameter tuning. The use of phrases like \"I don't think hand tuning is sufficient\" contributes to this critical tone.",
            "The tone is critical due to the use of phrases like 'questionable,' 'hard to tell based on the current experiments,' 'I feel that you need to provide more detailed experiments,' and direct challenges such as 'Please convince me otherwise.' These suggest the reviewer is pointing out weaknesses and demanding further evidence.",
            "The review presents both positive aspects (\"the method of MPINNs makes sense and seems to be novel\", \"I really like the paper\") and critical feedback (\"The present experimental results are somewhat inconclusive\", \"I am not entirely convinced by the results\"). The reviewer also offers constructive suggestions for improvement, indicating a balanced perspective.",
            "The reviewer expresses gratitude with \"Thank you!\", indicating a supportive and appreciative tone.",
            "The review is critical, pointing out several weaknesses in the experimental setup, missing details on hyperparameter tuning, and lack of convincing results. Phrases like \"I am not entirely convinced,\" \"Results do seem less convincing,\" and \"the experiments in the paper do not convince me\" indicate a critical tone.",
            "The review poses several pointed questions about the methodology and experimental setup, highlighting areas where the paper falls short. Phrases like 'major shortcoming,' 'difficult to judge,' and 'too preliminary' indicate a critical assessment. The reviewer also provides specific examples from their own experimentation to demonstrate potential flaws in the hyperparameter tuning.",
            "The review contains direct criticisms and specific requests for improvement. Phrases like 'In this paper, there are too many 'really' and 'then'', 'The authors should explain why MPINN works better instead of just stating the observation', and 'I would not recommend its publication in ICLR until all my comments are addressed' demonstrate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer clearly states their rejection decision while providing constructive feedback for improvement in a future resubmission. The reviewer acknowledges the potential of the method and appreciates the updates, but maintains that the current setup and experiments are not convincing. The comments provided are all aimed at strengthening the paper and do not contradict the overall rejection stance.",
            "The review is consistent because all points raised by the reviewer are logically connected and focus on the need for more rigorous experimental validation and clarification of claims regarding the MPINN method compared to standard PINNs. The reviewer consistently asks for more detailed experiments, fair comparisons, and fine-tuning to properly assess the performance and advantages of MPINNs.",
            "The review is consistent because it acknowledges the novelty and potential of the proposed method while also pointing out weaknesses in the experimental validation and presentation. Despite the identified shortcomings, the reviewer consistently recommends acceptance with minor revisions, emphasizing the significance of the idea over the current limitations in its demonstration.",
            "The review expresses a clear and consistent message of satisfaction, stating that the reviewer's concerns have been addressed. There are no contradictory statements or points of confusion within the review.",
            "The review is consistent in its critique, acknowledging the potential of the method but consistently highlighting weaknesses in the experimental validation, lack of clarity in methodology (hyperparameter tuning, network choices), and the preliminary nature of the results. The reviewer's overall assessment that the results are 'too preliminary to be accepted' is well-supported by the detailed questions and concerns raised throughout the review, focusing on the need for more rigorous experiments and justifications.",
            "The review is consistent because it highlights both the strengths (novelty, clear presentation) and weaknesses (experiments) of the paper. The reviewer appreciates the idea and presentation but points out the lack of rigorous experiments and benchmarks to validate the proposed method. The questions raised are all focused on addressing the experimental shortcomings, and the final assessment aligns with these points, indicating a willingness to reconsider the score if the authors address the experimental concerns.",
            "The review starts by recommending publication after addressing comments and polishing the language, suggesting a conditional acceptance. However, the summary states 'I would not recommend its publication in ICLR until all my comments are addressed,' which is a clear rejection until major revisions. This shift from a conditional acceptance to a rejection makes the review inconsistent in its overall recommendation."
        ]
    },
    {
        "paper_id": "nips_2021_ChWy1anEuow",
        "paper_title": "Risk Bounds for Over-parameterized Maximum Margin Classification on Sub-Gaussian Mixtures",
        "paper_abstract": "Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can fit the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this \"benign overfitting\" phenomenon of the maximum margin classifier for linear classification problems. Specifically, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classifier in the over-parameterized setting. Our results precisely characterize the condition under which benign overfitting can occur in linear classification problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression. \n",
        "review_ids": [
            "lIsVgrQeJek",
            "CgsnrO9qqgS",
            "0O57raFrAc4",
            "XPKaPAx6u4K",
            "4Czf1Pq_m4Z",
            "whAFJQR76kL"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for addressing my concerns. I am keeping my score. ",
            " Regarding point 5, in your \"corrected\" condition above, the LHS should be squared, $\\lVert \\mu \\rVert_2^2 \\geq C \\lVert \\mu \\rVert_\\Sigma$?\n\nRegarding point 3, are there any known results on estimating the mean vector in the regime considered in this paper (e.g., $d\\geq n^2$ in the isotropic case)?",
            "This paper studied the classification error of max margin linear classifier in high dimensional Sub-Gaussian mixtures. Tight upper and lower bounds are derived, which generalizes the prior works on Gaussian Mixtures.\n  This paper studied the classification error of max margin linear classifier in high dimensional Sub-Gaussian mixtures. This is an important problem, due to its connection to the implicit bias of first order algorithms, interpolation and the double descent phenomena. \n\nTight upper and lower bounds are derived, which generalizes the prior works on Gaussian Mixtures. Overall, this paper is well-written, the results are solid and I enjoyed reading it. A few comments are in order:\n\n(1) It might be better to expand the prior works section to include some relevant papers on high dimensional Gaussian Mixture. For example, the exp(- |mu|^4/(|mu|^2+d/n)) -type risk bounds also appeared in high dimensional Gaussian clustering literature, e.g.\n\nhttps://arxiv.org/pdf/1812.08078.pdf\n\nhttps://arxiv.org/pdf/2006.14062.pdf\n\nand many other references therein.\n\n\n\n(2) A major limitation I see from this work, is that its analysis heavily relies on the equivalence between max margin classifier and min-norm interpolator, see [19]. As the authors already mentioned in this paper (e.g. corollary 3.3), for example, in the text book example of isotropic Gaussian mixture with a constant separation, this can only happen when d > n^2, or the number of samples n is at most sqrt(d). In contrast, the n/d = constant setting is much more standard, and in certain sense more interesting. In my opinion, the results in this paper are far from a complete picture about max margin classifier - In other words, they are really about min-norm interpolator, which happens to be equivalent to max-margin classifier in the small sample regime (and therefore the title is a little bit misleading). \n\nOverall, my recommendation is weak accept - any comments and clarifications from the authors are welcome. -",
            "This paper proves new risk bounds for overfitting in the over-parameterized linear classification setting when the data is generated by a sub-Gaussian mixture. In certain regimes, the results show that overfitting can be benign, that is, overfitting still leads to a good test error. An important property of the proposed upper bound on the risk is that it is not directly dependent on the dimension, but rather on certain norms of the covariance matrix. This is useful because, as shown in the paper, if the eigenvalues of the covariance matrix decay fast enough, then requirements for benign overfitting become dimension independent.  This paper proves new risk bounds for overfitting in the over-parameterized linear classification setting when the data is generated by a sub-Gaussian mixture. In certain regimes, the results show that overfitting can be benign, that is, overfitting still leads to a good test error. An important property of the proposed upper bound on the risk is that it is not directly dependent on the dimension, but rather on certain norms of the covariance matrix. This is useful because, as shown in the paper, if the eigenvalues of the covariance matrix decay fast enough, then requirements for benign overfitting become dimension independent.\n\nThis paper is good step towards understanding benign overfitting, even though the paper only considers linear classification in sub-Gaussian mixture setting. Corollary 3.5 proves that even if the eigenvalues of the covariance matrix do not decay very fast, the requirement for small test error scale sub-linearly with dimension. In particular, as long as the the number of samples is a large enough constant, and the means of the classes are separated by $d^{1/4}$, the risk of maximum margin classifier is $o(1)$. \n\nOverall, the paper is well written, and the usefulness of the theorem is well illustrated with examples. The intuition is explained well and the theorem proof in the main paper is also helpful.\n\nMy first concern about the results is that the requirement $\\text{tr}(\\mathbf{\\Sigma})\\geq C \\max (n^{3/2}||\\mathbf{\\Sigma}||_2,n||\\mathbf{\\Sigma}||_F)$, which in the isotropic Gaussian case implies that the theorem requires $d\\geq n^2$. Thus, the result is only applicable to extremely over-parameterized setting. Is this requirement a necessary condition for such results? In the mild over-parameterized setting (say $d\\geq 100 n$), would overfitting no-longer be benign? This can be the case, for example, if we keep the data distribution fixed and keep increasing the number of samples $n$.\n\nAnother concern that I have is whether these results can be transferred to the setting of deep learning. In the linear classification setting considered in this paper, the results are essentially for the maximum margin classifier, with the motivation being that asymptotically, SGD converges to the maximum margin classifier. This is not the case for deep learning, where if we just benignly overfit the data, then there still are adversarial examples. Hence, overfitting in deep learning does not lead to maximum margin classifiers usually. Given that benign overfitting still leads to a good test accuracy for deep networks, an important question is whether the explanation offered by this paper is still valid for deep learning. Please see the main review.",
            "The paper studies risk bounds for the hard-margin SVM binary classification problem in the over-parametrized regime, where the number of samples is much smaller than the dimension of the instance space. It is assumed that the samples follow a distribution corresponding to a mixture of two sub-gaussian components, one component for each label. These components are assumed to have the same covariance matrix Sigma and opposing means mu and -mu. The main results of the paper are upper and lower bounds for the risk of the SVM classifier as trained on the data. By previous results, this also has implications to the minimum norm solution learned by gradient decent. The derived bounds are tighter than previous bounds and extend to the anisotropic setting. The lower bound derived matches the upper bound in some regimes of the parameters, suggesting optimality. They also discuss in some detail the role played by the eigenvalues of Sigma in the risk bounds, showing a nice separation to two regimes.  The paper is well written and easy to follow and the results seem technically correct. The results, however, are somewhat incremental as compared to previous work on the subject, hence my relatively low score.\n\nSome questions/comments: \n\n- In the introduction, the results are presented without the dependence on $\\Sigma$, stating rates that are not scale invariant as one would expect.\n\n- You assume that the distribution is centered. How does your bounds change when one also needs to estimate the center of the mixture, or equivalently, learn an additional bias parameter in the SVM classifier.\n\n- Please give the formal definition of the sub-gaussian/exponential norms, or at least state an appropriate reference where they are mentioned (line 104).\n\n- Is the condition $||\\mu||_2 \\geq C||\\Sigma||_2$ in Lemma 4.4 correct? It is not scale invariant.\n Yes.",
            "The paper studies maximum-margin classifiers in the setting where the dimension of the data is much larger than the number of samples. This data is assumed to come from a mixture of two subgaussians under a standard generative model. Notably, prior work shows that max-margin classifiers (i.e. Support Vector Machines) return the same estimator as the minimum-norm solution to an underconstrained least squares problem. The research mostly pertains to analyzing that least squares solution.\n\nThe paper focuses on providing sharper guarantees on when exactly the SVM is equivalent to the LS solution, and when this LS solution has low risk/misclassification probability. Their guarantees depend exclusively on the sub-gaussian data's true covariance matrix, the distance between the two subgaussian modes, and the number of samples. Notably, this does *not* depend explicitly on the dimension of the data. So, we can better understand how the spectrum of the covariance matrix impacts the misclassification probability.\n\nSome complementary lower bounds are given.  ### Overall Impression\nThis paper was a pleasure to read, and I confidently recommend it for publication.\nFurther, I verified much of the math in the appendix and found no errors.\n\n### Detailed Review\n\nThe setting of the paper is interesting. The focus on characterizing as-much-as-possible in terms of the covariance matrix is solid and pays off well. The structure of the paper is very clear and easy to read, providing intuitive interpretations of the core theorems along with a high-level overview of the proof.\n\nThe background information in the introduction is also well presented. While I am familiar with all the technical tools used in this paper, I had not read papers on benign overfitting in the past. Nevertheless, I was able to quickly understand the core ideas and felt comfortable reading this paper.\n\nI do have some questions and feedback for the authors:\n1. How fundamental is the covariance matrix's trace lower bound assumption(s)? You mention that the core theorems only assume the trace is large enough, but that the risk bound itself does not depend directly on this trace. Do we have reason to think this trace assumption is fundamental or necessary? Of all metrics, why is trace meaningful here?\n1. Intuitively parse the hard constraints on the trace of the covariance matrix in Theorem 3.1 [Line 167]. What kinds of covariance matrices fail this check? Do they have a flat spectrum? Does the sample complexity $n$ limit how flat the spectrum can be? [Lines 209 - 233] get close to discussing this, but fall short of explaining what covariance matrices / sub-gaussian structures fail this test, and how that depends on $n$.\n    - For example, Theorem 3.1 requires $n \\leq \\frac{tr(\\Sigma)}{\\\\|\\Sigma\\\\|_F}$ amongst other constraints. Is this a reasonable way to interpret the hard constraint? What does this really say about $\\Sigma$?\n1. Consider removing the line about logistic regression from your abstract. You don't really provide any new guarantee there.\n1. You should mention in the body of the paper that there are experiments in the appendix.\n1. The appendix is ordered in a hard-to-read way. I constantly found myself flipping back-and-forth between pages to remember what Lemma X.X refers to. Further, finding the proof of a specific lemma takes a while. Lastly, the order of proofs makes it hard to jump to the proofs that interest me. I would recommend a few changes:\n    - At the beginning of *every* proof in the appendix, fully restate the lemma/theorem/corollary.\n    - When you state a lemma without proof, link to its proof.\n    - List all defined symbols at the beginning or end of the appendix. There's a lot of symbols, and some like $\\varepsilon_\\lambda$ are defined within some lemma statements, which is easy to skip over on accident.\n    - Consider just reordering the entire appendix to be fully bottom-up? Just start with the lemmas that requires no other lemmas, and build up from there?\n1. All experiments in this paper would benefit from error bars. Consider highlighting the area between $25^{th}$ and $75^{th}$ quartiles on each plot? Figure 2(d) at $\\\\|\\mu\\\\|_2^2 = 4,6$ might be a bit non-linear, and some error bars would dissuade my concern, for example. It would also resolve the weird bump at $\\\\|\\mu\\\\|=2,\\alpha=0.8$ on Figure 2(b).\n\nSome typos and smaller confusions:\n1. [Line 64] \"our result shows that to achieve $o(1)$ population risk\" If $n$ is constant, what is this little-o depending on? $d$?\n1. [Line 293] The details are in Appendix A.1, not Appendix 4.1.\n1. [Line 319] Consider trying to unify the $Q=Z\\Lambda^{1/2}V^\\intercal$ notation with the content of [Line 120]. Maybe introduce the $Z$ notation there as an equivalent characterization? It's a little confusing as written, since this notation hasn't been introduced earlier, so it takes extra effort to verify. It would be more clear if I thought about this characterization when $Q$ was first defined and fresher in my memory.\n1. [Line 605] Consider justifying $\\\\|\\mu\\\\|_2^2 \\geq \\mu^\\intercal Q^\\intercal (QQ^\\intercal)^{-1} Q\\mu$, which I believe follows from $Q^\\intercal (QQ^\\intercal)^{-1} Q$ being a hat matrix?\n1. [Line 612] I don't think $v_i$ was defined anywhere.\n1. [Line 642] Unmatched parenthesis in the definition of $J_2$.\n1. [Line 733] \"the dependency of\" should be removed\n1. [Line 747] Should read \"holds\" not \"hold\" The limitations discussed are fine and sufficient. Admittedly, they are not particularly creative or interesting.\n\nSocietal Impact: Not applicable."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude (\"Thank you for addressing my concerns\") and indicates satisfaction by maintaining their original score (\"I am keeping my score\").",
            "The review poses direct questions and seeks clarification without expressing strong positive or negative opinions.",
            "The reviewer states they \"enjoyed reading it\" and ultimately gives a \"weak accept\" recommendation, indicating a generally positive sentiment despite pointing out limitations.",
            "The review expresses a generally positive assessment of the paper, highlighting its contributions to understanding benign overfitting, its clear writing style, and well-illustrated examples. The reviewer acknowledges the usefulness of the theorem and the helpfulness of the intuition and proof.",
            "The reviewer states that the paper is well-written, easy to follow, and the results seem technically correct. They also mention that the derived bounds are tighter than previous bounds and extend to the anisotropic setting.",
            "The reviewer states that the paper was a 'pleasure to read' and 'confidently recommend[s] it for publication.' This strong endorsement indicates a positive sentiment."
        ],
        "tone": [
            "Supportive",
            "Formal",
            "Balanced",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer is supportive, showing appreciation for the author's efforts and confirming their initial assessment remains unchanged, suggesting agreement and approval.",
            "The language used is technical and employs mathematical notation, indicating a formal and academic tone. Phrases like \"Regarding point 5\" and \"are there any known results\" contribute to this formality.",
            "The review acknowledges the paper's strengths (\"well-written, the results are solid\") while also pointing out limitations and suggesting improvements. The reviewer uses phrases like \"A major limitation I see\" and \"In my opinion\" to express their concerns constructively. The final recommendation of \"weak accept\" also indicates a balanced perspective.",
            "The review presents both positive aspects of the paper and specific concerns, maintaining a balanced perspective. Phrases like 'This paper is a good step...' and 'Overall, the paper is well written...' indicate appreciation, while the detailed questions and concerns about the results and their applicability demonstrate critical engagement.",
            "The review acknowledges the paper's strengths (well-written, technically sound, improved bounds) while also pointing out its limitations (incremental results) and raising specific questions and concerns. This blend of positive and critical feedback indicates a balanced tone.",
            "The reviewer expresses enthusiasm for the paper ('pleasure to read'), provides constructive criticism and suggestions for improvement, and ultimately recommends publication. This indicates a supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses gratitude for addressing concerns and states they are keeping their score. This is consistent as addressing concerns does not necessarily imply a score change, and maintaining the score after revisions is a coherent stance.",
            "The review is consistent as it raises valid questions and suggestions for improvement without any contradictory statements. Both points are asking for clarification and suggesting corrections, and they do not contradict each other.",
            "The review is consistent because it presents both positive aspects of the paper (well-written, solid results, important problem) and a significant limitation (heavy reliance on specific conditions that limit the generalizability and scope of the findings). The final recommendation of 'weak accept' logically follows from this balanced assessment, acknowledging the paper's contributions while pointing out a key weakness.",
            "The review is consistent because it first acknowledges the paper's contributions and strengths, then raises valid concerns and questions about the limitations and scope of the results. The reviewer does not contradict themselves but rather provides a balanced assessment, highlighting both the positive aspects and areas for further consideration or clarification.",
            "The review is consistent because it acknowledges the strengths of the paper, such as being well-written, easy to follow, and technically correct, while also pointing out a weakness, that the results are somewhat incremental. This weakness is presented as the reason for the relatively low score, which is a consistent judgment. The questions raised are requests for clarification and further improvement, not criticisms that contradict the positive aspects mentioned.",
            "The review is consistently positive and constructive. The reviewer recommends publication and provides suggestions for improvement, without contradicting their overall positive assessment."
        ]
    },
    {
        "paper_id": "iclr_2022_FCxWzalZp9N",
        "paper_title": "AF$_2$: Adaptive Focus Framework for Aerial Imagery Segmentation",
        "paper_abstract": "As a specific semantic segmentation task, aerial imagery segmentation has been widely employed in high spatial resolution (HSR) remote sensing images understanding. Besides common issues (e.g. large scale variation) faced by general semantic segmentation tasks, aerial imagery segmentation has some unique challenges, the most critical one among which lies in foreground-background imbalance. There have been some recent efforts that attempt to address this issue by proposing sophisticated neural network architectures, since they can be used to extract informative multi-scale feature representations and increase the discrimination of object boundaries. Nevertheless, many of them merely utilize those multi-scale representations in ad-hoc measures but disregard the fact that the semantic meaning of objects with various sizes could be better identified via receptive fields of diverse ranges. In this paper, we propose Adaptive Focus Framework (AF$_2$), which adopts a hierarchical segmentation procedure and focuses on adaptively utilizing multi-scale representations generated by widely adopted neural network architectures. Particularly, a learnable module, called Adaptive Confidence Mechanism (ACM), is proposed to determine which scale of representation should be used for the segmentation of different objects. Comprehensive experiments show that AF$_2$ has significantly improved the accuracy on three widely used aerial benchmarks, as fast as the mainstream method.\n",
        "review_ids": [
            "nE5pgBMDlOY",
            "3LiMiZWMNu",
            "EMqCsONEbnA",
            "SHri1GvrZ3"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "In this paper, the Adaptive Focus Framework (AF2), which adopts a hierarchical segmentation procedure and focuses on adaptively utilizing multiscale representations generated by widely adopted neural network architectures, is proposed. Particularly, a learnable module, called Adaptive Confidence Mechanism (ACM), is proposed to determine which scale of representation should be used for the segmentation of different objects. \nThis paper proposes  Adaptive Focus Framework (AF2) to deal with semantic segmentation task of aerial imagery. The strengths of this paper is that the writing is clear and easy to understand. The structure of this paper is ok. The weakness is that the idea is simple and not novel enough. In addition, the experiments cannot verify the effectiveness of the proposed method. The proposed AF2 tries to solve HSR aerial imagery semantic segmentation by making use of multi-scale representations. The whole  Adaptive Focus Framework (AF2) is simple, including Hierarchical features extractor, Preliminary predictor and Adaptive confidence mechanism for prediction selection (ACM).  \nAs the first two modules are popular in semantic segmentation and other related works, the main contribution of this work is the proposed ACM, which judges whether the prediction for each pixel is sufficiently confident or not in each level.  But actually, the proposed ACM is simple lacking novelty. The experiments in this work cannot verify the effectiveness of proposed method solidly. In Table 1, AF2-AFPN brings slightly better performance compared with PointFlow. On Potsdam, its performance is even worse. For general segmentation benchmark, only SFPN method is reported and compared, but in PointFlow, it has mIoU 80.3% on Cityscape. \n\nAll in all, i think this paper is not novel and solid enough to be accepted by ICLR.\n ",
            "This paper proposes a hierarchical segmentation procedure for aerial imagery semantic segmentation. The proposed method adaptively filters pixels in each scale representation by dynamically adjusting the filtering confidence to decide pixels will be segmented by the next level. Experiments have shown the proposed method can boost existing model architectures' performance to achieve SotA on several datasets. 1. To my understanding, this paper essentially proposes a new strategy to merge multi-scale predictions, by choosing the high-confidence output of earlier levels, and passing low-confidence output to downstream levels. Similar strategy could be seen as early as Viola-Jones cascade classifier, but in the domain of semantic segmentation, I do not remember seeing similar design, so in that sense this paper does have some small novelty, though it's incremental.\n\n2. That said, the ideas of merging multi-scale predictions in semantic segmentation have already been studied in many existing work, from the early FCN [1], to more focused studies on this such as [2] and [3]. The proposed AF2 can be considered a hand-crafted strategy to decide which output scale to take, which could be better than naive method like average-pooling, but I'm not convinced it will work better than learnable attention such as in [2].\n\n   2.1. Even comparing to max-pooling, it's possible that AF2 will settle on one class in early scale after passing the threshold, but later scale may reveal an even higher confidence from a different class.\n\n   2.2. The main problem of the paper is it failed to compare to any of these multi-scale merging strategy. For example, [2] claimed the attention method can boost baseline model result by 6%, which is higher than AF2's boost on AFPN (these are not really comparable due to different settings, but I meant the paper should have a fair comparison on this aspect).\n\n   - [1] Fully Convolutional Networks for Semantic Segmentation, CVPR 2015\n\n   - [2] Attention to Scale: Scale-aware Semantic Image Segmentation, CVPR 2016\n\n   - [3] Hierarchical Multi-Scale Attention for Semantic Segmentation, 2020\n\n3. Besides, a few other issues or questions of the paper:\n\n   3.1. The paper highlights the \"adaptive\" threshold and claims top-k won't be effective on aerial images, but there is no experiment designed specifically to validate how much improvements the adaptive mechanism can bring over baselines like fixed threshold or top-k.\n\n   3.2. How is the \"soft update factor\" in Eq.(3) decided?\n\n   3.3. Is r set to a fixed value for experiments on all datasets?\n\n   3.4. In Sec. 4.3, the paper states \"This hierarchical prediction method is independent to the proportion of foreground and background and thus can effectively solve the problems existed in the previous method.\" - I don't understand the logic here. I agree the hierarchical design is independent to the foreground/background ratio, but then how can it also help solving imbalance issue if they're independent? The paper proposed a method to selectively merge multi-scale predictions, but failed to compare to any other methods aiming for similar goal (e.g. max-pooling, attention). There is no experiment to validate the \"adaptive\" design of the mechanism either (compared to fixed threshold or top-k). Therefore, I recommend reject.",
            "This paper propose Adaptive Focus Framework (AF2), which adopts a hierarchical segmentation procedure and focuses on adaptively utilizing multiscale representations generated by widely adopted neural network architectures.\nThe main contribution is a learnable module, called Adaptive Confidence Mechanism (ACM) to determine which scale of representation should be used for the segmentation of different objects. The method achieves the state-of-the-art results on two ariel datasets(iSAID, Vaihingen). Strong Points:\n\n1,  The paper is easy to follow and understanding.\n2,  The proposed methods achieve the state-of-the-art results on two ariel datasets(iSAID, Vaihingen).\n\n\nWeakness:\nSorted by the importance.  Here are the details:\n\n1, Several related works are not cited and compared.  These work may decrease the novelty of this paper. \nThe difference of this work and these dynamic network only lies on the prediction source.\nThis work adpot the predictions as route gating which are used in [1][2][3][4].\n\n[1] Learning Dynamic Routing for Semantic Segmentation CVPR-2020\n[2] Fine-Grained Dynamic Head for Object Detection NIPS-2020\n[3] Dynamic Routing Networks arxiv-2019 \n[4] Improving Video Instance Segmentation via Temporal Pyramid Routing arxiv-2021\n\nUsing predictions for further refine is not very novel and may bring extra computation cost.\n\n\n2, Several statements are not true. \n\"e.g. PointFlow, is not available yet\"\nI search for the Github. https://github.com/lxtGH/PFSegNets This work is already opensourced about half year ago.\n\n\n3, Several statements are not necessary which may be decribed in short sentences.\n\nSec.3.1 and Sec.3.4 may be not necessary since these are the common knowledges for segmentation. \n\n4,  Several detailed design are missing. The choices of ASPP should be claimed. \nWhy use ASPP in the head?\nWhat about the GFlops and FPS of proposed methods?\nWhat is the baseline in Tab3?\n\n5, What about the generalization of Adaptive Confidence Mechanism? Will it still work well on other scene understanding tasks or datasets?\nIf not, it may be not suitable for the ICLR submission.\n\n  Compared with exsiting methods on scene understanding, the proposed Adaptive Confidence Mechanism is little weak.",
            "This paper proposes a pipeline for image segmentation. The  approach is validated  experimentally for remote sensing applications.\nA hierarchical segmentation procedure corresponding to a Feature Pyramid Networks (FPN) with Atrous  Spatial Pyramid is used.\n A preliminary detector take advantage of these feature maps for each resolution level (=layer) for computing class prediction to each pixel. In addition a confidence measurement to each pixel level prediction is also computed (Adaptive confidence mechanism).\nThe architecture is optimized using the cross entropy.\nResults show that the proposed pipeline improves the mIoU  scores compared to the state of the art techniques on three standard dataset.\n \nThe strength of the approach is that experimental results show improvement against state of  the art for mIoU score.\nIntuitively a multi-resolution approach (pyramid) to segment objects of various size is sound. \n\nHowever the paper lacks clarity and coherence:\n- foreground/background are mentioned extensively in the paper but these are not defined. How are these relating to the classes defined with the dataset used in the experiments?\n- class imbalance (between foreground/ background )   is mentioned as an issue in the abstract but it is unclear how this method addresses class imbalance.\n- the abstract also indicates that many techniques \"disregard the fact that the semantic meaning of objects with various sizes could be better identified via receptive fields of diverse ranges\" but it is unclear how the presented work addresses this shortcoming:  how semantic meaning of objects and association with size is used in the proposed pipeline? What are the objects segmented ? how are their real sizes translating into number of  pixels in the dataset used (  in aerial images are geolocated and correspond to an area (e.g. pixel == 50cm x 50cm) ) and relation to receptive fields of filters used?   In the results reported what objects correspond to \"small object\" and \"large object\" (e.g. a few objects are mentioned in Tab 1 , and as well in Tab.5)? \n- Several sentences lack clarity e.g.  \"For instance, pixels in complicated images usually have low confidence, while pixels in simple images have relatively higher confidence.\" What is a simple/complicated image?\n- mathematics are difficult to follow e.g. Eq 3\n \nThe proposed method looks promising in improving segmentation results in remote sensing application. \nHowever clarity has to be improved."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the idea is simple and not novel enough, and that the experiments do not verify the effectiveness of the proposed method. The conclusion is that the paper is not novel and solid enough to be accepted.",
            "The review concludes with a 'reject' recommendation, citing a lack of comparison to existing methods and insufficient validation of the proposed 'adaptive' design. The reviewer expresses being 'not convinced' about the method's superiority and highlights several issues and questions regarding the paper's claims and experimental setup.",
            "The review identifies several weaknesses in the paper, including missing citations, untrue statements, unnecessary details, missing design specifications, and concerns about the generalization of the proposed mechanism. The overall tone is critical, highlighting areas needing improvement.",
            "While acknowledging the method's potential and improved mIoU scores, the review heavily emphasizes the paper's lack of clarity and coherence, posing several critical questions and pointing out confusing aspects. This outweighs the positive aspects, leading to an overall negative sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"idea is simple and not novel enough,\" \"experiments cannot verify the effectiveness,\" and \"lacking novelty.\" These indicate a critical assessment of the work.",
            "The review uses phrases like 'I'm not convinced,' 'the main problem of the paper is it failed to compare,' and 'I don't understand the logic here.' It also points out specific shortcomings in the experimental design and validation of claims, indicating a critical stance towards the paper's methodology and presentation.",
            "The review uses direct and critical language, pointing out 'Weakness' and stating that 'Several statements are not true.' It also questions the novelty of the approach and its suitability for ICLR submission, indicating a critical assessment of the work.",
            "The review uses phrases like \"lacks clarity and coherence,\" poses direct, challenging questions, and provides specific examples for each criticism, indicating a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its criticism of the paper's novelty and experimental validation. The reviewer repeatedly emphasizes the simplicity of the idea and the lack of solid experimental support, leading to the consistent conclusion that the paper is not strong enough for ICLR.",
            "The review consistently argues that while the paper has some novelty, it lacks sufficient validation and comparison to existing methods, leading to a logical recommendation for rejection. The reviewer identifies potential novelty but then focuses on the weaknesses, primarily the lack of comparison and validation. The points raised are logically connected and support the final recommendation.",
            "The review is consistent because it identifies both strengths and weaknesses of the paper. While acknowledging the paper's clarity and strong performance, it raises valid concerns about novelty, citation completeness, factual accuracy, level of detail, and generalization ability. The reviewer's points are logically connected and contribute to a coherent overall assessment of the paper's quality and suitability for publication.",
            "The review is consistent in its assessment. While acknowledging the potential of the proposed method and its performance improvements, the reviewer consistently points out the lack of clarity and coherence in various aspects of the paper as the main weakness. The reviewer provides specific examples and questions throughout the review to support this point, without contradicting the initial positive acknowledgement of the method's potential."
        ]
    },
    {
        "paper_id": "iclr_2021_uUTx2LOBMV",
        "paper_title": "TextTN: Probabilistic Encoding of Language on Tensor Network",
        "paper_abstract": "As a novel model that bridges machine learning and quantum theory, tensor network (TN) has recently gained increasing attention and successful applications for processing natural images. However, for natural languages, it is unclear how to design a probabilistic encoding  architecture to efficiently and accurately learn and classify texts based on TN. This paper proposes a general two-step scheme of text classification based on Tensor Network, which is named as TextTN. TextTN first encodes the word vectors in a probabilistic space by a generative TN (word-GTN), and then classifies a text sentence using a discriminative TN (sentence-DTN).  Moreover, in sentence-DTN, its hyper-parameter (i.e., bond-dimension) can be analyzed and selected by the theoretical property of TextTN's expressive power. In experiments, our TextTN also obtains the state-of-the-art result on SST-5 sentiment classification task.",
        "review_ids": [
            "Do9XIx618qZ",
            "uaWQsMxRM1o",
            "DHzVxKHhyrE",
            "toVi7zvHNxk"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "After reading author replies:\nI would like to thank the authors to respond to my doubts on some of the results. But I decide to keep the review and the score, because Theorem 1 and Claim 1 are still not well explained. In particular, the explanation like \"if the 2nd inequality in Eq. 11 is violated, the network can not capture the amount of information measured by the entanglement entropy \" still looks like a conjecture or intuition rather than a mathematical statement. \n\n---------------------------------------------\n\nOriginality: High.\nThe proposed tensor network (TN) based text classification model looks new and interesting.\nIt consists of two parts: a word-level generative TN model, used to find concise representation of each word; and a sentence-level discriminative TN model, used to classify the sentence based on the outputs of the word-level TNs.\nBy using the word-level TN for input representation, the dimension explosion problem may be effectively avoided.\n\n\nClarity of model description: OK.\n\n\nClarity of training method: Low.\nThe training method of the overall model is not presented. Since there are two TNs, how to train the overall model may not be a trivial problem.\n\n\nClarity of analysis: Low.\nThe major problem of this paper is the analysis presented in Sec 3.2 and Sec 3.3.\nI understand that the authors may want to find some theoretical justification on how to choose the bond dimension in the sentence-level TN, as a function of the bond dimension of the word-level TN and the so called entanglement entropy.\nHowever, the result in Theorem 1 is incomprehensible. Only two inequalities are presented, without stating any condition or implication of the inequalities. Since both m and the bond dimension are hyper parameters, what does the 1st inequality mean? Is it a necessary condition on how to choose their values? What happens if the inequality is violated? Even the proof of this theorem does not answer these questions. Same doubts are on the 2nd inequality as well. Additionally, how does one even know what the entangle entropy of a model is before training the model?\n\nThe statement of Claim 1 is even more problematic. Not able to understand what it says.\n\n\nClarity of expeiremtal results: OK, but not clear enough.\nThe authors claim that when combined with BERT for word embedding, the proposed model can outperform the SOTA methods.\nHowever, there are several things that are not clear.\n1. It would be more fair to compared the combined model with BERT with some similar models that also use BERT for word embedding.\n2. It is claimed that the proposed method is better than word-GTN. But word-GTN is a unsupervised learning model. Why is it meaningful to make such a comparison?\n3. In the introduction, another TN based method \"TSLM\" is mentioned. Would it be more fair to compare the proposed method with TSLM, as both of them use TN for modeling?\n\n\nOverall, the proposed model looks interesting and shows some potential improvements over SOTA. However, the quality of the paper is degraded by the unclear claims and some questionable experimental results.",
            "### Summary \nThe paper proposes a tensor network for text classification. There are two components: (i) word-GTNs convert word embeddings to m-d probability encoding vectors, and (ii) a sentence-DND takes the word probability encoding vectors as input, combining them using matrix product state (MPS). Experiments on several text classification datasets e.g. SST, CR, MPQA show that the proposed method outperforms existing ones when using word2vec and BERT word embeddings. \n\n### Contributions \nThe contributions are two-fold:\n\n* The paper proves an effective lower-bound for the bond-dimension. This is then confirmed by experimental results.\n\n* The used sentence-DND combines all possible locations inducing a distribution over the class set. Existing sentence-DNDs employs only location. \n\n###  Pros\nI do like theorem 1 about a lower-bound of the bond-dimension based on entanglement entropy. It is very interesting to see how experimental results support the theorem nicely (Figure 3). \n\nThe experimental results are a plus. \n\n### Cons\n\n* Clarity: \n   - Although the appendices help a lot, the paper isn't easy to read, especially to people who are not familiar with tensor. For instance, what is a \"tensor contraction\" (right after equation 7)? \n   - The paper also doesn't show the complexity. For examples, each d-dim word vector is represented by a \\phi(w) tensors, which should have 2^d parameters. In this case, what is the number of parameters of W in equation 7? what is the complexity of the whole model?\n   - How to compute entanglement entropy (Appendix D4) isn't provided.\n\n* Originality: In general, the paper doesn't seem to propose significant ideas. The two components are already used. \n   - In fact word-GTNs are just a classifier with a fix-sized input. These GTNs are used in computer vision. However, why don't just use a neural network mapping d-dim word embeddings to a distribution over m latent features. \n   - The extension of sentence-DTN seems to be trivial (actually I was wondering why no-one hadn't proposed that before). However, the extension should come with more complexity, which wasn't shown in the paper. \n\n* Motivation: The paper isn't convincing why using tensor networks is a good idea. It is unclear how tensors help the task (e.g. what is compositionality here?)\n\n\n",
            "Summary:\nA tensor network model for text classification is introduced, which is constructed as the concatenation of a generative matrix product state (MPS) model for low-dimensional word embedding and a discriminative MPS model for classification. This model and different variants are assessed on multiple text classification datasets, with decent performance shown against a range of benchmarks.\n\nStrengths:\nThe TextTN model seems to be the first tensor network model applied to text classification. In that sense, the experimental results given are important for assessing the usefulness of tensor network models for real-world ML challenges, a question which has seen limited study so far.\n\nThe close correspondence shown between the accuracy in classification tasks and the entanglement entropy of the models (Figure 3) is interesting, and hints at the possibility of a compelling link between theoretical quantum many-body physics and practical considerations in ML.\n\nCritiques and Questions:\nThe word-GTN encoder (which converts high-dimensional word embeddings into low-dimensional inputs to the sentence-DTN) strikes me as being unnecessarily complicated, considering the small role it plays in the model. Given that this just outputs very low-dimensional word embeddings ($d=2$ in the paper), it would make sense to use a trainable word embedding here (or alternately a linear function of the original word embedding) in place of the word-GTN. On that note...\n\nThe \"TextTN w/o word-GTNs\" baseline in the ablation study (Table 3) seems rather misleading, as the paper text describes this as \"we directly average the word vectors of words in a sentence to obtain the sentence representation\". In other words, not only is the word-GTN removed, but the order of words is lost as well! I would request that this w/o word-GTN baseline to instead use a sentence-DTN whose inputs are low-dimensional vectors given by trainable low-dimensional word embeddings (or alternately a linear function of the word2vec word embeddings). This alternate baseline would make the TextTN significantly simpler (while still remaining a tensor network model), and would also bring it closer to the model of (Stoudenmire & Schwab 2016).\n\nRecommendation:\nI would recommend acceptance, owing to the new experimental evidence presented for the performance of tensor networks in NLP. However I do have some doubts about the encoding used for the model architecture, and request that the authors improve their ablation study to better justify the additional complexity coming from two separate linked matrix product state models.",
            "The paper proposed TextTN that applied Tensor Network on text input. They effectively solved the high dimensionality problem and achieved the state of the art on multiple text classification problems. The paper also proved the bond dimension and showed experiment results to justify their theory.\n\nIn the experiments in table 1, was there a reason why you don't use the pretrained BERT embeddings? Can you compare with the BERT baseline as well?\n\nIn your BERT + TextTN experiment in table 2, do you finetune the pretrained BERT embeddings?\n\nCould you please analysis on the efficiency of your model? It could be a benefit of TextTN over the baseline models.\n\nThe results on some experiments over the baseline models are not very significant (by ~0.1). I am concerned how much of them comes from hyper-parameter tuning vs. model structure. It's possible that the tasks are not very hard so there are not too much headroom. Can you evaluate it on harder tasks such as other tasks in the GLUE benchmark?\n\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the clarity of the paper, particularly regarding the theoretical analysis and experimental results. Phrases like \"incomprehensible,\" \"problematic,\" \"not clear enough,\" and \"quality of the paper is degraded\" indicate a negative sentiment.",
            "The review expresses several concerns regarding clarity, originality, and motivation. Phrases like 'paper isn't easy to read,' 'doesn't seem to propose significant ideas,' and 'paper isn't convincing' indicate a negative sentiment.",
            "The reviewer recommends acceptance of the paper, highlighting the new experimental evidence for tensor networks in NLP as a key strength. While they have critiques, the overall tone suggests a favorable assessment.",
            "The review contains both positive statements about the paper's achievements and critical questions about experimental choices and results. It acknowledges the paper's contributions while also raising concerns about comparisons, efficiency, and the significance of improvements."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses direct and critical language to point out flaws in the paper's methodology and presentation. For example, \"The major problem of this paper is the analysis presented in Sec 3.2 and Sec 3.3,\" and \"the result in Theorem 1 is incomprehensible\" demonstrate a critical tone.",
            "The tone is critical due to the reviewer pointing out several flaws in the paper, such as lack of clarity ('what is a tensor contraction?'), questioning the originality ('extension of sentence-DTN seems to be trivial'), and expressing doubts about the motivation ('paper isn't convincing why using tensor networks is a good idea').",
            "The review presents both strengths and weaknesses of the paper. It uses phrases like \"seems to be the first,\" \"is interesting,\" but also points out issues like \"unnecessarily complicated\" and \"rather misleading.\" The reviewer also offers constructive suggestions for improvement.",
            "The review demonstrates a balanced tone by acknowledging the paper's strengths (\"\". They effectively solved the high dimensionality problem and achieved the state of the art on multiple text classification problems.\") while also raising critical questions and concerns (\"The results on some experiments over the baseline models are not very significant (by ~0.1).\"). It maintains a professional and objective stance throughout."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently points out the lack of clarity in theoretical analysis (Theorem 1 and Claim 1) and questions the experimental setup. Even after reading the author's response, the reviewer maintains their concerns and keeps the original score, indicating a consistent stance on the paper's weaknesses.",
            "The review is consistent in its criticism of the paper. While acknowledging the interesting theorem and positive experimental results as 'Pros', the reviewer consistently raises concerns about clarity, originality, and motivation in the 'Cons' section. The reviewer does not contradict themselves and maintains a critical stance throughout the review.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the novelty of applying tensor networks to text classification and the interesting experimental results. At the same time, it raises valid critiques regarding the complexity of the word-GTN encoder and the misleading baseline in the ablation study. The recommendation for acceptance is conditional, based on addressing these critiques, which demonstrates a consistent line of reasoning throughout the review.",
            "The review is consistent as it raises valid questions and suggestions for improvement. It asks about experimental details (BERT embeddings, finetuning), model efficiency, and the significance of results compared to baselines, all of which are relevant to evaluating the paper's contributions and suggesting ways to strengthen it. There are no contradictory statements or inconsistent lines of reasoning within the review."
        ]
    },
    {
        "paper_id": "nips_2021_MGHO3xLMohC",
        "paper_title": "Nonparametric estimation of continuous DPPs with kernel methods",
        "paper_abstract": "Determinantal Point Process (DPPs) are statistical models for repulsive point patterns. Both sampling and inference are tractable for DPPs, a rare feature among models with negative dependence that explains their popularity in machine learning and spatial statistics. Parametric and nonparametric inference methods have been proposed in the finite case, i.e. when the point patterns live in a finite ground set. In the continuous case, only parametric methods have been investigated, while nonparametric maximum likelihood for DPPs -- an optimization problem over trace-class operators -- has remained an open question. In this paper, we show that a restricted version of this maximum likelihood (MLE) problem falls within the scope of a recent representer theorem for nonnegative functions in an RKHS. This leads to a finite-dimensional problem, with strong statistical ties to the original MLE. Moreover, we propose, analyze, and demonstrate a fixed point algorithm to solve this finite-dimensional problem. Finally, we also provide a controlled estimate of the correlation kernel of the DPP, thus providing more interpretability.\n",
        "review_ids": [
            "bpdAvuVOCQR",
            "xIebigsg_-4",
            "8MBLUfUGOeV",
            "T3r7mmyUxiS",
            "Zy9P2weXaKm",
            "4s2_YhXeRa6"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for their rebuttal; my score remains unchanged (7) and I recommend accepting this paper.",
            " Thank you for the detailed response to my questions/comments.  I\u2019ve read through the other reviews and rebuttal comments, and am satisfied with the remarks.  I maintain my review score of 7 (Good paper, accept). ",
            "This paper studies nonparametric MLE inference of continuous determinantal point processes (DPPs). The authors analyze various representations between discrete DPPs and continuous DPPs. In particular, they observe that the difference of normalization of DPP is bounded by the trace of a matrix. Then, they consider a new optimization problem penalizing the trace and propose a method of estimating likelihood/correlation DPP kernels. Moreover, they study an iterative method (a.k.a Picard iteration) for solving the MLE problem with the regularization term. Statistical guarantees of the MLE objective and correlation kernel are also studied.   This work tries to bridge the gap between continuous DPPs and discrete DPPs and provides some rigorous results. However, some derivation is unnatural and various analysis methods are already known. Moreover, as DPPs are used in many ML applications and mostly they are given by discrete settings, one should justify importance of continuous DPPs rather than discrete DPPs.\n\nThe equation (6) comes from penalizing the difference of normalization term as shown in Theorem 1. However, this only considers the error of normalization term and one needs to consider the first term (i.e., numerator in probability of DPP) in equation (5). Does the Theorem 1 similarly validate without identity operator?\n\nIn Theorem 6, the authors provide a statistical guarantee of correlation kernel. Is it possible to provide similar result in terms of the likelihood kernel in Algorithm 1?\n\nOverall, it needs to be improved the justification of the contributions of this paper. Also, it would be better to provide more concrete results (e.g., error estimation) rather than visualization.\n\n=========================================================================\n\nI have read authors response and other reviews and noticed there was a misunderstanding regarding continuous DPPs. Authors clarified this in their response. I update my score. - In line 192, it seems that m <= |C| + n since samples in \\mathcal{I} can contain points in C_l.\n- In Theorem 6, what is the \\kappa?\n",
            " I would like to thank the authors for the detailed answers to my questions. There was a misunderstanding regarding the continuous DPP and the Fredholm determinants. The reviewers comments clarified these concepts as well as other questions. \n\nBased on their response and comments from other reviewers, I agree that this work answers new results of nonparametric estimation of continuous DPPs which is not so studied in prior and can be also good hints for learning discrete DPPs. Besides, the result itself is technically sound and fairly well written. Following these reasons, I raise the score to 7.",
            "This paper introduces an algorithm to learn non-parametric, continuous determinantal point processes (DPPs). This algorithm relies on recent representer results for nonnegative functions in a RKHS and on a previous, related algorithm to learn discrete DPP parameterizations. \n\nThe authors prove that the proposed algorithm is guaranteed to increase the MLE objective function at each iteration, as well as prove statistical guarantees for the resulting solution: distance to an optimal parameterization, sample complexity.  Originality: The proposed method to learn a non-parameteric, continuous DPP is new; it is, to the extent of my knowledge, the first result on learning non-parametric, continuous DPPs.\n\nQuality: This paper is of high quality; the proofs are detailed, the theory meticulous (to the extent that I can judge it, being unfamiliar with results in Marteau-Ferey et al., 2020, and Rudi et al., 2020.). Furthermore, the theoretical analysis is regularly tied back to core concepts in DPPs (e.g., the effective sample size), and key arguments are regularly highlighted to support the provided analysis. \n\nClarity: With the caveat that I am not familiar with continuous DPPs (but familiar with their discrete counterparts), I found this paper very well written and accessible. The authors carefully frame their contributions in terms of previously known results within continuous and discrete DPP landscapes, draw the reader's attention to crucial arguments in their proofs, and describe in detail the assumptions and potential limitations of their work. Overall, this is a very well written paper.\n\nSignificance: The results are significant to the community, in particular given growing interest in the applications of continuous DPPs. This is the first result on learning non-parametric continuous DPPs, and also has interesting implications for potential extensions to learning algorithms for discrete DPPs. One potential limitation of this work is that the empirical verification focuses only on exponentiated quadratic kernels.\n\n\nQuestions & Comments:\n- You allude in the final section that the regularized objective might transfer to the finite DPP setting. Do you mean that by introducing a trace-based regularizer to the discrete Picard iteration, you may be able to recover a guarantee of the style of Theorem 4?\n- Have you tried learning L-ensembles with correlation kernels that are not of the same form as the RKHS kernel? Would you expect this to influence the learning process? The authors clearly describe the limitations of their work; I do not see a societal impact that requires discussing.",
            "This paper presents a new approach for maximum likelihood estimation (MLE) for continuous determinantal point processes (DPPs), which leverages recent work on kernel methods.  This approach involves nonparametric learning for a restricted class of the MLE problem, which is shown to fall within the scope of recent work on a representer theorem for nonnegative functions in an RKHS.  An optimization problem over matrices is characterized, and then a fixed point algorithm for solving the finite-dimensional optimization problem is proposed.  Statistical guarantees for the MLE approximation are presented.  Finally, the authors provide an empirical validation of their proposed method using experiments run on synthetic data sampled from a DPP.  The MLE learning approach for continuous DPPs proposed in this paper is novel, and appears to be a significant contribution.  As noted by the authors, substantial prior work exists on discrete or finite DPPs, but much less attention has been paid to continuous DPPs within the machine learning community.  This paper is reasonably well written, and appears to be one of the first contributions in the area of nonparametric learning for continuous DPPs.  The main theoretical contributions in Sections 3 - 5 of the paper appear to be sound, although I have not carefully checked the proofs.  Furthermore, the statistical guarantees in Section 5 establish approximation bounds on the MLE solution and correlation kernel that appear to be reasonably tight under realistic assumptions, with the exception of the lower bound on $p$ in Theorem 6.\n\nA few aspects of this paper could be improved.  First, the authors should discuss the computational complexity of the algorithms for estimating the DPP likelihood kernel (Alg. 1) and correlation kernel (Alg. 2).  What is the computational complexity of the fixed point algorithm in Sec. 4?  A very brief discussion of space and time complexity is given in Appendix S4.3, but this discussion should be significantly expanded upon.  Second, experiments that involve using the proposed approach to learn a continuous DPP from a real dataset should be performed.  Lastly, it would be helpful for the authors to provide a more detailed comparison of their nonparametric learning approach with prior work on parametric learning for continuous DPPs. The authors have included a short, but adequate, discussion of the limitations of their work in Section 7 of the paper.  Since this work involves very general algorithmic and theoretical contributions, a discussion of potential negative societal impact does not appear to be applicable."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly recommends accepting the paper, indicating a positive sentiment.",
            "The reviewer explicitly states they are \"satisfied with the remarks\" and maintains a positive review score of 7, indicating a favorable assessment.",
            "The review expresses both positive aspects (bridging the gap, rigorous results) and negative aspects (unnatural derivation, known analysis methods, lack of justification). The overall sentiment is therefore neutral.",
            "The reviewer expresses gratitude for the authors' detailed answers, acknowledges clarification of misunderstandings, and explicitly states agreement that the work presents new and technically sound results. The reviewer also mentions raising the score, indicating a positive evaluation.",
            "The review expresses a positive opinion of the paper, highlighting its originality, quality, clarity, and significance. Phrases like \"high quality,\" \"very well written,\" and \"significant to the community\" indicate a positive sentiment.",
            "The review expresses a generally positive opinion of the paper, highlighting its novelty and potential as a significant contribution. Phrases such as \"novel\" and \"significant contribution\" indicate a positive sentiment. While the review also includes suggestions for improvement, the overall tone suggests the reviewer finds the paper valuable."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer thanks the authors and explicitly recommends acceptance, showing support for the work.",
            "The reviewer expresses gratitude (\"Thank you for the detailed response\") and approval (\"satisfied with the remarks\"), indicating a supportive stance towards the authors and their work.",
            "The tone is balanced because it includes both positive feedback (\"This work tries to bridge the gap ... and provides some rigorous results.\") and constructive criticism (\"However, some derivation is unnatural and various analysis methods are already known.\"). It also poses specific questions and suggests improvements.",
            "The reviewer uses appreciative language like \"thank the authors\" and \"clarified these concepts.\" The statement \"I agree that this work answers new results...\" demonstrates support for the research and its contributions. Raising the score further reinforces the supportive tone.",
            "The tone is supportive, as the reviewer praises the paper's strengths, such as its detailed proofs, meticulous theory, and clear writing. They also offer constructive questions and comments to further improve the work. The reviewer's overall assessment suggests a positive and encouraging attitude toward the authors and their research.",
            "The review adopts a balanced tone by praising the paper's contributions while also pointing out areas for improvement. It uses formal language and provides specific feedback on the paper's content and presentation, indicating a careful and objective assessment. The reviewer acknowledges the paper's strengths and weaknesses, creating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer expresses a positive sentiment by thanking the authors, maintains a positive score (7), and recommends accepting the paper. All these points align and do not contradict each other.",
            "The reviewer expresses satisfaction with the authors' response and maintains their initial positive score, indicating a consistent positive evaluation.",
            "The review is consistent because it presents a coherent line of reasoning. Initially, the reviewer raises concerns about the justification and novelty of the work, along with specific questions about the methodology and results. After reading the authors' response and other reviews, the reviewer acknowledges a misunderstanding and updates their score, indicating a change in perspective based on new information. The subsequent questions in the second part of the review are more focused and specific, building upon the initial assessment rather than contradicting it. The reviewer's feedback evolves logically and constructively throughout the review process.",
            "The reviewer expresses positive feedback throughout the review, highlighting the authors' detailed answers, clarification of misunderstandings, the novelty and technical soundness of the work, and ultimately raising the score. There are no contradictory statements or conflicting opinions within the review.",
            "The review consistently praises the paper across all aspects: originality, quality, clarity, and significance. The reviewer highlights the novelty of the approach, the meticulous theory and detailed proofs, the accessibility of the writing despite the reviewer's unfamiliarity with continuous DPPs, and the significance of the results for the community. The questions raised are pertinent and constructive, seeking clarification and suggesting potential extensions, rather than pointing out flaws or inconsistencies in the paper or the reviewer's assessment.",
            "The review is consistent because it acknowledges the paper's novelty and contributions while also suggesting specific areas for improvement, which is a typical and constructive approach in peer review. There are no contradictory statements within the review."
        ]
    },
    {
        "paper_id": "iclr_2021_E3Ys6a1NTGT",
        "paper_title": "The Importance of Pessimism in Fixed-Dataset Policy Optimization",
        "paper_abstract": "We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments.",
        "review_ids": [
            "y1onO5W1NGG",
            "_NJ87T34PYO",
            "qUIfLNbAbne",
            "eyt6C-TqAg-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary:\n\nThe paper proposes a theoretical framework for analyzing the error of reinforcement learning algorithms in a fixed dataset policy optimization (FDPO) setting.  In such settings, data has been collected by a single policy that may not be optimal and the learner puts together a model or value function that will have explicit or implicit uncertainty in areas where the data is not dense enough.  The authors provide bounds connecting the uncertainty to the loss.  They then show that explicitly pessimistic algorithms that fill in the uncertainty with the worst case can minimize the worst case error.  Similarly, proximal algorithms that attempt to adhere to the collection policy (as often the case in model-free batch RL) have improved error compared to a naive approach but not as good as an explicitly pessimistic approach.\n\n\nReview:\n\nThe paper provides a general description of the pessimism performance bounds.  The theorems appear to be correct and the reasoning sound.  I also like the connection to the proximal approach, which is how most model-free batch RL algorithms approach the problem (by sampling close to the collection policy).\n\nHowever, the paper does need some improvement.  Specifically, a connection should be made to more existing literature on pessimism in safe, batch, or apprenticeship RL.  In addition, the paper spends a lot of time on definitions and notation that are not explicitly used while the most interesting empirical results are relegated to the appendix, which seems backwards.\n\nOn the connections to the literature, the idea of using pessimism in situations where you are learning from a dataset collected by a non-optimal teacher has been investigated in previous works in apprenticeship RL:\nhttp://proceedings.mlr.press/v125/cohen20a/cohen20a.pdf\nor\nhttps://papers.nips.cc/paper/4240-blending-autonomous-exploration-and-apprenticeship-learning.pdf \n\nSpecifically, the first (Bayesian) paper explicitly reasons about the worst of all possible worlds mentioned in the current submission and seems to have a lot of overlap in the theory.  Can the authors distinguish their results from Cohen et al.?  The second paper is an example where model-learning agents keep track of the uncertainty in their learned transition and reward functions and use pessimism to fill in uncertainty.  So the idea here is not quite new and better connections to this literature need to be made.\n\nThe other issue with the paper is its organization and writing. The theoretical results, while general, are not particularly complicated and don\u2019t seem to warrant the amount of notation and definitions on pages 1-3.  Specifically, the bandit example isn\u2019t really mentioned in the paper but the figure takes up a lot of valuable space.  Over a full page is used to define basic MDP and dataset terms that are widely known and commonly used.  The footnotes are whole paragraphs that seem to be just asides.  Finally, the grid word results are presented in a figure without any real associated text except for some generalities about what algorithms worked well,  Meanwhile, the most interesting and novel contributions of the paper, including the concrete algorithms for applying pessimistic learning, and the empirical analysis on Atari games, are stashed in the (very long) appendix.  I strongly suggest the authors reorganize the paper to highlight these strengths instead of notation and footnotes that are tangential to the paper.\n",
            "The message of this paper is that naive policy evaluations common in current (deep) RL algorithms, can lead to a dangerous overestimation of the value function. This overestimation of the value function can then lead to policy improvements with poor theoretical guarantees. To combat overestimation, the authors propose to penalize state-action pairs that are rarely visited. As an easier to implement alternative, and closer to existing algorithms in the literature, the authors also study another penalty term that penalizes deviation from the data generating policy. The authors show on a numerical example that the more principled penalty term that depends on visitation counts is better performing, and that the proximal penalty term only yields minor improvements over imitation learning (i.e. returning the data generating policy).\n\nThe main contribution of the paper is to decompose the sub-optimality upper bound into terms that either overestimate or underestimate the total reward that can be collected in the true MDP. The authors argue that the overestimation is especially problematic for (the many) RL algorithms that are subject to such overestimation, as there is a high chance of existence of a policy that performs poorly on the true MDP but has high reward on the empirical MDP (the MDP with empirical estimates of the reward and transitions), resulting in a large sub-optimality. \n\nAs far as I am aware, this decomposition is new. But I wonder if beyond formalizing the sub-optimality of naive algorithms, it has other theoretical or practical applications. The notion of pessimism is typical in the analysis of theoretically grounded algorithms (e.g. CPI in\nApproximately Optimal Approximate Reinforcement Learning, Kakade et al. 2002), where deviation from \u2018known\u2019 state-action pairs is typically maximally penalized with the worst possible value (i.e. a sub-optimality of 1 / (1-\\gamma)). So I wonder if the decomposition in overestimation/underestimation terms in Lemma 1 allows for new theoretical insights and algorithmic developments or if it is more of a rewriting, and similar results can be obtained by more carefully choosing the empirical MDP D, such that the overestimation term disappears with high probability even in the worst case and only the underestimation term remains. As is, I understand the reasons for exhibiting both underestimation/overestimation terms in order to analyze \u2018naive\u2019 algorithms in the sense of Sec. 4, but is there an advantage for this decomposition and for the algorithm in Sec. 5.1 compared to choosing the optimal policy without an penalty term but in a more carefully constructed MDP D\u2019 that doesn\u2019t allow for overestimation? Similarly, is there any benefit in not choosing \\alpha = 1 in Sec. 5.1? Is there an optimal choice for \\alpha for Sec. 5.2?\n\nAs for the practical implications, the results in Sec. 6 are quite depressing since algorithms with a proximal penalty are easier to implement than with the uncertainty penalty. What was the \\alpha in the experiments? I wonder if the results can be improved for the proximal algorithm if a better choice of \\alpha is used depending on the optimality of the data generating policy or on the size of the dataset.\n\nOverall, the paper is an interesting read and its message is well presented and supported. However, I am wondering if the theoretical contributions can serve another purpose than warning about the poor theoretical guarantees of \u2018naive\u2019 algorithms, and hope the authors can correct me if I underappreciated the importance of these derivations.",
            "Thank you for your thorough response.  Regarding '*Our work indicates that there is likely an \u201cuncertainty-aware CPI\u201d, which is able to be more sample-efficient while retaining the key property of safe monotonic improvement*' you might be interested in Safe Policy Iteration, Pirotta et al. 2013. I think it goes into a direction of a more refined and easy to implement pessimism than 1/(1-$\\gamma$), although it is not in the FDPO setting.\n\nOf course, it would have been better if the authors arrived at a concrete solution to close the gap between the naive algorithms with loose theoretical guarantees and the theoretical algorithms that are hard to implement. But I agree with R1 that the derivations of a unified framework might drive future researcher in this direction so I am raising my score accordingly. ",
            "**Summary:**\n\nThis paper attempts to unify prior work on fixed-dataset (aka \"batch\" or \"offline\") reinforcement learning. Specifically, it emphasizes the importance of pessimism to account for faulty over-estimation from finite datasets. The paper shows that naive algorithms (with no pessimism) can recover the optimal policy with enough data, but do so more efficiently. The pessimistic algorithms are divided into \"uncertainty-aware\" and \"proximal\" algorithms where the uncertainty-aware algorithms are shown to be more principled, but most prior work falls into the computationally easier proximal family of algorithms that is closer to imitation learning. These insights are proven both theoretically and with some small experiments.\n\n--------------------------------------------------------------------\n\n**Strengths:**\n\n1. A nice decomposition of suboptimality. The main workhorse of the paper is the decomposition provided in Lemma 1 which is novel and can provide some good intuition about the necessity of pessimism (although the intuition is only given in appendix G.3, which should definitely find it's way into the main text). The Lemma cleanly and formally demonstrates why we may expect over-estimation to be more damaging than under-estimation.\n2. A clear framework to examine prior work. The paper does well to capture the majority of recent work into a few broad families of algorithms: naive, proximal pessimistic, and uncertainty-aware pessimistic. The bound derived from the main Lemma for each algorithm family provide evidence to prefer uncertainty-aware algorithms. This is supported by the tabular experiments.\n3. The formal statements of Lemmas and Theorems seem to be correct and experimental methodology seems sound.\n\n--------------------------------------------------------------------\n\n**Weaknesses:**\n\n1. I am wary of the comparison of upper bounds done in the paper. Just because one algorithm has a lower upper bound does not prove superior performance. I agree that since all the proofs are derived from Lemma 1 and are very similar, the differences are indeed suggestive. However, the bound in Theorem 3 seems to be more loose than the others. For example, when $\\alpha = 0$ it does not recover the bound for the naive algorithm as would be expected. A more measured tone and careful description of these comparisons is needed. Claims like \"uncertainty-aware algorithms are strictly better than proximal algorithms\" in the conclusion are not substantiated. \n2. Lack of discussion of issues with implementation and function approximation. As the authors get into in Appendix G.6 and Appendix F.2 and briefly in the paper it is not clear how to implement the uncertainty-aware family of algorithms in a scalable way. I am not saying that this paper needs to resolve this issue (it is clearly hard), but this drawback needs to be made more clear in the main text of the paper, so as to not mislead the reader.\n3. Notation is heavy and sometimes nonstandard. I understand that the nature of this paper will lead to a lot of notation, but I think the paper could be made more accessible if the authors go back through the paper and remove notation that may only be needed in the proofs and may be unnecessary to present the main results. For example, the several different notions of uncertainty funtions might be useful in the appendix, but do not seem to all be necessary to present the main results. Similarly, the notion of decomposability is introduced and then largely forgotten for the rest of the paper. Some notation is nonstandard. For example: $d$ is used for number of datapoints (usually it would be dimension) and $ \\Phi$ is used as the data distribution (usually if would be a feature generating function or feature matrix).\n4. Abuse of the appendix. While I understant that the 8 page limit can be difficult, this paper especially abuses the appendix often sending important parts of the discussion and intuition for the results into appendix G. The paper would be stronger with some editing of the notation and organization of the main text to make room for more of the needed discussion and intuition in main body of the paper.\n\n--------------------------------------------------------------------\n\n**Recommendation:**\n\nI gave the paper a score of 7, and recommend acceptance. The paper provides a nice framing of prior work on fixed-dataset RL. While it leaves some things to be desired in terms of carefulness, scalability, and clarity, I think it provides a solid contribution that will be useful to researchers in the field.\n\nIf the authors are able to sufficiently improve the clarity of presentation as discussed in the weaknesses section, I could consider raising my score.\n\n--------------------------------------------------------------------\n\n**Questions for the authors:**\n\n1. It is natural to think that a practical proximal pessimistic algorithm would reduce the level of pessimism with the dataset size (so that it approaches the naive algorithm with infinite data). Do approaches like this resolve many of the issues that you bring up with proximal pessimistic algorithms (albeit by introducing another hyperparameter to tune)?\n\n--------------------------------------------------------------------\n\n**Additional feedback:**\n\nTypos:\n\n- The first sentence on page 4 is not grammatically correct.\n- In the statements of Lemma 1 and Theorem 1, $ \\pi^*_D$ is defined and never used.\n- In the statement of Theorem 1 $ u_{D,\\delta}^\\pi$ is defined but then only $ \\mu_{D,\\delta}^\\pi$ is used without being defined."
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive aspects (correct theorems, sound reasoning, connection to proximal approach) and negative aspects (lack of connection to existing literature, poor organization, emphasis on notation over empirical results).",
            "The review expresses both positive and negative aspects. It acknowledges the paper's interesting message and well-supported presentation, but also raises concerns about the practical implications and the broader theoretical significance of the contributions. The reviewer uses phrases like \"quite depressing\" but also \"interesting read\", indicating a mixed sentiment.",
            "The reviewer expresses gratitude for the authors' response and ultimately raises their score, indicating a positive overall assessment. Phrases like \"I agree\" and \"raising my score accordingly\" contribute to this positive sentiment.",
            "The reviewer recommends acceptance and gives a score of 7, indicating a generally positive assessment. They highlight the paper's strengths, such as its novel decomposition of suboptimality and its clear framework for examining prior work."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review points out several weaknesses in the paper, including the lack of connection to existing literature, the excessive focus on notation, and the placement of important results in the appendix. Phrases like \"does need some improvement\", \"not particularly complicated\", and \"tangential to the paper\" indicate a critical tone.",
            "The review presents both positive and negative feedback. It acknowledges the paper's strengths (interesting read, well-presented message) while also raising critical questions and concerns about the theoretical and practical implications of the work. The reviewer is questioning and probing, indicating a balanced perspective. The tone is also formal, using technical language and addressing the authors respectfully.",
            "The reviewer offers a helpful suggestion by pointing out relevant prior work (Safe Policy Iteration, Pirotta et al. 2013). Although they express a desire for a more concrete solution, they acknowledge the value of the authors' work in driving future research, showing a supportive attitude.",
            "The review provides both strengths and weaknesses of the paper. While the reviewer acknowledges the paper's contributions and novel aspects, they also point out areas for improvement with specific examples and justifications, maintaining a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the strengths of the paper, such as the correctness of theorems and the connection to proximal algorithms, while also pointing out areas for improvement, such as the lack of connection to existing literature and issues with organization and writing. The reviewer provides specific examples and suggestions for each point, maintaining a clear and consistent line of feedback throughout the review.",
            "The review is consistent in its assessment. It acknowledges the paper's message and presentation are good, but consistently questions the novelty and practical implications of the theoretical contribution, specifically the decomposition of sub-optimality and the proposed algorithms. The reviewer's questions and concerns are focused on the same core issue throughout the review, indicating a consistent line of thought.",
            "The reviewer acknowledges a weakness (lack of concrete solution) but also appreciates the theoretical contribution (unified framework) and raises the score accordingly, indicating a consistent evaluation.",
            "The review presents a balanced assessment, highlighting both strengths and weaknesses. The recommendation of acceptance with a score of 7 is consistent with the identified strengths and weaknesses. The reviewer's concerns are clearly articulated and logically lead to the final recommendation."
        ]
    },
    {
        "paper_id": "iclr_2020_H1edEyBKDS",
        "paper_title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
        "paper_abstract": "Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.",
        "review_ids": [
            "H1x4jhziFS",
            "BJeY0tInor",
            "HkegjQxaFH",
            "rklLgxopKB"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors describe a method for training plug and play language models, a way to incorporate control elements into pre-trained LMs. In contrast to existing work, which often trains conditioned upon the control element, the authors emphasize that their method does not require re-training the initial LM. This is exciting and a great research direction. It is evaluated in a number of different settings.\n\n1. The authors claim that this method is a baseline for controlled text generation (see e.g. the title). However, there does not appear to be any evaluation with any existing work that performs controlled text generation. I don't see how this can be proposed as a baseline for controlled text generation is there is no comparison to other methods. I imagine the authors will emphasize that that's not fair - because their method doesn't require retraining the language model - but it is relevant to demonstrate if there is a gap in performance or not. As is, there is only one baseline- unconditional language model - and to me this is mostly a way to calibrate the evaluators and not a way to compare their model against other models. \n\n2. Can the authors make a point or discuss the relationship of this work to neural style transfer? Compared to unsupervised style transfer approaches, which also use lists of words or attributes to learn to dis-entangle content and style, what are the benefits of the proposed approach and how would it compare?\n\n3. Can the authors discuss the effectiveness of their control mechanism for less logical control settings? For example, what if there was \"religion\" for \"the potato\" prompt? Does the model still respect these settings, or no? \n\n4. Can the authors add analysis on how much the model respects the control variables? This is quite common in existing controlled generation papers. If the model is updated to have the control variables and then is not provided with one at test time, what happens? Can you also control very easy to measure attributes, such as length?\n\nThis question ties in with a general point I am ambivalent to in this paper- that it is very long, but there is very little analysis done on what makes the method work, why it is better than other control methods or control baselines, where the proposed control mechanism is not effective, how the model scales if there are large quantities of topics rather than just a few of them, if the BoW and discriminator attribute models work well together or if certain attributes are easier to learn than others, so the model focuses more on those when there are conflicts, etc\n\n5. Missing citations: \n\nPrevious work has investigated controlling various attributes of text generation. Several of these works have also controlled multiple attributes simultaneously. For example, here's a list of a few of the works that were missed:\n\nKikuchi et al 2016\nFicler and Goldberg, 2017\nWang et al, 2017\nFan et al, 2018\nBaheti et al, 2018\nSee et al, 2019\nMartin et al, 2019\n\nThe related work section only focuses on very recent work, e.g. only one paper is discussed amongst a large body of existing work. I feel this is not an accurate reflection of how much previous work has investigated these techniques and analyzed how models deal with control variables. \n\nPlease also cite:\n- which dataset was used for story generation, appears to be missing\n- top-k sampling \n\n\nI have read the author response. Thanks for the details and additional analysis in the paper. ",
            "Thank you very much for your feedback on my reviews; really appreciate that.\n\nRegarding \"If you have any suggestions for other automatic evaluation metrics, we would be happy to consider including them.\", unfortunately, I don't have them, but I do think that measuring the content shifting among the generated texts could be useful. In this sense, we would be able to control the consistency of the targeted context or conversation. ",
            "The paper proposes a Plug and Play LM model for controlled natural language generation. Similar to the idea of the Plug and Play Generative Networks for vision, the model plugs in a discriminator, which is either a bag-of-words model or a single layer classifier. The added simple discriminator is then coupled with a pre-trained generative language model such as GPT-2, to obtain a conditional probability for generating controllable text. The authors evaluate the proposed model using human evaluation studies and quantitative perplexity metrics, aiming at measuring the relevance and fluency of the generated text. Their experimental results show that the text generated is fluent and aligned with the desired attributes.  \n\nThe proposed method is simple and makes sense to me. The idea of how one can make good use of large, pre-trained  generative language models is very neat here. However, I have two main concerns, as follows.\n\n1. The main focuses of the generated text seem to be dramatically changed in an unpredictable way while tailoring the control attributes. In this sense, how useful these kinds of text generation techniques are not clear to me. For example, the first two rows in Table 3 contain two paragraphs with very different main ideas to be conveyed. Similarly for sentences in Table 1. It seems that those sentences talk about very different topics/things to me, although they may reflect the desired control attributes.  Is there an automatic evaluation metric to subjectively evaluate the change of the focuses/ideas of two pieces of text?\n\n2. The model is a straightforward adaption of the Plug and Play Generative Networks from the vision community. \n\nIn short, the idea in the paper is simple and seems effective. On the other hand, the lack of a good evaluation metric makes me a bit uncertain about the contribution of the paper. I am willing to increase my evaluation score if I will be convinced by other reviews and comments.  \n",
            "The paper introduces an approach to the conditional generation of text, relying on pre-trained decoders, without fine-tuning and, in certain cases, without any training at all. The approach they introduce is following the framework known in NLP as noisy-channel modeling, previously standard in machine translation (in its SMT days), but undergoing certain revival recently (https://arxiv.org/abs/1611.02554, https://arxiv.org/abs/1910.00553,https://arxiv.org/abs/1908.05731,https://arxiv.org/abs/1907.06616). The authors do not mention this connection (they should!).\nVery differently from these previous approaches attempting to integrate the two factors in the search process (e.g., using reranking), the authors instead rely on gradient descent in the latent space of their model (Transformer), similarly to plug-n-play generative networks in image generation.  \n\nI find this approach interesting and like the paper overall. However, I do not see why authors do not compare to more direct ways of integrating the conditional component into the model. This would have been tricky in the NMT papers mentioned above, as the entire source sentences need to be reconstructred, however, it should be quite straightforward in this work, with conditioning on single categorical control variables (or maybe a couple in the additional experiments in sect 4.4). Especially, given that the authors already make the predictions of the control variable independently per prediction (e.g., see eq. (5) in section 4.2) / greedily per prefix (bottom lines, page 7). I would actually expect the proposed approach to work better (or at least differently) but it would be interesting to see it confirmed. E.g., for the experiments defining topics as sets of seed words (section 4.2), when integrating factors directly (unlike the proposed approach, Table 3), there will be no increase in the probability of generating relevant words before the first seed word is generated. \n\nAnother limitation is the lack of comparison to standard controlled generation work, i.e. those requiring training a model or/and fine-tuning pretrained decoder. I understand that the proposed approach falls in a different category and, of course, do not expect it to beat a fine-tuned model, but I'd like to get some feel for how much one loses by using this simpler method. There has been a lot of work on controlled generation in recent ~3 years, and they can also be combined with intializing and fine-tuning off-the-shelf pretrained decoders.\n\nThere is an interesting relation to the NIPS 2019 paper: https://arxiv.org/abs/1907.04944  They also rely on gradient descent to steer a pretrained language model. Their goal is to assess the degree of 'steerability' rather than building a controlled-generation model.\n\nGiven that style-controlled but otherwise unconditional generation may not have that many applications, I am curious how far you can push this approach. E.g., can you make it scale to more complicated data-to-text generation tasks (https://www.aclweb.org/anthology/D17-1239/)? Or, will the only application in this context be integrating new conditioning variables into pretrained conditional LMs? \n\nMinor: I am confused with the notation in \"Post-norm Geometric Mean Fusion\" section.  It says that softmax is applied to the product of probabilities. Maybe to a linear interpolation of log-probs? Or maybe that's not softmax at all? Something seems off here.\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Neutral",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review raises several critical concerns about the paper, including the lack of comparison to existing methods, missing citations, and insufficient analysis of the model's behavior and limitations. The reviewer also questions the validity of the method as a baseline for controlled text generation due to the absence of comparative evaluations.",
            "The reviewer expresses gratitude ('Thank you very much', 'really appreciate that') and offers a constructive suggestion.",
            "The review expresses both positive aspects (simplicity, effectiveness) and concerns (lack of evaluation metric, unpredictability of content change). The overall sentiment leans towards neutral due to the reviewer's willingness to be convinced otherwise.",
            "The review expresses both positive aspects ('I find this approach interesting and like the paper overall') and criticisms ('Another limitation is the lack of comparison', 'I am confused with the notation'). It suggests improvements and points out missing comparisons, resulting in a balanced, neutral assessment."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"I don't see how this can be proposed\", \"there is very little analysis done\", \"I feel this is not an accurate reflection\", and directly points out missing citations. This indicates a critical evaluation of the paper's claims and methodology.",
            "The reviewer is polite and helpful, offering a suggestion in a non-demanding way ('I do think that measuring the content shifting among the generated texts could be useful.')",
            "The review presents both positive and negative feedback. It acknowledges the simplicity and potential effectiveness of the idea while also pointing out concerns about the evaluation metric and unpredictability of content. The reviewer uses phrases like \"makes sense to me\" and \"very neat here\" but also expresses uncertainty with \"not clear to me\" and \"a bit uncertain\".",
            "The review offers constructive criticism, pointing out limitations and areas for improvement. Phrases like 'authors do not mention this connection (they should!)', 'I do not see why authors do not compare', 'Another limitation is the lack of comparison' and 'Something seems off here' indicate a critical yet helpful assessment of the paper's content and presentation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critique, focusing on the lack of comparison to existing controlled generation methods, insufficient analysis of the proposed method's effectiveness and limitations, and the inadequate related work section. While the review starts with a positive note about the research direction, the subsequent points consistently highlight areas needing improvement in terms of evaluation, analysis, and contextualization within the field. The reviewer's feedback is focused and does not present contradictory statements.",
            "The review is consistent because the reviewer first states they do not have specific suggestions for automatic evaluation metrics but then offers a relevant idea related to measuring content shifting to control consistency, which is a coherent and logical response.",
            "The review is consistent because it acknowledges the strengths of the paper (simplicity, effectiveness of the idea) while also raising valid concerns (unpredictable focus change, lack of novelty as it's an adaptation, and the need for better evaluation metrics). The reviewer's concerns and positive points are logically presented and do not contradict each other. The review provides a balanced perspective, appreciating the idea while pointing out areas for improvement or further consideration.",
            "The review is consistent because it expresses initial positive feedback ('I find this approach interesting and like the paper overall') and then proceeds to provide constructive criticism and suggestions for improvement. The reviewer points out limitations such as the lack of comparison to direct integration methods and standard controlled generation work, and raises a minor concern about notation. However, these points are presented as areas for improvement and do not contradict the initial positive assessment or each other. The reviewer's feedback is focused on strengthening the paper by suggesting further comparisons and clarifications, maintaining a consistent stance of constructive evaluation."
        ]
    },
    {
        "paper_id": "iclr_2021_JAlqRs9duhz",
        "paper_title": "Straight to the Gradient: Learning to Use Novel Tokens for Neural Text Generation",
        "paper_abstract": "Advanced large-scale neural language models have led to significant success in many natural language generation tasks. However, the most commonly used training objective, Maximum Likelihood Estimation (MLE), has been shown to be problematic, where the trained model prefers using dull and repetitive phrases. In this work, we introduce ScaleGrad, a modification straight to the gradient of the loss function, to remedy the degeneration issues of the standard MLE objective. By directly maneuvering the gradient information, ScaleGrad makes the model learn to use novel tokens during training. Empirical results show the effectiveness of our method not only in open-ended generation, but also in directed generation. With the simplicity in architecture, our method can serve as a general training objective that is applicable to most of the neural text generation tasks.",
        "review_ids": [
            "Y7_SCEsK8og",
            "3_jEVP1T5t",
            "hZDHvFAk2G",
            "84s-GIHdgFU"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "**I have updated this review after noting the authors\u2019 detailed response.**\n\nThis paper focuses on the problem of \u201cNeural Text Degeneration\u201d\u2014where text sampled from a language model can either be too repetitive and bland or too random and nonsensical. The authors focus largely on the former problem, proposing a finetuning loss that specifically incentivizes the use of tokens that have not yet been decoded in the given document. The authors test whether this improves repetition and unique token coverage with greedy decoding in open-ended generation. A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT). Similarly good results are obtained on Image Captioning with and without trigram repetition blocking. On Abstractive Summarization BeamSearch is used and again outperform MLE and UT. Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited. The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad. Multiple hyperparameter settings are shown, with some analysis on how gamma can be chosen to get a desired behavior. Finally, the authors analyze why UT may not be as effective: it penalizes gold repetitions too much and does little for other tokens.\n\nStrength:\n- The results are good for greedy decoding\n- The method is well motivated and well explained\n- The analysis regarding Unlikelihood Training is interesting\n\nWeaknesses\n-  The results shown do not make proper comparisons across models, baselines, and hyperaparameters over all tasks.\n- Results for stochastic decoding should have been shown across tasks.\n- Despite citing the need for awkward rules such as trigram repetition blocking as a reason to propose ScaleGrad, trigram repetition blocking still helps significantly.\n- Some details are hidden away in the appendix, which I had to read thoroughly in order to fully understand the comparison.\n\nI recommend to reject this paper, because the experimental comparisons are not quite fair and because of implicit argumentation about what Greedy decoding can or should do that is never made explicit.\n\nThe following two paragraphs are obsolete, because the authors shared experimental results from a larger set of experiments.\n> The results in Table 1\u2014which show the main metrics of interest on open-ended generation\u2014are missing two key points of comparison: ScaleGrad is only show with gamma=0.2, even though gamma=0.5 & gamma=0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings. This is despite the fact that two hyperparameter options for Unlikelihood Training are shown. In a footnote on page 6, for directed generation, the authors state \u201cAlthough UL was originally proposed for open-ended generation, it is applicable to directed generation. We did  the same scale hyper-parameter search for UL. Details can be seen in Appendix E.\u201d However, in Appendix E two hyperparameter settings for alpha are shown, the same two as used in Table 1, but two hyperparameter settings for gamma in ScaleGrad are shown _neither of which are shown in Table 1_ nor are repetition or uniqueness numbers shown for these hyperparameters settings anywhere in the paper or the appendices. This makes me question whether the improvements shown in Table 1 hold across hyperparameter settings as the authors claim in their analysis of Figure 1.\n\n> However, Figure 1 is missing necessary data points and comparison. First of all gamma=0.2 is not shown, though at least gamma=0.1,0.3 are so it can be somewhat inferred. That is suboptimal, but this graph does not even go up to gamma=0.8, which is what is used in the Abstractive Text Summarization experiment! Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1. Luckily, Figure 1 (c) can be compared, and if cross-referenced with Table 1, shows that Unlikelihood Training does better than ScaleGrad with a higher gamma. However, Figure 1 has no data on either Unlikelihood Training or a human baseline. It really should not be necessary to go looking through Table 1, Figure 1, and Appendix F to see that Unlikelihood Training is outperforming ScaleGrad on some metrics. Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.\n\nOn page 4 the authors write \u201cFollowing Welleck et al. (2020), we apply greedy decoding in our experiments in this section. This allows us to evaluate the modeling capability exclusively.\u201d We will get into the matter of comparison to Welleck et al. 2020, but I would like to begin by addressing whether Greedy Decoding is a neutral choice that only tests modeling capability, because it is clearly not. There is a spectrum of generation algorithms between probability maximization and straight-forward sampling. Greedy is closer to probability maximization, but it only maximizes local probabilities (Meister et al., 2020) and inevitably comes-up with lower probability outputs than Beam Search or Bound & Branch (Stahlberg & Byrne, 2019). Welleck et al., 2020 show that Greedy Decoding results in better text along their proposed metrics for open-ended generation.\n\nSince Greedy Decoding is not a \u201cneutral\u201d choice, I do not believe it is appropriate to exclude stochastic decoding baselines from the given comparisons. Stochastic decoding algorithms such as sampling, top-k sampling, and Nucleus Sampling usually do very well on repetition and uniqueness metrics. Indeed, they can be seen to outperform all the other models on Table 16 in Appendix H.\n\nIn the analysis section, tables are quite limited in their coverage. In Table 6 no comparisons are made to systems that have not been trained with ScaleGrad, and these algorithms were not reported on in Table 1 so no comparison can properly be made even if the reader goes searching for the data.  In Table 8, Unlikelihood Training is not included in the comparison even though it does very similarly to ScaleGrad on the same task in Table 5. Finally, Table 5 shows that trigram blocking still helps significantly on ScaleGrad trained systems. This is understandable, but disappointing since getting rid of these kind of rules is described as the reason for proposing ScaleGrad.\n\nAltogether, I feel the comparisons made in this paper are not quite convincing and the argument about why Greedy decoding, a deterministic algorithm, should even be able to match the properties of a large, noisy distribution is not properly fleshed-out.\n\nMeister, Clara, Ryan Cotterell, and Tim Vieira. \"If Beam Search Is the Answer, What Was the Question?.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n\nStahlberg, Felix, and Bill Byrne. \"On NMT Search Errors and Model Errors: Cat Got Your Tongue?.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
            "## Summary\n\nThis work proposes an effective modification of language model token-level distribution during the training which prevents some forms of degeneration such as repetitions and dullness. The approach is based on the idea of encouraging the model to use tokens which were not observed in the previous context so far. In other words, this method changes softmax distribution such that unseen/novel tokens is being rescaled with a given hyper-parameter $\\gamma$ (eq.4). Authors conduct several experiments using different tasks such as open-ended generation, image captioning and abstractive text summarization. As a result they confirm substantial improvement over the standard mle training and **token-level** unlikelihood training. In addition to analysis of their method, authors discuss a potential issue of unlikelihood training criterion and how their approach avoids this issue.\n\n## Strong points\n\n1. Main method is easy to understand and I believe is easy to implement: this work can be a motivating example for future research towards degenerative text generation. From my understanding there are some interesting future work such as setting individial $\\gamma$ for some tokens, specific masks for novel token sets etc.\n\n2. Large-scale experiments **with some human evaluation**. I enjoyed seeing improvements on multiple tasks including summarization (with automatic metrics at least). In addition, analysis of stochastic decoding used in directed generation is meaningful and highlights the importance of this work. Appendix includes detailed description of each experiment protocol including the protocol of human evaluation. Convincing examples of generated continuations are given in the appendix.\n\n3. Code with implemented method and experiments is provided: code is based on fairseq custom module which makes it relatively easy to extend and do more research with it.\n\n## Weak points\n\n1. Misleading comparison choice: authors claim to compare their approach with unlikelihood training (UL) and choose **token-level** UL loss without even mentioning the existence of **sequence-level** UL loss which works better based on the original paper. In fact, the whole narrative looks like sequence-level version does not exist. Simply stating that ScaleGrad is being compared with token-level UL (which works worse than sequence-level) would make future conclusions more clear.\n\n2. Some relevant work got completely ignored. I am not aware of the full variety of prior work for this popular problem these days, but there is one i am aware of: https://arxiv.org/pdf/2003.11963.pdf, where authors *do similar gradient analysis* as here. If this one is missed, I wonder what else may be missing in the related work here.\n\n3. No human evaluation for text summarization. Given known weakness of automatic metrics in text summarization task and the fact that authors did human eval for text completion, I wounder why they decided to exclude it from here (I can totally see budget limitation as one of the factors, and saying this explicitly would be helpful).\n\n4. The potential issue of UL (sec. 5.4) does not look convincing. From my understanding the main line there is \"*UL essentially rejects the ground truth token in such special cases (subject to the choice of the value of $\\alpha$).*\". This statement on its own is not clear to me and seems to be disconnected from the previous one: \"*In this case, the norm increases as pk increases, which contradicts with the optimization principle.*\" I agree that in this case ($\\alpha=1, p_\\text{neg}>0.5$) the norm increases as $p_k$ (prob of ground truth token) increases, but I don't see any problem or contradiction here. From my understanding when $p_\\text{neg}$ goes above some threshold, then norm of the gradient of $p_k$ is growing proportionally to $p_k$. Keep in mind that as $p_k$ is increasing, $p_\\text{neg} > \\frac{1}{1+\\alpha}$ is eventually dissatisfied (because of softmax property), i.e. I don't see any issue. Would be great if authors can elaborate more about this.\n\n## Recommendation\n\nOverall I vote for accepting this work as long as main concerns will be addressed/discussed. This is a decent approach with a strong experimental evidence and it will be useful for text generation community in the future research. I would be even more satisfied if authors can discuss / clarify / address the points I highlighted above.\n\n## Comments and questions\n\n1. stochastic beam search was mentioned as one of the efforts to solve text generation issue, but I believe it is more about doing sampling without replacement on the sequence-level. I am curious if authors may provide some perspective on how stochastic beam may reduce the degeneracy (e.g. compared to simple beam search).\n\n2. In sec. 2.1 teacher forcing is described as being used \"*used to train neural text generation models for faster convergence.*\". I wonder how can one use MLE criterion on the token level without teacher forcing? E.g. if we use predicted token as the context in the next time step, then we have no target truth token for the next time step (since the context is changed). In other words I think that teacher forcing is essential if we aim to maximize the probability of training sequences.\n\n3. In section 2.2: \"*thus reformalizing the probability distribution*\", this wording reformalizing sounds a bit weird to me, but it is clear what authors had in mind.\n\n4. Did you think about combining UL loss (both seq level and token level) with ScaleGrad? From my understanding it is possible since ScaleGrad emphasize novel tokens, and this softmax from scalegrad may be used in the UL loss, which may help even further! Importantly, sequence-level UL would allow to use ScaleGrad on the sequence-level, since there is no need for ground truth target there, and while penalizing the repeating words with UL, ScaleGrad would emphasize the novel words (sounds promising to me). Overall the paper narrative looks as combating the UL method (with some misleading gaps about token vs. sequence level), but to me it looks like they may work together!",
            "The authors propose to modify a language model's token-level distributions by rescaling the output probability of tokens that do no appear in the context ('novel tokens'). The authors show improvements over MLE and token-level unlikelihood in terms of repetition, with increases in perplexity. \n\n\n#### Clarity and significance\n- **ScaleGrad motivation**. There are many different ways to change the gradient, e.g. any regularization function, any scaling of the output probabilities, or even gradient clipping modifies the gradients that are used to update the model. As a result, the presentation of their method as \"a modification straight to the gradient of the loss function\" seems odd, and the name ScaleGrad suggests that they are proposing the general notion of rescaling gradients. Instead, they propose to scale a novel set's output probabilities then renormalize.\n\n- **Specific solution**. The method is specifically designed around the 'novel set', which could limit its significance. The authors speculate that they can alter the novel set (e.g. for importance or factual correctness), but this appears to be nontrivial.\n\n- **Unlikelihood discussion**. The discussion in 5.4 deals with the case of $p_{neg}>0.5$, meaning that the probability of the ground-truth token $p_*$ is $<0.5$ (due to normalization). If I'm understanding their argument, the authors argue that the resulting gradient contradicts the fact that the gradient should go to zero at an optimum. However, *the model is not at an optimum* if $p_<0.5$. Could the authors clarify the statements \"the model is not to learn to predict the ground truth tokens correctly\", \"contradicts the optimization principle\", and \"essentially rejects the ground truth token\"? \n\n- **Method for promoting novelty**. It's unclear why this specific method (renormalizing over the novel set) is the best or simplest method for promoting novelty. A downside is that we no longer know which objective the model is optimizing. In the appendix, the authors discuss a variant that uses an additional loss (Section I), yet do not perform an empirical comparison with that or other 'novelty promoting' variations. They argue in Section I that this suffers 'the similar issue as the UL loss', but that issue was unclear (see point above).\n\nOverall I'm borderline on this paper: the authors do perform a lot of experiments and show improvements, but I'm hesitant that scaling novel tokens and renormalizing the model's output distribution is significant.",
            "The paper presents a  technique to encourage generating certain tokens (i.e. non-repetitive ones) in text generation. The idea is to scale the softmax probability  for certain words (in the novel set) by a factor of gamma. The authors show how this affects learning by deriving the effect on the gradient. \n\nMany experiments are presented both on open ended generation (language modeling, story telling) as well as abstractive text summarization to justify the method. The model seems to encourage more diversity than unlikelihood training in open ended generation (while still maintaining a lower perplexity). The gains in summarization are marginal. \n\nI have two questions:\n(1) How are lambda and alpha connected to \\gamma in Section 3? This make the method section clearer. \n(2) How much is the model discouraged from generating stop words like \"the\" or \"a\" (and how does this affect fluency)\n\nPros:\n-Well justified and simple method to solve a relevant problem in text generation. \n-Lots of experiments, gains in open ended generation seem decent. \n\nCons:\n-Gains on summarization are marginal / non-existent suggesting that this is not as large of a problem for more constrained tasks.\n-Some clarity on the questions above would be helpful. \n\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer recommends rejecting the paper due to concerns about experimental comparisons, implicit argumentation, and the use of greedy decoding. They explicitly state that the comparisons are 'not quite fair' and 'not quite convincing'.",
            "The reviewer recommends acceptance of the work, stating it is a \"decent approach with strong experimental evidence\" and \"will be useful for text generation community\".",
            "The review expresses concerns about the clarity, motivation, and significance of the proposed method. Phrases like 'seems odd,' 'could limit its significance,' 'unclear,' and 'hesitant' indicate a negative sentiment. The reviewer also raises questions about the method's justification and potential limitations.",
            "The review expresses overall positive feedback, highlighting the well-justified and simple method, relevant problem-solving, and decent gains in open-ended generation. While acknowledging marginal gains in summarization and requesting clarifications, the positive aspects outweigh the negatives."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses critical language, pointing out weaknesses in the methodology ('The results shown do not make proper comparisons across models', 'Some details are hidden away', 'comparisons made in this paper are not quite convincing') and questioning the authors' reasoning ('I do not believe it is appropriate to exclude stochastic decoding baselines').",
            "The review includes both strong points and weak points, providing constructive criticism and suggestions for improvement. It maintains a professional and objective tone while offering encouragement and recognizing the value of the work.",
            "The tone is critical due to the reviewer's questioning of the method's motivation ('ScaleGrad motivation'), specific design ('Specific solution'), and unclear aspects of the unlikelihood discussion ('Unlikelihood discussion'). The reviewer also points out a lack of comparison with alternative methods ('Method for promoting novelty').",
            "The review uses both positive and negative points ('Pros' and 'Cons') to assess the paper. It acknowledges the strengths of the method while also pointing out areas for improvement and limitations, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review maintains a consistent stance by highlighting both strengths and weaknesses of the paper, ultimately arguing for rejection due to concerns about experimental comparisons, the limited scope of analysis, and the lack of justification for focusing solely on greedy decoding. The reviewer's points logically build upon each other to support the final recommendation.",
            "The review is consistent because it acknowledges the strengths of the paper (novelty, ease of implementation, experimental results, code availability) while also pointing out valid weaknesses (comparison choice, missing related work, lack of human evaluation in summarization, unclear explanation). The recommendation to accept with revisions aligns with this balanced assessment, indicating that the reviewer finds the paper valuable but in need of improvement. There are no self-contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent in its critique, focusing on the clarity of motivation, the limited scope of the specific solution based on the 'novel set', the understanding of unlikelihood discussion, and the justification for the chosen method for promoting novelty. The reviewer's concerns are logically connected and do not contradict each other, leading to an overall borderline assessment.",
            "The review is consistent because the reviewer acknowledges the strengths of the paper, such as the well-justified and simple method and decent gains in open-ended generation, while also pointing out weaknesses like marginal gains in summarization and the need for clarification on certain aspects of the method. The pros and cons are logically aligned with the initial summary and questions, presenting a balanced and coherent assessment."
        ]
    },
    {
        "paper_id": "iclr_2019_HklnzhR9YQ",
        "paper_title": "Approximation and non-parametric estimation of ResNet-type convolutional neural networks via block-sparse fully-connected neural networks",
        "paper_abstract": "We develop new approximation and statistical learning theories of convolutional neural networks (CNNs) via the ResNet-type structure where the channel size, filter size, and width are fixed. It is shown that a ResNet-type CNN is a universal approximator and its expression ability is no worse than fully-connected neural networks (FNNs) with a \\textit{block-sparse} structure even if the size of each layer in the CNN is fixed. Our result is general in the sense that we can automatically translate any approximation rate achieved by block-sparse FNNs into that by CNNs. Thanks to the general theory, it is shown that learning on CNNs satisfies optimality in approximation and estimation of several important function classes.\n      \n      As applications, we consider two types of function classes to be estimated: the Barron class and H\\\"older class. We prove the clipped empirical risk minimization (ERM) estimator can achieve the same rate as FNNs even the channel size, filter size, and width of CNNs are constant with respect to the sample size. This is minimax optimal (up to logarithmic factors) for the H\\\"older class. Our proof is based on sophisticated evaluations of the covering number of CNNs and the non-trivial parameter rescaling technique to control the Lipschitz constant of CNNs to be constructed.",
        "review_ids": [
            "SJxraILVRQ",
            "HJeo4wAshX",
            "rJxsSvvcnQ",
            "ryxrITy5nm"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I do not have further questions.",
            "This manuscript shows the statistical error of the ERM for nonparametric regression using the family of a Resnet-type of CNNs. Specifically, two results are showed. First, the authors show that any block-sparse fully connected neural network can be embedded in CNNs. Second, they show the covering number of the family of CNNs. Combining with the existing results of the approximation error of neural nets (Klusowski&Barron 2016, Yarotsky 2017, Schmidt-Hieber 2017), they show the L2 statistical risk. \n\nDetailed comments:\n\n1. The intuition of using block-sparse FNN seems unclear. It seems that when $M=1$, it reduces to the sparse NN considered in [Schmidt-Hieber 2017]. In the proof of Corollary 5, the authors directly use the error of approximating Holder smooth function by sparse FNN and show that the construction in [Schmidt-Hieber 2017] is actually block-sparse. Thus, it seems unclear why we should consider such block-sparse family. Can any sparse NN be embedded in the family of CNNs?\n\n2. In the Related Work, the authors only compare with 2 previous work on the approximation error of CNN. Actually, this work is more related to [Schmidt-Hieber 2017] due to borrowing the results. It would be better to see what the novelties are compared with that work, especially in terms of the proof techniques.\n\n3. The authors claim that the construction of approximator for Holder functions in [Schmidt-Hieber 2017] is block sparse. It would be nice to give more details of the construction since this is not claimed in [Schmidt-Hieber 2017].",
            "The authors demonstrate the function expression properties for the Residual type convolutional neural networks to approximate the block sparse fully connected neural networks. Then it is shown that such Res-CNNs can approximate any function as long as it can be expressed by the block-sparse FNNs, including the Barron class and Holder class functions. The price to pay is that the number of parameters is larger than that of the FNNs by a constant factor. \n\nThe idea for connecting the expressive ability of CNNs with FNNs is interesting, which can fully take advantage of the power of FNNs to understand CNNs. However, it is not very clear how the convolutional structure of CNNs help in the analysis of approximating FNNs. For example, in the analysis of C.1 and C.2, it will help better understand why CNNs may work from a high-level intuition when the authors construct the filters. \n\nMoreover, it will also help better understand the expressive power of CNNs if the authors can provide some extended discussion on why approximating the block-sparse FNNs rather than arbitrary feed-forward networks. Is there any fundamental reason (or a counterexample) this cannot be realized, or is there to some extent a technical barrier in the analysis? \n\nMinor issue\n\nOn page 20, \u201cBounds residual blocks\u201d -> \u201cBounds for residual blocks\u201d\n",
            "The paper studies approximation and estimation properties of CNNs with residual blocks in the context\nof non-parametric regression, by constructing equivalent fully-connected architectures (with a block-sparse structure),\nand leveraging previous approximation results for such functions.\nExplicit risk bounds are obtained for regression functions in Barron and Holder classes.\n\nThe main contribution of the paper is Theorem 1, which shows that a class of ResNet-type CNNs\ncontains a class of \"block-sparse\" fully-connected networks, with appropriate constraints on various size quantities.\nThis result allows the authors to obtain a general risk bound for the ResNet CNN that minimizes empirical risk\n(Theorem 2, which mostly follows Schmidt-Hieber (2017)),\nas well as adaptations of the bound for the Barron and Holder classes, by relying on existing approximation results.\n\nThe construction of Theorem 1 is interesting, and shows that ResNet CNNs can be quite powerful function approximators,\neven with a filter size that is arbitrarily fixed.\nHowever, the obtained CNN approximating architectures look quite unrealistic compared to most practical use-cases of CNNs,\nsince they specifically try to reproduce a fully-connected architecture, leading to residual blocks of depth ~= D/K,\nwhich is very deep compared to usual CNNs/ResNets (considering, e.g. K=3 and D in the hundreds for images).\nIn particular, CNNs are typically used when there is some relevant inductive bias such as equivariance\nto translations (and invariance with pooling operations) to take advantage of,\nso removing this inductive bias by approximating fully-connected architectures seems a bit twisted.\nThe approach of reducing the function class to be approximated would seem more relevant here,\nas in the cited papers Petersen & Voigtlaender (2018) and Yarotsky (2018), and perhaps the results of\nthe present paper can be useful in such a scenario as well.\n\nSeparately, the presentation of the paper could be significantly improved,\nfor instance by introducing relevant notions more clearly in the introduction and related work sections,\nand by providing more insight and discussion of the obtained results in the main paper.\n\nMore specific comments:\n- Section 1, p.2: define M? define D? M seems to be used for different things in different paragraphs\n- Section 2: Explain what is \"s\" in the Barron class, or at least point to the relevant definition in the paper\n- Section 3.1:\n  * 'estimation error' is usually called '(expected) risk' in the statistical literature (also in the introduction). estimation error would have to do with relating R and R^hat\n  * why is the estimator \"regularized\"?\n- Definition 2: shouldn't it be D_m^(0) = D instead of 1?\n- Theorem 1: What is L? Also, it would be helpful to sketch the construction in the main paper given that this is the main result.\n- Section 4.2: M_1 is the Lipschitz constant of what function?\n- Section 5.1: \"M = 1\" this is confusing, maybe use a different letter for the ridge expansion? The discussion on 'relative scale' could be made clearer.\n- Section 5.2, 'if we carefully look at their proofs': more details on this should be provided.\n"
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Neutral",
            "Neutral"
        ],
        "sentiment_reason": [
            "The statement is a simple declaration of having no further questions, indicating neither positive nor negative feelings.",
            "The review provides constructive criticism and asks for clarification, indicating a neutral stance rather than outright support or rejection.",
            "The review expresses both positive aspects (interesting idea) and concerns (clarity, motivation). It also includes a minor correction, indicating a balanced assessment.",
            "The review acknowledges the interesting construction and potential of the paper's main theorem but also points out limitations regarding the realism of the architectures and presentation issues. The reviewer provides specific comments and suggestions for improvement, indicating a balanced perspective."
        ],
        "tone": [
            "Neutral",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The language is straightforward and lacks any emotional coloring or specific stylistic features.",
            "The reviewer uses phrases like \"seems unclear,\" \"directly use the error,\" and \"borrowing the results\" to express doubts about the novelty and clarity of the work. The questions posed are challenging and suggest weaknesses in the manuscript's justification and presentation.",
            "The review acknowledges the interesting idea but also points out areas for improvement in clarity and justification. Phrases like \"it is not very clear\" and questions about the motivation indicate a critical yet constructive approach. The inclusion of a minor correction adds to the balanced nature.",
            "The review contains several points of critique, such as the unrealistic nature of the CNN architectures ('quite unrealistic compared to most practical use-cases'), the 'twisted' approach of approximating fully-connected architectures, and the need for significant improvement in the presentation ('presentation of the paper could be significantly improved'). The reviewer also asks clarifying questions and points out inconsistencies, indicating a critical assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consists of a single statement and does not contain any contradictory information.",
            "The review is consistent because all comments are focused on clarifying the novelty and motivation of the proposed approach, particularly in relation to existing work by Schmidt-Hieber 2017. The reviewer consistently questions the justification for using block-sparse FNNs and asks for more details and comparisons to related research.",
            "The review is consistent as it provides constructive criticism and suggestions for improvement without contradicting itself. The reviewer acknowledges the interesting idea but raises valid questions about clarity and justification, maintaining a consistent critical yet helpful tone throughout.",
            "The review is consistent because the reviewer acknowledges the theoretical contribution of the paper (Theorem 1 and 2) as interesting, but raises concerns about the practical relevance of the constructed CNN architectures and the presentation clarity. The reviewer's critique is focused and the specific comments support the overall assessment without contradicting each other."
        ]
    },
    {
        "paper_id": "nips_2022_Qry8exovcNA",
        "paper_title": "Explaining Graph Neural Networks with Structure-Aware Cooperative Games",
        "paper_abstract": "Explaining machine learning models is an important and increasingly popular area of research interest. The Shapley value from game theory has been proposed as a prime approach to compute feature importance towards model predictions on images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for GNN explanation, where the task is to identify the most important subgraph and constituent nodes for GNN predictions. We claim that the Shapley value is a non-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Specifically, we define a scoring function based on a new structure-aware value from the cooperative game theory proposed by Hamiache and Navarro (HN). When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, resembling message passing in GNNs, so that node importance scores reflect not only the node feature importance, but also the node structural roles. We demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity over strong baselines on chemical graph property prediction and text graph sentiment classification. Code: https://github.com/ShichangZh/GStarX \n",
        "review_ids": [
            "VSnE_eeAjZ",
            "Kf1nPMKMov9",
            "o0nWt3eJc0k",
            "s1ak2Hr5_lq",
            "hEUVB0RVAjP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " The authors have addressed my comments.",
            " Thanks for the detailed response. Generally, I think my concerns have been addressed and my questions have been answered. ",
            " - This paper proposes a new GNN explanation method called Graph Structure-aware eXplanation (GStarX) based on HN value from the cooperative game theory, and claims it is able to leverage the critical graph structure information and improve the GNN explanation.\n-  This paper also tries to explain why and how this method is more effective compared with existing methods based on Shapley Value claimed to be non-structure-aware. \n- Experiments conducted on chemical graph property prediction and text graph sentiment classification are used to demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity\n - Strengths\n    - This paper is first to apply HN-Value (borrowed from game theory) to GNN explanation and illustrates its effectiveness. \n    - Compared with non-structure-aware GraphSVX, HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, so that node importance scores reflect not only the node feature importance, but also the structural roles.\n    - SubgraphX is also structure-aware for GNN explanation, but it has limitation of only generating one single connected subgraph as explanation. GStarX can overcome the issue by generating multiple disconnected subgraphs as explanation.\n- Weaknesses\n    - The paper does not clearly explain how Equation 6 and 7 are transformed into the form of matrix multiplication in Algorithm 3, and some necessary explanations are needed for better understanding.\n    - The paper claims that \"Connecting GNNs and the HN surplus allocation through the message passing lens\" and uses a molecule example shown in Figure 1 (c) to demonstrate  the connection between structure-aware HN value and the GNN message passing. A graph for sentiment classification in Figure 1(b) is used to qualitatively show that the structure-aware HN value can eliminate the inappropriate word coalition. But for better understanding, it seems there is lack of clear illustration using a unified explanation mechanism, which can naturally connecting these examples of message passing and elimination.\n    - The example in Figure 1 (b) is only qualitatively studied, and for consistency and better illustration, it should be included in section 5 with the detailed quantitative analysis.\n - Why the fidelity is used as the only metric for evaluation? How about metrics such as AUC and accuracy used in some other GNN explanation papers?\n- Is the evaluation in Table 1 (with 8 different Sparsity) meaningful or fair?\n- Algorithm 1 illustrates the main framework of the paper, but it seems Algorithm 3 is equally important for better understanding. It may be more appropriate to include Algorithm 3 in the main body of the paper. \n There is no potential negative societal impact of this paper.",
            " The paper proposes a structure aware scoring function, referred to as HN value, to explain decisions of a GNN for the task of graph and node classification. HN value originally proposed in reference [6] of the paper has its roots in cooperative game theory like Shapley values but unlike Shapley values, it is structure aware.  An algorithm called GStarx is proposed which computes HN values corresponding to nodes in a graph later presented as explanations. Higher the HN value higher the node importance. The superiority of the method is demonstrated via experiments in 6 graph classification datasets.  Strengths:\n\n1. The paper proposes a new scoring function based on cooperative game theory to find node importances.\n2. The paper is well written and structured. \n3. The overall study is carried out well with various concerns, regarding application to node classification, comparison to closely related work based on C-Shapely, at least addressed in the Appendix.\n\nWeaknesses:\n1. The proposed method can only be applied to find node importances ignoring the differing feature importances. Note that a GNN uses both node features and structure to make predictions. Some of the features might be more important than others towards a decision.\n2. There are a few clarifications required based on the current text which I also elaborate under Questions. \n\n\n\n\n\n 1. I am still wondering about the lack of feature importances in the explanation. For a graph we are given nodes, their features and the connections. The current paper address the problem of finding the most important nodes, which might be reasonable for the datasets used currently in the paper. But for real world datasets both for node and graph classification, node features may play an important role. How do or can we find feature importances together with node importances with the current methodology? \n\n2. Definition 4.2 is a bit confusing.  It seems like the condition |S/G|=1 will only be satisfied if the induced graph on nodes in S is a clique.  But intuitively, the first condition should correspond to graph induced on S to be a connected subgraph. The authors should elaborate on definition 4.2 a bit more and justify the choice even if it was originally proposed in [6]. The example given in lines starting from 180 is too simple as it only considers the case when |S|=1.\n\n3. In the GStarX algorithm, what is f^0_c^*?\n\n4. It is not clear if the obtained explanations are actually sparse as the importance scores are continuous. For example, a method can lead to a uniform importance score distribution over the nodes, i.e., all nodes are almost equally important. One should use entropy based metric for sparsity as proposed in https://arxiv.org/abs/2105.08621  to show that indeed the importance score distribution is not very uniform.\n\nOne minor question: What does HN stand for? I even looked up in the original paper [6], could not find the full form! One of the main limitations of this work is that there is no direct way to compute feature importances while explaining a decision which might be very important in some real world datasets and applications. Consequently in presence of feature explanations more evaluation metrics might be required as one can just not simply remove a feature while computing fidelity/faithfullness as argued in https://arxiv.org/abs/2105.08621\n\nThe authors should explicitly acknowledge this limitation while still providing examples of application areas for example for text graphs or graphs with dense feature representations extracted using some unsupervised representation learning methods. In those cases, providing additional feature explanations might not be useful as the features themselves are not directly interpretable. \n",
            " The explainability of GNNs is crucial for modeling graph data. This work is based on the HN value instead of the Shapley value to explain the GNNs, which involve the graph structure information.   Pros:\n\nThe proposed method is quite clearly defined. Most of the time, I enjoyed reading this paper because of its clear and concise introduction to the preliminaries of this work (including Shapley value and HN value). Considering graph structure information via HN value to explain the GNNs is reasonable. The experiments have demonstrated the effectiveness of the proposed GStarX algorithm. \n\nCons:\n\nOne limitation of the approach is its lack of technical originality. Although using the HN value is interesting and innovative on this issue, this paper did not provide any original theoretical analysis on the benefit of leveraging graph structure information via HN value to explain the GNNs. Meanwhile, for the examples in Figure 1, only comparing the basic Shapley method is not enough to demonstrate the advantages over other SOTA methods. And, for Figure 1 (b), it is hard to say whether the coalition {\u201cnot\u201d, \u201cgood\u201d} contributes negatively when using the GNNs to model two unconnected nodes: \u201cgood\u201d and \u201cnot\u201d. Please describe the original theoretical contribution of the paper.\n\n N/A"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The statement \"The authors have addressed my comments\" indicates satisfaction and agreement, suggesting a positive sentiment.",
            "The reviewer explicitly states that their concerns have been addressed and questions answered, indicating satisfaction with the response.",
            "The review presents both strengths and weaknesses of the paper, without expressing a clear positive or negative overall judgment. It offers constructive criticism and suggestions for improvement.",
            "The review identifies several weaknesses, including the lack of feature importance consideration, confusion regarding Definition 4.2, unclear aspects of the GStarX algorithm, and concerns about the sparsity of explanations. The reviewer also highlights the limitation of not being able to compute feature importances and suggests acknowledging this limitation.",
            "The review expresses positive aspects such as clear definition, enjoyable reading experience, reasonable approach, and demonstrated effectiveness. Despite criticisms, the overall sentiment leans towards positive due to the acknowledgment of the paper's strengths."
        ],
        "tone": [
            "Neutral",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The statement is factual and lacks strong emotional coloring, indicating a neutral tone.",
            "The reviewer expresses gratitude (\"Thanks for the detailed response\") and indicates agreement and satisfaction, which reflects a supportive tone.",
            "The review uses a balanced approach by listing both strengths and weaknesses. It also asks clarifying questions and suggests improvements rather than simply criticizing. Words like 'effectiveness,' 'limitation,' 'needed,' 'lack of clear illustration,' and 'meaningful' contribute to this balanced tone.",
            "The review uses questioning and critical language, such as \"I am still wondering about the lack of feature importances\", \"Definition 4.2 is a bit confusing\", \"It is not clear if the obtained explanations are actually sparse\", and \"One of the main limitations of this work\". These phrases point out flaws and areas needing improvement.",
            "The review presents both 'Pros' and 'Cons' sections, offering both positive feedback (e.g., \"clearly defined,\" \"enjoyed reading\") and constructive criticism (e.g., \"lack of technical originality,\" \"not enough to demonstrate the advantages\"). This balanced approach indicates a neutral tone overall."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is a concise and positive statement. It indicates that the reviewer's previous comments have been addressed by the authors, implying satisfaction and agreement. There are no contradictory statements or points of inconsistency within this short review.",
            "The review expresses a positive sentiment and indicates that the reviewer's concerns have been addressed and questions answered, showing internal consistency.",
            "The review consistently identifies both strengths and weaknesses of the paper, offering constructive criticism and suggestions for improvement without contradicting its overall assessment. The reviewer acknowledges the novelty and potential of the proposed method while pointing out areas that need clarification, further explanation, or more rigorous evaluation. There are no conflicting statements or viewpoints within the review.",
            "The review is consistent in its criticism, primarily focusing on the lack of feature importance consideration in the proposed method. This point is raised as a weakness, elaborated in questions, and reiterated as a significant limitation in the concluding remarks. While acknowledging strengths like novelty and presentation, the core concern about feature importance remains consistent throughout the review.",
            "The review is consistent because the pros section praises the clarity and understandability of the paper and the reasonable approach of using HN value. The cons section then points out the limitations in terms of technical originality and experimental validation, which are valid criticisms that do not contradict the positive points mentioned in the pros section. The reviewer consistently evaluates different aspects of the paper without contradicting themselves."
        ]
    },
    {
        "paper_id": "iclr_2022_X_hByk2-5je",
        "paper_title": "Lossless Compression with Probabilistic Circuits",
        "paper_abstract": "Despite extensive progress on image generation, common deep generative model architectures are not easily applied to lossless compression. For example, VAEs suffer from a compression cost overhead due to their latent variables. This overhead can only be partially eliminated with elaborate schemes such as bits-back coding, often resulting in poor single-sample compression rates. To overcome such problems, we establish a new class of tractable lossless compression models that permit efficient encoding and decoding: Probabilistic Circuits (PCs). These are a class of neural networks involving $|p|$ computational units that support efficient marginalization over arbitrary subsets of the $D$ feature dimensions, enabling efficient arithmetic coding. We derive efficient encoding and decoding schemes that both have time complexity $\\mathcal{O} (\\log(D) \\cdot |p|)$, where a naive scheme would have linear costs in $D$ and $|p|$, making the approach highly scalable. Empirically, our PC-based (de)compression algorithm runs 5-40 times faster than neural compression algorithms that achieve similar bitrates. By scaling up the traditional PC structure learning pipeline, we achieve state-of-the-art results on image datasets such as MNIST. Furthermore, PCs can be naturally integrated with existing neural compression algorithms to improve the performance of these base models on natural image datasets. Our results highlight the potential impact that non-standard learning architectures may have on neural data compression.",
        "review_ids": [
            "aS2zgIPBgv4",
            "tmZXCJaAuXw",
            "EqrpET_oNEH",
            "9o6JYz6OfEm",
            "ZFU8TFz7HhT",
            "5a3byG6_yvb"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Probabilistic circuits are a formalism, developed relatively recently, for describing multivariate probability distributions. PCs are represented using directed acyclic graphs (DAGs), with an operational semantics, and it is relatively straightforward to deduce which operations (marginalization, maximisation, estimation of moments, etc.) are tractible based on the structure of the DAG and the locations of input variables.\n\nThis paper studies the application of PCs to lossless compression, identifying the specific type of marginalisation which is necessary for auto-regressive, arithmetic coding-style compression, and the necessary conditions a PC must satisfy in order for this marginalisation to be tractible. In section 3, a procedure is described for particularly efficient marginalisation by sharing computation.\n\nExperiments show that the approach is competitive in terms of compression rate with other recent 'neural compression' works, and results are presented showing that PCs can be much faster than some existing methods, although I have serious concerns about these (see below). Overall, I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, that I was not previously aware of, the methodological contribution wasn't clear enough and that the experiments were presented in a way which was potentially misleading.\n\nMy main issues with the methodological contribution (specifically section 3.2), are that I felt it was relatively difficult to understand (i.e. the presentation could have been clearer), and that as a result I don't know how significant the contribution is. On a high level, it seemed like a couple of observations were made:\n\n(a) For many models, it is important to decode/encode data in an order which 'fits' the model, leading to computational efficiency.\n(b) Work sharing between encode/decode steps is necessary for efficiency.\n\nTo someone, such as myself, who is an expert on neural lossless compression, these are both obvious points. The O(log D |p|) computational efficiency didn't seem particularly radical to me either. However, I'm unfamiliar with the PCs literature, so maybe these are significant observations to that community, and it's possible that I missed something important, so please clarify.\n\nBefore I discuss the experiments, another significant issue with the overall framing was the statement in the second paragraph of the introduction that the methods presented here are 'tractible', with the implication being that other neural compression methods are not (in some sense). I understand where this came from, since tractibility is a key property in the PCs literature, but I still felt this could be misleading, since neural compression and decompression with existing methods, whilst they may have different performance characteristics to PCs, certainly _are_ tractible.\n\nI had two main issues with the experiments. The first relates to the presentation of the timings of the method. I think it's easy to be misled, and to mislead, with timings in machine learning (and the emerging field of neural lossless compression in particular), and my issue with timings in this work is that the paper is presented as though a significant breakthrough has been made in terms of runtime, but the PC method is only compared to slow implementations of existing methods, which were not optimized for speed. One example of a faster implementation of a neural compression method is BB-ANS with a small VAE (from Townsend et al., 2019), implemented at https://github.com/j-towns/craystack. Running the example code there on a CPU, the compression/decompression for the binarized MNIST test set (a slightly easier task than raw MNIST) is 3.26s/2.82s, an order of magnitude faster than the PC timings in the paper. For extra context, on the same machine gzip takes 0.22s to compress and 0.06s to decompress the raw MNIST test set, so in my opinion neither the Craystack BB-ANS implementation nor the 15s encode and 44s decode of the PCs implementation should be considered fast. This context should have been made clearer, rather than trying to imply that a genuine breakthrough had been made in runtime.\n\nMy other serious issue with the presentation of the experiments were the vague claims of \"state-of-the-art performance on natural image datasets\", in the abstract and at the very end of section 2, discussed in more detail in section 5. It's not at all clear on what task the authors are claiming state of the art performance. At the end of sec 5, the suggestive statement is made that compression/decompression of natural images \"can be done easily\" with the implemented method. Have the authors actually implemented this? If compression wasn't implemented then this should be stated and the authors should explain why not, if it was implemented this should be clearly stated. Unfortunately, even if compression was implemented, a claim of state of the art performance would still be incorrect, because the bitrates achieved in a recent paper by Zhang et al. (https://arxiv.org/abs/2109.02639) are significantly better.\n\nSome more minor points and suggestions:\n - At the beginning of sec 3.2, why does \\pi need to be 'random'? Don't you just mean to say that \\pi is \"some ordering\" (i.e. no need to suggest that it is a random variable).\n - In section 4.1 I think it's best to consistently use the word 'latent' rather than 'hidden', since this is the standard terminology in the deep generative modelling community.\n - It might be a good idea to combine the different related work sections in one place.\n - In 'related work' near the bottom of page 8, there's a slight grammatical error: replace \"grow PC structures to fit better the data\" -> \"grow PC structures to better fit the data\". I felt that whilst this was a reasonably well written paper highlighting the interesting concept of PCs, which I was not previously aware of, the methodological contribution wasn't clear enough and that the experiments were presented in a way which was potentially misleading.\n\nEDIT: Score changed to 6, see comment below.",
            " It seems that overall, significant improvements have been made to the paper. I've changed my score to a weak accept. I would greatly appreciate it if the authors could explicitly state _in the paper_ that they have not implemented compression with the PC+IDF model. For example, by changing the sentence beginning \"Compression and decompression with the PC+IDF model can be done easily...\" to \"Although we leave the implementation to future work, compression and decompression with the PC+IDF model should be straightforward since...\".\n\nI'm still slightly concerned about the way PCs are presented as being strictly + significantly faster than other \"suboptimal\" DGM-based appoaches. Surely in practice this would depend on things like the size of the DGM, and presumably the implementation and the hardware used. In a fully optimized implementation, is it not possible, for example, that a small VAE (with say one or two convolutional layers) would outperform a PC on speed, because of better vectorization and parallelization? It's a shame that Townsend (2019) doesn't provide timings for full MNIST, since the compression rate of the small VAE (1.41bpd) is actually not too far from the PC rate.",
            " I am satisfied with detailed illustration of PC, which is much clearer to follow.\nHowever, there are some suggestions to make the paper better:\n1.\tFor coding with VAE, I hope to find a coding algorithm that avoids bits-back coding. Unfortunately, it seems that PC could not solve this issue as decoding with p(z|x) is unlikely to be avoided with PC. In fact, PC performs much faster than auto-regressive models, which can be highlighted.\n2.\tFor PC+IDF, the authors admit that the coding time is hard to summarize due to the coders, and the complexity is similar compared with IDF++. But on the other hand, I suggest just summarize the inference time between IDF++ and PC+IDF, and show the time consumption by replacing the mix-logistic prior to PC.\n3.\tIt seems that PC model is a non-neuron model, thus I doubt the performance on complex images. Results on complex images like CIFAR10, ImageNet32, ImageNet64 with ONLY PC model are recommended. \n",
            "This paper proposes a new probabilistic model named PC for efficient encoding and decoding. The authors claim that PC greatly reduces the time complexity for inference. Experiments show that PC achieves 5-20x faster than SOTA neural compression algorithms, and performs SOTA results on MNIST datasets. Strengths:\nThis paper provides a new probabilistic modelling framework named PC for lossless compression. This framework is new in AI lossless compression community and achieves desired compression ratio and compression bandwidth in MNIST. By incorporating PC with explicit generative models in which the prior can be replaced with PC, the model achieves SOTA compression ratio in real-color images.\n\nWeaknesses:\nThis work is somewhat hard to follow in the following aspects:\n1.\tIn the abstract, the authors highlighted the drawback of VAEs such that \u201cbits-back coding brings poor single-sample compression rates\u201d. However, the proposed PC seems have no relevant with bits-back coding. I wonder what is the advantage of PC over VAEs in terms of \u201cbits-back coding\u201d?\n2.\tThe authors claim that the complexity of PC is O(log D |p|) where |p| is number of neural network units. But I am not able to figure out how to incorporate neural networks with PC in O(log D |p|). In particular, consider flow models with PC, it just seems to replace the prior with PC, to what follows, the complexity seems to be O(log D + |p|). Moreover, for auto-regressive models, it seems that PC cannot reduce the complexity to O(log D |p|) but remains O(log D |p|). More discussions on how to incorporate generative models with PC with O(log D |p|) complexity is encouraged.\n3.\tThe PC framework is hard to follow. In Definition 2-4, it is better to give definitions on \u201cscope of variable\u201d, and examples on \u201cSmoothness\u201d, \u201cDecomposability\u201d and \u201cSD\u201d. For Fig. 1, detailed probability corresponding to the figure is recommended.\n4.\tFor Sec. 5, the compression bandwidth of PC+IDF compared with IDF is recommended.\n The idea of PC for lossless compression is new and interesting. However, following the main contribution of PC and the main idea of PC is somewhat difficult. Giving simple examples on PC for clarification is recommended.",
            "The manuscript addresses the question of using deep generative models in lossless compression. As highlighted by the authors, the main issue here is related to the cost of computing probabilistic queries or, in some cases such as GANs, the inability to compute them.\nThus, this paper suggest using Probabilistic Circuits (PCs) for lossless compression. PCs allow for tractable probabilistic inference, which enables efficient compression.\n\nExperimental results are favourable in two ways. First, PCs are faster for compression, achieving results from 5 to 20 times faster than competitor neural networks. Second, PCs achieve competitive compression rates on various datasets. A strength of this paper is its clever use of PCs in compression. First, it highlights the importance of tractability in lossless compression, which is an unexplored perspective in related works. Then, PCs are presented as a class of models with tractable inference, while still being expressive enough. This balance between expressiveness and computational cost is key this manuscript's main contribution.\n\nIt is worth clarifying the significance of the technique for improving marginal inference from O(D.|p|) to O(log(D).|p|). This algorithm has relevant similarities with Partial Propagation described in [1]. Thus, a comment on differences here could highlight the novelty in Algorithm 2.\n\nSection 4 provides a practical way for scaling up the use of PCs in compression. However, it is not clear from the manuscript the contributions made in the scaling up process. That is, if there was any new required technique developed for applying Hidden Chow-Liu Tree to the specific problem of compression.\n\nExperimental results are encouraging and convincing.\n\n\nMinor comments:\n* There is a typo with \"p(i.e.,\" in Section 2\n* Theorem 1 significance is not clear from the manuscript and it seems incremental at a first glance. While there is no questioning of its practical implications, a formal treatment could highlight its broader applications.\n\n\n[1] C.J. Butz, J.S. Oliveira, A. dos Santos, A.L. Teixeira, P. Poupart, A. Kalra, An Empirical Study of Methods for SPN Learning and Inference, Ninth International Conference on Probabilistic Graphical Models (PGM), 49--60, 2018. The manuscript provides a class of tractable models for lossless data compression. These models are well defined and algorithms are shown. Parts of the work need clarifying for novelty and significance. However, experiment results with faster and competitive compression rates are very convincing.",
            "The paper showcases an application of Probabilistic Circuits to lossless compression, and achieves competitive compression performance to state of the art method. The PCs output a marginal distribution over the data, which the authors then use to compress with arithmetic coding or other methods. This has several advantages, one being that bits-back coding is not needed, enabling single-sample compression.\n\n Strengths:\nThe contributions of the paper are: (i) reducing the complexity with respect to D from linear to sub-linear and (ii) integration with existing neural compressors. The authors use Hidden Chow-Liu Tree (HCLT) , a PC model initially proposed for simple density estimation tasks containing binary features, to scale up to achieve state-of-the-art performance on various image datasets.  Furthermore since HCLTs cannot be easily vectorized, the authors implemented customized GPU kernels for parameter learning and marginal query computationbased on Juice.jl, an open-source Julia package. This way the authors were able to get significant improvement in the performance.\n\nWeaknesses:\n The baselines BitSwap and IDF, which are definitely NOT the state of the art in compression efficiency and compute time. The comparison that is missing is with respect to 'Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding' (Ruan, Ullrich, Severo, et al.). In that paper, the authors claim to compress all 10,000 MNIST images in less than 100s (Figure 7). Comparison with BitSwap (and possibly IDF) should be done with caution. In BitSwap, the entropy coder runs on CPU while the model can be run on GPU or CPU. It's not clear what the experimental setup was here for BitSwap, PC, and IDF experiments. Exactly what setup the authors claim SOTA should also be clarified (e.g. model on GPU, compression on CPU).\n\n\nComments regarding writing\nPage 4: \n- Not clear what happens when 2 different input units have the same variable. This is exactly the case of Figure 1, where there are 2 input nodes for each of X1, X2, and X3.\n\nPage 5:\n- If the inputs are descendants of the same child unit, then doesn't this imply the opposite? That is, you definitely need to multiply them based on equation 2? It would be easier if the authors followed along with a concise and concrete example.\n- The example in Figure 1 does not help, as the distance from n1 and n2 to the root node is too short for the reader to properly follow along. \n\n Overall the work is interesting and timely and I am leaning towards an accept if the authors can convincingly address the points above."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Neutral",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the clarity of the methodological contribution, potentially misleading experimental presentation, and unsubstantiated claims of state-of-the-art performance. Phrases like 'serious concerns,' 'potentially misleading,' and 'not clear enough' indicate a negative sentiment.",
            "The reviewer states that \"significant improvements have been made to the paper\" and changes their score to a \"weak accept.\" This indicates a positive overall sentiment.",
            "The review expresses satisfaction with the illustration but also provides suggestions and raises doubts about the model's performance, indicating a neutral overall sentiment.",
            "The review identifies several weaknesses in the paper, including difficulties in understanding the framework, inconsistencies in claims about complexity, and lack of clarity in definitions and explanations. The reviewer uses phrases like \"hard to follow\" and \"somewhat difficult,\" indicating a negative assessment.",
            "The review expresses overall positive feedback, highlighting the paper's strengths such as the clever use of PCs in compression, the emphasis on tractability, and encouraging experimental results. While it points out areas for improvement, the reviewer is convinced by the experimental results and acknowledges the paper's contribution.",
            "The review identifies both strengths and weaknesses of the paper. The overall assessment leans towards acceptance if the authors address the concerns, indicating a balanced perspective."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses direct and evaluative language to point out flaws in the paper's methodology and experimental setup. Phrases like 'serious concerns,' 'methodological contribution wasn't clear enough,' 'potentially misleading,' and questioning the significance of the contribution indicate a critical tone. The reviewer also points out specific instances of misleading presentation and lack of clarity.",
            "The reviewer acknowledges the improvements made to the paper and offers constructive suggestions for further clarification. The phrase \"I would greatly appreciate it\" indicates a supportive and collaborative tone.",
            "The review starts with a positive statement ('I am satisfied') but then transitions to providing constructive criticism and suggestions ('some suggestions to make the paper better'). It also expresses doubts ('I doubt the performance on complex images'), indicating a balanced perspective.",
            "The review points out specific flaws and areas for improvement in the paper, using direct questions and suggestions. The reviewer challenges the authors' claims about complexity and requests more clarity in the explanations. Phrases like \"I wonder,\" \"it seems,\" and \"More discussions are encouraged\" convey a critical but constructive tone.",
            "The review adopts a balanced tone, acknowledging both the strengths and weaknesses of the manuscript. It uses positive language such as 'favourable,' 'strength,' 'clever,' 'encouraging,' and 'convincing,' but also points out areas needing clarification ('not clear,' 'typo,' 'significance is not clear') and suggests improvements (clarifying novelty, formal treatment).",
            "The review acknowledges the strengths of the paper (reducing complexity, integration with neural compressors, performance improvements). However, it also points out weaknesses (baselines not state-of-the-art, missing comparison, unclear experimental setup) and offers specific suggestions for improvement, including comments on writing clarity. The reviewer uses cautious language ('should be done with caution') and ultimately expresses a conditional positive outlook ('leaning towards an accept if the authors can convincingly address the points above')."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently expresses concerns about the clarity of the methodological contribution and the potentially misleading presentation of the experimental results, providing specific examples and justifications for these concerns throughout the review. The reviewer's critique is focused and logically structured, moving from general impressions to specific issues in different sections of the paper without contradictions.",
            "The review is consistent because the reviewer acknowledges the improvements in the paper and expresses a remaining concern in a constructive manner. The reviewer's points are logically connected and aim to refine the paper rather than contradict their initial positive assessment which led to a score change to weak accept.",
            "The review is consistent because it starts with a positive comment and then provides constructive suggestions for improvement. The suggestions are logically connected and do not contradict each other. The reviewer points out both strengths and potential weaknesses of the paper in a balanced way.",
            "The review is consistent because it acknowledges the novelty and potential of the proposed method (PC) in the strengths section, while the weaknesses section consistently focuses on the lack of clarity, justification, and detailed explanations of the PC framework. The reviewer appreciates the idea but criticizes the presentation and supporting arguments, maintaining a consistent critical but constructive tone throughout the review.",
            "The review is consistent because it acknowledges the strengths of the manuscript, such as the clever use of Probabilistic Circuits for lossless compression, faster compression times, and competitive compression rates. At the same time, it constructively points out areas for improvement, mainly focusing on clarifying the novelty and significance of certain aspects like Algorithm 2 and Theorem 1, and the contributions in the scaling up process. The reviewer's positive assessment of the experimental results and the overall approach is not contradicted by the suggestions for improvement, which are aimed at enhancing the clarity and impact of the work.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the application of Probabilistic Circuits and competitive performance, while also pointing out specific weaknesses, primarily regarding the choice of baselines and clarity in writing. The reviewer's overall assessment is balanced, leaning towards acceptance if the authors address the raised points, which aligns with the identified strengths and weaknesses. There are no contradictory statements within the review; the reviewer provides a coherent and reasoned evaluation."
        ]
    },
    {
        "paper_id": "iclr_2021_VG3i3CfFN__",
        "paper_title": "PhraseTransformer: Self-Attention using Local Context for Semantic Parsing",
        "paper_abstract": "Semantic parsing is a challenging task whose purpose is to convert a natural language utterance to machine-understandable information representation. Recently, solutions using Neural Machine Translation have achieved many promising results, especially Transformer because of the ability to learn long-range word dependencies. However, the one drawback of adapting the original Transformer to the semantic parsing is the lack of detail in expressing the information of sentences. Therefore, this work proposes a PhraseTransformer architecture that is capable of a more detailed meaning representation by learning the phrase dependencies in the sentence. The main idea is to incorporate Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results show that the proposed model captures the detailed meaning better than Transformer, raises local context awareness and achieves strong competitive performance on Geo, MSParS datasets, and leads to SOTA performance on Atis dataset in methods using Neural Network.",
        "review_ids": [
            "7dO025WVpw3",
            "RSQy37Iuia3",
            "hmHor482DA2",
            "UAgO0xMUWXg"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper modifies the self-attention mechanism in transformers to function at the phrase level, rather than at the token level, as a means to improve alignments between input phrases and logical form predicates for Semantic Parsing tasks. They achieve this by using LSTMs on the token representations to form n-gram representations in the attention head, and then performing attention on these n-gram representations. They call this modified transformer, a PhraseTransformer. They are able to improve over baseline transformers on 3 datasets.\n\n**Strengths and reasons to accept**\n\n1. The model improves upon existing neural models on three dataset, achieving SOTA on MSparS. \n2. The idea is adequately motivated and the model analyses appropriately corroborate the motivation.\n3. The model is described well. Other than the tiny size of the figures and grammatical errors, the model is quite easy to follow.\n\n**Weaknesses and possible reasons to reject**\n\n1. I'm concerned about repeated evaluations on the test set i.e. the model might be overfit to the test set. For example the numbers reported in Table 2, seem to be test set numbers, since they exactly match the numbers in Table 3. This suggests that n-gram sizes were fine-tuned on the test set, which is concerning.\n2. How does adding multiple LSTMs to the attention affect the runtime complexity and parallelization ability of the attention mechanism? I feel that the LSTMs add a lot of overhead to the attention mechanism, and might not scale to larger datasets. Can the LSTM be replaced with a simpler operation to achieve the same result? \n3. Although they outperform recent neural models, don't forget about Wang et al. 2014 (Morpho-syntactic Lexical Generalization for CCG Semantic Parsing) who are still the SOTA on ATIS with an accuracy of 91.3 \n4. The datasets used are quite old and performance on these datasets is quite saturated. Might be useful to evaluate on newer semantic parsing datasets such as TOP or other SQL based datasets such as WikiSQL or SPIDER.\n\n**Other issues:**\n\n1. The citation format is quite terrible in the paper. There are no separators whatsoever between the cited papers and the main text. \n2. Lots of grammatical errors. Very hard to understand in certain spots. \n3. Figure 2 is extremely tiny on print and even on screen.\n\n**Update after rebuttal - I would like to keep my score**\n1. Although the authors have provided dev set numbers, the fact that test set numbers were computed, is highly concerning. In fact, for ATIS, the best dev model is different from the best test model. The other models should never have been evaluated on the test set. \n2. Am still concerned about the datasets being old and saturated, and would love to see results on more recent datasets.",
            "*Summary of the paper*: One drawback of the transformer architecture is that they often fail to capture local interactions within the sentence. In this paper, the authors propose the PhraseTransformer architecture which incorporates Long Short-Term Memory (LSTM) into the Self-Attention mechanism of the original Transformer to capture more local context of phrases. Experimental results on three semantic parsing datasets show that the phrase level transformer is better at capturing the local information than the original transformer.\n\n*Strength of the paper*:  This paper proposes to use phrase-level information to capture the local dependencies of the transformer architecture and the experimental results show that semantic parsing could benefit from such local dependencies. The authors also conduct empirical experiments -- section 4.3.2 to show why their method is helpful. The paper is well-structured and easy to follow.\n\n*Weakness of the paper*: \n\n(1) The idea of using different-granularity representation including phrase-level representations of transformer architecture has been proposed before. For example, Hao, et al. 2019 study multi-granularity representations for self-attention in machine translation; Yang, et al. 2018 study the localness of the self-attention mechanism; Nguyen, et al. 2020 study the tree-structured representations of the self-attention networks. The authors should have done a detailed literature review along this line of works. \n\n(2) The proposed method is quite empirical and from Table 2 it is unclear how to set the n-gram size for different layers. A more detailed experiment demonstrating how different n-gram sizes in different layers affects the model would be quite helpful. \n\n(3) The improvement over the original transformer is marginal except on the Atis dataset, the claims would be more convincing if the authors could conduct similar experiments on other tasks. \n\n(4) For the error analysis part, I would like to see a more systematic analysis rather than two examples posted in the paper: is there a specific type of error being corrected by the phrase-level representation? In what scenario will the phrase-level representation help most?\n\n*Reason for score*: Overall, I vote for rejecting this paper. I like the idea of trying different-granularity representations for the transformer. However, such kind of ideas has been proposed before and this paper does not bring new insights into using such representations.  \n\n*Reference*: \n\nHao, Jie, et al. \"Multi-Granularity Self-Attention for Neural Machine Translation.\" arXiv preprint arXiv:1909.02222 (2019).\n\nYang, Baosong, et al. \"Modeling localness for self-attention networks.\" arXiv preprint arXiv:1810.10182 (2018).\n\nNguyen, Xuan-Phi, et al. \"Tree-Structured Attention with Hierarchical Accumulation.\" arXiv preprint arXiv:2002.08046 (2020).\n",
            "This paper extends Transformer to integrate representations of ngrams for semantic parsing. The key idea is to split input sentences into ngrams of different orders and utilize LSTM to build their representations before feeding them to the layers inside a transformer. The experimental results show that this modification leads to marginal improvement on three benchmark datasets of semantic parsing.\n\nThe encoders of the previous neural semantic parsing methods only learn the dependencies between tokens but ignore the local context around the tokens. Therefore, to exploit the local context information, this work provides some main contributions: \n-- This work introduced a novel Transformer encoder to encode the n-grams in each utterance.  \n-- This work showed the effectiveness of this new model on three benchmark datasets. \n-- This work displayed the model capacity by visualizing the alignment between the source tokens and the target logical forms, and the similarity of the phrase representations.  \n\nThe method is evaluated on three datasets, Geo, Atis and MSParS with two evaluation metrics, Exact Matching and Logic Matching. Compared with Exact Matching, Logic Matching is able to compare the variants of the logical forms.\n\nStrengths:\n\n-- It is a good method to exploit the local context information with the Transformer architecture. And this method seems to be easy to implement.\n\n-- There is a thorough evaluation which displays the model performance, and visualizes the attention alignment and the similarity of the phrase representations. \n\nWeakness:\n\n-- The first thing I am concerned about is the novelty. Although in semantic parsing, there is no previous work that utilizes local context information with a Transformer, there are many similar architectures in machine translation. The proposed Transformer-based model is also totally applicable to machine translation scenarios, which makes it necessary to compare the model performance with the machine translation models that exploit local context as well.\n\n-- The second is that the performance is not significant enough. Although this work claims that the performance is superior to the baselines on two benchmark datasets, it should be noted that the other baselines report only the Exact Match accuracy while this work reports the Logic Match performance. Considering only the Exact Match comparison, the proposed method is only 0.4% and 0.02% higher than the baselines on Atis and MSParS, respectively, which is not significant at all. If using Logic Match as the main metric, it would be better to re-evaluate the baselines with Logic Match as well for fair comparison.\n\n--  The description of the model in Sec. 3 is not clear enough.\nHow does the model split an input sentence into ngrams? Do the adjunct ngrams have overlapped subwords/characters or not?\n\n-- I found this contribution too close to the following work.\nHao, Jie, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. \"Modeling recurrence for transformer.\" arXiv preprint arXiv:1904.03092 (2019).\n\n-- In addition:\n\n---- The citation of logic matching seems to be incorrect.\n\n---- The sentence \u201cW is parameters, LayerNorm, FeedForward are the functions by proposed by \u2026 .\u201d is misleading because LayerNorm and feed forward networks are not proposed by Vaswani et al.\n\n---- What is model 4 and model 5 in Table 3?\n\n---- Figure 5 (b) is hard to read\n\nMinor issues and improvement suggestions:\n\n-- Compare the proposed method with the baselines which are from machine translation fields which also exploit the local context.\n-- Improve the model performance. \n\nReasons to accept: \n-- Proposed an interesting Transformer-based encoder to exploit the local context information.\n\nReasons to reject:\n-- Novelty is not enough. The evaluation results can not show the superiority of this model.\n\n",
            "##########################################################################\nSummary:\nThis paper takes on a challenging task, i.e., semantic parsing from natural language text.  The paper proposes an improved version of the transformer for the task of semantic parsing, called PhraseTransformer to overcome the limitations (i.e., failure to capture local sentence context effectively) of the transformer. It proposes to use LSTM networks and the Self-Attention mechanism of the original Transformer and provides experimental results for up to 4-grams representations of the sentences to better accommodate the local contexts and achieves state-of-the-art (SOTA) performance on ATIS dataset, and competitive performance on other datasets such as Geo and MSParS. \n \n##########################################################################\nReasons for score: \n \nOverall, I like the idea and I would like the paper to be accepted. The main reason for my acceptance is that in a natural language many values are actually phrases (e.g., one way) and it is very critical to capture the meaning of n-grams as one unit (i.e., one representation for the full n-gram) and consider their context locally to better understand the meaning in the context of the sentence, and effectively parse the natural language texts. \n \n##########################################################################Pros: \n \n1. The paper tackles a very important problem, i.e., parsing natural language text into a semantic frame. Natural language is complex and the semantic frame is easy to handle, which facilitates many down-stream NLP tasks. \n \n2. The proposed approach is also very interesting, i.e., captures the meaning of the phrases or n-grams in the context of the whole sentence.\n \n3. Reasonable experiments have been provided to prove the efficacy of the proposed approach. Moreover, it also outperforms the SOTA model on the ATIS dataset.\n \n##########################################################################\nCons: \n \n1. Although the paper compares with several baselines and SOTA models, an important model for comparison is skipped, i.e., SpanBERT: Improving Pre-training by Representing and Predicting Spans, recently published in ACL 2020 (July 2020 published more than two months ago). Still, I will not recommend rejecting this paper only because of this.\n \n2. The paper fails to explain in detail how n-grams are generated? And how these representations are put back to generate predictions at the word level. Since some n-gram would make sense to form a phrase (e.g., \u201cone way\u201d or \u201cSan Francisco\u201d), but many would not be valid phrases (e.g., \u201ca one\u201d, \u201cway ticket\u201d, \u201cticket from\u201d are not valid phrases in natural language text). Also, it is not trivial to know in advance what is the suitable value for \u201cn\u201d in the n-grams for a given sentence. Please explain this step: not sure but maybe a working example can explain it in a much better way. \n \n##########################################################################\nQuestions during the rebuttal period: \n \nPoint # 1 in the cons section is not very critical for my decision, it may help the authors to improve their paper.\n\nPoint # 2 is critical for me to understand the key idea. I would love to see a working example that shows a sentence, how n-grams are generated (also, how n is decided), and how final predictions are performed. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses several concerns, including potential overfitting to the test set, the impact of LSTMs on runtime complexity, the use of outdated datasets, and issues with citation format and grammatical errors. The reviewer maintains their score even after the rebuttal, indicating persistent reservations.",
            "The reviewer ultimately recommends rejecting the paper due to a lack of novelty and marginal improvements over existing methods. Phrases like \"does not bring new insights\" and concerns about the empirical nature of the method contribute to the negative sentiment.",
            "The review identifies several weaknesses including lack of novelty, insignificant performance gains, unclear model description, and concerns about the contribution being too similar to existing work. It also points out issues with citations and clarity in the paper.",
            "The reviewer states 'Overall, I like the idea and I would like the paper to be accepted. The main reason for my acceptance is that in a natural language many values are actually phrases' and lists several pros, indicating a positive overall sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"concerned about repeated evaluations\", \"highly concerning\", \"citation format is quite terrible\", and \"lots of grammatical errors\". These expressions convey a critical assessment of the paper's methodology, results, and presentation.",
            "The review identifies several weaknesses with specific critiques. The reviewer points out missing literature review, lack of clarity in parameter settings, marginal improvements, and insufficient error analysis. The use of questions like \"is there a specific type of error being corrected...\" indicates a critical perspective.",
            "The review uses phrases like 'I am concerned about the novelty,' 'performance is not significant enough,' 'description of the model is not clear enough,' and 'I found this contribution too close to the following work.' These phrases indicate a critical assessment of the paper's merits.",
            "The reviewer explicitly states their desire for the paper to be accepted and frames their criticisms as opportunities for improvement ('it may help the authors to improve their paper'). The reviewer also acknowledges the paper's strengths and importance, suggesting a supportive stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents a balanced perspective, highlighting both strengths and weaknesses of the paper. The reviewer's concerns, particularly regarding potential overfitting and the use of older datasets, are consistently maintained throughout the review and even after the rebuttal. There are no contradictory statements or shifts in opinion within the review.",
            "The review is consistent because the weaknesses listed logically support the rejection recommendation. The reviewer acknowledges the paper's strengths but points out significant shortcomings in novelty, empirical validation, and impact, which justify the negative evaluation.",
            "The reviewer acknowledges the method's strengths, such as being a good approach to exploit local context and easy to implement. However, the weaknesses, particularly the lack of novelty and marginal performance improvement, are consistently emphasized as major concerns that outweigh the strengths, leading to a negative overall assessment and reasons to reject. The reviewer's points are logically connected and lead to a consistent conclusion about the paper's limitations.",
            "The review is consistent because the reviewer expresses a positive overall opinion and recommends acceptance. The listed cons are framed as minor issues or requests for clarification and improvement, not as reasons for rejection. The reviewer explicitly states they \\\"like the idea\\\" and \\\"would like the paper to be accepted\\\", and that one of the cons is not critical for their decision."
        ]
    },
    {
        "paper_id": "nips_2021_pBKOx_dxYAN",
        "paper_title": "SNIPS: Solving Noisy Inverse Problems Stochastically",
        "paper_abstract": "In this work we introduce a novel stochastic algorithm dubbed SNIPS, which draws samples from the posterior distribution of any linear inverse problem, where the observation is assumed to be contaminated by additive white Gaussian noise. Our solution incorporates ideas from Langevin dynamics and Newton's method, and exploits a pre-trained minimum mean squared error (MMSE) Gaussian denoiser. The proposed approach relies on an intricate derivation of the posterior score function that includes a singular value decomposition (SVD) of the degradation operator, in order to obtain a tractable iterative algorithm for the desired sampling. Due to its stochasticity, the algorithm can produce multiple high perceptual quality samples for the same noisy observation. We demonstrate the abilities of the proposed paradigm for image deblurring, super-resolution, and compressive sensing. We show that the samples produced are sharp, detailed and consistent with the given measurements, and their diversity exposes the inherent uncertainty in the inverse problem being solved.\n",
        "review_ids": [
            "XXSqDkwb2X",
            "ZBMif7sUhYH",
            "AuboLatzvn",
            "WJ40-G0-VQC",
            "wEyaUkJ2YqE",
            "mNjQov44yjd"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a technique based on annealed Langevin dynamics to solve noisy linear inverse problems. The technique is built on top of [18, 19] and proposes to perform the Langevin dynamics in the spectral space of the linear degradation operator $H$. To improve the performance of the proposed method, the authors use coordinate wise step sizes obtained by approximating the hessian with its diagonal. Qualitative results from the proposed method are presented for compress sensing, inpainting, super-r\u00e9solution and deblurring.  \nReview Summary\n--------------\n\nThis paper proposes a technique based on annealed Langevin dynamics to solve noisy linear inverse problems. IMO, the novelty of this paper is limited as it seems to boil down to applying [19] in the spectral space of the operator $H$ (**M1**). Moreover, the numerical experiments are quite limited, with mainly qualitative results and no comparison with other methods (**M2**). For these reasons, I recommend rejecting this paper. As I might be wrong for the spectral space part, I will carefully read the author response and revise my score if necessary.\n\n\nMajor comments and questions\n----------------------------\n\n* **M1:** One main concern for this paper is its limited novelty. The posterior sampling using Langevin dynamics has been described in [18] and [19] proposed a first adaptation to the inverse problem for specific operator $H$. The core contributions of the papers are thus computing the conditional score function in the spectral space of the operator and proposing to use per-coordinate step sizes:\n    - **A:** The authors states that the extension of Langevin dynamics to spectral space is far from trivial but I am not really convinced about this. Looking at the equation (10), we can see that the dynamic is independent for each coordinate in the spectral space and the propose algorithm boils down to the one in [19] but applied to all variables in spectral space, where the noise levels are different for each coordinate. Equation bellow l.182 simply states that the added noise $\\eta_{T,k}$ should sum to $z_T$ (*i.e.* the standard assumption but in the spectral space). The most complex part was to show that $\\nabla_{\\tilde x_{T, >}}\\log p(\\tilde x_{T, 0}|\\tilde x_{T, \\not 0}) = 0$ but this result is taken as an assumption (l.60 in the supplementary) while it should probably holds as $V^T$ is unitary and thus the added noise are independent for each coordinate in spectral space. Overall, it seems that the proposed derivation could be simplified a lot while retaining their efficiency.\n    - **B:** For the step-sizes adaptation, it makes sense that it should be adapted on each coordinates as the noise levels are not the same. However, the benefit of using such a scheme is not demonstrated in the paper.\n\n* **M2:** Another major concern for this paper is with the experimental results. Indeed, the paper does not propose any comparison with other methods. For inverse problem solution sampling, the authors could compare their results with the one from [30]. The mean results could also be compared with Plug-and-Play or Regularization by denoising methods, in particular with the PSNR from Table.1. Without such validation, it is hard to assess wether this method provides good results or not. Also, adding an ablation study to show the benefit of the proposed step-size compared to using the classical one would help convincing that it indeed accelerate the dynamic.\n\n\nMinor comments, nitpicks and typos\n----------------------------------\n\n\n* **m1:** (Numerical experiments) The part of the LSUN and CelebA datasets used to train the NCSNv2 models used to approximate the prior distribution are not properly described in the text. From the code, it seems that the training set is used and that the inverse problems are constructed from the test/validation sets but this should be properly described in the paper.\n\n* **m2:** (Numerical experiments) The parameters that are used in the experiments are not described in the paper. In particular, the sequence of sigma that is chosen (it seems to be geometrical from the code) and the step sizes\n\n* **m3:** (Eq.(10) & Algorithme.1 - l.9) the notation $|\\sigma_0^2 I - ... |^\\dagger$ is confusing. I think it would be clearer to use $(\\sigma_0^2 I - ... )^\\dagger$ as done in Eq.(8). * **L1:** One major limitation of such method is that it is limited to simple noise models as one needs to derive an expression for the score function. Moreover, the noise level must be known in advance for the method to work. These limitation should be mention in the section.6. or discuss in the experimental results, for instance showing the results when the noise is more complex or only an approximation of the noise level is given.\n\n* **L2:** Another limitation that should be mentioned is that this technique has only been tested for inverse problem in imaging. For more complex inverse problems where the ground truth is unknown and the noise more complex, such as in astrophysics or neuroscience, there is little chances to be able to apply such techniques to recover the signal of interest.",
            " I thank the authors for the answers and clarifications they provided. Originally, my main concerns with the paper were its novelty compared to [19] seems a bit inflated and the limited numerical evaluation/lack of application of the paper.\n\nRegarding the numerical evaluation (**M2**), the authors propose in their answer to add a more quantitative evaluation and comparison with other algorithms. I find their answer satisfactory as it will allow to better evaluate the method.\n\nRegarding the novelty (**M1**), the main proposition is to apply the method from [19] in the spectral space with adapted step sizes, as stated in the rebuttal by the authors. Re-reading l.70-75, it seems that this is already hinted in the manuscript however, the derivations in the paper does not make this very clear, making it hard to see what is a contribution or not. This gives me a mixed feeling about the paper.\n\nMoreover, I am not totally clear about what would be realistic the applications of such methods (**L2**). As stated by reviewer KMnW, for most inverse problem, we aim for the best possible reconstruction for a given criterion. While I agree that Langevin dynamic can help quantify uncertainty and provide multiple reasonable images, I feel that the statement `When perceptual quality becomes our prime objective one should concentrate on producing a sample from the posterior distribution` is too strong.\n\nOverall, I still think this paper should be reworked a bt before publication, with clearer exposition and at least a more concrete application. That being said, I will understand if this paper is accepted as is based on the other reviews. I will therefor raise my rating to 5.",
            " I thank the authors for the careful replies to my questions. I keep my recommendation that the paper should be comfortably accepted. \n",
            "The paper describes an apparently new approach to solving noisy inverse problems.\n\nIt shows how the approach can be applied to three different noisy inverse problems:\nwhich it calls image deblurring, super-resolution, and compressive sensing. \n\nIt validates the approach on synthetic data which is synthesized\n(eg blurred and noise added) starting from publicly available imagery.\n\nIt is carefully written and I find it readable.  The paper describes an apparently new approach to solving noisy inverse problems.\n\nIt shows how the approach can be applied to three different noisy inverse problems:\nwhich it calls image deblurring, super-resolution, and compressive sensing. \n\nIt validates the approach on synthetic data which is synthesized\n(eg blurred and noise added) starting from publicly available imagery.\n\nIt is carefully written and I find it readable.\n\nOriginality: I have seen work on langevin dynamics in inference problems before,\nbut I think that work was theoretically focused and the computational efforts\nwere just preliminary efforts rather than mature tool construction.\nI believe the originality here is in the effort to deliver something\nthat's been carefully derived, using sophisticated identities\nrather than handwaving, and carefully tested on (stylized approximations to) \n'real-world' problems.\n \nQuality: The paper is very well written at the narrative level. \n\nClarity: I find it very clear.\n\nSignificance: I am unable to judge whether the proposed method is an improvement over\nexisting methods. Does this method do better than the state of the art on the problems\nin its purview (denoising, super-resolution, compressed sensing) where so many previous\nefforts have been made and carefully documented? I don't know.\n\n I think what they call compressed sensing is a not at all compressed sensing.\nThey are not doing random undersampling either in Fourier space nor do they\nseem to be doing random undersampling in some other randomly orthogonally transformed domain.\nI think they are simply dealing with undersampled data in the original pixel domain.\nPossibly this is a real problem in certain settings, but it shouldn't be\ncalled compressed sensing.\n\nI think the identity (3) that they call Brilliant is well known to Mathematical Statisticians for\ndecades as \"Stein's Integration by Parts trick\" after Charles Stein, a leading mathematical\nstatistician of the 20th century. \nSee for example Stein, Charles M. (November 1981). \"Estimation of the Mean of a Multivariate Normal Distribution\". The Annals of Statistics. 9 (6): 1135\u20131151. doi:10.1214/aos/1176345632 (many other statistics papers use this idea, \nphysicists also know it well by other names)",
            "This paper proposes to combine Langevin dynamics and Newton's method to arrive at a posterior sampling algorithm, where the prior distribution is given by an MMSE Gaussian denoiser. The proposed method is evaluated on multiple imaging inverse problems like deblurring, super-resolution, etc.  Strengths: the method proposed in this paper is original and technically sound. \n\nThe authors utilize denoiser-based deep prior and blurred posterior function, which is very similar to the approach used in (Bigdeli et al. 2017). It could be helpful if similarities and differences with that work are discussed.\n\nWeakness:\n- The introduction section extensively discusses the properties of the proposed method before actually introducing the method. IMHO the introduction should be precise, and more space can be dedicated to the experimental results section.\n- The experimental results section heavily relies on subjective results. Although understandably, objective metrics like PSNR cannot reflect the perceptual quality, it is still desirable to include some objective results comparing to some baseline methods in the main text.\n- In the bibliography, venues should be properly capitalized (e.g. line 335)\n\nReferences:\nBigdeli, Siavash A., et al. \"Deep mean-shift priors for image restoration.\" Proceedings of the 31st International Conference on Neural Information Processing Systems. 2017. Have the authors adequately addressed the limitations and potential negative social impact of their work: Yes.",
            "This paper proposes a method to sample from the posterior density in the Bayesian formulation of linear (imaging) inverse problems using Langevin dynamics. The main contribution of the paper is a way to generalize the idea (which is not new) beyond diagonal observation operators (denoising and inpainting), by resorting to an SVD of the operator.\n\n   This paper proposes a method to sample from the posterior density in the Bayesian formulation of linear (imaging) inverse problems using Langevin dynamics. The main contribution of the paper is a way to generalize the idea (which is not new) beyond diagonal observation operators (denoising and inpainting), by resorting to an SVD of the operator and some \"intricate\" derivations. \n\nThis is a very nice paper and the proposed formulation is solid and constitutes an important contribution to the topic of imaging inverse problems. However, there are a few minor issues that should be addressed. \n\nA fundamental limitation of the proposed method, which should be clearly acknowledged, is that sampling from the posterior is not applicable/acceptable in many domains. Look, for example, at the samples in the compressive sensing or super-resolution experiments; in some samples, the teeth are visible in others not. If this was a medical image reconstruction/restoration scenario, this could correspond to some samples showing a tumor, others not. In such a context, what is required is THE best possible reconstruction (under some optimality criterion), not samples from the posterior, which can vary significantly. \n \nUnlike what the authors claim, it is not true that \"most image restoration techniques aim to form an MMSE estimator\". In fact, the vast majority of image restoration approaches are formulated as optimization problems, which can be interpreted as MAP estimators or, from a regularization theory point of view, as regularized inversion of the (ill-posed) observation operator. There are a few attempts at leveraging MAP/regularization approaches to yield MMSE estimates, such as https://epubs.siam.org/doi/abs/10.1137/120902276, but it is certainly not the majority. \n\nThe argument that \"the MMSE solution averages all these candidate solutions, being the conditional mean of the posterior of x given y, leading to an image with loss of fine details\" is not new, but it certainly requires some sort of justification, as it is far from evident that this is true. Unless the posterior is strongly multimodal, the posterior expected value could very well itself be a sample. As such, without any comment about the properties of the posterior, this is nothing but an empirical observation. \n\nIn lines 104-107, the authors write \u201cthat using MMSE Gaussian denoisers iteratively for handling general linear inverse problems has been already proposed in the context\nof the Plug-and-Play-Prior (PnP) method and RED, and their many follow-up papers. However, none of the cited methods use explicitly MMSE denoisers, but general-purpose denoisers. In fact, at the heart of the PnP is the idea that you can replace the proximity operator of some regularizer with a (Gaussian) denoiser, but this equivalence sees it as a MAP denoiser, not an MMSE one. The only work where PnP is used with an explicit MMSE denoiser is https://doi.org/10.1109/TIP.2018.2869727\n\nThe derivations and explanations in lines 149 to 166 are unnecessarily \u201cintricate\u201d (to use a word that the authors have used a lot). If you simply start with the equation between lines 161 and 162 and use the definitions in line 160, then everything (namely equation (5)) falls directly from there. In particular, you don\u2019t need equation (4), which anyway is not referred to in the rest of the paper and is trivial from the formulation between lines 161 and 162. \n\nIt would be interesting to see if the formulation (and the resulting computation) is significantly simplified in the case where H is a periodic convolution (thus with M=N), in which case H is diagonalized by the discrete Fourier transform and, consequently, multiplications by U and V can be done with O(N log N) cost using FFTs.\n\nIn the experiments, the main claim is that, although having a lower PSNR (which is natural, since they are not the MMSE estimate, which maximizes the PSNR), the samples are more \"visually appealing\". This claim could be, at least in part, supported on other quality metrics, such as SSIM (structural similarity index measure), which correlates better with human quality perception.\n\nFinally, there is no point in showing the \u201ddegraded\u201d images in the compressive sensing experiments, since these are not degraded images, but observations that are meaningless to a human observer. \n\n\n\n\n\n Some limitations have not been addressed, as mentioned in the main review above. Namely, that this type of approach may not make sense in some contexts, such as for medical image reconstruction. My recommendation is that this limitation should be briefly discussed in the paper.\n "
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Neutral",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses concerns about the paper's novelty and the limited experimental results, recommending rejection. Phrases like \"limited novelty\", \"not really convinced\", and \"major concern\" indicate a negative sentiment.",
            "The review expresses both positive and negative aspects. The reviewer appreciates the authors' responses but still has reservations about the novelty and application of the method. The reviewer is willing to increase the rating but suggests further work.",
            "The reviewer explicitly states they \"keep my recommendation that the paper should be comfortably accepted,\" indicating a positive sentiment.",
            "The review expresses both positive aspects (carefully written, readable, clear) and negative aspects (concerns about originality, mislabeling of compressed sensing, and misattribution of a known identity).",
            "The review identifies both strengths and weaknesses, leading to a balanced perspective and a neutral sentiment.",
            "The review identifies several limitations and inaccuracies in the paper, using phrases like 'fundamental limitation,' 'not true,' 'not new,' 'unnecessarily intricate,' and pointing out issues with the experimental setup and claims. While acknowledging the paper's contribution, the reviewer focuses on these shortcomings, resulting in an overall negative assessment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review employs direct criticisms, such as \"the novelty of this paper is limited\", \"not really convinced about this\", and \"Another major concern for this paper is with the experimental results\". The reviewer also points out limitations and areas for improvement, indicating a critical tone.",
            "The reviewer presents both positive feedback (acknowledging the authors' responses and willingness to increase the rating) and critical feedback (expressing concerns about novelty, clarity, and application). The language is measured and constructive, indicating a balanced perspective.",
            "The reviewer expresses gratitude (\"I thank the authors\") and reinforces their positive recommendation, indicating a supportive tone.",
            "The review provides both positive feedback (e.g., \"carefully written\", \"readable\", \"very clear\") and critical feedback (e.g., concerns about originality, mislabeling of compressed sensing, and misattribution of a known identity). It is a balanced assessment of the paper's strengths and weaknesses.",
            "The review points out weaknesses such as the structure of the introduction, the reliance on subjective results, and a formatting error. The reviewer uses phrases like \"IMHO\" and \"it is still desirable\" which suggest a critical but constructive approach.",
            "The tone is critical, pointing out flaws and inaccuracies in the paper. Examples include: 'A fundamental limitation...', 'Unlike what the authors claim, it is not true...', 'The argument...is not new, but it certainly requires some sort of justification...', 'The derivations and explanations...are unnecessarily intricate...', 'Finally, there is no point in showing the 'degraded' images...' These phrases demonstrate a critical evaluation of the paper's content and presentation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer's detailed comments and concerns logically support the overall negative assessment and the recommendation for rejection. The reviewer clearly identifies weaknesses in novelty and experimental validation, and these points are elaborated in the major comments. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because it presents a coherent evaluation, showing a logical progression from initial concerns to conditional acceptance without contradiction.",
            "The review is consistent because the reviewer thanks the authors for their replies and maintains their positive recommendation for acceptance. There are no contradictory statements.",
            "The review is consistent because the reviewer's positive comments on writing quality and clarity are not contradicted by their concerns about significance, terminology (compressed sensing), and attribution of a known identity. The reviewer's critiques are logically separate points and do not create internal contradictions within the review.",
            "The review is consistent because it highlights both the strengths (originality and technical soundness) and weaknesses (presentation and experimental evaluation) of the paper without contradicting itself. The reviewer provides constructive criticism and suggestions for improvement while acknowledging the positive aspects of the work.",
            "The review is consistent in its critique, highlighting both the strengths and weaknesses of the paper. It acknowledges the paper's contribution while consistently pointing out specific limitations, inaccuracies, and areas for improvement. The reviewer maintains a clear and coherent line of reasoning throughout the text, without contradicting themselves."
        ]
    },
    {
        "paper_id": "nips_2021_pvjfA4wogD6",
        "paper_title": "Video Instance Segmentation using Inter-Frame Communication Transformers",
        "paper_abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. However, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality.In this work, we propose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip.Specifically, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene.The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay.The code is available at https://github.com/sukjunhwang/IFC\n",
        "review_ids": [
            "AZgws3wYVEA",
            "ZMErSbcgMdm",
            "eZBr9an5Sr",
            "Ut318yi9o7K",
            "MU1xvsRHUz",
            "C7H43_ZVsN8"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes inter-frame communication transformers(IFC) for video instance segmentation. Compared to a full spatial-temporal transformer with complexity THW x THW, the proposed IFC module reduces it to THW + T(HW)^2 while preserving the temporal and spatial attentions. The proposed video instance segmentation network outputs a clip-level prediction, which is then stitched along the time stamps via Hungarian matching between the tracklet tubes. The experiments on YouTube-VIS 2019 dataset shows the effectiveness of the proposed method.  * The proposed memory tokens are memory-efficient and effective at learning temporal relationships between frames. \n\n* The paper provides extensive experiments and ablation studies. \n\n* The paper is overall well-written and easy to follow.\n\n* It seems that the proposed method provides a good efficiency-accuracy trade-off. How does it compare with MaskProp [10]?\n\n* If I understand it correctly, the span of instance tracking is controlled by T (which defaults to 5 time stamps). How does the proposed model handle long-term tracking of the instances (e.g. re-appeared objects after 5+ or T+ frames)? How much is the performance drop caused by missing such long-term tracking? \n\n* Why is the reported score in Table 2 (41.0 AP) and in Table 3 (39.0) different? Is it that one is the best score and the other is average score out of multiple runs (5 runs)? I could not find the explanation for this gap, and this should be clarified.\n\n* The proposed method looks similar to 'decomposed T + HW' attention. How is the 'T + HW' version implemented? and why is it worse than the proposed method?\n\n* How is the 'full THW' implemented? It's opposed to an expectation that the 'full THW' model should be the upper bound in terms of accuracy. What is missing in the 'full THW' model, and what makes the proposed method perform better? The authors have adequately addressed the limitations and potential negative societal impact of their work.\nFor long-term tracking of the instances, please see the main review.",
            "The paper extends transformer-based video instance segmentation framework VisTR in three aspects. First, the intra- (i.e., space) and inter-frame (i.e., time) relation computations in the transformer encoder are decomposed. Specifically, the extra memory tokens are introduced for efficient intra- and inter-frame communication. Second, the fixed object queries are used in the transformer decoder and thus the model can take a varying number of input frames. Moreover, the contextualized object queries are forwarded to the segmentation head so that the adaptive convolutional weights can be generated for effective tracking and segmentation. Third, the authors presented to use 3D IoU to link the already tracked tracklets and the currently generated tracklets to produce final video-level tracklets. Combining all together, the final framework is evaluated on the benchmarks and achieves new state-of-the-art.  [Originality]\n\nThe main contribution of this paper lies in presenting a solid transformer-based video instance segmentation framework that pushes the state-of-the-art result significantly and is fast. The idea of introducing memory tokens for effective intra- and inter-frame relation computation is novel and interesting. \u200b\nHowever, there are also several ideas that are not technically new. \n\n- The similar memory-token-based relation computation is also described in the following work. However, I will not count the following paper as a concurrent work since it is published in arXiv near the submission. Please compare the proposal with the following work.\n\n[1] End-to-End Video Object Detection with Spatial-Temporal Transformers, arXiv 2021 \n\n- The fixed object queries which are shared across the frame are also tried in the original VisTR paper (please refer to the original paper Table-2).\n- The idea of using adaptive convolutional weights for instance segmentation is a promising technique that is verified in various previous literature (e.g., CondInst, SOLO, Max-deeplab).\n- Using an FPN-like spatial decoder obviously gives an extra performance gain in pixel-level prediction tasks.\n- The mask-based sequence matching is a direct extension of VisTR's box-based matching. While clearly effective, it consumes more memory than the box-based counterpart.\n- The final loss functions (i.e., dice loss + focal loss) are from the VisTR.\n- The 3D IoU score for clip-linking is a direct video-level extension of MaskProp's averaged 2D IoU scores.\n\n[Quality]\n\n- The authors well-combined the previous techniques into a single framework.\n- Extensive ablation studies show the efficacy of the proposals.\n- The video results are quite impressive.\n\n[Clarity]\n\n- As the key technical contribution is on the memory token, please discuss and compare it with the recent space-time factorization methods in more detail. For example, the following paper also introduces space-time factorization methods for video embeddings in the transformer.\n\n[1] ViViT: A Video Vision Transformer, arXiv 2021\n\n[2] Is Space-Time Attention All You Need for Video Understanding?, ICML 2021\n\n- The method section is well-written and easy to follow. Table-1 shows a clear advantage of the proposal over the baselines.\n\n- Typos\n[1] Line 291 Our model is shows -> Our model shows\n\n[Significance]\n- While there are several components that are not new, each component is combined in a novel way and the final model achieves non-trivial gain over the baselines both in accuracy and speed.\n- As transformer-based video models are becoming more and more important these days, the work provided has several strong points (memory token-based transformer encoder design and efficient tracklet matching) that need to be shared with the community.\n\n[Question & Suggestion]\n- The position encoding remains to be the same as VisTR. Did the authors have tried to drop it or try different types of embeddings? For example, the position prior can be embedded in the self-attention operation such as Swin Transformer.\n- Why do more memory tokens fail to provide performance gain (e.g., 8 vs. 16 in Table 3-(c))? More discussions need to be provided in the manuscript.\n- Please detail the visualization process in Figure 2.\n- Please provide a full model FLOPS comparison by adding an extra column in Table 3.\n- The authors can try to make the whole framework purely based on the Transformer (e.g., replacing the ResNet backbone with SETR). None",
            " I'm not sure why this was tagged for ethical review. I suppose there are long-term implications for surveillance that could be problematic. But, it's a bit of a long walk to get there from this paper.   N/A N/A",
            " I believe this paper was mistakenly flagged for ethics review by pxiA N/a N/a",
            "This paper proposes Inter-frame Communication Transformers (ICT) to solve the Video Instance Segmentation (VIS) task in one framework. In particular,  they propose to utiize concise memory tokens as a means of converying information as well as summarizing each frame content.  The method achieves the state-of-the-art performance  on YouTube-VIS 2019.   \nStrength:\n\n1, The overall writing is good. \n\n2, The experiment results on YouTube-VIS are good. \n\n3, The motivation is clear: reducing computation cost and of transformer and reducent information in video by mean of passing tokens. \n The main issue of this paper is the novelty.  In particular, they just apply the core idea of MaxDeeplab [1] into video domain. Thus there are no fundamental new messages in case of feature or object query fusion. \nHere are the details. \n\n1.  Architecute and Loss : The end-to-end solution for VIS is firstly proposed by VisTR[3].  It has been proven effectively for predicting multiple queries across time. Moreover, the decoder head and loss are the same with original DETR[2] (Panoptic Segmentation) paper.  Thus in case of architecture and loss, this is not novel. \n\n2. Method: The overall architecture is based on VISTR.  The core contribution of this paper is Inter-Frame Communication Encoder which contains Encode-Receive and Gather-Communicate. However, I can not tell the difference between the Query and Pixel Interaction in MaxDeeplab. They are same component by encoding the appreance information into Object Query via attention layers (while this paper call it token, also misleading and hard to under why called token).  Gather-Communicate process is most used in preovious video segmentation method in previous work by link encoded query across time [4].  \n\nIn summary, there is no new concept or new message contribution. The overall contributions are  incremental and there are many similar components in related fields.  At least, it dose not reach the \n \n\nThere are other issues in case of experiment comparison and writing. \n. \n1. It is unfair to compare privous work including SG-Net and CrossVIS since most previous work only uses previous frame information. This paper and VisTR use the whole clip as inputs. Moreover, the FPS is also misleading since VisTR-framework process one clip each time rather than one image. \n\n2.  What is pretrained model result on COCO since it has hudge effect on YouTube-VIS ?\n\n3.  What is difference between token and query in DETR? \n\n\nReference: \n\n[1] MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers CVPR-2021\n\n[2] End-to-End Object Detection with Transformers ECCV-2020\n\n[3] VISTR:  End-to-End Video Instance Segmentation with Transformers CVPR-2021\n\n[4] Temporally Distributed Networks for Fast Video Semantic Segmentation CVPR-2020\n\n",
            "- The paper proposes a video instance segmentation method based on Transformers. The authors propose to aggregate temporal features through memory tokens. Specifically,  each frame is summarized by several memory tokens.  A Transformer decoder is used to perform the inter-frame feature learning using these tokens.\n- The paper shows strong performance on the Youtube-VIS benchmark, outperforming state-of-the-art methods in both speed and accuracy.  # Strength\n- Using memory tokens to perform inter-frame representation learning is highly motivated.\n- The proposed method is highly efficient and effective, e.g., 46.5 fps/41.0 AP for near-online inference.\n\n# Weakness\n- Limited contributions. For each component in the proposed framework, only the memory token is the new technique. Others are proposed in previous papers, e.g., DETR and VisTR.\n- According to the ablations, the biggest improvement (2.2 AP) is from the bipartite matching strategy, i.e., using mask-based matching instead of box-based one. It makes this work less impressive.\n- The conclusion of 'we find that the memory token has more interests to instances that are relatively difficult to detect;' is not convincing. Are the visualized figures from only one of the memory tokens?  More analysis is needed to support this claim.\n- The conditional convolutional weights, i.e., single-layer 1x1 conv, look more similar to SOLOv2 [A] instead of CondInst [7]. Related discussion is missed. \n- Many typos and grammar errors, e.g.,  L137, L188, L291\n\n[A] Wang et al. Solov2: Dynamic, faster and stronger.  In NeurIPS, 2020.\n The limitations are not discussed. The potential negative societal impact is provided."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Neutral",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment, highlighting the method's efficiency, effectiveness, and the paper's clarity. Phrases like 'memory-efficient and effective,' 'extensive experiments,' 'well-written and easy to follow,' and 'good efficiency-accuracy trade-off' indicate a positive evaluation.",
            "The review acknowledges the paper's significant contributions, including pushing the state-of-the-art results, novel memory token idea, impressive video results, and non-trivial gains in accuracy and speed. It also highlights the importance of the work for the community.",
            "The reviewer expresses uncertainty (\"I'm not sure\") and downplays the ethical concerns (\"a bit of a long walk\"). While acknowledging potential issues, the overall sentiment is not strongly positive or negative.",
            "The reviewer expresses a belief about a procedural matter (the paper being mistakenly flagged) without expressing a strong positive or negative opinion about the paper's content or quality.",
            "The review expresses concerns about the paper's novelty, stating that it's an application of existing ideas (MaxDeeplab) to the video domain and lacks fundamental new contributions. The reviewer also points out unfair comparisons in the experiments and raises questions about specific implementation details.",
            "The review highlights several weaknesses, including limited contributions, reliance on existing techniques, and concerns about the validity of certain claims. The reviewer also points out typos and grammar errors, further contributing to a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Neutral",
            "Neutral",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is balanced, acknowledging the strengths of the paper (efficiency, experiments, clarity) while also raising specific questions and concerns about implementation details, performance discrepancies, and comparisons to other methods. The reviewer constructively probes for more information and clarification.",
            "The review provides both positive and negative feedback, highlighting strengths and weaknesses. This balanced approach indicates a fair and objective assessment.",
            "The language is relatively objective and avoids strong emotional expressions. Phrases like \"I'm not sure\" and \"I suppose\" suggest a neutral stance. The reviewer uses \"But\" to connect the ethical concerns to the paper, showing a balanced opinion.",
            "The tone is matter-of-fact and objective, simply stating a belief about a procedural error. There is no indication of strong emotion or bias.",
            "The review uses phrases like \"no fundamental new messages,\" \"not novel,\" \"no new concept or new message contribution,\" \"incremental,\" and \"unfair\" to express critical feedback. The reviewer also uses questioning to express doubt and highlight weaknesses.",
            "The tone is critical, as evidenced by phrases like \"Limited contributions,\" \"less impressive,\" \"not convincing,\" and \"Related discussion is missed.\" The reviewer directly points out flaws and areas needing improvement."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its assessment, highlighting both the strengths of the paper (efficiency, experiments, writing) and areas for improvement (comparison with other methods, long-term tracking, clarification on score discrepancies and implementation details). The reviewer's questions and comments are all relevant and contribute to a constructive critique without any self-contradiction.",
            "The review consistently acknowledges the paper's strengths, such as achieving state-of-the-art results and novel combination of techniques, while also pointing out areas for improvement and lack of complete novelty in some individual components. The reviewer appreciates the overall contribution despite suggesting further comparisons and discussions, indicating a consistent and balanced assessment.",
            "The reviewer expresses uncertainty about the ethical tagging but acknowledges potential long-term ethical implications, while also stating that the connection to the paper is weak. These points are consistent as the reviewer is not contradicting themselves, but rather expressing a nuanced opinion.",
            "The review is consistent as it expresses a single, clear opinion that the paper was mistakenly flagged for ethics review and does not contain any contradictory statements.",
            "The reviewer acknowledges the strengths of the paper, such as good writing and experimental results, but consistently points out the lack of novelty as the main weakness. The reviewer argues that the proposed method is largely based on existing works like MaxDeeplab, VisTR, and DETR, and that there are no fundamental new contributions. The concerns about experimental comparison and missing details further support the overall critique of incremental novelty without contradicting the initial positive points.",
            "The review is consistent because it clearly separates strengths and weaknesses, providing specific points for each. The reviewer acknowledges the paper's strong performance and efficiency as strengths, while also pointing out limitations in novelty, justification of claims, and writing quality as weaknesses. These points are logically distinct and do not contradict each other, forming a balanced and consistent evaluation."
        ]
    },
    {
        "paper_id": "iclr_2022_i7h4M45tU8",
        "paper_title": "Neural Temporal Logic Programming",
        "paper_abstract": "Events across a timeline are a common data representation, seen in different temporal modalities. Individual atomic events can occur in a certain temporal ordering to compose higher level composite events. Examples of a composite event are a patient's medical symptom or a baseball player hitting a home run, caused distinct temporal orderings of patient vitals and player movements respectively. Such salient composite events are provided as labels in temporal datasets and most works optimize models to predict these composite event labels directly. We focus uncovering the underlying atomic events and their relations that lead to the composite events within a noisy temporal data setting. We propose Neural Temporal Logic Programming (Neural TLP) which first learns implicit temporal relations between atomic events and then lifts logic rules for composite events, given only the composite events labels for supervision. This is done through efficiently searching through the combinatorial space of all temporal logic rules in an end-to-end differentiable manner. We evaluate our method on video and on healthcare data where it outperforms the baseline methods for rule discovery. ",
        "review_ids": [
            "lhY5o4IABdW",
            "wwoQAR4jb--",
            "fFqTaImtBMi",
            "MLnWEwIwBXU",
            "kUXLlhiaQ7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Location-Aware Graph Convolutional Networks for Video Question Answering. AAAI 2020\n\nHopper: Multi-hop Transformer for Spatiotemporal Reasoning. ICLR 2021",
            "This paper proposed a neural nets based approach for Temporal Logic Programming. A key claimed contribution is on formulating combinatorial rule space search as predicate selection indicator vectors assignment so as to make it differential to enable end2end training from atomic event probabilities as inputs.   Strength\n\n\t1.  formulating combinatorial rule space search as predicate selection indicator\n\nWeakness\n\n\t1. The proposed model's parameterization depends on the number of events and predicates making it difficult to generalize to unseen events or required retraining. \n\t2. The writing needs to be improved to clearly discuss the proposed approach.\n        3. The experiments baselines are of the authors' own design; it lacks a comparison to the literature baselines  using the same dataset. If there is no such baseline, please discuss the criteria in choosing such baselines.\n\n\nDetails: \n\n\t1. Page 1, \"causal mechanisms\", causality is different from temporal relationship. Please use the terms carefully. \n\t2. Page 3, it seems to me that M_T is defined over the probabilities of atomic events. The notation as it is used not making it difficult to make sense of this concept. Please consider providing examples to explain M_T. \n\t\t\n\t3. Page 4, equation (2), it is not usual to feed probabilities to convolution. \n\t\ta. Please discuss in section 3 how your framework can handle raw inputs, such as video or audio? Do you need an atomic event predictor or human label to use your proposed system? If so, is it possible to extend your framework to directly have video as input instead of event probability distributions?  Can you do end2end training from raw inputs, such as video or audio? (although you mentioned Faster R-CNN in the experiment section, it is better to discuss the whole pipeline in the methodology).\n\t\tb. Have you tried discrete event embeddings to represent the atomic and composite events so as the framework can learn distributional embedding representation of events so as to learn the temporal rules?\n\t4. Page 4, please explain what you want to achieve with M_A = M_C \\otimes M_D. It is unusual to multiple length by conv1D output. Also please define \\otimes here. I am guessing it is elementwise multiplication from the context.\n\t5. Page 4, \"M_{D:,:,l}=l. This can be thought as a positional encoding. It is not clear to me why this can be taken as positional encoding?  \n\t6. Page 6, please detail how do you sample top c predicates. Please define what is s in a = softmax(s). It seems to me the dimension of s with \\sum_i (c i) can be quite large making it softmax(s) very costly. \n\t\n\t\n This is a good initial attempt to attack the neural temporal logic programming. However, the steps in the proposal --- temporal compression, predicate modeling, composite event prediction, combinatorial inference, and rule reduction --- requires more clarity and refinement to reach a publication state. Overall, the proposal has little element of neural networks. To me it is more on matrix formulation of ILP problem. It is Okay to me but please consider renaming the paper title. ",
            "The paper proposes an end-to-end differentiable strategy (called neural TLP) to learn unknown temporal relations between atomic events (like after(miss, swing), \u201cmiss occurs after swing\u201d in the baseball example), subsequently used to predict composite events (like strike). The strategy consists of a cascade of a smoothing stage to filter  out noise from the input time series, an interval time extractor, a stage predicting temporal relations (such as before, after and during) and a linear output layer. Furthermore, the paper proposes a post-hoc procedure to extract propositional logical rules relating atomic events to composite ones from the last layer. The performance of the proposed strategy is evaluated on video recognition (CATER) and healthcare (MIMIC-III) datasets against two baselines, namely a LSTM neural net and a simplified version of the proposed strategy. **Strengths**\n\n- The paper is well written.\n- The proposed solutions, namely the differentiable layers to predict the temporal relations between atomic events and the post-hoc strategy to induce propositional rules from the last linear layer, are simple but non trivial.\n- Results are promising.\n\n**Weaknesses**\n\n- It is not clear how the work positions with respect to recent literature in spatio-temporal reasoning and video question answering. For example, the authors should have a look at [1]-[2]\n- While experimental results are interesting and promising, they provide only a partial picture, as they neglect recent neural advances. Indeed, comparisons are made against only simple baselines and should be conducted against more \u201cstructured\u201d neural approaches, see for instance graph-based approaches [1]-[2], where code is available.\n\n**Questions**\n\n1. How scalable is the proposed strategy in terms of number of objects, number of atomic events and compositionality of the target events?\n2. As far as I understand, the proposed strategy considers only the temporal relations among atomic events and objects are considered independent of each other. Is that correct? If so, why is that the case?\n3. While I understand the intuition about the strategy to extract the time intervals of atomic events (as shown in Figure 3), I do not see how well this strategy can perform in the continuous and more noisy setting. How reliable are the obtained estimates?\n4. In CATER experiments, is the perception component (i.e. Faster R-CNN) trained jointly to the Neural TLP  or are two components trained separately?\n5. The names parameter and structure learning are a bit confusing in this context. In their classical definition, parameter learning considers learning the parametric relations among variables in a given graph. In Neural TLP, the temporal relations are not given, rather learnt. Why is this considered as parameter learning?\n6. Can you mention something about the logic expressiveness of the induced rules? The paper is clearly written and easy to follow. I like the idea around the proposed differentiable operators used to predict the temporal relations among atomic events. Also, the experimental results are interesting and promising. However, they are preliminary. I would like to see more in-depth analysis on the correcteness of the time intervals for atomic events, the scalability of the proposed strategy and a comparison against recent structured neural approaches for spatio-temporal reasoning. Furthermore, the authors should discuss and relate their work against existing literature in the field of video question answering.",
            "The paper proposed a new approach called neural logic programming to recover composite events from complex time series data using atomic events and their temporal relations.  Strengths: the two-step approach (i.e., parameter learning followed by structure learning) does seem novel for the problem. \n\nWeakness: the empirical results are rather unconvincing. Not only that the baselines (two LSTMs) are rather weak, but also that the proposed approach seems to be even worse than the baselines in terms of mean average precision (which is the mainstream metric used on CATER) in Table 1. Besides, Table 3 suggests that the proposed approach is marginally better than logistic regression.\n\nMinor:  In figure 2, \u201c(1) parameter learning\u201d seems to be in the wrong place. That should refer to predicate prediction according to page 3, paragraph 1. Although there is some novelty in the proposed approach, the empirical results are too weak to validate the effectiveness of the approach. I do not think the paper in its current form is good enough to be published in ICLR.",
            "This paper presents an approach for learning temporal rules from data. The idea is to extract atomic events, and their temporal relationships between them. Subsequently, composite events are learned using composite event labels for supervision. The whole approach is formulated as an optimisation problem, after which standard techniques are applied to solve it.\n The paper addresses an important problem of learning temporal rules from data, for which many applications exist (as the paper convincingly motivates). \n\nA very strong point of the paper is to learn interpretable rules using modern neural network machinery. This can certainly inspire future work. \n\nThe paper is not always easy to follow. It would have been useful to more explicitly state the aims of the proposed method. For example, on page 4 the paper refers to Temporal Relation Networks, but they are dismissed because \"timelines can be long in our setting\", yet, it is not clear what this refers to. For me, it was also quite unnatural to say that composite events are a-temporal (e.g. in the baseball\nexample on page 2). Why does it make sense that you cannot say anything about the timeline of the 'strike' event? Also in the remainder of Section 3.1, various choices are made, which were not all obvious to me. \n\nIn the structure learning part, I was missing the connection to probabilistic rule learning approaches (e.g. ProbFOIL). The essence\nseems to be to learn rules where the temporal atoms have a probability attached to it, unless I am missing something.\n\nIn the evaluation, the paper convincingly shows that variations of the proposed method performs less well. What was not convincing to me\nwere two aspects:\n(1) Why previous approaches are unsuitable for the proposed settings, e.g. probabilistic rule-learning, sequence mining, etc. \n(2) I have doubts about the validity of the medical example. For example, isn't it the case that 'oral water' is simply responsible for \nurine flow rather than after(oral_water, spo2_sao2=high) ? I can understand that a doctor would say that you \"captured important explanatory variables\" in the proposed rules, but that does not mean that it is in any way true. In fact: doesn't this show a downside of the approach in the sense that only temporal events can be used in rules?\n Overall, I thought this is an interesting contribution to the ICLR conference. I do have some doubts about the completeness and validity of the experiments."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Neutral",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review consists only of paper titles and publication venues, lacking any subjective assessment or opinion to indicate a positive or negative sentiment. It's simply providing information.",
            "The review identifies several weaknesses, including difficulty generalizing the model, unclear writing, and inadequate baselines. The reviewer also points out numerous issues in the details of the paper, indicating significant concerns about the clarity and correctness of the approach. The concluding remarks suggest the paper needs substantial refinement.",
            "The review expresses both positive aspects (strengths, promising results, well-written) and negative aspects (weaknesses, unclear positioning, limited comparisons). The reviewer also raises questions and suggests improvements, indicating a balanced perspective.",
            "The review expresses concerns about the paper's empirical results, stating they are \"unconvincing\" and \"worse than the baselines.\" The reviewer concludes the paper is \"not good enough to be published.\"",
            "The review expresses both positive and negative feedback. It acknowledges the paper's strengths, such as addressing an important problem and learning interpretable rules. However, it also raises concerns about clarity, justification of choices, and the validity of the experimental results. The overall assessment is that it's an interesting contribution but with doubts about completeness and validity."
        ],
        "tone": [
            "Neutral",
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The text presents information in a factual manner, without expressing any personal feelings or opinions. The listing of paper titles and publication venues is devoid of emotional tone.",
            "The review uses phrases like \"difficult to generalize,\" \"writing needs to be improved,\" \"lacks a comparison,\" \"not making it difficult to make sense,\" and \"not clear to me.\" These phrases highlight specific shortcomings and express a critical assessment of the paper's quality and clarity. The overall tone is questioning and demanding clarification and improvement.",
            "The review uses a mix of supportive language (e.g., \"The paper is well written,\" \"Results are promising,\" \"I like the idea\") and critical language (e.g., \"It is not clear,\" \"provide only a partial picture,\" \"comparisons are made against only simple baselines\"). The reviewer also poses questions, demonstrating an objective and analytical approach.",
            "The reviewer uses phrases like \"rather weak,\" \"even worse,\" and \"marginally better\" to express their dissatisfaction with the results. The overall assessment is negative, indicating a critical stance towards the paper's quality.",
            "The review presents both positive and negative aspects of the paper in a measured way. It uses phrases like \"A very strong point...\", \"The paper is not always easy to follow\", and \"Overall, I thought this is an interesting contribution... I do have some doubts...\" which indicate a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review only contains paper titles and publication information, and does not express any opinions or judgments. Therefore, there is no review content to analyze for consistency or contradiction.",
            "The review is consistent. It identifies a strength but primarily focuses on weaknesses and areas for improvement, providing specific details and suggestions for revision. The overall tone is constructive and aims to guide the authors towards improving the paper for publication. The reviewer consistently points out areas needing refinement in clarity, methodology, and experimental validation.",
            "The review is consistent in its assessment. It highlights both the strengths of the paper, such as clarity, simplicity of the proposed method, and promising initial results, and its weaknesses, primarily concerning the lack of comparison with recent literature and more advanced baselines. The questions raised are pertinent and aim to address the identified weaknesses and further understand the proposed method. The final summary reinforces this balanced perspective, acknowledging the paper's merits while emphasizing the need for further analysis and contextualization. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because it acknowledges the novelty of the proposed approach as a strength, but then points out significant weaknesses in the empirical results. The reviewer argues that the weak empirical results, where the proposed method performs worse than baselines and only marginally better than logistic regression, undermine the paper's contribution. The conclusion that the paper is not ready for publication is logically derived from the identified weakness in the empirical evaluation.",
            "The review consistently highlights both positive aspects (importance of the problem, interpretability of rules, interesting contribution) and negative aspects (clarity issues, lack of justification for certain choices, doubts about experimental validity) of the paper. There are no contradictory statements or shifts in opinion within the review. The reviewer maintains a critical but appreciative tone throughout, offering constructive criticism while acknowledging the paper's potential contribution."
        ]
    },
    {
        "paper_id": "iclr_2018_SyOK1Sg0W",
        "paper_title": "Adaptive Quantization of Neural Networks",
        "paper_abstract": "Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.",
        "review_ids": [
            "rkIpwEslz",
            "Hkcb6tG-M",
            "HkrUKW5eM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I have read the responses to the concerns raised by all reviewers. I find the clarifications and modifications satisfying, therefore I keep my rating of the paper to above acceptance threshold.\n\n-----------------\nORIGINAL REVIEW:\n\nThe paper proposes a method for quantizing neural networks that allows weights to be quantized with different precision depending on their importance, taking into account the loss. If the weights are very relevant, it assigns more bits to them, and in the other extreme it does pruning of the weights.\n\nThis paper addresses a very relevant topic, because in limited resources there is a constrain in memory and computational power, which can be tackled by quantizing the weights of the network. The idea presented is an interesting extension to weight pruning with a close form approximate solution for computing the adaptive quantization of the weights.\n\nThe results presented in the experimental section are promising. The quantization is quite cheap to compute and the results are similar to other state-of-the-art quantization methods. \nFrom the tables and figures, it is difficult to grasp the decrease in accuracy when using the quantized model, compared to the full precision model, and also the relative memory compression. It would be nice to have this reference in the plots of figure 3.  Also, it is difficult to see the benefits in terms of memory/accuracy compromise since not all competing quantization techniques are compared for all the datasets.\nAnother observation is that it seems from figure 2 that a lot of the weights are quantized with around 10 bits, and it is not clear how the compromise accuracy/memory can be turned to less memory, if possible. It would be interesting to know an analogy, for instance, saying that this adaptive compression in memory would be equivalent to quantizing all weights with n bits.\n\nOTHER COMMENTS:\n\n-missing references in several points of the paper. For instance, in the second paragraph of the introduction, 1st paragraph of section 2.\n\n- few typos:\n*psi -> \\psi in section 2.3\n*simply -> simplify in proof of lemma 2.2\n*Delta -> \\Delta in last paragraph of section 2.2\n*l2 -> L_2 or l_2 in section 3.1 last paragraph.",
            "Revised Review:\n\nThe authors have addressed most of my concerns with the revised manuscript. I now think the paper does just enough to warrant acceptance, although I remain a bit concerned that since the benefits are only achievable with customized hardware, the relevance/applicability of the work is somewhat limited.\n\nOriginal Review:\n\nThe paper proposes a technique for quantizing the weights of a neural network, with bit-depth/precision varying on a per-parameter basis. The main idea is to minimize the number of bits used in the quantization while constraining the loss to remain below a specified upper bound. This is achieved by formulating an upper bound on the number of bits used via a set of \"tolerances\"; this upper bound is then minimized while estimating any increase in loss using a first order Taylor approximation.\n\nI have a number of questions and concerns about the proposed approach. First, at a high level, there are many details that aren't clear from the text. Quantization has some bookkeeping associated with it: In a per-parameter quantization setup it will be necessary to store not just the quantized parameter, but also the number of bits used in the quantization (takes e.g. 4-5 extra bits), and there will be some metadata necessary to encode how the quantized value should be converted back to floating point (e.g., for 8-bit quantization of a layer of weights, usually the min and max are stored). From Algorithm 1 it appears the quantization assumes parameters in the range [0, 1]. Don't negative values require another bit? What happens to values larger than 1? How are even bit depths and associated asymmetries w.r.t. 0 handled (e.g., three bits can represent -1, 0, and 1, but 4 must choose to either not represent 0 or drop e.g. -1)? None of these details are clearly discussed in the paper, and it's not at all clear that the estimates of compression are correct if these bookkeeping matters aren't taken into account properly.\n\nAdditionally the paper implies that this style of quantization has benefits for compute in addition to memory savings. This is highly dubious, since the method will require converting all parameters to a standard bit-depth on the fly (probably back to floating point, since some parameters may have been quantized with bit depth up to 32). Alternatively custom GEMM/conv routines would be required which are impossible to make efficient for weights with varying bit depths. So there are likely not runtime compute or memory savings from such an approach.\n\nI have a few other specific questions: Are the gradients used to compute \\mu computed on the whole dataset or minibatches? How would this scale to larger datasets? I am confused by the equality in Equation 8: What happens for values shared by many different quantization bit depths (e.g., representing 0 presumably requires 1 bit, but may be associated with a much finer tolerance)? Should \"minimization in equation 4\" refer to equation 3?\n\nIn the end, while do like the general idea of utilizing the gradient to identify how sensitive the model might be to quantization of various parameters, there are significant clarity issues in the paper, I am a bit uneasy about some of the compression results claimed without clearer description of the bookkeeping, and I don't believe an approach of this kind has any significant practical relevance for saving runtime memory or compute resources. ",
            "The authors present an interesting idea to reduce the size of neural networks via adaptive compression, allowing the network to use high precision where it is crucial and low precision in other parts. The problem and the proposed solution is well motivated. However, there are some elements of the manuscript that are hard to follow and need further clarification/information. These need to definitely be addressed before this paper can be accepted.\n\nSpecific comments/questions:\n- Page 1: Towards the bottom, in the 3rd to last line, reference is missing.\n- Page 1: It is a little hard to follow the motivation against existing methods.\n- Page 2: DenseNets and DeepCompression need citations\n- Lemma 2.1 seems interesting - is this original work? This needs to be clarified.\n- Lemma 2.2: Reference to Equation 17 (which has not been presented in the manuscript at this point) seems a little confusing and I am unable to following the reasoning and the subsequent proof which again refers to Equation 17.\n- Alg 2: Should it be $\\Delta$ or $\\Delta_{k+1}$? Because in one if branch, we use $\\Delta$, in the other, we use the subscripted one.\n- Derivation in section 2.3 has some typographical errors.\n- What is $d$ in Equation 20 (with cases)? Without this information, it is unclear how the singular points are handled.\n- Page 6, first paragraph of Section 3: The evaluation is a little confusing - when is the compression being applied during the training process, and how is the training continued post-compression? What does each compression 'pass' constitute of?\n- Figure 1b: what is the 'iteration' on the horizontal axis, is it the number of iterations of Alg3 or Alg2? Hoping it is Alg3 but needs to be clarified in the text.\n- Section 3: What about compression results for CIFAR and SVNH? "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they find the clarifications and modifications satisfying and maintain their rating above the acceptance threshold, indicating a positive assessment.",
            "The reviewer states that the authors have addressed most of their concerns and now believe the paper does 'just enough' to warrant acceptance. This indicates a shift from a more critical stance in the original review to a more positive one.",
            "While the reviewer acknowledges the interesting idea and motivation, they raise several concerns about clarity, missing references, confusing explanations, and typographical errors. The phrase \"hard to follow\" and the need for clarification \"before this paper can be accepted\" indicate a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer expresses satisfaction and uses encouraging language like 'interesting extension' and 'promising' when discussing the paper's contributions. They also offer constructive criticism and suggestions for improvement rather than focusing solely on flaws.",
            "The reviewer acknowledges the improvements made by the authors ('addressed most of my concerns') and ultimately recommends acceptance. However, they also express lingering concerns about the relevance and applicability of the work due to its reliance on customized hardware. This combination of positive and critical feedback indicates a balanced tone.",
            "The review adopts a critical tone by pointing out specific weaknesses in the manuscript, such as missing references, unclear motivations, confusing explanations, typographical errors, and lack of information. Questions like \"is this original work?\" and phrases like \"hard to follow\" convey a critical stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive about the paper's topic, idea, and results, while providing constructive criticism and suggestions for improvement. The reviewer acknowledges the paper's strengths and areas where it could be enhanced, without contradicting themselves. The reviewer praises the relevance of the topic, the interesting idea, and the promising results, while also pointing out areas for improvement in clarity and presentation of results, as well as minor issues like missing references and typos. This combination of positive feedback and constructive criticism is consistent and common in peer reviews.",
            "The reviewer's stance is consistent. Initially, the reviewer expressed concerns about clarity, dubious claims about compute benefits, and practical relevance. In the revised review, the reviewer acknowledges that most concerns are addressed and the paper is now acceptable, but still maintains a reservation about the limited relevance/applicability due to the need for customized hardware. This shows a consistent concern about the practical impact and broad applicability of the research, even though other aspects have improved.",
            "The review is consistent because all comments are focused on areas for improvement and clarification, supporting the initial statement that some parts of the manuscript are hard to follow. The reviewer points out specific issues like missing references, unclear motivations, confusing notations, and questions about experimental details, all contributing to the need for revision and clarification. There are no contradictory statements or conflicting opinions within the review."
        ]
    },
    {
        "paper_id": "iclr_2022_gccdzDu5Ur",
        "paper_title": "Combining Diverse Feature Priors",
        "paper_abstract": "To improve model generalization, model designers often restrict the features that their models use, either implicitly or explicitly. In this work, we explore the design space of leveraging such feature priors by viewing them as distinct perspectives on the data. Specifically, we find that models trained with diverse sets of explicit feature priors have less overlapping failure modes, and can thus be combined more effectively. Moreover, we demonstrate that jointly training such models on additional (unlabeled) data allows them to correct each other's mistakes, which, in turn, leads to better generalization and resilience to spurious correlations.",
        "review_ids": [
            "Wk7VWusNiR",
            "43Usdck9Ifh",
            "m7MjLVvGt7R",
            "1dO9Lch9ix",
            "o6BLRaz_PYN",
            "7bC9SxbWvZY",
            "aWEDeWw1PnM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your thorough rebuttal! I am satisfied with your replies to the other reviewers and would like to maintain my rating.",
            " In light of the addition of stacking experiments and also the additional discussion of where spurious correlations are likely to occur, I have raised my score to a 6.",
            "The paper presents multiple techniques for training models with different feature priors (i.e. inclinations to focus on different aspects of the training data) and combining them, either post hoc via ensembles or by allowing the models to provide augmented pseudo-labelled training data to each other via co-training. When using simple ensembling techniques, ensembles with a diversity of feature priors are show to perform better than ensembles where the individual models have similar feature priors. Co-training is shown to boost performance substantially when models with diverse feature priors supply pseudo-labels to each other. The problem domain is image classification. The feature priors concern shape and texture. Different preprocessing and or architecture constraint techniques are used for different models so as to predispose them to focus on shape but not texture or vice versa. The implementation of shape feature priors (via edge detection preprocessing) and texture feature priors (via limited receptive field via bagnet) makes a lot of sense and does a good job of illustrating a concrete example of a collection of different feature priors.  Some of the co-training experimental results are strong. \n\nOne concern I have is that the ensemble results presented in section 3.2 are generated using very primitive ensembling techniques. Appendix A.4 says the combination techniques were simply max, average and lowest rank. It is more common to treat this kind of ensembling as a 2nd-level machine learning problem with the outputs of the models forming the inputs to a 2nd-level model. I would not have expected a fancy 2nd-level model but I was hoping at a minimum that the first-level models would be combined via. e.g. linear regression on first-level outputs from a held-out validation set. Fancier combinations (e.g. neural nets) are also possible, of course. I would encourage the authors to read a few writeups by winners of Kaggle competitions and/or read about the ensembling done in the Netflix Prize to get a better sense of what constitutes state-of-the-art ensemble combination techniques.\n\nI was also concerned about the assumption made on page 7 that spurious correlations are likely not to exist in unlabelled data, because unlabelled data supposedly comes from a more diverse collection process. While this may be true in some cases, there will also be many real-world situations where all the input data, whether labelled or unlabelled, comes from the same distribution and it may all have the same spurious correlation. It is often the case that a small portion of the data is labelled simply because it is very time intensive for humans to do the labeling but nonetheless, the remaining unlabelled data comes from the same distribution. I hope that in future versions of this work, the authors make clear that practitioners should think hard about whether their unlabelled data will have the same spurious correlations as their labelled data, rather than assuming that this is likely the case.\n\nFor these reasons, I can only give the paper a 5. I would prefer that the ensemble section be redone with more sophisticated ensembling and/or removed and I would prefer that the absence-of-spurious-correlation-in-unlabelled data assumption be presented more cautiously. \n \nA minor complaint: from a presentation point of view, it is non-standard and a bit strange to add additional related work in section 6. I don't normally expect to read about related work after the results and towards the end of the paper. Related work is normally presented earlier in a paper. The authors might consider moving this section to an earlier point in the paper. \n\nOn page 8, I found the bolding of +BagNet cotraining results to be a bit confusing. Normally the 'winning' algorithm results are bolded, which in this case is Canny. I realize that the message of the huge boost of cotraining for +BagNet is what is intended but it still confused me that the bolded numbers were not the best numbers.\n\nIt also would be nice to show the method on another domain aside from image classification, although I realize space constraints might make this difficult. The authors might consider removing the ensembling section in future versions of the work and instead using that space for cotraining results on another type of problem.\n\n*** Update after author rebuttal *** In light of the addition of stacking experiments for the ensembling, I have raised my score to a 6. The authors achieve some positive results from cotraining of groups image classification models designed to focus on shape but not texture or vice versa. However, their ensembling results are acheived using very primitive ensembling which is not state of the art. They also overstate the odds that spurious correlations are unlikely to exist in unlabelled data.",
            " I thank the authors for their response. However, I still think that the paper lacks the technical novelty, and the results presented are the results of an empirical study. I keep my initial score. ",
            "The paper is an empirical study of combining multiple feature priors along with some pre-processing to solve a variety of computer vision tasks. Positives\n\n+ The study seems to be interesting and maybe useful for practitioners.\n\nConcerns\n\n- Very meagre contribution in terms of technical novelty and framework. \n\n- Looks like an empirical study without much conviction and direction.\n\n- Experimental evaluation and comparisons seem dated, not state of the art.\n\n- The work is very much below the expected standards of ICLR. The paper is clearly below par, can be rejected.",
            "The paper proposes a formalized framework for imposing priors on the feature extraction in deep visual processing models. There has been earlier work on encouraging certain feature representations (e.g. suppressing the focus on texture in feature extraction) and also making feature representations robust to domain shift. The core contribution of this paper is the systematic formulation and investigation of how different, distinct feature priors leads to complementary feature representations that can be combined to provide more robust data representations - in other words, creating synthesized multi-view data representations.\nThe paper ties back to early (1998) work on co-training (which essentially is multi-modal bootstrapping) and ties this to the more recent body of work on self-supervision and self-training.\nExperiments are performed with classical shape- and texture-biased models, and show that the hypothesis - that diverse feature priors are able to robustly create a set of complementary data views - holds. This paper has a number of strengths, that combined makes me recommend the paper for acceptance:\n+ The topic of this paper, creating and combining robust, generalizable and diverse feature representations, is of high relevance to a large portion of the ICLR audience.\n+ It provides an interesting and valuable formal framework for steering feature representations in different directions, creating multi-view representations of the data.\n+ It is well written, well organized, technically correct, and easy to read.\n+ The experimental design is sound and well done.\n\nOne weakness can be pointed out, not however any cause for not accepting this paper in my opinion:\n- The experiments are performed on old datasets, CIFAR-10 and STL-10, both with quite clear class structure and simplistic image setting (e.g. the object centered in the image). It would be interesting to see experiments on more difficult data with fine-grained and hierarchical class structure for example. \n This paper has a number of strengths, that combined makes me recommend the paper for acceptance: Of high relevance, well written and correct, proposes a valuable framework, and contains sound and well designed experiments.\nOne weakness can be pointed out, not however any cause for not accepting this paper in my opinion: The experiments are performed on old datasets.\n\nIn summary, I propose acceptance for this paper and believe it will be of interest to a large portion of the ICLR audience.\n",
            "The goal of the paper is to improve model generalisation. The authors consider feature priors as distinct perspectives on the data. The results show that models trained with diverse sets of various feature priors have less overlapping modes and are more efficiently combined.   Strengths. The experimental part is relatively clear. \n\nWeaknesses. The paper is not very clearly written. First, I would appreciate some (even informal) definition of a feature prior. Later in the text, the co-training is mentioned, and it seems that using different priors = co-training using different views. Is it the idea of the paper? I did not really understand the first contribution: \"We demonstrate that training models with diverse feature priors results in them making mistakes on different parts of the data distribution, even if their overall accuracy is similar.\" What is meant?\n\nAs far as I understand, there are two \"priors\" only explored in the paper: shape and texture. \n The current contribution is an exploratory work, combining several state-of-the-art methods (for instance, self-training and co-training are used in the experiments). \nThere is a lack of technical novelty. \n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses satisfaction with the author's responses and intends to maintain their original rating, indicating a positive assessment.",
            "The reviewer states they have \"raised my score to a 6\", indicating a positive change in their evaluation of the work.",
            "The review expresses concerns about the ensembling techniques used, the assumption about spurious correlations in unlabelled data, and other issues like presentation and bolding. The reviewer initially gave the paper a score of 5 and only raised it to 6 after rebuttal, indicating persistent reservations.",
            "The reviewer explicitly states that the paper 'lacks technical novelty' and reiterates their initial score, implying dissatisfaction.",
            "The review expresses significant concerns about the paper's contribution, novelty, experimental evaluation, and overall quality, ultimately recommending rejection.",
            "The reviewer explicitly recommends acceptance and highlights several strengths of the paper, such as its relevance, interesting framework, writing quality, and sound experimental design. The identified weakness is not considered a reason for rejection.",
            "The review identifies several weaknesses, including lack of clarity in writing, lack of a clear definition of 'feature prior,' and a lack of technical novelty. The reviewer also expresses a lack of understanding regarding the first contribution."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Critical",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Thank you for your thorough rebuttal!\" and \"I am satisfied with your replies\" which convey a supportive and appreciative tone.",
            "The reviewer's language suggests a supportive stance, acknowledging the authors' efforts to address previous concerns by mentioning the addition of stacking experiments and discussion of spurious correlations, which led to an increased score.",
            "The review uses phrases like \"One concern I have,\" \"I was also concerned,\" \"I would prefer that the ensemble section be redone,\" and \"I found the bolding...to be a bit confusing.\" These phrases clearly indicate a critical tone, pointing out flaws and suggesting improvements.",
            "The reviewer uses phrases like 'lacks technical novelty' and 'results of an empirical study' to express their criticism. The statement 'I keep my initial score' reinforces a negative stance.",
            "The review uses phrases like \"Very meagre contribution,\" \"without much conviction and direction,\" \"seem dated, not state of the art,\" and \"clearly below par, can be rejected,\" which indicate a critical and negative assessment.",
            "The reviewer uses encouraging language, praising the paper's strengths ('high relevance,' 'valuable formal framework,' 'well written,' 'sound and well done'). The reviewer also explicitly states their recommendation for acceptance.",
            "The tone is critical due to the identification of weaknesses such as unclear writing ('The paper is not very clearly written'), lack of understanding ('I did not really understand'), and lack of novelty ('There is a lack of technical novelty'). The reviewer also uses questioning to point out issues."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses satisfaction with the rebuttal and states the reviewer will maintain their rating. This indicates a consistent viewpoint as the reviewer's positive assessment is reinforced by the authors' response.",
            "The review is consistent because the reviewer clearly states that they increased the score due to the addition of stacking experiments and discussion of spurious correlations, which is a logical reason for raising the score.",
            "The reviewer consistently points out concerns regarding the primitiveness of ensembling techniques and the potentially flawed assumption about the absence of spurious correlations in unlabelled data, both before and after the author rebuttal. While the reviewer acknowledges the authors' effort to address the ensembling issue by adding stacking experiments and increases the score, the core criticisms remain unchanged, indicating a consistent evaluation perspective throughout the review process.",
            "The reviewer consistently maintains their initial negative assessment and score.",
            "The review acknowledges a minor positive aspect (potential practical use) but strongly emphasizes negative aspects like lack of novelty, direction, outdated experiments, and low quality, ultimately recommending rejection. The concerns and the final recommendation are aligned, making the review consistent.",
            "The review consistently praises the paper, highlighting its strengths such as relevance, valuable framework, writing quality, and sound experiments. While a minor weakness (old datasets) is mentioned, it is explicitly stated not to be a reason for rejection, and the review concludes with a clear recommendation for acceptance. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because it raises several concerns about the clarity of the paper, specifically regarding the definition of 'feature prior' and the explanation of the first contribution. It also consistently points out the lack of technical novelty, focusing on the exploratory nature of the work and the combination of existing methods."
        ]
    },
    {
        "paper_id": "nips_2021_v4vjMuXF-B",
        "paper_title": "Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics",
        "paper_abstract": "Deep neural networks excel at image classification, but their performance is far less robust to input perturbations than human perception. In this work we explore whether this shortcoming may be partly addressed by incorporating brain-inspired recurrent dynamics in deep convolutional networks. We take inspiration from a popular framework in neuroscience: predictive coding''. At each layer of the hierarchical model, generative feedbackpredicts'' (i.e., reconstructs) the pattern of activity in the previous layer. The reconstruction errors are used to iteratively update the network\u2019s representations across timesteps, and to optimize the network's feedback weights over the natural image dataset--a form of unsupervised training. We show that implementing this strategy into two popular networks, VGG16 and EfficientNetB0, improves their robustness against various corruptions and adversarial attacks. We hypothesize that other feedforward networks could similarly benefit from the proposed framework. To promote research in this direction, we provide an open-sourced PyTorch-based package called \\textit{Predify}, which can be used to implement and investigate the impacts of the predictive coding dynamics in any convolutional neural network. \n",
        "review_ids": [
            "SHNC-0Llx_4",
            "KXeC_gkN0W",
            "J2_Okd_leed",
            "KBOWhsxAgAt",
            "s_8jhw4Pyc",
            "qckgDDA7DoV",
            "Vrh0ONKFwSX",
            "1Rr28Xf8yV",
            "R-dJge_7KG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I'd like to thank the authors for their careful response. I am happy to see the new experiment results. I do think the experiments make the paper more comprehensive, and the paper is above the acceptance threshold. ",
            "The authors draw inspiration from neuroscience and propose augmenting feedforward neural networks with recurrent dynamics to improve their adversarial and corruption robustness. They provide a PyTorch package that can augment any convolutional neural network with recurrence. Their empirical tests show that recurrence mostly helps with robustness.  *Review summary:*\n\nThe proposed method is well explained and the provided recurrence software package could be a valuable contribution to the community. The paper could be improved by reporting more interpretable metrics and addressing inconsistencies in the results. Additionally, the literature review needs to be improved and the novelty made more explicit. I do not believe the paper is good enough for publication in its current form, but am ready to revise my score if my concerns are adequately addressed.\n\n*Strengths:*\n\nThe paper is well written in terms of grammar and fluency. The introductory paragraphs provide good motivation for exploring recurrence in DNNs, and the model description is clear and concise. The concluding and broader impact statements are also well argued. The breadth of experiments (denoising, corruption robustness, and adversarial robustness) is helpful for providing a holistic understanding for how recurrence influences resilience to perturbations. While I didn\u2019t look at the code itself, I believe that the provided codebase could be helpful for important future work in this space. I also appreciate the detailed comparisons to [22] and [24] in the Appendix.\n\n*Weaknesses:*\n\nThis is a fairly long list that is not in order of importance (sorry). However, I hope many of these issues can be easily resolved by the authors.\n\na) I recommend additional background discussion on related prior work. There is a wealth of research attempting to use predictive coding ideas to improve DNN robustness. If we only focus on recurrence then the scope broadens further {2, 3, 8}. Although note that {8} heavily draws on the concept of analysis-by-synthesis, which is also a core idea in predictive coding. A more related example would be sparse coding, which is a predictive coding model that has been used for denoising for decades {6, 7}. Recently, several papers have also demonstrated adversarial robustness with sparse coding {1, 4, 5}. I have provided some examples here, but I would encourage the authors to look at citations of and within them to inform a proper discussion on the context of their work such that readers can properly assess the novelty of their ideas. An additional (minor) issue is that the existing references should also be updated. Many papers cited as arxiv preprints are now published in peer-reviewed venues.\n\nb) It would be easier to compare the networks (VGG vs EfficientNet) if the authors used shared vertical axes in adjacent plots. This recommendation applies to almost every subfigure. The study\u2019s goal is clearly not to compare VGG against EfficientNet, but understanding the relative differences between baselines can give better context for the change in performance when recurrence is added.\n\nc) It is not clear to me what motivated the stopping criteria for the recurrence in any given experiment. The authors appear to arbitrarily stop the model at 4, 8, 10, 15, and 20 time steps. Why is this? I would like the authors to extend the time steps as long as necessary (given computational constraints) to give a complete picture of how recurrence influences the representations. Presumably all of the curves will eventually converge to a steady state -- when is that? If not, why? In a few instances (e.g. figure 4 for eps=0.003 and figure 3(b) for PEfficentNet), the performance does a reversal, which should at least be discussed.\n\nd) When interpreting the gaussian noise experiment results (figure 2) the authors state that \u201cBoth networks demonstrate significant accuracy improvement across timesteps under noisy conditions, while maintaining a performance close to the feedforward level for clean images.\u201d I do not believe this statement is supported by the data. For one, the use of the word \u201csignificant\u201d implies that they did some sort of significance test, which they did not as far as I can tell. Perhaps more importantly is that for certain settings (e.g. the blue and orange curves in figure 2(a)) the recurrence appears to hurt performance. Due to the axis rescaling (see point b above), it is hard to see whether these differences matter. Finally, they should note what data that accuracy was computed on -- I assume one of the ImageNet test sets?\n\ne) I do not understand the metric used in figure 2(b). I believe a normalized MSE distance of 1.0 indicates that the reconstruction quality with recurrence matches that without. The authors should be explicit about this. This metric is not as helpful to understand as reporting the raw MSE values (this is a theme across my suggestions). I find it interesting that there is not a consistent correlation or trend when comparing MSE against noise level. In fact, the PVGG net and PEfficientNet appear to have opposite trends, where the noise free case has the highest score for PVGG and almost lowest for PEfficientNet. There is a similar inconsistency (and similar metric confusion) in figure 2(d), where there is not a monotonic relationship between layer and distance, as I would expect. I would appreciate it if the authors would comment on this. \n\nf) I think it would be helpful if the authors used the same images for the two networks in figure2(C) so that readers can understand the relative difference in the baselines used.\n\ng) In terms of the corruption experiments, given that the clean accuracy has changed I think it makes more sense to use the relative CE score from [8] than the regular CE. Also, the authors do not say if they are using the same AlexNet baseline as was used in [8]. At timestep 0 they always report CE of 1.0, which leads me to guess that they are using the standard VGG or EfficientNet score as the denominator. At a minimum the authors should be explicit about how they are computing the metric instead of deferring to the citation (unless they are indeed computing the score exactly as was done in [8]). Ideally they should use AlexNet as their denominator so that the reader can properly compare their performance with other works in the field. I appreciate that the authors intended to focus on the added value of feedback, instead of achieving SOTA on some benchmark. However, it is still helpful to understand the added value if we have a good relative context.\n\nh) The authors alternate between showing individual CE scores without hyperparameter tuning in figure 3.(a) and mCE with tuning in figure 3.(b). But in the appendix figure 6 we can see that there are tuned CE scores that perform worse with recurrence (e.g. zoom & fog for VGG). To clarify the results I recommend that the authors provide more information in the main paper. Specifically, they should acknowledge negative CE results from the tuned/untuned models in the main paper. And if the goal is to compare the results \u201cwith tuning\u201d against \u201cwithout tuning\u201d, then they should use the same metric in both scenarios. \n\ni) The robustness metrics used are also not standard in that field. The appendix figures 8, 9, and 10 are much more in line with what is typically reported. I don\u2019t see the added benefit of the presentation in figure 4 when we can easily see the \u201cbaseline corrected\u201d score by comparing time step 0 with those that are > 0 (this argument applies to many of the above points as well). Again, even though the authors wish to stress a relative comparison it is important that they report their results in such a way that it can be viewed in the context of other work. As a point of comparison, the results reported in {9} also used FoolBox and are more aligned to typical setup and formatting.\n\nj) I\u2019m not sure if the concerns raised in {4, 8} about backpropagating through recurrence are relevant here, but it would be helpful if the authors would explicitly say that the white-box attacks did not suffer from gradient obfuscation issues (see {9}). Even better would be if they additionally used gradient-free attacks as in {8}.\n\n{1} Sulam et al., Adversarial Robustness of Supervised Sparse Coding, https://proceedings.neurips.cc/paper/2020/hash/170f6aa36530c364b77ddf83a84e7351-Abstract.html\n\n{2} Krotov et al., Dense associative memory is robust to adversarial inputs, https://direct.mit.edu/neco/article-abstract/30/12/3151/8426/Dense-Associative-Memory-Is-Robust-to-Adversarial?redirectedFrom=fulltext\n\n{3} Frosst et al., DARCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules, \u200b\u200bhttps://arxiv.org/abs/1811.06969\n\n{4} Paiton et al., Selectivity and robustness of sparse coding networks, https://jov.arvojournals.org/article.aspx?articleid=2772000\n\n{5} Kim et al., Modeling Biological Immunity to Adversarial Examples, https://openaccess.thecvf.com/content_CVPR_2020/html/Kim_Modeling_Biological_Immunity_to_Adversarial_Examples_CVPR_2020_paper.html\n\n{6} Lu et al., Sparse coding for image denoising using spike and slab prior, https://www.sciencedirect.com/science/article/abs/pii/S0925231212007643?via%3Dihub\n\n{7} Oja et al., Image Feature Extraction and Denoising by Sparse Coding, https://link.springer.com/article/10.1007/s100440050021\n\n{8} Schott et al., Towards the first adversarially robust neural network model on MNIST, https://openreview.net/forum?id=S1EHOsC9tX\n\n{9} Athalye et al. Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples http://proceedings.mlr.press/v80/athalye18a.html Yes",
            " Thank you again for your reply. The provided figures and promised updates for myself as well as the other reviewers is sufficient for me to consider this paper worthy of publication. I have increased my score two points.",
            " We are quickly approaching the discussion deadline. Would you mind confirming that you received and understand the final requests in my above comment? I do not believe it'll be difficult for you to make those changes, but some confirmation that they will be done would raise confidence in general for the final assessment.\n\nThank you",
            " Thanks for adding the discussion. It is very important for readers to know why the predictive coding is implemented like this. My comment is addressed after adding the discussion.",
            "Taking inspiration from the theory of predictive coding in neuroscience, the authors implement CNNs with layerwise predictive activity, as a form of unsupervised training. Experiments show that this approach is helpful in case of image corruption.  The paper is generally well written and easy to follow.  The illustrations are effective. The section dedicated to the description with code snapshots of the developed library is unnecessary in the paper and could be relegated to supplementary material or link documentation. I would use the space available to provide more references to the state of the art and describe the proposed approach in more detail.\n\nThe idea of predictive coding applied to neural network layers is not new. The authors should expand the related works section to be more inclusive of more recent work.  The novelty of the work is very limited. Since the model is not new, they only show that the application brings advantages in the case of corrupted images with perturbations. \n\nThe experimental part is also limited. The authors only evaluate one type of image noise. It would have been interesting to see if this theory could contribute to improving robustness against adversarial attacks. ",
            " Thank you for taking the time to consider my suggestions. Overall I think the quality of the paper will be improved when accounting for the promised updates after my own and the other reviews. However, I do still have a few clarifications below that I would like the authors to consider. I hope that these will be minor points and the discussion can conclude quickly.\n\na) I appreciate the care to include this related context. Note also that at least one of the sparse coding models for robustness, {5}, could be considered a \u201cDNN\u201d and I believe related work from that group also fits a \u201cDNN\u201d classification. I trust the authors will take care in framing the sparse coding literature in the context of their contribution.\n\nb) I think including figures with absolute values in the appendix with a reference to it in the main manuscript would be helpful. See my note below about axis rescaling in general.\n\nc) Please include this explanation in the main manuscript, and if possible make an effort to have some sort of consistent criteria for stopping the recurrence, even if that criteria changes per experiment type. Even with your response, I still don\u2019t understand a few stopping criteria beyond \u201carbitrary.\u201d For example, the apparent convergence of PVGG16 and PEfficientNetB0 in figure 3(b) is totally different. EfficientNet seems to do a reversal, and is possibly reaching a plateau. However, VGG is continuing to decline and might show a similar reversal with more inference steps. You mentioned that VGG is easier to run, so why stop early? Maybe I\u2019m reading these incorrectly, if so please clarify. Another example would be figures 2(a) and 2(b). I am guessing that both of those metrics can be computed simultaneously without additional cost, so why were you able to run 20 time steps for EfficientNet when computing accuracy but only 15 when computing the MSE distances, and of course the same argument applies for VGG when comparing the two metrics.\n\nd) I think your revised sentence is more accurate and clear. I would appreciate an attempt to explain why the accuracy does not consistently improve in low-noise settings, or at least an acknowledgement of it, but I understand that this might have to be left as an open question. By \u201caxis rescaling\u201d, I mean a physical distance on the y axis corresponds to a different Improvement in Accuracy between the two subplots. To say it another way, the distance from 0 to 3 in the PVGG subplot is about the same as the distance from 0 to 6 in the PEfficientNet plot. See my note below for a longer discussion.\n\ne) Thank you for the clarification, this makes sense and seems reasonable. Including the absolute values in the appendix would be sufficient. I guess the answer to my followup question will have to be relegated to future work, which is acceptable to me.\n\nf) Additional images could be put in the appendix if one wanted to show more variety.\n\ng-j) Thank you, I appreciate the clarifications. Please take care to justify when you are using CD vs. mCE, and I look forward to seeing the gradient-free attack results.\n\nAdditional note: I should have been more clear about my general point with the axis rescaling. My suggestion is for you to equalize the y-axis values in many of your adjacent plots. For example, in figure 2D you have specific sets of subplots for PVGG16 & PEfficientNetB0 and within those subplots your y-axis values are adjusted to show a relative minimum. There are other examples in the responses above. This makes it difficult to observe overall trends, and can over-exaggerate perceived differences. I suggest that you look through each grouping of subplots carefully and assess if they really _need_ to have different (unshared) y-axes. Without a justification the default action should be to share the axes.\n",
            "This paper implemented the predictive coding dynamics into deep neural nets, and observed the increase in robustness against various corruptions. The authors also provided a pytorch-based package for easy implementation of the proposed dynamics into deep neural nets.   The predictive coding principle has gained increasing attention in the machine learning community. Despite not being the first to incorporate the predictive coding dynamics into neural networks, this paper did a good job in formulating the predictive coding principle in a clean way and they were able to test out the ideas on large scale datasets like Imagenet. The paper was well written and provided enough details to reproduce their results. In general, I enjoyed reading this paper. \nHowever, I do have some comments and questions regarding the proposed mechanism as well:\n1) In Eq. 2, the feedforward representations were updated to reduce the reconstruction error in the lower level (the alpha term). However, if we believe that the feedforward representations for noisy images are far away from that of the clean images and will cause wrong classification, why reducing the distance between the feedback representation and this \"wrong\" forward representation helps robustness? I understand that experimentally we see some positive results, but it would be good to comment on this point. \nAlso, it may be good to take a pretrained robust feedfoward network (for example, network trained with data augmentation techniques or adversarial training), and see whether the proposed mechanism can still improve the robustness.\n2) In the paper, the authors only reported relative improvements, but I am curious to see the absolute value of robustness accuracy. Since the authors take pretrained networks, I am wondering if similar robustness improvements can also be achieved by training the forward model differently (for example, using data augmentation or adversarial training). This is related to the last point I mentioned in 1). It would be good to show whether feedback can further improve robustly trained feedforward models.\n3) Since the authors introduced more weights in the model (the feedback path), could the robustness improvement be a result of using larger model? It would be good to take a deeper or wider feedforward model to match the number of parameters in the feedback model and see if the improvement is still there. \n4) In the pseudo code, it looks like the authors loop through all PCoder modules first, and then move on to the next time step. How would this compared with performing several update steps first until one PCoder is convergent and then move on to the PCoder in the next level?\n5) For the adversarial robustness experiment, I was wondering how the authors perform the attack? Are they performing end-to-end attack considering the predictive coding dynamics? If not, this should be evaluated. In addition, when performing end-to-end attack, adversarial attack may suffer from gradient obfuscation problem, so it would also be good to add black-box attack evaluation.\n6) How sensitive is the method's performance with respective to the coefficient \\alpha in Eq. 2? In Table 2 and Table 3 in appendix, \\alpha is fixed to be 0.01. But this term is an very important term for the predictive coding principle. Also the authors scaled the gradient based on feature size, so it seems that this is a sensitive hyper-parameter. \n The authors have provided sufficient discussion on this.",
            "The authors proposed a new algorithm to implement a popular neuroscience theory, i.e. predictive coding, in deep neural networks (DNN). The feedforward connection weights are pretrained for object recognition and the feedback connection weights are trained for image reconstruction, and then predictive coding introduces recurrent computations with the trained DNNs. The authors use two image recognition DNNs (VGG16 and EfficientNetB0) in experiments. They found that the recurrent computation brings improvement in object recognition and image/feature reconstruction over time given noisy inputs. This property of predictive coding improves DNN robustness to noisy examples and adversarial attacks. The authors also provide a python package to allow community to explore the predictive coding algorithm.  Predictive coding is an important theory in neuroscience and recently some researchers have explored several implementations with deep neural network to help understand the theory. How neurons recurrently process information is still unknown in predictive coding theory. This paper proposed a new implementation (though have some similarity to previous works) and did extensive experiments and found desirable properties of recurrent computation over time. This is very interesting and valuable to both neuroscience and deep learning communities. The writing is clear and easy to understand.\n\nThe feedforward connection weights in DNN are trained with forward propagation for object recognition (pretrained) and the feedback connect weights are trained with reconstruction of lower layer features. No recurrent computation is involved in training the connection weights, which may not be the case in the brain. It would be good if the author can discuss this in the perspectives of both neuroscience and machine learning. Yes."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Neutral",
            "Positive",
            "Negative",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('thank the authors'), approval ('happy to see'), and a positive assessment of the paper's quality ('above the acceptance threshold').",
            "The review expresses concerns about novelty, inconsistencies in results, lack of interpretable metrics, and the need for a better literature review. The reviewer states, \"I do not believe the paper is good enough for publication in its current form,\" indicating a negative overall assessment.",
            "The reviewer explicitly states that the paper is \"worthy of publication\" and increased their score, indicating a positive shift in their evaluation.",
            "The review expresses a need for confirmation and raises a question about the completion of changes, indicating a neutral stance.",
            "The reviewer expresses gratitude (\"Thanks for adding the discussion\") and indicates satisfaction (\"My comment is addressed after adding the discussion\").",
            "The review expresses concerns about the novelty and limitations of the work, stating that the idea is not new, the experimental part is limited, and the section with code snapshots is unnecessary. These criticisms indicate a negative overall sentiment.",
            "The reviewer expresses appreciation for the authors' consideration and believes the paper's quality will improve. They acknowledge clarifications and find some revisions accurate and clear, indicating a generally positive outlook.",
            "The review expresses both positive and negative feedback. It acknowledges the paper's strengths (well-written, clear formulation, experiments on large datasets) but also raises several questions and concerns, indicating a balanced perspective.",
            "The review expresses positive feedback, highlighting the paper's interesting and valuable contribution to both neuroscience and deep learning. Phrases like \"very interesting and valuable,\" \"desirable properties,\" and \"clear and easy to understand\" indicate a positive sentiment."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Supportive",
            "Neutral",
            "Supportive",
            "Critical",
            "Supportive",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'I'd like to thank' and 'I am happy to see', indicating a supportive and encouraging tone. They also explicitly state that the paper is 'above the acceptance threshold'. ",
            "The review provides a detailed list of weaknesses (labeled a through j) and uses phrases like \"It is not clear to me,\" \"I do not believe this statement is supported by the data,\" and \"I don't see the added benefit.\" These phrases, along with the extensive list of criticisms, indicate a critical tone. The reviewer also points out inconsistencies and lack of clarity in the presentation and interpretation of results.",
            "The reviewer expresses gratitude (\"Thank you again\") and uses encouraging language (\"sufficient for me to consider this paper worthy of publication\").",
            "The tone is neutral as it's a polite request for confirmation and assurance, using phrases like \"Would you mind confirming\" and \"raise confidence\" without expressing strong positive or negative feelings.",
            "The reviewer uses positive language like \"very important\" and \"thanks,\" indicating a supportive and encouraging tone.",
            "The review uses phrases like \"unnecessary,\" \"limited,\" and \"not new,\" indicating a critical assessment of the paper's contributions and scope. The reviewer also suggests specific improvements, such as expanding the related works section and exploring different types of image noise.",
            "The reviewer uses phrases like \"Thank you for taking the time to consider my suggestions\", \"I appreciate the care to include this related context\", and \"Thank you, I appreciate the clarifications\". They also offer constructive suggestions and express understanding of limitations, suggesting a supportive stance.",
            "The review starts with positive comments ('did a good job', 'well written', 'enjoyed reading') but then transitions to specific criticisms and questions ('However, I do have some comments and questions'). The reviewer uses polite language and frames concerns as questions, suggesting a balanced and constructive approach.",
            "The review uses encouraging language and constructive criticism. It acknowledges the paper's strengths and suggests a specific improvement to strengthen the work. The reviewer uses \"It would be good if the author can discuss this...\" which indicates a supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review expresses consistently positive feedback, praising the authors' response, new experiments, comprehensiveness of the paper, and explicitly stating the paper is above the acceptance threshold.",
            "The review is consistent in its assessment, highlighting both strengths and weaknesses of the paper. While acknowledging positive aspects like clarity of writing and potential of the provided software, it consistently points out significant issues related to methodology, result interpretation, metrics, and comparison to prior work. The reviewer's overall judgment that the paper is not ready for publication in its current form is well-supported by the detailed points in both strengths and weaknesses sections, without any self-contradiction.",
            "The review expresses a positive change in opinion after the authors' response. The reviewer explicitly states that the provided figures and updates are sufficient to consider the paper worthy of publication and mentions increasing the score, indicating a consistent positive evaluation based on the authors' actions.",
            "The review is consistent because the reviewer's statements are aligned and do not contradict each other. The reviewer is politely asking for confirmation and expressing confidence that the requested changes will be made, which are consistent goals in a review context.",
            "The review is consistent because the reviewer expresses gratitude for the addition of a discussion and confirms that their comment has been addressed as a result of this addition. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because it provides both positive and negative feedback without contradicting itself. The reviewer acknowledges the paper's strengths, such as being well-written and having effective illustrations, while also pointing out weaknesses related to novelty, limited experimental scope, and the need for more references. The suggestions for improvement, like expanding the related works section and removing unnecessary code snapshots, are logically connected to the criticisms and aim to address the identified limitations.",
            "The review is consistent because it maintains a constructive and helpful tone throughout. The reviewer provides specific feedback and suggestions for improvement in a logical and coherent manner, without contradicting themselves. The reviewer acknowledges the authors' efforts and responses, while still pointing out areas that need further clarification or improvement, maintaining a consistent focus on enhancing the paper's quality.",
            "The review is consistent. It begins with positive remarks about the paper's contributions, clarity, and reproducibility. Then, it transitions to constructive criticism and questions, focusing on specific aspects of the methodology, experiments, and potential improvements. The questions are logically connected and aim to enhance the paper's robustness analysis, parameter sensitivity understanding, and comparison with existing methods. There are no self-contradictory statements within the review; the reviewer's concerns and questions are presented as areas for further clarification and investigation, not as fundamental flaws that negate the initial positive assessment.",
            "The review is consistently positive overall, highlighting the paper's contributions and significance. The critique regarding the lack of recurrent computation during training is presented as a suggestion for improvement and further discussion, rather than a contradiction to the positive evaluation of the work's value and interest."
        ]
    },
    {
        "paper_id": "nips_2021_MGX69TBAi07",
        "paper_title": "Smoothness Matrices Beat Smoothness Constants: Better  Communication Compression Techniques for Distributed Optimization",
        "paper_abstract": "Large scale distributed optimization has become the default tool for the training of supervised machine learning models with a large number of parameters and training data. Recent advancements in the field provide several mechanisms for speeding up the training, including {\\em compressed communication}, {\\em variance reduction} and {\\em acceleration}. However, none of these methods is capable of exploiting the inherently rich data-dependent smoothness structure of the local losses beyond standard smoothness constants. In this paper, we argue that when training supervised models,  {\\em smoothness matrices}---information-rich generalizations of the ubiquitous smoothness constants---can and should be exploited for further dramatic gains, both in theory and practice. In order to further alleviate the communication burden inherent in distributed optimization, we propose a novel communication sparsification strategy that can take full advantage of the smoothness matrices associated with local losses. To showcase the power of this tool, we describe how our sparsification technique can be adapted to three distributed optimization algorithms---DCGD, DIANA and ADIANA---yielding significant savings in terms of communication complexity.  The new methods always outperform the baselines, often dramatically so.\n",
        "review_ids": [
            "m-q_FX7Dt0I",
            "503ICd1AG2Z",
            "YNVA1Nar2EY",
            "FqqkqjR7Ay",
            "rimy5CuE60U",
            "-Qry7NyjwH9",
            "yJcHORu6xOhH",
            "SO6Mn2X99mf"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for responding to my review. After reading the response and other reviews, I am happy with my previous assessment.\n\nYes, I meant loss functions that cannot be described by the structural assumption in Lemma 1.",
            " A diagonal sparsification sounds a lot like the block-specific $L_i$ constants often used in block coordinate descent methods. (See Wright 2015 survey.) I agree it is useful, but it is not first presented in this work. ",
            " To clarify, I'm not bringing this up to say that only proofs can be 50 pages, or that proofs that are 50 pages are fun to read. I'm just pointing out that usually when an appendix is very long, it's to accommodate something like a proof that is too long and burdensome to put in the main paper, and doesn't really add interest to most readers. To me, as a reviewer, it seemed like a good chunk of this appendix was meant to add to the quality of the paper, and I'm surprised the authors chose to not showcase it, but I am not judging that beyond saying that I did not read it carefully and don't feel its contributions should be a huge part of the decision. If that is ok with the authors, then it's completely fine.",
            "  - Why do you think that the extreme case of $m_i = 1$, rank-1 smoothness matrices may not be beneficial ? Please note that the theory is the same, and Figure 4 confirms this numerically.\n\nWell, the question is how well it compares against just knowing the element-wise Lipschitz constant, as is practice in, say,  coordinate descent methods. ",
            "  - We are not aware of any alternative approach to do what we are doing in our paper: we are not aware of any prior work that proposes the use of smoothness matrices as objects of key interest in communication-efficient distributed training, and proposes their use in the design of smoothness-matrix-aware bespoke compression operators.\n\nYes, but the idea of using quadratic approximations is a fundamental tool in optimization. To me, this seems like a global Hessian approximation, which is a fairly commonly used trick. Anyway I'm not saying I've seen this exact implementation, but at the very least the work could be presented in the context of other Hessian approximation tools, such as Newton subsampling, or Kernel subsampling. \n\n - You do not make it clear what approach you have in mind when mentioning leverage scores. \n\nLeverage scores are commonly used to approximate kernel matrices, for example in row/column sampling for Nystrom sampling. There are many papers on this. (e.g. Gittens, Mahoney 2013)",
            "The papers consider the standard distributed optimization scenarios, where a server wants to minimize the average of $n$ loss functions, which are evenly distributed across $n$ participating worker nodes. Typically, optimization problems designed for such distributed optimization scenarios assume a blanket smoothness constant for the average and the individual functions involved. Instead, the authors consider the notion of matrix smoothness, where the distance between the points in defining the smoothness is measured with matrix norm (of a smoothness matrix). Under the assumption that the individual loss function satisfies matrix smoothness with different matrices, the authors design compression algorithms using these smoothness matrices and then use them in conjunction with various distributed optimization algorithms. The authors show that as a result of their smoothness-aware compression, they improve over the convergence guarantees of the standard baseline algorithms considered, namely, DCGD, DIANA, ADIANA.  Originality:\n\n  a) The notion of matrix smoothness considered in this paper is a novel one. None of the present distributed algorithms have exploited loss function smoothness to this granularity to the best of my knowledge. As the authors show, this notion can further improve the performance of distributed optimization algorithms.\n\n  b) The compression operator proposed is tailored to the smoothness matrix of the loss function and therefore improves over the performance of generic compression operators proposed in the literature.\n \n c) The distributed optimization algorithms used by the authors seem to only differ in the compression part with respect to the ones present in the literature.\n\nPresentation:\n\na) The paper's presentation is excellent. The introduction clearly mentions the paper's main ideas, contributions, and comparison to prior work.\n\n b) A minor point would be to clarify if the authors introduce the notion of matrix smoothness or was it already present in the literature. (Either way, my review would remain unchanged based on this point, but it will be instructive for the reader.)\n\n\nSignificance: \n\n a) The idea of using smoothness to this granularity is a novel one and can significantly improve the performance of distributed optimization algorithms.  However, it is unclear to me how the precise smoothness matrix could be identified for complicated loss \nfunctions. \n\nb) This work could also inspire other research works to explore other properties of functions to this granularity (e.g., Strong Convexity).\n\nTo summarize, I like this paper and recommend that it be accepted.\n\n Yes.",
            "This paper proposes a new notion of smoothness called matrix smoothness which leads to better theory bounds and a data-dependent compression scheme. The compression scheme is applied to three existing algorithms resulting in improved convergence bounds in smooth and strongly convex setting. Simple experiments are done to show the effectiveness of the proposed scheme.  The idea of using smoothness matrices to improve convergence bounds as well as perform data-dependent compression seems novel but kind of infeasible for large-scale problems (see next section). Theoretically, the proposed scheme seems to offer a significant speed-up when importance sampling is used. The paper is reasonably well-written. Some comments:\n1) Since you consider strongly convex and smooth objectives, it\u2019d be good to show how much speed-up you get in practice vs. how much you expect theoretically.\n2) Is it possible to give an estimate of the speed-up when you use uniform sampling and not importance sampling?\n3) In Theorem 2 for DCGD+, sigma^{*} can become large if $L_i^{*}$ is poorly conditioned (then the max eigenvalue of its inverse will become large)?\n4) line 195 - typo \u201ccompression\u201d\nPlease see my concerns in the next section. I have concerns about the feasibility of the proposed method. First, I expect it\u2019d be hard to estimate the matrices $\\mathbb{L}_i$ especially in high-dimensional deep learning settings. Second, modern ReLU networks don\u2019t satisfy smoothness so this method may not be applicable there. It\u2019d be nice to know if it can be extended for non-convex and non-smooth deep learning problems. The proposed compression scheme would probably increase per-client or per-device computation because you need to invert the $L_i$\u2019s. ",
            "This paper analyzes several optimization concepts: compression, variance reduction, and acceleration, using the assumption of smoothness matrices rather than the smoothness parameter. Sketching matrices are designed as sum_{i\\in S} L^{1/2}_i c_i * L^{1/2,dagger}_i where L_i is the \"smoothness matrix\" associated with the ith data sample. Taking this additional information into account, which for generalized linear models is not too hard to compute, it is suggested that there can be a factor of min(n,d) improvement in unaccelerated methods and sqrt(min(m,n)) benefit in accelerated methods.   This work is interesting, and though the contribution feels incremental, the L_i matrices are indeed not complicated to precompute for most applications. Overall, the writing is fairly clean and the results do seem novel, though it is unclear to me if they are a bit incremental, as it seems to be not that much of an improvement over using leverage scores. \n\nThe one big weakness is that it seems most of the paper is actually in a 50 page appendix, which is not just extra experiments and a few lengthy proofs, but quite a bit of clarifying text and interpretations as well. Moreover many of the proofs pushed to the appendix are short, so it is not the lengthiness of the proofs that is causing such a big appendix. The impact is that while I can check a few pages of the appendix, I cannot reasonably judge all 50 pages in this short review period, nor do I think that is fair compared to the other works that squeezed all the big ideas in the first 8 pages. At the very least, the authors should condense their main story to clearly and succinctly outline how to get their main constant factor improvements. That being said, the parts I was able to read seem reasonable; the proof techniques seem standard and the results follow expected trends. \n\nSpecific critiques:\n - The definition of L_i in (5) should be rank m_i for problems like logistic regression, but could be of higher rank in a neural network model. These two regimes seem a bit separate: sketching should make a big impact in shallow models (like logistic regression) but in the extreme case of m_i = 1, L_i matrix shouldn't be that much more handy than L_i scalar, and it seems that this is somewhat the example shown in the experimental results. On the other hand, a deep model that can overfit may have a more complex L_i. Am I misinterpreting this? Are the experiments using L_i that have more interesting rank? These details should be included in the main paper, not in the appendix. \n\n - Are the results significantly better than using leverage scores? What are the main scenarios where this method would give benefit? \n\n - Can the authors point to a specific scenario where L_i the matrix is giving a significant benefit over L_i a smoothness constant? e.g. give a specific sparsification scheme that is used in practice, or a dataset type with such features in the data matrix?  No negative societal impact"
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Neutral",
            "Neutral",
            "Negative",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses satisfaction with the authors' response and their previous assessment, indicating a positive overall sentiment.",
            "The review acknowledges the usefulness of the method but points out a lack of novelty, suggesting a neutral stance.",
            "The reviewer expresses a neutral stance, clarifying their perspective and avoiding strong positive or negative judgments. They acknowledge the authors' choices without explicitly criticizing or praising them.",
            "The review poses a question and seeks clarification, without expressing strong positive or negative opinions. The language is inquisitive rather than judgmental.",
            "The review expresses skepticism about the novelty of the approach and suggests the work should be contextualized within existing literature on Hessian approximation and kernel methods. The reviewer uses phrases like 'To me, this seems like a global Hessian approximation' and 'the work could be presented in the context of other Hessian approximation tools,' indicating a critical stance.",
            "The reviewer states \"I like this paper and recommend that it be accepted.\"",
            "The review expresses concerns about the feasibility of the proposed method for large-scale problems, the difficulty of estimating the smoothness matrices, and the applicability of the method to modern ReLU networks and non-convex/non-smooth deep learning problems. These concerns outweigh the positive aspects like novelty and improved convergence bounds.",
            "The review expresses concerns about the paper's structure, particularly the reliance on a large appendix containing essential information. The reviewer also questions the significance of the improvement over existing methods and the practical benefits of using smoothness matrices."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Neutral",
            "Neutral",
            "Critical",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer thanks the authors and expresses happiness with their previous assessment, indicating a supportive tone. The phrase \"I am happy with my previous assessment\" is a clear indicator of support.",
            "The reviewer uses phrases like 'I agree it is useful' which indicates a positive aspect, but then immediately follows with 'but it is not first presented in this work', indicating a critical aspect. This balance suggests a balanced tone.",
            "The tone is neutral, characterized by phrases like 'To clarify,' 'I'm just pointing out,' and 'I am not judging that beyond saying.' The reviewer focuses on explaining their perspective and avoiding strong emotional expressions.",
            "The tone is neutral as it is a question seeking clarification. Phrases like \"Why do you think\" and \"Please note\" are polite but do not indicate strong emotion or bias.",
            "The tone is critical, as evidenced by phrases such as 'To me, this seems like...' which implies disagreement and a questioning of the paper's originality. The reviewer also points out a lack of clarity ('You do not make it clear...') and suggests relevant literature that should be considered, indicating areas where the paper is lacking.",
            "The reviewer uses positive language such as \"excellent presentation\", \"novel one\", \"significantly improve\", and explicitly recommends acceptance.",
            "The review raises concerns about the practical applicability and limitations of the proposed method, using phrases like \"kind of infeasible,\" \"concerns about the feasibility,\" \"hard to estimate,\" and \"may not be applicable.\" These indicate a critical tone.",
            "The review uses phrases like \"the contribution feels incremental\", \"it is unclear to me if they are a bit incremental\", \"the one big weakness\", \"I cannot reasonably judge all 50 pages\", and poses direct questions about the novelty and benefits of the proposed method compared to alternatives."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer explicitly states they are happy with their previous assessment after reading the author's response and other reviews. This indicates consistency with their previous evaluation. The second sentence clarifies a point, further supporting the consistency.",
            "The review is consistent because the reviewer's points are logically connected and do not contradict each other. The reviewer relates the method to existing methods, acknowledges its usefulness, and points out a lack of novelty, all without internal contradiction.",
            "The reviewer consistently explains their perspective on the appendix, clarifying their understanding of its typical purpose and their approach to evaluating it in this specific case. There are no self-contradictory statements.",
            "The review is consistent because the reviewer raises a question about a potential negative claim from the authors regarding the extreme case, while also acknowledging supporting evidence (theory and Figure 4). The reviewer then transitions to a relevant comparison with existing methods (coordinate descent), which is a logical progression in evaluating the work. There is no self-contradiction in the reviewer's points; they are seeking clarification and context.",
            "The reviewer acknowledges the novelty of the specific combination of smoothness matrices in distributed training but suggests contextualizing the work within the broader field of optimization and approximation techniques, specifically mentioning Hessian approximations and leverage scores. The reviewer is not contradicting themselves, but rather providing constructive feedback to improve the paper by placing it in the context of existing literature and clarifying certain aspects.",
            "The review is consistent in its positive assessment of the paper. The reviewer repeatedly highlights the novelty and significance of the matrix smoothness concept and the tailored compression method. While raising a minor point about clarifying the novelty of matrix smoothness and questioning the practical identification of the smoothness matrix, these points do not contradict the overall positive recommendation. The reviewer consistently expresses a favorable opinion of the paper and recommends acceptance.",
            "The review is consistent because it acknowledges the novelty and theoretical contributions of the paper, but consistently raises concerns about the practical feasibility and applicability of the proposed method, especially in large-scale and deep learning settings. The reviewer's questions and concerns are all focused on the practical limitations and potential challenges in implementing the proposed method.",
            "The review is consistent because it acknowledges the interesting aspects and potential novelty of the work while also pointing out significant weaknesses related to the paper's structure (heavy reliance on the appendix), clarity of contribution (incremental improvement, comparison to leverage scores), and specific technical questions. The reviewer's positive and negative points are about different aspects of the paper and do not contradict each other. The reviewer is consistently critical of the presentation and the perceived incremental nature of the contribution, while still acknowledging the potential value of the core ideas."
        ]
    },
    {
        "paper_id": "iclr_2020_Syx7WyBtwB",
        "paper_title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge",
        "paper_abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.",
        "review_ids": [
            "Hkgqk1x6Fr",
            "rklgSCRptr",
            "BJxDW5Jl5B"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper presents a method intended to allow practitioners to *use* explanations provided by various methods. Concretely, the authors propose contextual decomposition explanation penalization (CDEP), which aims to use explanation methods to allow users to dissuade the model from learning unwanted correlations. \n\nThe proposed method is somewhat similar to prior work by Ross et al., in that the idea is to include an explicit term in the objective that encourages the model to align with prior knowledge. In particular, the authors assume supervision --- effectively labeled features, from what I gather --- provided by users and define an objective that penalizes divergence from this. The object that is penalized is $\\Beta(x_i, s)$, which is the importance score for feature s in instance $i$; for this they use a decontextualized representation of the feature (this is the contextual decomposition aspect). Although the authors highlight that any differentiable scoring function could be used, I think the use of this decontextualized variant as is done here is nice because it avoids issues with feature interactions in the hidden space that might result in misleading 'attribution' w.r.t. the original inputs.\n\nThe main advantage of this effort compared to work that directly penalizes the gradients (as in Ross et al.) is that the method does not rely on second gradients (gradients of gradients), which is computationally problematic. Overall, this is a nice contribution that offers a new mechanism for exploiting human provided annotations. I do have some specific comments below.\n\n- I am not sure I agree with the premise as stated here. Namely, the authors write \"For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective\" -- I would argue that an explanation may be useful in and of itself by highlighting how a model came to a prediction. I am not convinced that it need necessarily lead to, e.g., improving model performance. I think the authors are perhaps arguing that explanations might be used to interactively improve the underlying model, which is an interesting and sensible direction.\n\n- This work, which aims to harness user supervision on explanations to improve model performance, seems closely related to work on \"annotator rationales\" (Zaidan 2007 being the first work on this), but no mention is made of this. \"Do Human Rationales Improve Machine Explanations?\" by Strout et al. (2019) also seems relevant as a more recent instance in this line of work. I do not think such approaches are necessarily directly comparable, but some discussion of how this effort is situatied with respect to this line of work would be appreciated.\n\n- The experiment with MNIST colors was neat. \n\n- The authors compare their approach to Ross and colleagues in Table 1 but see quite poor results for the latter approach. Is this a result of the smaller batch size / learning rate adjustment? It seems that some tuning of this approach is warranted. \n\n- Figure 3 is nice but not terribly surprising: The image shows that the objective indeed works as expected; but if this were not the case, then it would suggest basically a failure of optimization (i.e., the objective dictates that the image should look like this *by construction*). Still, it's a good sanity check.\n\n\n",
            "The paper presents a way of using generated explanations of model predictions to help prevent a model from learning \"unwanted\" relationships between features and class labels. This idea was implemented with a particular explanation generation method from prior work, called contextual decomposition (CD). For a given feature, the corresponding CD can be used to measure its importance. The proposed learning objective in this work optimizes not only the cross entropy loss, but also the difference between the CD score of a given feature and its explanation target value. Experiments show that this new learning algorithm can largely improve the classification performance.\n\nI like the high-level idea of this work and agree that there is not much work on using prediction explanations to help improve model performance. However, there are two major concerns of the model and experiment design. \n\nFirst, it seems like the proposed method requires whoever use it already know what the problem is. For example, \n\n- in section 3.3, the model inputs include a collection of features and the corresponding explanation target values.\n- in section 4.1, it is already known that some colorful patches only appear in some non-cancerous images but not in cancerous images. \n- it is even more obvious in section 4.2 and 4.3, because in both experiments, the training and test examples were altered on purpose to create some mismatch. \n\nMy question is that if we already know the bias or the mismatch, why not directly use this information in the regularization to penalize some features? Is it necessary to resort to some explanation generation methods?\n\nMy second concern is more like a personal opinion. In the experiment of section 4.2, if the colors are good indicators of these digits in the training set, I don't it is wrong for a model to capture these important features. However, the way of altering examples in the same class with different colors in training and test sets seems questionable, because now, the distributions of training and test images are different. On the other hand, if we already know color is the issue, why not simply convert the images into black-and-white? A similar argument can also be applied to the experiment in section 4.3\n\nOverall, I like the idea of using explanations to help build a better classifier. However, I am concerned about the value of this work. \n",
            "The authors propose to add a regularizer to the loss function when training a prediction model. In particular, the regularizer considers explanations during the model training; if the explanations are not consistent with some prior knowledge, then explanation errors will be introduced. \n\nThe motivation for the proposed research is interesting and has some merit. However, I am a bit worried that the proposed approach is somewhat ad hoc. I can imagine there are various explanations that can be generated for the same model. There can also be different prior knowledge available for a particular problems. Which prior knowledge and explanations to use seem to affect a lot about the learned model. But there is no principled approaches for making the selection.\n\nIn some sense, standard regularizers such as L1 or L2 are are intrinsic regularizers, while the proposed regularizer is extrinsic regularizer. I think the extrinsic regularizer certainly has some merit, but it is also hard to regulate.\n\nFor instance, consider the example in Figure 2 about the presence of patches. Isn't that a too specific knowledge about the dataset, which in turn makes the proposed approach not general? I have doubts on how useful a method is if it relies on such specific prior knowledge about the data."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states \"Overall, this is a nice contribution that offers a new mechanism for exploiting human provided annotations.\" and \"The experiment with MNIST colors was neat.\"",
            "The review expresses concerns about the core premise and experimental design of the paper, questioning the necessity and validity of the proposed method. Phrases like \"major concerns,\" \"seems questionable,\" and \"concerned about the value of this work\" indicate a negative sentiment.",
            "The reviewer expresses concerns about the ad hoc nature of the approach, the lack of principled selection methods, and the reliance on specific prior knowledge, indicating a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review offers both positive feedback (\"nice contribution\", \"neat experiment\") and constructive criticism (suggestions for improvement, pointing out missing related work, questioning the premise). This balanced approach indicates a nuanced and thoughtful evaluation.",
            "The review uses a critical tone by directly questioning the assumptions and experimental setup of the paper. Phrases like \"it seems like the proposed method requires whoever use it already know what the problem is,\" \"My question is that if we already know the bias or the mismatch, why not directly use this information,\" and \"the way of altering examples...seems questionable\" demonstrate a critical evaluation of the work.",
            "The review uses phrases like \"a bit worried,\" \"somewhat ad hoc,\" \"no principled approaches,\" and \"I have doubts\" to express criticism and skepticism about the proposed method."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the paper's contribution as valuable and novel, while also providing constructive criticisms and suggestions for improvement. The reviewer points out both strengths (e.g., avoiding second gradients, nice MNIST experiment) and weaknesses or areas for improvement (e.g., premise of explanations, lack of discussion of related work, questions about experimental setup and figure significance). The criticisms are aimed at strengthening the paper and do not contradict the overall positive assessment of the contribution.",
            "The reviewer consistently raises concerns about the practical value and necessity of the proposed method, given that it seems to require prior knowledge of the bias it aims to mitigate. The reviewer questions why explanation methods are needed if the bias is already known and suggests simpler alternatives like direct regularization or data preprocessing. The reviewer's arguments are logically connected and build upon the initial appreciation of the high-level idea to a critique of the implementation and experimental design.",
            "The review is consistent in expressing concerns and doubts about the proposed approach. While acknowledging the motivation's merit, the reviewer consistently raises concerns about the ad-hoc nature of the method, the subjectivity in choosing explanations and prior knowledge, the lack of principled selection methods, and the potential limitations in generality due to reliance on specific prior knowledge. The reviewer's arguments flow logically and build upon each other to support a critical assessment of the approach's practicality and generalizability."
        ]
    },
    {
        "paper_id": "iclr_2021_N33d7wjgzde",
        "paper_title": "Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning",
        "paper_abstract": "Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bounding boxes, labeled points and scribbles. This task is challenging, as coarse annotations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image.\n      \n      We formulate weakly supervised segmentation as a semi-supervised metric learning problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image similarity, semantic annotation, co-occurrence, and feature affinity They act as priors; the pixel-wise feature can be learned from training images with any partial annotations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discriminative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose. Our code is publicly available at https://github.com/twke18/SPML.",
        "review_ids": [
            "-CIOT0L8ObO",
            "tTFY4-YUaPv",
            "ZTbEayq8pm-",
            "T9QuyHfckA6",
            "QcnkjfLgdEQ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary:\n\nThe paper proposes a unified framework for weakly-supervised semantic segmentation that can take various types of weak labels as the input, e.g., points, scribbles, boxes, image tags. The authors formulate it as a contrastive learning framework by considering pixel-to-segment relationships, i.e., for each pixel, finding positive and negative segments to perform contrastive learning objectives. Specifically, the paper introduces four types of relationships between pixels and segments, i.e., image similarity within the image, semantic relationship, semantic co-occurrence, and feature affinity across images. In experiments, results with various weak labels show SOTA performance on the PASCAL and DensePose datasets.\n\nPros:\n\nThe paper is well written and is easy to follow. The proposed unified framework using the idea of pixel-to-segment contrastive learning is interesting for the weakly-supervised semantic segmentation task. \n\nThe proposed four types of pixel-to-segment relationships seem mostly reasonable and effective for different types of weak label inputs.\n\nExtensive experiments are conducted to demonstrate the effectiveness of the proposed framework and show strong performance on the PASCAL and DensePose datasets.\n\nCons:\n\nAlthough using pixel-to-segment contrastive learning is interesting for weakly-supervised semantic segmentation, the technical contribution is a bit limited as it mostly derives from the recent work in Hwang et al., 2019.\n\nIt is not clear to fully understand the effectiveness of the introduced four types of relationships. For example, for semantic co-occurrence, it is doubtful to form positive pairs simply based on the co-occurrence, as the features in two images could be very different even they share at least one same label. This situation could be more significant if performing on more challenging datasets that contain diverse scenes and more categories, e.g., COCO, ADE20K. The authors should make comments on this or provide more analysis for reasoning this design choice, e.g., one easy thing to consider could be weighting this relationship based on the co-occurrence rate.\n\nIt appears that feature affinity is less effective or not used in some settings. Although the authors explain the reason in the appendix (due to noisy label propagation), it may not be a principal way to integrate this feature affinity into any dataset. Moreover, Table 1 shows different configurations for different label types, and it could also vary across different datasets. Although there are ablation studies provided in Table 5 and 6 of the appendix, it is difficult to draw conclusions on how to choose the proper weights. In this way, it makes the practical usage harder to scale up to more challenging datasets.\n\nAnother concern is the motivation and usefulness of the unified framework. In the literature, there exist specific frameworks optimized for different types of weak labels. Previous frameworks could not use all the weak labels at the same time, which is on the contrary the advantage of the proposed method. Therefore, it is of great interest to see whether the method can simultaneously leverage different weak labels and achieve better performance, e.g., using image tags + boxes against only boxes.\n\nOverall, the paper presents an interesting and effective framework. I would like to hear the feedback from the authors about the above-raised points and may update the rating (my current rating is more like a borderline).\n",
            "Summary:\nThis paper talks about a novel weakly-supervised semantic segmentation (WSSS) approach which leverages a single pixel-to-segment contrastive learning formulation. The key idea is to map each pixel into a point in the feature space so that the pixels in the same semantic categories are embedded closely. It is interesting to note that they have also incorporated the analysis of unlabeled pixels across the images to harvest their patterns/clusters for better discrimination.\n\nPros:\n- Motivation was well described.\n- It is interesting to see that four different types of pixel-to-segment (where same segmentation entities were sometimes regarded as different categories) relationships were leveraged in a combined manner to eventually pull up the performance.\n- Experiments are reasonably carried out both qualitatively and quantitatively.\n\nCons/Questions:\n- Would there be a reasonable explanation of why a big performance gain was acquired for the \"labeled points\" case while the jump was relatively small on other weak supervision cases (image tags, bounding boxes, scribbles)?\n- A paper by Sun et al. (Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation, ECCV 2020) also claims that their approach is novel in that they make use of the intra-image information which is somewhat similar to the proposed approach. A paper by Fan et al. (Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation, CVPR 2020) seems highly relevant to the proposed approach, especially with the CAM-driven WSSS approaches. \nSince these recent papers have not been included in the reference of the submitted manuscript, it would be interesting to elaborate on how the proposed approach is unique/strong when compared.\n- For Table 2, what is the accuracy measure for the numbers shown?\n- It is hard to tell how the lambda values were chosen. (Table 1)\n- Some descriptions/explanations are rather redundant and verbose which makes it hard to read.\n\nTypo:\nIn Section 1, \"have have\"  --> \"have\"",
            "The submission proposes a unified framework for weakly supervised semantic segmentation which is compatible with different types of annotations including image tag, bounding box, points and scribbles. With different kinds of labels, the method derives positive and negative segments for each labels and via metric learning the network learns to predict class label for each pixel. While the proposed method obtained STOA or close to SOTA performance on VOC2012 and densepose dataset, the reviewer feels that the novelty of the proposed method is not significant enough. A unified framework for semantic segmentation is appealing but it isn't new. E.g. Guided Attention Inference Network, TPAMI 2019 proposed a way to use gradient-based attention weakly supervised semantic segmentation framework which is compatible with image tags, bounding boxes and pixel-level annotations. In their work, different kinds of annotations are converted into supervision on class-specific attention maps and no parameters need to be tuned based on the ratio/types of annotations are used in the training, whereas the proposed method here needs different parameter choices for different lables/dataset. In the reviewer's eyes the submission follows a similar direction, although the framework design and objectives are different. The reviewer hope the authors can clarify their contribution and add discussions comparing to other similar approaches. ",
            "In this paper, the authors proposed a metric learning-based semi-supervised semantic segmentation approach. In the proposed method, unlike the conventional semantic segmentation scheme that cast semantic segmentation as pixel-wise classification, they formulate semantic segmentation as pixel-segment contrastive learning. To that end, the authors introduced different positive and negative samples mining mechanisms such as low-level image similarity, semantic annotation, semantic co-occurrence, and feature similarity. Furthermore, the proposed method leverage unlabeled data in discriminative feature learning both within and across images. The validity of the proposed method is demonstrated on Pascal Voc and Dense Pose benchmark datasets.\n\n##########################################################################################\n\n*Strength:\u00a0The formulation of pixel-to-segment based contrastive learning is intriguing. In addition to that, the authors introduced a new insight into collecting positive and negative samples that would be leveraged in contrastive learning. Moreover, the samples involve pixels that come from both intra and inter images.\n\n\n#################################################################################\n\n*Weakness:- The sub-modules used in the proposed approaches are adopted from existing methods thus there is no enough novelty in there.\n-The resulting improvement achieved is impressive however the work lacks theoretical novelty.\n\nQuestion: As some of the semantic relationships are basically generating pseudo-labels, for example, Feature Affinity. Have you conducted any experimental analysis to assess the impact of accumulated error that comes from these Pseudo labels?\n\n######################################################################################\n\n*Reason for score:\nThe proposed work is more of engineering, and it does not have any theoretical novelty; however, formulating semantic segmentation in a contrastive learning context is interesting. Furthermore, the authors demonstrate the effectiveness of the proposed method by showing improvements over the state of the art methods. ",
            "\nWeakly-supervised image/object segmentation can naturally be formulated as a pixel-level semi-supervised problem. To solve the insufficient supervision problem in weakly-supervised segmentation, the paper proposes to use four kinds of heuristics to provide supervision for training pixel-level metric learning. The takes the advantages of image segments generated from edge detectors (HED & gPB), then low-level image similarity, semantic annotation, semantic co-occurrence, and feature affinity.\n\nThe paper has obvious advantages summarized as follows.\n1. It unifies various weakly-supervised segmentation tasks by proposing pixel-level metric learning.\n2. The paper obtains clear SOTA results and significantly promotes the development of weakly-supervised segmentation.\n\nThe weakness of the paper is its clarity. Lots of details are missing in the paper, e.g., I cannot find how to apply semantic co-occurrence applied in training segmentation models using image classification models. I think this is not the authors' fault. The problem is caused due to there are so many contents in the paper. Thus, I think source codes should be released and well-organized to ensure the reproducibility of the paper.\n\nBesides, the terms in the papers can be re-considered. (1)  Although contrastive learning is very popular now, I think the method is better termed as metric learning - its original name. (2) I think there are some redundancy in semantic annotation, semantic co-occurrence, and feature affinity. The relation between them should be clarified and justified.\n\n\n\n"
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive and negative aspects of the paper, acknowledging its strengths (well-written, interesting approach, strong performance) while also pointing out limitations (limited technical contribution, unclear effectiveness of some relationships, concerns about scalability and practical usage). The concluding statement suggests a borderline acceptance.",
            "The review highlights several positive aspects of the paper, including a well-described motivation, an interesting approach to combining pixel-to-segment relationships, and reasonably carried out experiments. While it raises questions and concerns, the overall tone suggests a favorable assessment.",
            "The review expresses concerns about the novelty of the proposed method, stating it's not significant enough. It also points out similarities to existing work and criticizes the need for parameter tuning based on different labels/datasets.",
            "The review expresses positive sentiment by highlighting the intriguing formulation of pixel-to-segment contrastive learning and the new insights into collecting positive and negative samples. It also acknowledges the effectiveness of the method and its improvements over existing approaches.",
            "The review highlights the paper's advantages, such as unifying weakly-supervised segmentation tasks and achieving state-of-the-art results. While it points out weaknesses related to clarity, the overall assessment acknowledges the paper's significant contribution."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review provides both positive (\"The paper is well written and is easy to follow.\", \"Extensive experiments are conducted to demonstrate the effectiveness\") and negative feedback (\"the technical contribution is a bit limited\", \"it is doubtful to form positive pairs simply based on the co-occurrence\"). The reviewer also offers suggestions for improvement, indicating a balanced and constructive approach.",
            "The review presents both 'Pros' and 'Cons/Questions' sections, providing a balanced assessment of the paper's strengths and weaknesses. The language is mostly neutral and objective, although some questions are phrased politely ('Would there be a reasonable explanation...').",
            "The tone is critical, using phrases like \"novelty of the proposed method is not significant enough\" and \"follows a similar direction.\" The reviewer also directly expresses their opinion (\"In the reviewer's eyes...\") and hopes for clarification, indicating a skeptical stance.",
            "The review presents both strengths and weaknesses of the paper, using neutral language to describe each aspect. Phrases like 'intriguing formulation' and 'effectiveness of the proposed method' indicate positive aspects, while statements like 'no enough novelty' and 'lacks theoretical novelty' point out limitations. The inclusion of a question also contributes to the balanced tone.",
            "The review presents both positive aspects (advantages, SOTA results) and negative aspects (lack of clarity, missing details) of the paper in a relatively objective manner. It uses phrases like 'obvious advantages' but also points out 'weakness' and suggests improvements."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents a balanced assessment, highlighting both the strengths (well-written, interesting approach, good experimental results) and weaknesses (limited technical contribution, unclear effectiveness of some components, practical concerns) of the paper. The concerns raised are valid and logically follow from the description of the method, and the overall tone is consistent with a borderline rating, reflecting a nuanced perspective rather than self-contradiction.",
            "The review is consistent because it presents both positive aspects (Pros) and areas for improvement (Cons/Questions) of the paper in a balanced and constructive manner. The reviewer acknowledges the novelty and interesting aspects while also raising valid concerns and questions that are relevant to the paper's quality and clarity. There are no contradictory statements within the review.",
            "The review is consistent because it acknowledges the strengths of the submission (unified framework, good performance) while raising a valid concern about the novelty of the approach by comparing it to existing similar works. The reviewer's arguments are logically connected and focused on the perceived lack of significant novelty.",
            "The reviewer consistently praises the practical aspects and engineering contributions of the work, such as the formulation of contrastive learning for semantic segmentation and the effectiveness of the method. Simultaneously, the reviewer consistently points out the lack of theoretical novelty and the reliance on existing methods for sub-modules as weaknesses. The question about pseudo-labels further reinforces the concern about the engineering-heavy nature and potential practical limitations.",
            "The review is consistent because the reviewer acknowledges both the strengths and weaknesses of the paper. The reviewer praises the paper for its unified approach and strong results, while also pointing out a lack of clarity due to missing details. The suggestions for improvement, such as releasing source code and reconsidering terminology, are directly related to the identified weaknesses and are constructive. There are no contradictory statements or conflicting opinions presented in the review."
        ]
    },
    {
        "paper_id": "iclr_2022_iPHLcmtietq",
        "paper_title": "Phase Collapse in Neural Networks",
        "paper_abstract": "Deep convolutional classifiers linearly separate image classes and improve accuracy as depth increases. They progressively reduce the spatial dimension whereas the number of channels grows with depth. Spatial variability is therefore transformed into variability along channels. A fundamental challenge is to understand the role of non-linearities together with convolutional filters in this transformation. ReLUs with biases are often interpreted as thresholding operators that improve discrimination through sparsity. This paper demonstrates that it is a different mechanism called \\emph{phase collapse} which eliminates spatial variability while linearly separating classes. We show that collapsing the phases of complex wavelet coefficients is sufficient to reach the classification accuracy of ResNets of similar depths. However, replacing the phase collapses with thresholding operators that enforce sparsity considerably degrades the performance. We explain these numerical results by showing that the iteration of phase collapses progressively improves separation of classes, as opposed to thresholding non-linearities.",
        "review_ids": [
            "UC9ohhfPV1b",
            "-2Cfka_GrSI",
            "DApFYowMJ-N",
            "5XiYyI-7Cm-",
            "G4HNUTrRuMM",
            "1AZpZGVqhha"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies within-class variability which reduces along the layers of deep neural networks. They mainly question the effect of sparsity and soft-thresholding introduced by ReLU. They show that these classification improvements by eliminating spatial within-class variabilities rather come from a phase collapse, which eliminates the phase of network coefficients. Eliminating the phase of zero-mean filters improves the separation of class means, hence increase in classification accuracy. They introduced a complex-valued neural network in which spatial filters are defined as complex multiscale wavelets and learning is reduced to $1 \\times 1$ complex filters across channels. Their results show that such a network is able to reach ResNet-18 performance on CIFAR10 and ImageNet.\n\n\n\n\n ### Strength:\nThis paper is a nice extension to the related works [Zarka et al (2020, 2021), Ulicny et al.(2019)]. The main contribution (and difference to the literature) is that they hypothesize classification performance mostly results from iterated phase collapses. They supported their hypothesis  (necessary and sufficient) by first explaining the performance of iterated phase collapses by showing that it progressively improves linear discriminability. Secondly, they replaced the phase collapse with a soft-thresholding and showed that this considerably decreases the classification accuracy.\n\n\n### Correctness: \nThe core assumptions seem correct to me, e.g. the images classes are often locally invariant to translation. Small translations are used to be approximated by a phase shift. The empirical results are correct, the theoretical results seem reasonable, but I did not fully examine the proofs in the appendix or reproducibility of the code in the supplementary materials.\n\n### Weakness: \nGenerally, the writing quality is not good and hard to read, e.g. long sentences. Page 3, before the (Krizhevsky et al., 2012) reference, I am not sure if filters are usually localized oscillatory patterns or filters usually localize oscillatory patterns? Sometimes the text is not scientifically addressed, e.g. page 4: $\\mathbf{E}[X_y * \\psi] = 0$ then $\\mathbf{E}[\\rho_b(X_y * \\psi)] $ is \"usually\" close to zero. What does usually mean here? Formula4: \"One can verify...\"? It is now clear to me how. And \"This integral is already well approximated by a sum over 4 phases\"?  MNIST probably needs to be cited for \"The MNIST database of handwritten digits (1998)\". The operator \"$*$\" is not defined. It is also not clear to me why the authors used skip-connection-based neural networks? What about CNNs without skip connections? \n\n\n\n>update after rebuttal: I would like to thank the reviewers for addressing my concerns. However, I found it difficult to find the changes in the rebuttal version. I now see the Results with/without skip connections in Table1 and Table2. The text is still hard to read and follow (this is also addressed by other reviewers). As promised by the authors the text would be improved (e.g. shorter sentences). In conclusion, I would increase my score. The paper mainly questions the effect of sparsity and soft-thresholding introduced by ReLU. The authors show that the classification improvements by eliminating spatial within-class variabilities come from a phase collapse. This counts as a contribution compared to related works. The hypothesis is clearly defined and supported using theory (and assumptions) and experiments. However, there are pitfalls that need to be addressed. ",
            " Thank you for your response. The authors have clarified my concerns. Therefore I am leaving the score unchanged as marginally accept.",
            " I would like to thank the reviewers for addressing my concerns. However again I found it difficult to find the changes in the rebuttal version e.g. results with/without skip connections. Highly recommend that you mention figures/sections when addressing concerns and questions. The text is still hard to read and follow (this is also addressed by other reviewers). I hope that the authors would consider this for the final version. I conclusion I would be happy to increase my score to acceptance (7).\n",
            "This paper proposes that using phase modulus operators instead of thresholding non-linearites is beneficial to classification performance in the case of scattering like networks. The authors show that such a network with some learnable operators can come close to performance of small ResNets when they use the modulo operators and performance degrades when using other non-linearieis. This is demonstrated on ImageNet and CIFAR-10 and is accompanied with relevant analysis and theory.  I think the paper addresses some interesting questions and it is somewhat interesting that a network with a fixed wavelet basis can achieve reasonable performance on ImageNet classification. I also think the paper provides quite a bit of theory to support some of the claims.\n\nHowever I feel the paper suffers from several weaknesses:\n\n* It is not clear to what is the purpose of the proposed model? if it is to show that such a model can indeed be comparable to current architectures then OK, but it does not go beyond that. I don't feel there is a lot \"actionable\" conclusions here nor that we learned anything fundamental about how these models work.\n\n* There is almost no discussion as to what the learnable operators P do - what happens if they are not used? how do they relate to other parts of the model and, more importantly, to what ResNets learn?\n\n* Why are the resulting networks so parameter heavy? they have many more parameters when compared to the corresponding ResNets and perform worse. What can be done?\n\n* How does the proposed model scale? with the number of layers? number of activations at each layer? etc. I think this is a potentially interesting paper but I feel it leaves quite a bit to be desired. \n\npost author response update:\n\nI have read the other reviewers comments as well as the author responses - it would seem that I have been harsh in my scoring and this has been revised. I think this is a valuable contribution to our understanding of the field and a nice demonstration that fixed wavelet filters can, under the right circumstances, perform as well as learned networks. My confidence in the score still remains low as I feel was not the right person to review this.",
            "The authors present some theoretical results, followed by some experiments, in support of their argument that the linear separability for image classification in deep convolutional neural networks mostly relies on a phase collapse phenomenon. By eliminating the phase of zero mean filters they improve the separation of class means.They present a Phase Collapse Scattering network and demonstrate Resnet-like accuracy. The authors argue that phase collapse are both necessary and sufficient to discriminate class means on complex datasets. Strengths\n-The paper is interesting in that it presents theoretical results followed by solid experimental results related to the theoretical results demonstrating that the phase of the signal can be removed while improving the classification performance of DNNs\n-The paper is for the most part well written\n-The authors present good results on CIFAR10 and Imagenet. While the results do not seem state of the art I think they are sufficiently good in support of the authors' arguments\n\nWeaknesses: \n-Figure 1:The authors assume color printers are used (orange/blue). I suggest they use bold/solid, dashed lines to distinguish between the different steps\n-Page 2: clarify why both directions (necessary & sufficient) hold. I see how the sufficient condition holds but it is not clear how the necessary direction emerges. Does it make sense to phrase this as a conjecture?\n-Eq 4: It may help many readers to derive this in the appendix, it should be a quick derivation\n-The ResNet results while good are not state of the art. If SOA feasible using the PCS framework? If yes why wasn't a SOA presented in the paper. If not, what do the authors believe hinders achieving SOA?\n-Page 3: Clarify better when the random vector X_y represents. Does it represent the features used by the classifier? In that case I do not see how the classes are linearly separable if E[X_y] are all different. Do you mean this is a sufficient or necessary condition? For example if X_1, X_2 are 2D vectors, each representing two concentric circles. The two sets of features represent different classes but the means are the same (the circle center). Obviously we can have E[X_y] with the same mean but represent different classes. Limiting ourselves to linear separability makes the problem trivial. I suggest the author rewrite this section to clarify what they mean\nPage 3: The authors make a lot of assumptions that X_y are stationary. How important is this assumption in their results? Do they think perhaps that a significant set of problems, such as those often dealt with by RNNs, are not well modelled as stationary? Overall I found this an interesting paper. It is well written, and has a fairly solid experimental section in support of some not so well known theoretical ideas as to why deep neural nets achieve good classification\nOverall here is my summarized assessment:\n\noriginality: The authors present some theoretical results to justify their experimental framework. The paper is not just blindly attempting different DNN architectures. From this point of view I consider this a fairly original piece of work\n\nquality: I believe the exposition and experimental results are of sufficient quality for ICLR. The paper is in general well written, though as I indicate elsewhere certain parts need to be improved. \n\nclarity: I mentioned above in the \"Weaknesses\" section some clarity issues that I noticed. The authors should address these issues. I was ambivalent as to whether the paper should be rejected because of these clarity issues, but since they can all be addressed I think I decided to be a bit lenient. But for the final ICLR submission they should be addressed.\n\nsignificance: Unless I missed it, are the authors providing source code? The paper's significance and clarity will be improved if source code is provided, to make the results reproducible",
            "This is a very interesting paper that is based on scattering networks, for which it shows that the so called \u2018phase collapse\u2019 leads to state-of-the-art results on par with modern architectures, like ResNet. To derive this, the paper shows that having neural networks on complex numbers is similar to a structure deep network (like with wavelet filters) on the reals. Phase collapse is then when there is an operation, like the modulus (absolute value function), which eliminates the phase from the complex number and maintains only the amplitude. All in all, the paper shows that maintaining the amplitude while eliminating the phase, is what brings the high accuracies, while the reverse (keeping the phase, eliminating the amplitude) yields terrible accuracies. As a corollary, I would say that the paper explains why non-linearities, like ReLUs, are so successful. The strengths of the paper are:\n- I find the novelty and the results, both theoretical and empirical, fascinating and thought provoking. Making a link between operators like ReLUs to complex numbers, their phases, and collapse is a great idea. In particular, I find the results in Table 2 very convincing.\n\n- The theoretical analysis is nice, and not to hard to follow, once one commits. It would be interesting to make more explicit the analysis for different types of nonlinearities. For instance, if sigmoid are soft-thresholding functions, I suppose the same is also for tanh. However, tanh tends to work better. Why is that the case, under this framework? What about other non-linear activations, like the swish function?\n\n- The results are generally strong and quite convincing, at least in ablation comparisons.\n\nThe weaknesses of the paper:\n- I found the writing involved, if not subpar for the quality of the paper. It is clear that the authors have a very good understanding of the area, however, they do not make it very easy for the average reader to understand their thought process, The sentences are not really immediately connected, that is there exist jumps, which sometimes can be inferred based on follow-up text, while other times they are simply hard to understand. It could be that for readers who are better versed in the literature, the text is easier. However, any paper should do a minimum effort to communicate the message clearly. Certainly, the introduction contains several forward references and that at least can be improved.\n\n- I found section 2 particularly hard to follow, as various concepts are being introduced at random points, without any clear structure. Almost ironically, I couldn\u2019t find any direct definition of what comprises phase collapse, which made matters all that much harder. While it is sort of expected/intuitive what phase collapse could mean, given that the paper is on the theoretical/conceptual side of things, it makes it hard to follow the thought process. \n\nThe only \u2018definition\u2019 I found was that phase collapse eliminates the phase of a complex number with a modules just below eq (3). It is not clear if this a general definition, just a logical deduction based on fundamental concepts, or something that the paper introduces. In any case, it is also unclear why this holds. I assume it is because the modulus of the exponential is 1, so the phase that is introduced by the exponential is eliminated?\n\n- It is also unclear how equation (5) is derived. Again, is it a logical deduction based on the nature of the ReLU? Or is it derived based on the aforementioned definitions of filtering with wavelets and complex numbers? It should be immediately clear from the text what is expected to be background knowledge, what is derived knowledge, what is a simple deduction.\n\n- From a technical point of view, what I found unclear was what is the point of stacking \u2018phase collapse\u2019 operators, since one of them is supposed to remove all the phases from the feature, that is, it learns translation invariant features. From deep networks, I assume that deeper layers combined the wavelet activations from the previous layers. However, I find hard to reconcile this with \u2018translation invariance\u2019. For instance, a full connected layer is translation invariant, while a convolutional layer is translation equivariant. Does phase collapse then correspond to fully connected layers, and if yes, what is the point of stacking them? After all, with fully connected layers it is also of little added value to stack them, usually.\n\n- Complementing on the previous point, can it be that adding the skip connections is beneficial such that to make the operations also translation equivariant, thus not completely eliminate all phases and translation information? Something like that is analyzed in the bottom of page 5, but maybe this point can be furthered.\n\n- I find the motivation of the projection operators P a logical one in the context of deep networks, however, not really convincing in the context of phase collapse scattering networks. Do they contribute anything to the amplitude or the phase? What would be the benefit, considering that all phases are anyways immediately eliminated afterward? Do they help with having class means that are well separated perhaps? Maybe there can be a motivation that relates more to the main story of the paper.\n\n- Last, I would like to clarify that in my attempt to find out more about \u2018phase collapse\u2019 as a concept, I found out the arXiv version of the paper accidentally. I got a bit annoyed by that, as I believe it biases my opinion, and I do not find it fair against other papers. Based on the novelty and the strong results, I vote for acceptance. I have one remark that I couldn\u2019t completely understand and it would be nice if the authors could help me there. All in all, I find that I learned something from this paper, which I think is a great result for any publication."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment, acknowledging the paper's contribution, the support for the hypothesis with theory and experiments, and the reviewer's decision to increase their score after the rebuttal. The phrase 'This counts as a contribution compared to related works' and 'The hypothesis is clearly defined and supported using theory (and assumptions) and experiments' indicates a positive assessment.",
            "The reviewer explicitly states that the authors have addressed their concerns, leading to a positive assessment and maintaining a 'marginally accept' score.",
            "The reviewer expresses gratitude and states they would be happy to increase their score to acceptance, indicating a positive overall sentiment.",
            "The reviewer initially expressed concerns but revised their opinion after reading other reviews and the author's response, ultimately stating it's a \"valuable contribution\" and a \"nice demonstration.\" This indicates a positive shift in sentiment.",
            "The reviewer expresses overall positive feedback, highlighting the paper's interesting theoretical results, solid experimental results, and good writing quality. They also acknowledge the authors' good results on CIFAR10 and Imagenet.",
            "The reviewer ultimately votes for acceptance based on the novelty and strong results, indicating a positive overall sentiment. Phrases like \"very interesting paper\", \"fascinating and thought provoking\", \"great idea\", \"results are generally strong and quite convincing\", and \"I learned something from this paper\" contribute to this positive assessment."
        ],
        "tone": [
            "Balanced",
            "Neutral",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review provides both strengths and weaknesses of the paper, pointing out areas for improvement while also acknowledging the paper's contributions. It maintains a formal and objective tone throughout the assessment, using specific examples and questions to support its points. While ultimately leaning positive, it does not shy away from criticism, indicating a balanced evaluation.",
            "The tone is neutral as it simply states the reviewer's satisfaction with the authors' response and the unchanged score, without expressing strong emotions or opinions. The language is straightforward and objective.",
            "The reviewer thanks the authors, expresses hope for improvement in the final version, and is willing to increase their score, showing a supportive attitude despite criticisms.",
            "The review initially presents both positive aspects (\"interesting questions,\" \"reasonable performance,\" \"quite a bit of theory\") and negative aspects (weaknesses related to purpose, discussion, parameter count, and scaling). The final statement shows a revised opinion.",
            "The review presents both strengths and weaknesses of the paper. While praising the originality and experimental results, it also points out areas needing clarification and improvement. The reviewer's use of \"Strengths\" and \"Weaknesses\" sections explicitly contributes to the balanced tone.",
            "The review provides both strengths and weaknesses of the paper, offering constructive criticism alongside positive feedback. While the reviewer expresses enthusiasm for the core idea and results ('fascinating and thought provoking'), they also point out areas where the writing and clarity could be improved ('I found the writing involved'). This balanced approach indicates a neutral tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently maintains their opinion about the strengths and weaknesses of the paper throughout the review and update.  They initially point out the paper's contribution and hypothesis as strengths, while criticizing the writing quality and clarity as weaknesses. In the update after rebuttal, they acknowledge some improvements but reiterate the poor writing quality, while still deciding to increase the score due to the paper's contribution. This shows a consistent evaluation of the paper's merits and flaws.",
            "The reviewer states that the authors clarified their concerns and therefore maintains the initial score of marginally accept. This is consistent as addressing concerns and keeping a marginally accept score are not contradictory.",
            "The reviewer acknowledges the authors' efforts in addressing concerns and expresses willingness to increase the score to acceptance despite mentioning remaining issues like difficulty in finding changes and readability. The final positive statement of increasing the score aligns with the initial thank you and constructive suggestions, indicating a consistent overall positive direction towards acceptance despite some reservations.",
            "The reviewer initially points out weaknesses but explicitly revises their opinion after reading author responses and other reviews, leading to a more positive assessment. This change of opinion is clearly stated and justified, demonstrating a consistent process of evaluation and revision rather than self-contradiction.",
            "The review is consistent because the reviewer identifies both strengths and weaknesses of the paper, and the overall assessment aligns with these points. The reviewer appreciates the originality and quality of the work, while also pointing out areas for improvement in clarity and completeness. The final summarized assessment reinforces the initial points, indicating a consistent evaluation of the paper's merits and shortcomings.",
            "The review is consistent because it acknowledges both the strengths (novelty, strong results) and weaknesses (clarity, structure) of the paper, and the final recommendation for acceptance is based on the strengths outweighing the weaknesses. The reviewer's judgment is aligned with their detailed assessment."
        ]
    },
    {
        "paper_id": "iclr_2019_H1eMBn09Km",
        "paper_title": "Using GANs for Generation of Realistic City-Scale Ride Sharing/Hailing Data Sets",
        "paper_abstract": "This paper focuses on the synthetic generation of human mobility data in urban areas. We present a novel and scalable application of Generative Adversarial Networks (GANs) for modeling and generating human mobility data. We leverage actual ride requests from ride sharing/hailing services from four major cities in the US to train our GANs model. Our model captures the spatial and temporal variability of the ride-request patterns observed for all four cities on any typical day and over any typical week. Previous works have succinctly characterized the spatial and temporal properties of human mobility data sets using the fractal dimensionality and the densification power law, respectively, which we utilize to validate our GANs-generated synthetic data sets. Such synthetic data sets can avoid privacy concerns and be extremely useful for researchers and policy makers on urban mobility and intelligent transportation.",
        "review_ids": [
            "BylCid5VyV",
            "S1log2v6nQ",
            "SyezMqrt37",
            "SyeIpvuunX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thanks for clearing up some of the issues in the paper. I appreciate that you want to keep this paper 'simple' in terms of model and do other work in the future. However, I feel that you would need to do some of these other items to get this paper to a higher level for a top conference such as this one.",
            "The authors propose an interesting idea of generating synthetic data sets for ride sharing. In particular, they split the space/time into small spatial/temporal cells (50mx50m and 5min) each containing number of requests (or a scaled version of it), and train a conditional GAN to output these cell values given an input 5-min time label. They validate the results using metrics from graph and fractals theory.\nWhile the idea is interesting, the execution of the paper is lacking. Some details are missing and especially key things such as metrics should be explained better.\n- How is the data represented? It says that pixel represents the number of ride requests, how exactly? Then, in the next paragraph it is said that pixel represents presence/absence of ride requests, so which one is it. This is a critical part of the proposal and is not well explained.\n- y-axis in Figure 2 is not explained.\n- Metrics should be better explained. How are edges defined, when you only model requests, not destinations? This is far from being clear.\n- In addition, how is D2 defined? Do we compute one for each time, or how? What exactly is \"side e\", what is a \"side\" here? Basically both metrics are not well defined.\n- \"We can claim strong similarity ...\", what is this justified by?\n- Second paragraph in Section 3.1 is not clear, reads very strangely.\n-  Labels are being mentioned before being defined, adding to confusion.\n- It is clear from Figure 2 that workdays and weekends are very different, yet the authors chose to ignore that fact during modeling. They do mention that we can choose any labeling we want, but still strange that for the experiments this was not taken into account.\n- The authors mention that cells as 1.2km x 1.2km, but Figure 3 shows much different resolution. Seems that the figure is just given as an example, but reading the text one gets an impression that the figure was actually used in the paper. This needs to be clarified.\n- For the classifier, it says that \"time sequence of the data\" is a label, what does this mean? You mean the actual label, or some time sequence? This is confusing, although it seems that simply the 5-min label was used.\n- Could we add the metrics to the loss, to enforce them as the authors say that that would result in strong similarity?\n- One of the major flaws of the paper is missing baseline. It is very difficult to appreciate the results without any reference result.\n- Again, I am not sure how results in Section 5.2 are computed when only requests are modeled.\n- Footnote 2 in the conclusion mentions baselines, yet there are none mentioned in the paper.",
            "The paper works on a very interesting problem: generating ride hailing demand map using deep learning technique. The idea is novel and interesting. The paper adopts two metric to evaluate the performance of the algorithm and shows that the performance is good. However, the problem is a little far from real world cases, which limits the contribution of the paper.\n\nThe title of the paper is very attractive. Before reading the paper, I was very excited and wanted to see the algorithm could generate the driving trajectory by using GAN. However, it can only generate the pickup location, which makes me a little disappointed. In real world applications, in riding hailing industry, the demand/supply estimation have been wide investigated. And it can be very accurate. It is not clear why we need to generate it. On the other hand, the paper only adopts conventional GAN to this application. Technically the contribution is not significant. The paper only considers time slot for generating the new data. In this area, much more information has been used. The authors are suggested to survey the smart transportation or riding sharing research area. The training solution is not satisfied. The model is only trained for each small area in the city. The training set is very limited, which may make the model overfit. Thus the experiments and the results are not convincing. In current GIS or transportation area, usually we would use a unique model for the whole city. At least the authors should discuss their algorithm for the scale issue.\n\nOverall, the paper works on a very interesting problem. However, the current solution should be improved.\n",
            "The paper produces a heat-map of ride-share requests in four cities in the USA. For each city 'block' they produce a time-sequence of 2016 images representing a week-long run from combining each 5-minute interval. This is used with a GAN to produce new data. The techniques applied, although not commonly used in the context of ride sharing / hailing, have been used extensively in other literature.\n\nSome major points on the paper:\n1) A GAN approach is normally used to generate more data when enough real data is not obtainable. However, here you only use one week of data from a much larger set. Surely, it would be better to make use of all the weeks available?\n\n2) It is not clear how the heat-maps once produced could be used in the future. There is a hint in the results section about how they can be converted back to ride requests, but this is not clearly defined.\n\n3) There are a number of cases where you state that some approach has been found to be better. However, no evidence is presented for how you determined this to be true.\n\n4) The conversion of data to heat-maps has been used extensively in prior research. Although I'm not directly aware of the use in machine learning I am aware of the use in transport - \"Interactive, graphical processing unit- based evaluation of evacuation scenarios at the state scale\". The novelty here seems to be the application to this specific problem.\n\nMore specific points:\n- \"Our real ride request data sets consist of all the ride requests for an entire week for the four cities.\" - it's not clear - are all four cities used to train one model?\n\n- \"Hence the week-long data should be quite representative.\" - This fails to take into account such things as national holidays or other major events such as sports. Did your chosen week contain one of these?\n\n- \"Hence we believe the ride request data sets also reflect the overall urban mobility patterns for these cities.\" - This is a huge assumption, which would seem to need evidence to back it up.\n\n- \"and lump together all the ride requests within each interval.\" - Presumably you mean that all time values are to the granularity of 5 minutes? \n\n- \"We arbitrarily sized each block to represent an image of 24\u000224 pixels\" - this seems particularly small.\n\n- \"Each image of that block is labeled with a time interval (for our experiments, the hour in a day).\" - Can the variability within an hour not make this more difficult? \n\n- \"We find that small networks are appropriate for the training data\" - evidence to support this.\n\n- \"This network is pre-trained on the training data\" - which training data are you referring to?\n\n- \"This is found to increase the efficiency of the training process\" - evidence?\n\n- \"In this work we set the block size for each of the four cities to be\n1200 \u0002 1200 meters\" - how was this value arrived at?\n\n- You state that GPUs were no more efficient, it would be good to see more analysis of this.\n\n- \"To help enhancing the scalability\" -> \"To help enhance the scalability\"\n\n- \"and other useful functions\" - such as?\n\n- Figure 4 would probably work better as a speedup graph.\n\n- \"Running times for sampling ride requests from the trained models and stitching the images of all the blocks together are significantly less than the training times, and are not included in these results.\" - at least some figures to give an idea of scale should be provided."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses reservations about the paper's suitability for a 'top conference' without further improvements, indicating a negative sentiment towards its current state.",
            "The review expresses significant concerns about the paper's execution, clarity, and completeness. Phrases like \"execution of the paper is lacking,\" \"some details are missing,\" \"metrics should be explained better,\" and \"major flaws of the paper is missing baseline\" indicate a negative overall sentiment.",
            "The review expresses disappointment and criticism regarding the paper's practical application, technical contribution, training solution, and experimental results. Phrases like 'a little disappointed,' 'not clear why we need to generate it,' 'technically the contribution is not significant,' 'not satisfied,' 'overfit,' and 'not convincing' indicate a negative assessment.",
            "The review raises several major concerns about the paper's methodology, data usage, clarity, and lack of supporting evidence. Phrases like 'not clear,' 'no evidence is presented,' 'huge assumption,' and multiple requests for evidence or justification indicate a critical assessment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer acknowledges the authors' intent but states that 'you would need to do some of these other items to get this paper to a higher level,' which is a critical assessment of the paper's current standing.",
            "The review adopts a critical tone by pointing out specific flaws and ambiguities in the paper. Phrases like \"not well explained,\" \"far from being clear,\" \"not clear, reads very strangely,\" \"adding to confusion,\" and direct questions challenging the authors' methodology contribute to the critical tone.",
            "The tone is critical, as evidenced by phrases like 'problem is a little far from real world cases,' 'makes me a little disappointed,' 'not clear why we need to generate it,' 'technically the contribution is not significant,' 'training solution is not satisfied,' 'model is only trained for each small area,' 'experiments and the results are not convincing,' and 'current solution should be improved.' These phrases point out specific flaws and shortcomings in the paper's approach and execution.",
            "The review employs a critical tone, pointing out several weaknesses in the paper's approach and conclusions. The reviewer uses direct questions and statements of doubt, such as 'Surely, it would be better...?', 'It is not clear...', 'This is a huge assumption...', and 'evidence to support this,' which convey a critical perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the positive aspects of the paper (addressing issues, simplicity) while also clearly stating the need for further improvements to reach the standard of a top conference. There are no contradictory statements within the review.",
            "The review is consistently critical of the paper, pointing out multiple areas where the paper lacks clarity, detail, and justification. The reviewer raises concerns about data representation, unexplained figures, poorly defined metrics, lack of baselines, and confusing descriptions. There are no contradictions within the review itself; it maintains a consistent stance of critical evaluation throughout.",
            "The review is consistent because it starts by acknowledging the interesting problem and novel idea, but then consistently points out the limitations and weaknesses of the proposed solution. The reviewer highlights concerns about real-world applicability, technical contribution, training methodology, and experimental validation, maintaining a consistent critical perspective throughout the review. The concluding remarks reiterate the initial positive point about the problem's interest while emphasizing the need for improvement, thus maintaining consistency.",
            "The review is consistent because all the points raised by the reviewer are focused on improving the paper's rigor, clarity, and justification. The reviewer consistently asks for more evidence, clarification on methodology, and justification for assumptions. There are no contradictory statements or conflicting feedback within the review."
        ]
    },
    {
        "paper_id": "nips_2022_XVfOai2ytN1",
        "paper_title": "On global convergence of ResNets: From finite to infinite width using linear parameterization",
        "paper_abstract": "Overparameterization is a key factor in the absence of convexity to explain global convergence of gradient descent (GD) for neural networks. Beside the well studied lazy regime, infinite width (mean field) analysis has been developed for shallow networks, using on convex optimization technics. To bridge the gap between the lazy and mean field regimes, we study Residual Networks (ResNets) in which the residual block has linear parameterization while still being nonlinear. Such ResNets admit both infinite depth and width limits, encoding residual blocks in a Reproducing Kernel Hilbert Space (RKHS). In this limit, we prove a local Polyak-Lojasiewicz inequality. Thus, every critical point is a global minimizer and a local convergence result of GD holds, retrieving the lazy regime. In contrast with other mean-field studies, it applies to both parametric and non-parametric cases under an expressivity condition on the residuals. Our analysis leads to a practical and quantified recipe: starting from a universal RKHS, Random Fourier Features are applied to obtain a finite dimensional parameterization satisfying with high-probability our expressivity condition.",
        "review_ids": [
            "UFLVK8ahZqb",
            "jqQvl8iMbM",
            "eeixXld848F",
            "gYSJCKS65HN",
            "5WVgiWu93uM",
            "WC4oML3XwH",
            "e-_1BRk1kJX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " The paper have nine content page, all the other stuff are in the appendix\nThis can be simply fixed by split the pdf. It's very rude to reject a paper by this means.\n\nI hope the ac can ignore this rude reviewer.",
            " The submission rules should be quite clear from the CFP. \nhttps://neurips.cc/Conferences/2022/CallForPapers\n\nIn particular, \n1. \"Submissions are limited to nine content pages\"\n2. \"Submissions that violate the NeurIPS style ... or page limits may be rejected without further review.\"\n3. \"Papers may be rejected without consideration of their merits if they fail to meet the submission requirements\"\n\nReviewer xZax just made an ad hominem fallacy.",
            " The reviewer will respect any decision by the AC, but hope this paper will not get rejected only because the careless review by u7jP. One point I want to raise is that the review u7jP chose \"3\" in the confidence part, while he/she obvious did not read the paper at all. This shows unprofessional and hence this review should be disregarded.",
            " The response has addressed my question and clarified the meaning of \"global convergence\" in the paper. The reviewer will keep the score. ",
            " This paper studies the convergence of continuous-time residual networks with linear parameterization in the residual block trained by GD. Due to linear parameterization, the residual block functions are understood to be functions from an RKHS. Gradient of the loss function wrt the parameters (in this case a family of RKHS functions parameterized by time) is derived using standard techniques in optimal control. Then, a functional PL condition is proved for the gradient, which leads to local convergence. To achieve global convergence, two approaches---dimensionality lifting and random Fourier feature approximation---are studied for a special kernel. Final theorem says \"global\" convergence can be achieved when the network takes random Fourier features, is sufficiently wide, the matrices A and B are appropriately chosen, and the parameter is initialized from 0.  Originality: The novel parts of this work lie in the functional PL condition for the loss of the model studies, as well as the the approaches to extend local convergence to global convergence in section 5. Though, the reviewer has question regarding the \"global convergence\" which is left for the questions part. The continuous time residual networks with linear parameterization has been explored in previous works, e.g. in [1]. The local convergence results based on PL condition is also not novel to the current work. \n\nQuality and clarity: The paper is clear well organized.\n\nSignificance: The analysis is theoretical significant because it reveals the good loss landscape for one type of flow-based model. The flow-based model is theoretically interesting because they are infinite-depth limit of deep neural networks, especially ResNets. It is also practically important, with neural ODE as a representative application. \n\n\n[1] E W, Ma C, Wu L. Machine learning from a continuous viewpoint, I. Science China Mathematics. 2020 Nov;63(11):2233-66. 1. To the reviewer's understanding, \"global convergence\" in optimization means the algorithm converges from any initialization, which does not need to be close to the minimum. However, in section 5 of the paper, convergence to global minimum is proven for a specific initialization, v^0=0. This is not global convergence. Is there a way to improve the results to real global convergence, i.e. the GD converges to global minimum from any (reasonable) initialization? If not the statements in the paper might need to be changed. The limitations have been addressed in the paper. ",
            " This paper violates the NeurIPS submission rules, by putting supplementary materials (page 14-31) into the main paper submission, and will not be reviewed. This paper violates the NeurIPS submission rules, by putting supplementary materials (page 14-31) into the main paper submission, and will not be reviewed. This paper violates the NeurIPS submission rules, by putting supplementary materials (page 14-31) into the main paper submission, and will not be reviewed. This paper violates the NeurIPS submission rules, by putting supplementary materials (page 14-31) into the main paper submission, and will not be reviewed.",
            " This paper provided a local Polyak-Lojasiewicz inequality for a kernelized neural ODE-model. This paper's analysis is quite interesting, but still relationship between the proposed work and the previous should be improved.  This paper provided a local Polyak-Lojasiewicz inequality for a kernelized neural ODE-model. This paper's analysis is quite interesting, but still the relationship between the proposed work and the previous should be improved. \n\n\nThis paper present a nice Polyak-Lojasiewicz property for the Neural ODE model. I think this paper definitely will make impact in this area. I'm willing to upgrade my score if the paper can make the relationship between the proposed work and the previous more clear.  (detailed points see  Questions)\n\n I guess the authors need to discuss the realtionship between [1], which provided the convergence properties of Gradient Descent for linear ResNets. Can your analysis be considered as iinfiinte dimension analysis of [1]?\n[1] M. HARDT AND T. MA, Identity Matters in Deep Learning, arXiv:1611.04231 [cs, stat] (2018). arXiv: 1611.04231.\n\nWhat's the relationship between your analysis and the Resnet's NTK (which can be written down as an ODE). What's the further degree of freedom increased in your paper. At the same time, this paper is missing reference [2]. I'm curious about the relationship between your paper with [2]. Can the analysis in [2] be considered as a PL analysis?\n \n[2] Ding Z, Chen S, Li Q, et al. On the Global Convergence of Gradient Descent for multi-layer ResNets in the mean-field regime[J]. arXiv preprint arXiv:2110.02926, 2021.\n\nThe assumption 2 is the core assumption to imply the PL condition. Can the analysis in line 256-258 be more concrete?  In there a principled way to construct such RKHS? Thi limitation of this paper is two fold in my mind:\n- i don't know how strong is assumption 2\n- the hidden weight is not trained. [ but i think this model is still more powerful than NTK model."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Positive",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses strong disapproval and frustration with the rejection based on a formatting issue, using terms like \"very rude\" and hoping the area chair will ignore the reviewer.",
            "The review expresses disapproval and criticizes the author by pointing out a logical fallacy. The tone is accusatory, suggesting a problem with the author's argument.",
            "The review expresses strong disapproval of another reviewer's assessment, calling it 'careless' and suggesting the reviewer 'did not read the paper at all.' The use of the word 'unprofessional' further contributes to the negative sentiment.",
            "The reviewer explicitly states that their question was addressed and the meaning of a key term was clarified. They also indicate they will maintain the score, implying satisfaction.",
            "The review presents both positive aspects (clarity, organization, theoretical significance) and negative aspects (concerns about 'global convergence' claim, lack of complete originality). This balanced perspective leads to a neutral sentiment.",
            "The review states the paper 'violates the NeurIPS submission rules' and 'will not be reviewed,' indicating a negative assessment and rejection.",
            "The reviewer finds the analysis interesting and believes the paper will have an impact. They are willing to upgrade their score if certain clarifications are made, indicating a positive overall sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical due to the use of the phrase \"very rude\" to describe the rejection and the reviewer's action. The reviewer is directly criticizing the decision and implying unprofessional behavior.",
            "The reviewer uses direct and critical language, accusing reviewer xZax of making an \"ad hominem fallacy.\" This specific accusation indicates a critical evaluation of the reviewer's argument and conduct.",
            "The tone is critical due to the direct accusations of carelessness and lack of reading, as well as labeling the behavior as 'unprofessional.'",
            "The review expresses satisfaction with the authors' response and indicates a positive outcome ('will keep the score'). The language is straightforward and encouraging.",
            "The review acknowledges the paper's strengths (clear writing, theoretical significance) while also raising critical questions and concerns about the 'global convergence' claim and originality. This mix of positive and negative feedback creates a balanced tone.",
            "The tone is critical due to the repeated accusation of violating submission rules and the direct statement that the paper 'will not be reviewed.' The repetition emphasizes the severity of the issue.",
            "The review expresses both positive aspects (interesting analysis, potential impact) and critical points (need for clarification, missing references, assumption strength). The reviewer also poses questions and suggestions for improvement, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it identifies a minor formatting issue (content and appendix in one PDF), suggests a simple solution (split the pdf), and argues against rejecting the paper based on this minor issue, deeming such rejection as 'rude'.",
            "The review is consistent because it presents factual information about submission rules and a separate observation about a reviewer's fallacy without any contradiction between them. The two parts of the text are independent and do not create any inconsistency.",
            "The review is consistent in its criticism of reviewer u7jP. It argues that u7jP's review is careless and unprofessional because they chose a low confidence score (3) while seemingly not having read the paper at all. The reviewer's point is focused on discrediting u7jP's review due to perceived lack of effort and expertise, and there are no contradictory statements within this argument.",
            "The review expresses satisfaction that the question has been addressed and the meaning clarified, and in line with this positive feedback, the reviewer decides to keep the score, indicating a consistent stance.",
            "The review is consistent because the reviewer provides a balanced assessment, highlighting both the strengths (novelty in functional PL condition and approaches to extend local convergence, clarity, significance) and weaknesses (questionable 'global convergence' claim) of the paper without contradicting themselves. The critique about 'global convergence' is a specific point of concern regarding the paper's claims, not an internal inconsistency within the review itself.",
            "The review consistently states that the paper violates NeurIPS submission rules by including supplementary materials in the main paper and will not be reviewed. The reviewer repeats the same statement multiple times without any contradiction.",
            "The reviewer consistently points out the strengths of the paper, such as the interesting analysis and potential impact, while also consistently highlighting areas for improvement, primarily the need to clarify the relationship with prior works and the assumptions made. There are no contradictory statements or conflicting opinions expressed in the review."
        ]
    },
    {
        "paper_id": "nips_2022_bot35zOudq",
        "paper_title": "Pushing the limits of fairness impossibility: Who's the fairest of them all?",
        "paper_abstract": "The impossibility theorem of fairness is a foundational result in the algorithmic fairness literature. It states that outside of special cases, one cannot exactly and simultaneously satisfy all three common and intuitive definitions of fairness - demographic parity, equalized odds, and predictive rate parity. This result has driven most works to focus on solutions for one or two of the metrics. Rather than follow suit, in this paper we present a framework that pushes the limits of the impossibility theorem in order to satisfy all three metrics to the best extent possible. We develop an integer-programming based approach that can yield a certifiably optimal post-processing method for simultaneously satisfying multiple fairness criteria under small violations. We show experiments demonstrating that our post-processor can improve fairness across the different definitions simultaneously with minimal model performance reduction. We also discuss applications of our framework for model selection and fairness explainability, thereby attempting to answer the question: Who's the fairest of them all?",
        "review_ids": [
            "n4B4GAUJgEC",
            "2gUsGDdYzpB",
            "SBxJuEMqAJG",
            "HiS_X4--oMx"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I would like to thank the authors for their responses which address most of my concerns. However, I find the use of \u201ctractable\u201d misleading as it usually means \u201ctheoretically tractable\u201d. I suggest replacing this term with the detailed explanation given in the authors\u2019 responses.",
            " This paper pushes the limit of fairness impossibility theorem where all three group fairness definitions cannot be satisfied simultaneously. Based on an integer-programming approach, the authors propose a framework to yield a certifiably optimal post-processing method for simultaneously satisfying multiple fairness criteria under small violations. In experiments, the framework with minimal model performance reduction achieves fairness improvement across different definitions simultaneously. As an application, the framework advises on model selection and fairness explainability.  Strength: \n- The motivation of the paper is very interesting. \n\nWeakness:\n- There is no experimental comparison with existing fairness works.\n - While the proposed method belongs to post-processing fairness algorithms, experiments do not include any comparison with post-processing algorithms. \n- Tables and figures are uninformatively presented. \n- In Table 3, few cells are in bold fonts. No interpretation is given. \n- A motivating example is missing, where authors could compare theoretical fairness impossibility result vs. their proposed solution. - In Figure 1, what does different colours mean?\n- Why do authors compare with an in-processing algorithm while the proposed work is a post-processing algorithm? There is no negative societal impact of this work.",
            " This paper proposes an optimization-based method to simultaneously reduce the gap in demographic parity, equalized odds, and predictive rate parity. The fairness problem is first formulated as a non-convex optimization task and then solved using mixed integer programming. Experimental results that compares the proposed method against two existing baselines are provided. Strengths:\n1. This paper provides a novel solution to the well-known impossibility theorem in algorithmic fairness. The motif of the paper is novel and very relevant to the fairness community.\n2. One major strength of this paper is that this methodology enables the user to select the model from the frontier that offers the best trade-off among all fairness metrics and performance. This feature is very appealing to models that are deployed under multiple applications.\n3. This paper is well-organized. All concepts are clearly explained. The authors provide sufficient intuition along with rigorous definitions. \n4. The formulation of the constrained optimization problem fits well with the fairness application. The constraints are well-explained and intuitively make perfect sense.\n5. The proposed MILP solution is novel and suitable to this application. MILP is computationally tractable and globally optimal (although the problem formulation accepts tolerance).\n\nWeakness:\nThe major concern for this approach is on its generalizability. Will the transformation matrix (X) learned on a tune/train set transfer (with great fairness & accuracy performance) to the test set? How is the binning method impact the generalizability? Technically, one can make B -> infinity and learn a X that achieves (almost) perfect performance on both fairness and accuracy perspective. But this performance will not generalize to the unseen test set.  \n1. What change should be on the target and constrains if protected attributes are non-binary?\n\n2. In table 1, INT significantly under-performs IP for ACS coverage and COMPAS. What's the reason behind this gap for these two datasets?\n\n3. It would be great if the authors could provide additional ablation experiment on the size of bins and see how B impacts the performance. Please see the weakness in the previous section for major concern. Minor concerns are:\n\n1. Figure 1 needs to be improved: clearer labels for x,y axis and legend.\n\n2. The experiment set up seems unclear. How was the training/validation/test set split? At which subset the X matrix is learned? \n\n",
            " This paper studies how to deal with the trade-off among three common fairness notions: demographic parity, equalized odds, and predictive rate parity. The paper proposes a post-processing approach that aims to modify the decision made by any prediction model. The variable is the probability of flipping the decision from one to another for each group (in the case of continuous decision it is to move instances from one to another). Then, a combinatorial optimization problem is defined which contains the formulations of all fair constraints. The paper develops an efficient approach to solve the problem via mixed integer programming. Finally, experiments show that the proposed method can help the decision maker balance the trade-off between model utility and different fairness constraints. Strength\n\nThe proposed optimization framework is not limited to binary classification. The reformulation makes the optimization problem tractable. The optimization framework is also scalable since its complexity does not depend on the number of features and samples in the data. The experiment results are interesting which show that the proposed method is practically meaningful to practitioners and decision makers.\n\nWeakness\n\nAlthough the size the optimization problem only depends on the number of groups and the number of bins, in many cases the number of groups may be much larger than two. For example, when we consider intersectional fairness for multiple protected features, and when we consider conditional fairness notions (e.g., conditional demographic parity) which can produce a large number of subgroups by conditioning on certain features, the number of groups will increase significantly. The paper does not analyze the scalability of the proposed method under such situation.\n A critical question that may affect my final rating is that, in Eq. (1), when the number of instances in b\u2019 after transformation is computed, why only considering the instances that are transformed into b\u2019? What about the instances that are transformed out of b\u2019?\n\nOther questions:\n\nMixed integer programming in general is still NP-hard. Why does the paper claim that the proposed MIP reformulation is tractable?\n\nThe paper states that the challenge for solving the optimization problem comes from Constraint (3) and (4f) which introduce non-convexities. What if we remove Constraint (3) and (4f)? For Constraint (3), since the most commonly used fairness notions are demographic parity and equalized odds, in some cases it is fine not to consider the third one. For Constraint (4f), it seems to me that its purpose is to further preserve the utility of the solution in addition to the objective function. Thus, it is not a necessary condition in the optimization.\n Yes."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude to the authors for addressing their concerns, indicating a positive overall assessment.",
            "The review identifies several weaknesses, including a lack of experimental comparison with existing fairness works, uninformative presentation of tables and figures, missing interpretations, and a lack of a motivating example. These criticisms outweigh the identified strength.",
            "The review highlights several strengths of the paper, including its novelty, relevance, clear organization, and well-fitting formulation. While weaknesses are pointed out, the overall tone suggests a positive assessment of the work's contributions.",
            "The review presents both strengths and weaknesses of the paper, along with some questions and suggestions. The language used is objective and analytical."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The use of 'I would like to thank the authors' shows a supportive tone.",
            "The review uses direct and critical language to point out flaws in the paper, such as \"There is no experimental comparison,\" \"Tables and figures are uninformatively presented,\" and \"A motivating example is missing.\"",
            "The review presents both strengths and weaknesses of the paper. It uses phrases like 'major strength,' 'very appealing,' and 'well-organized,' but also raises concerns about generalizability and requests clarification on certain aspects. This balanced approach indicates a neutral tone overall.",
            "The review provides both positive aspects (Strengths) and negative aspects (Weakness) of the paper. It also raises questions and suggestions, indicating a balanced assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses initial satisfaction that most concerns were addressed, but then raises a specific, actionable suggestion for improvement regarding the term 'tractable'. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent in its criticism, focusing on the lack of experimental comparison with post-processing methods and unclear presentation of results (figures and tables).",
            "The review is consistent because it clearly outlines both the strengths and weaknesses of the paper without contradicting itself. The strengths highlight the novelty and clarity of the proposed method, while the weaknesses point out valid concerns regarding generalizability and experimental details. The reviewer's critique is constructive and focuses on areas for improvement and further clarification, rather than presenting conflicting viewpoints.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the optimization framework's generality and scalability in certain aspects, while also raising valid concerns about scalability in scenarios with a large number of groups and questioning the tractability claim of the MIP formulation. The weaknesses are presented as questions and potential limitations, not as contradictions to the strengths."
        ]
    },
    {
        "paper_id": "iclr_2020_B1lFa3EFwB",
        "paper_title": "Stablizing Adversarial Invariance Induction by Discriminator Matching",
        "paper_abstract": "Incorporating the desired invariance into representation learning is a key challenge in many situations, e.g., for domain generalization and privacy/fairness constraints. An adversarial invariance induction (AII) shows its power on this purpose, which maximizes the proxy of the conditional entropy between representations and attributes by adversarial training between an attribute discriminator and feature extractor. However, the practical behavior of AII is still unclear as the previous analysis assumes the optimality of the attribute classifier, which is rarely held in practice. This paper first analyzes the practical behavior of AII both theoretically and empirically, indicating that AII has theoretical difficulty as it maximizes variational {\\em upper} bound of the actual conditional entropy, and AII catastrophically fails to induce invariance even in simple cases as suggested by the above theoretical findings. We then argue that a simple modification to AII can significantly stabilize the adversarial induction framework and achieve better invariant representations. Our modification is based on the property of conditional entropy; it is maximized if and only if the divergence between all pairs of marginal distributions over z between different attributes is minimized. The proposed method, {\\em invariance induction by discriminator matching}, modify AII objective to explicitly consider the divergence minimization requirements by defining a proxy of the divergence by using the attribute discriminator. Empirical validations on both the toy dataset and four real-world datasets (related to applications of user anonymization and domain generalization) reveal that the proposed method provides superior performance when inducing invariance for nuisance factors. ",
        "review_ids": [
            "BJxoDJtnKS",
            "HJg0D4d0YH",
            "rkg8J4Vg9r"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "*Summary*\n\nThe paper proposes a new method to learn data-driven representations, being invariant to some specific nuisance factors which are detrimental for the selected (supervised) classification task.\nAuthors build upon the existing probabilistic framework termed Adversarial Invariant Induction (AII) from (Xie et al., 2017). \n\nThey claim to explore it under a both theoretical and practical point of view, demonstrating the limitations of maximizing a variational upper bound on conditional entropy as a proxy to achieve invariance. \n\nLeveraging these observation, authors propose a novel method, called \u201cinvariance induction by discriminator matching\u201d (IIDM) that is based on a regularized classification loss, penalized by a Kullbach-Leibler divergence between conditional distributions of the nuisance factor.\n\nExtremely convincing experiments are carried out on a synthetic and a real benchmark in multi-source domain generalization (PACS).\n\n\n\n*Pros*\n1. The genesis of the proposed IIDM is extremely paced since smoothly derived from the AII framework.\n2. Experimental results on a synthetic benchmark (a version of rotated MNIST) and on a popular benchmark for domain generalization (PACS) proved the effectiveness of IIDM\n\n\n\n *Cons*\n1. The paper is hard to get, if the reader is not familiar with related literature\n2. It is not fully clear from the paper which parts are original and which are inherited from prior work.\n3. The structure of the paper needs to be improved (check my comments in the section beneath)\nSome of the proposed methodologies are not clear (IIDM+)\n\n\n\n\n*Detailed Comments*\n\nThe problem considered by authors is surely interesting and addressing a popular topic in computer vision and deep learning. \n\n1. Unfortunately, the paper, as it is is hard to get for scholars which are not expert of the AII formalism, which, in my opinion is not enough detailed. Therefore, in my opinion clarity is something that authors should try to work hard on: for instance, during the rebuttal time, authors can write from scratch an entire new Section in which they explain in plain terms the main outcomes of their paper, without entering too much into technical details.\n\n\n2. Additionally, the structure of the paper needs, in my opinion a major re-styling, still for the sake of better readability:\n2.a. A visualization of the proposed method (for instance, using flow-diagrams) in comparison with the existing AII should help in rapidly getting the factors of novelty of the proposed IIDM method. I would also encourage authors to add a pseudo-code\n2.b. Since authors claim two major contributions (understanding AII + IIDM), I would like to see those two contributions thoroughly presented and dissected in two separated sections of the paper. I am not fully convinced with the actual writing style in which the two contributions seem to be intertwined together.\n\n\n3. Although already convincing, the experimental part can be improved:\n3.a. Instead of a version of rotated-MNIST, authors can test on the \u201cdigits-five\u201d setting (MNIST, MNIST-M, SVHN, UPS, SYN) as done in several works like http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Deep_Cocktail_Network_CVPR_2018_paper.pdf.\n3.b. In addition to multi-domain generalization, authors could also have tried more classical unsupervised domain adaptation settings or, even, single-source domain generalization as in https://papers.nips.cc/paper/7779-generalizing-to-unseen-domains-via-adversarial-data-augmentation.\n\n\n\n*Final Evaluation*\n\nI think that the main aspect that authors should face during the rebuttal is to make the paper more easy to read and better separate the two contributions (understanding AII and IIDM). What I am not convinced at all about the writing style of the authors since when reading the paper I am not always capable of understanding what is novel (since proposed by authors) and what is inherited from prior work. But, maybe, the reason for this is that I am not an expert of the specific related field - but, even so, I think that the paper needs to be understood from the broadest audience possible.\n\nInstead, I am familiar with multi-source/single-source domain generalization (and adaptation) and, after my careful analysis of the experiments, I see a lot of potential in the approach. I would me more than interested in checking the performance of the proposed method over some of the novel benchmarks that I have recommended. It would be nice if authors add more experiments, but I know that this is always a complicated request during a rebuttal period.\nGlobally, if I were asked to only rate the experimental part, I would have promoted for full acceptance. Unfortunately, the theoretical part of the paper is not fully clear to me and, therefore, I am not confident in calling for a full acceptance only based on the experiments. \n\nIn brief, I would go for a weak reject, looking forward to the authors\u2019 rebuttal and the opinion of the other Fellow Reviewers.",
            "The paper points out that the practical behavior of AII assumes the optimality of the attribute classifier, which is rarely held in practice. And claims that the paper analyzes the practical behavior of AII both theoretically and empirically, indicating that AII has theoretical difficulty as it maximizes variational upper bound of the actual conditional entropy. Then it argues an ugly modification based on a wrong property of conditional entropy. \n\n- The paper says that it analyzes AII theoretically and empirically. But it only shows the practical drawback of AII intuitively without any theoretical proof.\n- In Section 3, the paper says : 'In general, maximizing the upper bound of the function of interest $f$ does not guarantee the minimizing the $f$ '. \n- Also in the section 3, the paper says : 'Figure 2-(b) visualizes how distribution move during the optimization of AII on synthesized data'. And I want to ask why the caption of Figure 2-(b) is IIDM ?\n- The proposition 1 on which the modification proposed in the paper is based will be not true when the distribution of attributes is not uniform by Bayses' s law which is used in the so called proof of the proposition 1. Which means that $p(z|a_i)=p(z|a_j) \\Leftarrow p(a_i|z)=p(a_j|z)$ if and only if $p(a_i)=p(a_j)$.\n- In the proof of equation 4 which is the main theorem of the total work. We can find $-\\sum q_{\\phi}^i(a)\\log\\mathbb{E}_{p_{\\theta}^j(z)}[q_{\\phi}(a|z)]\\ge -\\sum q_{\\phi}^i(a)\\mathbb{E}_{p_{\\theta}^j(z)}[\\log q_{\\phi}(a|z)]$.  The inequality direction is reversed. \n",
            "** Summary\nThe paper studies the problem of representation learning under invariance constraints (i.e., the representation should be invariant wrt some attributes). The authors first review the adversarial invariance induction (AII) approach and they point out its limitations and then they propose a novel variant, which introduces an explicit regularization to minimize pairwise divergence (i.e., different attributes should lead to the same conditional distribution over the learned representation). The authors support the modified objective function both from a formal point of view and with an extensive empirical validation\n\n** Evaluation\nThe paper lies a bit outside my area of expertise. Although the paper tries to capture intrinsic limitations of the AII approach and build a more stable algorithm, my impression is that too many elements in the discussion and derivation remain too vague at the current stage and they would require better and clearer explanation.\n\nDetailed comments:\n1- In many parts of the paper the notation is not very rigorous and sometimes it may create confusion. In general, the writing needs to be improved in many parts:\n- In eq. 1, it is not explained what the expectation is wrt. It should be x drawn from p(x). But it would be better to make it explicit.\n- The setting defined in sect.2 should be more rigorous: it is not clear what y is and what are the attributes we would like to be invariant for. This notation is not fully consistent with eq.1.\n- In the first paragraph of Sect.3, it would be very helpful to have a more complete sketch of the algorithm. In general, you mention in the Sect. 2 that you focus on the supervised case, but then it is not clear whether this is the case across the paper.\n2- While the intuition behind using the divergence is sound, as it is mentioned, it seems to suffer from the same issue as the original AII: minimizing a lower bound does not guarantee that we are minimizing the actual objective. Having the support of 0, does not seem to make it much more sensible.\n3- As the description of the algorithm advances in Sect.4.2, it is clear that many additional choices need to be made in order to have a full workable algorithm (e.g., how to estimate q_\\phi^i). In the paper, the actual algorithm is never reported in detail and this makes the experiments very hard to reproduce in my opinion.\n4- At the best of my understanding, the algorithm may become more and more intractable as the number of attribute values grows. In fact, you need to check the divergence for each pair of values and estimated distributions.\n5- The empirical analysis seems well executed and a good level of detail is reported on how the datasets are managed and the experiments are set up.\n\nMinor comments:\n- The caption of Fig.1 is not very clear. Many elements at this stage of the paper are not defined yet (e.g., \"our proposal minimize the proxy of divergence ...\").\n- p2: \"as the assumption of ... is rarely holds\", remove \"is\"\n- p3: \"(encoder) that parameterized\", remove \"that\"\n- last paragraph of Sect.2 is very confusing.\n- p4: \"updation\" is not a word\n- p4: \"In general, minimizing the upper bound...\" does not seem correct.\n- p4: \"even such a simple case\" -> \"even in such a simple case\"\n- Sect 4.1 \"the above theoretical\", I would not really say there is much theory behind the analysis in the previous section.\n- p5 \"an ttribute\" -> \"an attribute\"\n- p6 \"the average discriminator's perception\" what is the perception?"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer recommends a \"weak reject\" due to the theoretical part not being clear, and the paper being hard to read.",
            "The review expresses several criticisms of the paper, including lack of theoretical proof, inconsistencies in the text, flaws in the proposed modification, and a reversed inequality in the main theorem's proof. These points indicate a negative assessment of the paper's quality and validity.",
            "The review is critical due to concerns about clarity, rigor, and reproducibility, as evidenced by phrases like \"too many elements remain too vague,\" \"notation is not very rigorous,\" and \"experiments very hard to reproduce.\""
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer provides constructive criticism and points out several areas where the paper needs improvement, such as clarity, structure, and separation of contributions. The final evaluation includes a recommendation for \"weak reject.\"",
            "The review uses phrases like 'ugly modification,' 'wrong property,' 'without any theoretical proof,' and points out errors in equations and reasoning. This direct and fault-finding approach demonstrates a critical tone.",
            "The tone is critical due to specific criticisms and suggestions for improvement. The reviewer highlights weaknesses in notation, explanations, algorithm description, and theoretical grounding, using phrases like \"the writing needs to be improved,\" \"not very rigorous,\" and \"does not seem correct.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it praises the experimental results and potential of the method while consistently criticizing the clarity, structure, and theoretical presentation of the paper. The final recommendation of a weak reject logically follows from this balanced assessment, prioritizing the need for improvement in clarity and theoretical understanding despite the promising experimental findings.",
            "The review is consistent because all points raised are criticisms of the paper, focusing on lack of theoretical proof, potential errors in methodology, and specific flaws in proofs and details, without any contradictory statements or positive feedback.",
            "The review is consistent in its critique, highlighting both the potential of the approach and the lack of clarity and rigor in the current manuscript. While the summary is somewhat positive in outlining the paper's topic, the evaluation and detailed comments consistently point out areas for improvement, focusing on vagueness, unclear notation, insufficient algorithm description, and lack of theoretical support. The reviewer appreciates the attempt to address limitations of existing methods but finds the current presentation lacking in clarity and detail, leading to concerns about reproducibility and scalability."
        ]
    },
    {
        "paper_id": "iclr_2018_H1uP7ebAW",
        "paper_title": "Learning to diagnose from scratch by exploiting dependencies among labels",
        "paper_abstract": "The field of medical diagnostics contains a wealth of challenges which closely resemble classical machine learning problems; practical constraints, however, complicate the translation of these endpoints naively into classical architectures. Many tasks in radiology, for example, are largely problems of multi-label classification wherein medical images are interpreted to indicate multiple present or suspected pathologies. Clinical settings drive the necessity for high accuracy simultaneously across a multitude of pathological outcomes and greatly limit the utility of tools which consider only a subset. This issue is exacerbated by a general scarcity of training data and maximizes the need to extract clinically relevant features from available samples -- ideally without the use of pre-trained models which may carry forward undesirable biases from tangentially related tasks. We present and evaluate a partial solution to these constraints in using LSTMs to leverage interdependencies among target labels in predicting 14 pathologic patterns from chest x-rays and establish state of the art results on the largest publicly available chest x-ray dataset from the NIH without pre-training. Furthermore, we propose and discuss alternative evaluation metrics and their relevance in clinical practice.",
        "review_ids": [
            "HJZ2MKRbM",
            "S1KuIB5gz",
            "BkE5LPlZG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper proposes to combine the recently proposed DenseNet architecture with LSTMs to tackle the problem of predicting different pathologic patterns from chest x-rays. In particular, the use of LSTMs helps take into account interdependencies between pattern labels. \n\nStrengths:\n- The paper is very well written. Contextualization with respect to previous work is adequate. Explanations are clear. Novelties are clearly identified by the authors.\n- Quantitative improvement with respect to the state the art. \n\nWeaknesses:\n- The paper does not introduce strong technical novelties -- mostly, it seems to apply previous techniques to the medical domain. It could have been interesting to know if there are more insights / lessons learned in this process. This could be of interest for a broader audience. For instance, what are the implications of using higher-resolution images as input to DenseNet / decreasing the number of layers? How do the features learned at different layers compare to the ones of the original network trained for image classification? How do features of networks pre-trained on ImageNet, and then fine-tuned for the medical domain, compare to features learned from medical images from scratch? \n- The impact of the proposed approach on medical diagnostics is unclear. The authors could better discuss how the approach could be adopted in practice. Also, it could be interesting also to discuss how the results in Table 2 and 3 compare to human classification capabilities, and if that performance would be already enough for building a computer-aided diagnosis system.\n\nFinally -- is it expected that the ordering of the factorization in Eq. 3 does not count much (results in Table 3)? As a non-expert in the field, I'd expect that ordering between pathologic patterns matters more.",
            "This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images. \nAuthors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification. Experiments are clearly described and results are significantly better compared to state of the art.\n\nThe only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain. Considering that their paper is titled as a work to use \"dependencies\" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation. \n\nOn the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently. To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well. \n\nThat said, the paper has several positive aspects in all areas:\n\nOriginality - the paper presents first combination of DenseNets with LSTM-based output factorization,\nWriting clarity - the paper is very well written and clear.\nQuality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done,\nSignificance - Apart from the issue of intractable inference which is arguably a large limitation of this work, the application in medical field is significant. \n\n",
            "Well written and appropriately structured. Well within the remit of the conference.\nNot much technical novelty to be found, but the original contributions are adequately identified and they are interesting on their own.\n\nMy main concern (and complaint) is not technical, but application-based. This study is (unfortunately) typical in that it focuses on and provides detail of the technical modeling issues, but ignores the medical applicability of the model and results. This is exemplified by the fact that the data set is hardly described at all and the 14 abnormalities/pathologies, the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint. If I were a medical expert, I would not have a clue about how these results and models could be applied in practice, or about what medical insight I could achieve.\n\nThe bottom line seems to be: \"my model and approach works better than the other guys' model and approach\", but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much "
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review presents both strengths and weaknesses of the paper, indicating a balanced assessment. While acknowledging the paper's clarity and quantitative improvements, it also points out the lack of strong technical novelties and unclear impact on medical diagnostics.",
            "The review acknowledges the paper's strengths (impressive results, clear description, better results than state-of-the-art, originality, writing clarity, quality, significance) but also points out a major limitation (intractable inference) and suggests a missing baseline (multi-task learning). The overall sentiment is therefore balanced.",
            "The review expresses concerns and complaints about the lack of medical applicability and detail regarding the data set used in the study. Phrases like \"main concern (and complaint)\", \"ignores the medical applicability\", and \"I would not have a clue\" indicate a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a balanced tone by explicitly listing both 'Strengths' and 'Weaknesses'. It uses constructive language, such as 'It could have been interesting to know' and 'The authors could better discuss', suggesting improvements rather than outright criticism. The reviewer also admits to being a 'non-expert' in a specific area, showing humility and openness to further explanation.",
            "The review adopts a balanced tone by praising positive aspects ('impressive set of results', 'clearly described', 'significantly better') while also offering critical feedback ('major limitation', 'not tractable for inference', 'missing multi-task baseline'). The reviewer uses phrases like 'On the other hand' and 'That said' to transition between positive and negative points, indicating a balanced assessment.",
            "The tone is critical due to the direct complaints and concerns raised about the study's focus and lack of medical applicability. Phrases like \"My main concern (and complaint)\" and the overall questioning of the study's practical relevance contribute to the critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents a balanced assessment with both strengths and weaknesses, without contradicting itself. The reviewer provides constructive criticism and raises valid questions.",
            "The review consistently highlights both the strengths and weaknesses of the paper. It praises the results, originality, and clarity while also pointing out a significant limitation regarding the practical applicability of the proposed method due to intractable inference and the lack of comparison with a multi-task baseline. The reviewer's points are logically connected and do not contradict each other, presenting a balanced assessment of the paper.",
            "The review is consistent because it starts with acknowledging the paper's structure and conference relevance, but then focuses on a central critical point: the lack of medical applicability and focus on technical details over practical medical insight. The reviewer consistently argues that the paper lacks application context and medical justification, without contradicting their initial positive remarks about writing style and structure."
        ]
    },
    {
        "paper_id": "nips_2022_-zYfrOl2I6O",
        "paper_title": "CASA: Category-agnostic Skeletal Animal Reconstruction",
        "paper_abstract": "Recovering a skeletal shape from a monocular video is a longstanding challenge. Prevailing nonrigid animal reconstruction methods often adopt a control-point driven animation model and optimize bone transforms individually without considering skeletal topology, yielding unsatisfactory shape and articulation. In contrast, humans can easily infer the articulation structure of an unknown character by associating it with a seen articulated object in their memory.  Inspired by this fact, we present CASA, a novel category-agnostic articulated animal reconstruction method. Our method consists of two components, a video-to-shape retrieval process and a neural inverse graphics framework. During inference, CASA first finds a matched articulated shape from a 3D character assets bank so that the input video scores highly with the rendered image, according to a pretrained image-language model. It then integrates the retrieved character into an inverse graphics framework and jointly infers the shape deformation, skeleton structure, and skinning weights through optimization. Experiments validate the efficacy of our method in shape reconstruction and articulation. We further show that we can use the resulting skeletal-animated character for re-animation. \n",
        "review_ids": [
            "VF7oPoJf3B3",
            "1tNX2xG4mm",
            "VFydtWFE-cJ",
            "VnZTuK6QUN",
            "6XzKgobF_FU"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Most of my questions are answered adequately. I'd like to raise score to 7. The interesting part of the paper is the 3D skeletal model retrieval given a large database, which provides reasonable constraints when the target object falls roughly within the database. The remaining concern is that the method does not appear faithful to the data (horns of the cow in Fig. 4), which could be  due to either lack of observation or unnecessary regularization terms, which needs further discussion.\n\nSuggestion on figures\n- Please visualize the retrieved model together with the reconstruction (similar to lasr did), so that readers understand how much the shape is updated by optimization.\n- Lasr results in Fig. 8 is inconsistent with Fig.7  \n- Fig 6. Note casa/lasr apply symmetry constraint on the occluded body part, but banmo/viser baseline do not. This should be better explained.",
            " Thank you for the detailed responses as well as additional experiments to address the raised concerns. \n\nNow that my major concerns are cleared, I raised my score to accept. However, as R1 mentioned, the loss functions on inverse graphics are not substantially novel and I would strongly recommend toning down the contribution claim on neural inverse graphics and better clarifying the novelty. Also since the proposed approach does per-instance optimization, the generalization claim in the sentence of L58-60 does not make sense. I would highly recommend removing it. Thanks!",
            " This paper focuses on the problem of 3D reconstruction of animals from a monocular video. The proposed method is claimed to be Category-agnostic, which means they can deal with different categories such as dogs, horses, but they are all quadruped. The major contributions is the category-agnostic reconstruction which is realized by first retrive a 3D template model from an asset. Then the template model is deformed and optimized to fit to the input video.  Strengths:\nFirst, having a good initial template to start with the following up optimization will certainly reduce the deformation space and make the optimization to be more feasible. They have demonstrated better visualization results by adopting this retrived template model. The idea is pretty easy to understand and the paper is organized and written well.\n\nWeakness:\n1) Obtaining this template model by retrival using the pre-trained CLIP model is more or less an engineering work. I don't think this could be claimed as a technical contribution. The performance is improved mainly due to this retrived template, while the compared approaches start with some general shape for example, a sphere.\n2) The optimization pipeline or the loss function is not novel. It is pretty much a standard optimization, I'm a bit confused what the authors want to claim on this optimization problem. In addition, the skeletal representation of articulated models, they are just standard way of dealing with those kinds of animals. I'm not sure what the authors want to emphasize on this.\n3) Missing important comparison:  -- BANMo: Building Animatable 3D Neural Models from Many Casual Videos. \n I have some questions or confuse on the some technical details of the proposed method, \n1) How to optimize both the bone length, joints angles, skinning weights together? Do we need to optimize one while fixing others? Furthermore, what is the improvement of optimizing skinning weights, what if the skinning weights are not optimized, but using those from retrived template?\n\n2) How is the optimization initialzed? How can we achieve the initial fitting to images or videos, including initial global pose?\n\nThe authors might also want to include the response to the issues I raised in Weakness above. The major limitation is lack of technical contribution. Using the pre-trained CLIP model as features to retrive the 3D template is good, but I'm not sure this could be claimed as technical novelty. And the following optimization is also pretty standard. The authors should point out what are the major technical contributions that really stands out.",
            " This paper presents a category-agnostic character animation reconstrucion from a casual video input. To alliviate the ill-posed nature of the problem, this work first queries the closest template model from the database using CLIP features. As the following inverse graphics optimization stage can warm start with the retrieved template, the proposed approach produces better results both qualitatively and quantitatively over prior methods. Additionally, the paper introduces a large amount of synthetic dataset for qualitative evlauation of predicted attributes with diverse categories. This paper has the following strengths:\n- This work presents an interesting use of CLIP feature. To retrieve the closest animal template from the database, the CLIP features are extracted from both input video and synthetically rendered database. This type of semantic-based retrieval is more general and flexible than hand-crafted descriptors for the retreival task. \n- The paper presents an impressive qualitative results even from videos in the wild. \n- The proposed dataset would be a great resource for community for both training and evaluation to assess the accuracy of predicted attributes from diverse categories. \n\nThere are several weaknesses of this work:\n- The qualitative results of baseline methods (LASR, ViSER) are substantially worse than what\u2019s presented in their original papers. I\u2019m wondering if the results are cherrypicked or their code was not properly run. I would highly recommend reaching out to the authors of these papers to confirm that these results are expected. Please answer to this in the rebuttal to prove that the experiments are credible.\n- L58-60: I\u2019m very confused about the notion of generalization here. The paper also discuss train/test split in L258. However, as far as I understand, this work presents an instance-specific training and there is no cross-instance generalization. Please clarify.\n\nOverall the paper presents an interesting approach and the results are impressive. However, the aforementioned concerns prevent me from giving a higher score at this stage.  - Please answer to the comments above.\n- It is not clear if CLIP is necessary for this retrieval task as text is not involved at all. It would be great if the use of various feature backbone including CLIP and ResNet pretrained with ImageNet is evaluated. \n- From the exposition, it is not clear how to compute CLIP feature from videos (CLIP only provides embedding per frame). Please elaborate.\n\nOther comments:\n- L43: The neural inverse graphics framework in general has been extensively used in prior works, and not novel. Please state more concretely what is novel over the prior works in terms of the optimization.\n- L231: Please add citation to Cobra-tools. The limitation is discussed, but its societal impact is not.",
            " The authors propose CASA, a method for recovering 3D shape and skeletal movements from monocular videos. Given a video, it first retrieves a shape and a skeleton from a library with 200+ animals, then optimizes both shape and articulations with differentiable rendering. Results are shown synthetic real datasets, with applications in re-posing. A new dataset PlanetZoo with 200+ animated animals is introduced. **Strengths**\n- The proposed PlanetZoo dataset is interesting, as I'm not aware of a dataset of similar size (>200 animals) and quality (with texture, skeleton and deformation). It has potential to be used to evaluate and benchmark the performance of animal 3D reconstruction algorithms. From that perspective, it would be helpful to highlight the features of the dataset, and compare with the existing ones (such as deforming things 4D).\n- The method is sensible. It leverages 3D shape and skeletal priors of specific categories to improve the reconstruction quality. Since the library is large, the method can be applied to a wide range of animal categories.\n- The usage of skeleton structure also helps the re-posing application and user controllability.\n\n**Weakness**\n- Quality of results. The results are not faithful to the input data even test-time optimization is used. For instance in Fig.7, the reference cow image does not contain a horn but the reconstruction has horns, possibly from the template being retrieved. This is not desirable. I'm also confused as I did not notice a term that prevents the template shape to change when there is disagreement with the image evidence during optimization. This makes it unclear whether the optimization works properly.\n- The experimental comparison can be made fairer. LASR/ViSER does not have access to a 3D shape library. Proper baselines would be methods that use a 3D shape template, such as ACSM [A], ACFM, or proving LASR a 3D template.\n- There are some missing details (see questions). The one I'm most concerned about is the initialization of root transformations, which is crucial for reducing the ambiguity between shape and deformation. For instance, if the heading direction is rotated by 180 deg at initialization (the head becomes the tail), the optimization might focus on deforming the shape without correcting the heading.\n- Ablation study is missing in the main paper. How well does retrieval perform? As it is a major contribution, a quantitative evaluation and thorough analysis is expected. For the ablation study on flexible bone model, a qualitative comparison is desired as numbers does not give much insight in this context.\n- Fig. 3 is slightly misleading as it is not clear which parameters are per-frame and which are per-video. The subtitles also conflated per-video deformation (stretch and deform) vs per-frame deformation (rigging), vertex deformation (deform) vs bone deformation (stretch). \n\n**Other related work**\n- [A] Articulation-aware canonical surface mapping. CVPR 20.\n- [B] Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects, CVPR 22. - How many categories does the dataset contain? What are the categories?\n- How are the root node transformations represented and initialized? Are they the same as LASR/ViSER?\n- During model retrieval, how to sample viewpoints when rendering the 3D model? How many viewpoints are needed? How long does it take to sampling and matching? Also, pairwise matching should produce SxNxT scores, where S is the number of shapes, and N is the number of renderings per shape, and  T is the number of frames. Then how is T and N marginalized?\n Yes."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses both positive aspects (questions answered, interesting part of the paper) and concerns (lack of faithfulness to data). It also includes specific suggestions for improvement, indicating a balanced perspective.",
            "The reviewer states their major concerns are cleared and they raised their score to accept, indicating a positive overall assessment.",
            "The review expresses several concerns about the paper's technical novelty and contribution. Phrases like \"more or less an engineering work,\" \"not novel,\" \"pretty much a standard optimization,\" and \"lack of technical contribution\" indicate a negative sentiment.",
            "The review acknowledges both strengths and weaknesses of the paper, leading to a balanced assessment. It expresses concerns and suggestions for improvement without strong positive or negative language.",
            "The review focuses on weaknesses and areas for improvement, such as the quality of results, experimental comparison, missing details, and ablation study. The reviewer also points out misleading aspects of the figures and expresses confusion about the method's optimization process."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The tone is balanced. It starts with a positive note ('Most of my questions are answered adequately') then transitions to a critical observation ('the method does not appear faithful to the data'). The reviewer provides constructive suggestions ('Please visualize the retrieved model...') indicating a desire to improve the paper.",
            "Supportive",
            "The tone is critical, as evidenced by the reviewer's direct questioning of the authors' claims and the use of phrases such as \"I don't think this could be claimed as a technical contribution,\" \"I'm a bit confused what the authors want to claim,\" and \"The authors should point out what are the major technical contributions that really stands out.\"",
            "The review presents both positive aspects ('interesting use of CLIP feature', 'impressive qualitative results', 'great resource for community') and critical points ('qualitative results of baseline methods are substantially worse', 'very confused about the notion of generalization', 'not clear if CLIP is necessary'). The reviewer also offers constructive suggestions ('Please answer to this in the rebuttal', 'Please clarify', 'Please elaborate').",
            "The reviewer uses specific examples and questions to highlight the shortcomings of the paper, such as the horn on the reconstructed cow in Fig. 7 and the lack of clarity in Fig. 3."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses an overall positive sentiment ('Most of my questions are answered adequately', 'I'd like to raise score to 7', 'interesting part of the paper') while pointing out specific areas for improvement (data faithfulness, figure suggestions). The reviewer's concerns are constructive and do not contradict the positive assessment of the paper's core contribution.",
            "The reviewer expresses satisfaction with the authors' responses and raises the score to accept. They provide constructive feedback for improvement regarding novelty claims and generalization, but these are suggestions for refinement rather than contradictions within the review itself.",
            "The review is consistent in its assessment that the paper's main weakness is the lack of technical novelty. The reviewer repeatedly points out that the template retrieval using CLIP is more of an engineering effort rather than a technical contribution, and the optimization pipeline is standard. Despite acknowledging strengths like improved visualization and clarity, the core criticism remains focused on the limited technical innovation.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the interesting use of CLIP features, impressive qualitative results, and the valuable dataset. At the same time, it raises valid concerns and weaknesses regarding the credibility of baseline comparisons, clarity on generalization, the necessity of CLIP, and the computation of CLIP features from videos. The reviewer maintains a balanced perspective by highlighting both positive and negative aspects and providing constructive criticism without contradicting themselves.",
            "The review is consistent because it acknowledges the strengths of the proposed dataset and method's approach, while also raising valid and logically connected weaknesses concerning the quality of results, experimental setup, missing details, and lack of ablation studies. The reviewer's points flow logically and do not contradict each other, presenting a balanced critique of the paper."
        ]
    },
    {
        "paper_id": "iclr_2022_KVYq2Ea90PC",
        "paper_title": "A Study of Face Obfuscation in ImageNet",
        "paper_abstract": "Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective for privacy protection; nevertheless, object recognition research typically assumes access to complete, unobfuscated images. In this paper, we explore the effects of face obfuscation on the popular ImageNet challenge visual recognition benchmark. Most categories in the ImageNet challenge are not people categories; however, many incidental people appear in the images, and their privacy is a concern. We first annotate faces in the dataset. Then we demonstrate that face blurring and overlaying---two typical obfuscation techniques---have minimal impact on the accuracy of recognition models. Concretely, we benchmark multiple deep neural networks on face-obfuscated images and observe that the overall recognition accuracy drops only slightly (<= 1.0%). Further, we experiment with transfer learning to 4 downstream tasks (object recognition, scene recognition, face attribute classification, and object detection) and show that features learned on face-obfuscated images are equally transferable. Our work demonstrates the feasibility of privacy-aware visual recognition, improves the highly-used ImageNet challenge benchmark, and suggests an important path for future visual datasets. ",
        "review_ids": [
            "hkhY-LAudvC",
            "2zWZaOHfB5h",
            "wmytw7qfx33",
            "muaJz2tXtFF",
            "Y-UbKxlTgxo",
            "6m-g0uRpct-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " There are millions of images in ImageNet, it's impossible to check every image to find the face which is not detected by face detectors. And how to evaluate/judge the quality of the labels from crowd workers.  \n\nMoreover, Imagenet is an impact work in computer vision. This paper is only done some little changes on it, the contribution is incremental. It cannot meet the high standard of this conference.",
            " First, thanks for your efforts on this work. It is still not well answered my question when some faces are not detected.  It still cannot clearly explain how to process the problems of the false positive/negative detections.  How do you use different detections to correct false negatives? I will still keep my rating.",
            " Thanks for the reply of the authors. I truly appreciate the efforts done by the work for addressing the privacy issue of the ImageNet dataset. However, I would still want to see the influence and difference w/ and w/o face obfuscation for  larger and more complex datasets which can get benefit from ImageNet pretraining. I will still keep my rating.",
            "This paper presents an empirical study on the effect of face obfuscation in the ImageNet dataset. The main conclusion is that face obfuscation does not decrease the utility of the dataset. Specifically, the authors showed that various networks trained on the obfuscated dataset only experienced small accuracy drop on the image classification task. The authors also discussed the impact on different categories, showing that face obfuscation hurt more to the object categories that are more closely related to faces (i.e., the bounding boxes of which overlap more with faces). Last but now least, experiments has been conducted to show that face obfuscation also does not have a significant impact on the transferability of the features learned from the new dataset. All these conclusions are inline with intuitions since ImageNet is not primarily focused on human activities / faces. Strengths:\n* The main contribution of this work is that it provided empirical evidence on the effect of face obfuscation on the ImageNet dataset. Through comprehensive experiment, the authors showed that face obfuscation does not decrease the utility of the dataset.\n* Another contribution that should not be overlooked is that the authors annotated all faces in ImageNet in a semi-automatic manner and promised that they will make the annotations publicly available to other researchers.\n* The paper is very well written and includes a lot of details on the experiment protocol. Thus It should be straightforward for other researchers to reproduce the results and to extend the study.\n\nWeaknesses:\n* Since blur and cutout are commonly used data augmentation techniques, it is to be expected that face obfuscation would not have a big impact to visions tasks that have little to do with faces. Although it is commendable that this is now shown empirically though the study, this work also does not bring interesting new insights into the topic.\n* The authors showed that categories that are closely related to faces are indeed affected more by face obfuscation. This paper would be more interesting (from a technical point of view) if the authors could additionally investigate into methods for alleviating such impact. This paper is very well written and it provides empirical evidences to support the intuition that face obfuscation does not decrease the utility of the ImageNet dataset. However, my main concern is that the paper has no technical novelty and it also does not bring sufficiently new insights to the community.",
            "The paper mainly discuss the privacy issue for human for the widely used ImageNet dataset and how to handle them.\nIn addition, the paper does a very detailed empirical experiments to study the performance influence for various tasks, including object recognition, scene recognition, face attribute, and object detection if all the faces in the ImageNet are obfuscated. The main strength of the paper is to address the privacy issues of ImageNet and is to provide an alternative face obfuscated version.\nIn addition, the authors conduct very thorough experiments across different tasks and different architecture for the study of the performance influence with the obfuscated dataset. It shows new dataset still is effective for transfer learning for various vision task with few performance drop. However, the weakness is that the main part of the paper is to examine the performance influence of different settings and is of limited technical novelty. For the verification of some downstream tasks, the coverage of tasks is not enough. Although the paper mainly focuses on providing plenty of empirical results to evaluate the influence of using the face obfuscated ImageNet dataset and is of limited novelty, it provides a lot of insights to show the effectiveness and feasibility of privacy preserving ImageNet.\n\nI have few concerns of the selection of transferring tasks. For example, the resolution of CIFAR-10 is only 32x32 and only for 10 classes. Similarly, Pascal VOC is also relatively small and easy dataset as compared with COCO or other recently released object detection dataset.\nMost of the images in the CelebA dataset are in frontal pose and have much fewer variations than other unconstrained face dataset, like IJB-C, etc. Since these datasets are relatively simpler than others which are more close to real-world scenarios, I wonder if the same experimental results and findings are still valid for harder datasets with more variations.",
            "The main concern addressed in this paper is the privacy problem that may result from images in ImageNet databases containing unexpected faces. The authors propose a two-step face filtering method. First, the authors use a detector called Amazon Rekognition to detect the ImageNet database. Then, the authors further optimize the detector output through the crowdsourcing platform Amazon Mechanical Turk (AMT) to reduce false positives and false negatives in automated detection. For the detected faces, the authors took two approaches, distinguishing between blurring and overlaying, and tested their effectiveness on different models separately. The accuracy of the two approaches was reduced by 0.9% on average compared to the original database on the ILSVRC classification challenge. And using the database that has blurred or covered the faces still maintains the transferability of the original database in the tests of downstream tasks.\nContribution\uff1a\n1. The authors perform a very time-consuming and labor-intensive task for accurate labeling and filtering of faces in the ImageNet database and statistical analysis of the classes of faces contained in ImageNet.\n2. The authors demonstrate experiments related to classification tasks and pre-trained model training using a database containing blurred or covered faces, proving that the theory is feasible and that the dropped accuracy is acceptable.\n3. In terms of ethics, using blurred or covered face data for training can reduce privacy concerns. The study of the ImageNet database in terms of privacy can provide an important reference for subsequent databases The main weakness:\n1. I was confused by the statement in section (Part III, second paragraph) as to whether the face annotation process was manually filtered only for faces that were successfully detected (generated prediction boxes5) by Amazon Rekognition. Although I can infer from Appendix A Stage1 that only the data with successful detector predictions should be put into the AMT platform. Perhaps it would be better to include a brief description of Amazon Rekognition in the text. \n2. And if some faces are not detected by Amazon Rekognition, how do you tackle this problem? This is an important problem for the privacy issue in this paper because this paper focuses on the privacy issue.\n3. The novelty of this paper is limited reference to the proposed method in the paper.\n\nThe main strengths:\n1. The experimental part of the paper is depicted in great detail and completely. Rigorous validation of the obfuscation approach is done on two different methods on 15 different models. And in the appendix, the detailed method of blurring, and the problems that may happen in the process of labeling are under clearer explanation.\n2. The ethics of machine learning has been widely debated, with the issue of face privacy being of particular concern. There are many similar discussions, for example, there are some papers proposing to remove images associated with people from the database. The feasibility of obfuscation processing of faces proposed in this paper is a good way to minimize privacy issues without reducing the number of databases at the same time. This paper has some contributions in exploring the ethicality of datasets, especially in the current very popular ImageNet database, but it exists some flaws (see weakness). The solutions and results in this paper are open sources and feasible, and this work will inspire subsequent exploration of privacy protection in publicly available datasets. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the feasibility of verifying ImageNet data and criticizes the paper's contribution as 'incremental' and not meeting the conference's 'high standard'.",
            "The reviewer expresses dissatisfaction with the paper, stating that it \"is still not well answered my question\" and \"still cannot clearly explain\" certain issues. The final sentence, \"I will still keep my rating,\" implies the reviewer is maintaining a negative assessment.",
            "The reviewer acknowledges the authors' efforts but expresses a desire for further analysis on more complex datasets. This indicates a neutral stance, neither strongly positive nor negative.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges the strengths of the paper, such as providing empirical evidence and detailed experiment protocols. However, it also raises concerns about the lack of technical novelty and new insights.",
            "The review acknowledges the paper's strengths, such as addressing privacy issues and providing a face-obfuscated dataset. It also highlights the thorough experiments and insights into the effectiveness of the new dataset. While pointing out weaknesses, the overall assessment leans towards positive due to the recognition of the paper's contributions.",
            "The review highlights several contributions and strengths of the paper, praising the authors' efforts, experimental detail, and ethical considerations. While weaknesses are mentioned, the overall tone suggests a positive evaluation."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical due to the use of phrases like 'impossible to check', 'only done some little changes', and 'contribution is incremental'. The reviewer questions the paper's value and suitability for the conference.",
            "The tone is critical due to the use of phrases like \"not well answered,\" \"cannot clearly explain,\" and the implication of unchanged negative rating. The questions are framed as challenges to the authors' work, rather than seeking clarification.",
            "The review begins with appreciation (\"I truly appreciate the efforts\") but then expresses a remaining concern (\"However, I would still want to see...\"). This combination of positive acknowledgment and constructive criticism indicates a balanced tone.",
            "The review offers both praise and criticism, using phrases like \"Strengths,\" \"Weaknesses,\" and \"my main concern.\" It fairly presents both the positive aspects (well-written, comprehensive experiments) and the negative aspects (lack of novelty, expected results).",
            "The review presents both positive and negative aspects of the paper. It uses phrases like \"main strength,\" \"very thorough experiments,\" and \"provides a lot of insights\" to praise the work, while also using phrases like \"limited technical novelty\" and \"coverage of tasks is not enough\" to express concerns. The reviewer also uses tentative language like \"I wonder if\" when voicing concerns about the experimental results.",
            "The review presents both strengths and weaknesses of the paper. It uses formal language and provides specific examples to support its claims, indicating a balanced and objective assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because both points raised are negative assessments of the paper. The first point questions the practicality of verifying ImageNet labels, and the second point argues that the paper's contribution is incremental and doesn't meet conference standards due to its reliance on ImageNet. Both points contribute to a negative evaluation without contradicting each other.",
            "The review is consistent because the reviewer expresses negative feedback by stating that their questions are still not answered and the explanation is still unclear. The reviewer maintains a negative stance by saying they will keep their rating, implying no change in their initial negative assessment.",
            "The reviewer appreciates the authors' efforts in addressing the privacy issue but maintains their previous rating and expresses a desire for further experiments on larger datasets. This indicates a consistent stance, as the reviewer acknowledges the positive aspects while still highlighting an area for improvement that influences their overall assessment.",
            "The review is consistent. The reviewer acknowledges the strengths of the paper, such as providing empirical evidence and detailed experiments, but consistently points out the lack of technical novelty and new insights as the main weakness. The reviewer's points in both strengths and weaknesses sections align with the overall assessment that the paper is well-executed but lacks significant novelty.",
            "The review is consistent because it acknowledges the strengths of the paper, such as addressing privacy issues and conducting thorough experiments, while also pointing out weaknesses like limited technical novelty and concerns about the selection of transfer learning tasks. The concerns are logically connected to the identified weaknesses, and there are no self-contradictory statements.",
            "The review is consistent because it presents a balanced view by highlighting both strengths and weaknesses of the paper. The criticisms are constructive and do not contradict the appreciation for the paper's contributions, especially in addressing ethical concerns and providing a feasible solution."
        ]
    },
    {
        "paper_id": "nips_2021_SBNs7EULzqq",
        "paper_title": "Non-Asymptotic Analysis for Two Time-scale TDC with General Smooth Function Approximation",
        "paper_abstract": "Yue Wang, Shaofeng Zou, Yi Zhou",
        "review_ids": [
            "8ATsRnJGxfe",
            "mHcYmpTl623",
            "lhbdXikiSHi"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper provides finite-sample analysis on the two time-scale TDC algorithm with non-linear function approximation, proposed by [5] Bhatnagar et al. Both the i.i.d. setting and the Markovian setting are considered. The convergence rate is near-optimal given the target MSPBE function is generally non-convex.  The paper is clearly written and easy to understand.  \n\nBased on the asymptotic results given in [5] Bhatnagar et al., it is natural to consider the finite-sample analysis and the non-asymptotic rate $1 / \\sqrt{T}$ achieved here is a bit surprisingly good. The downside though is that the whole analysis falls under the framework where the outside loop (the slower component $\\theta$) is to optimize a general non-convex function with stochastic gradients, while the inner loop (the faster component $\\omega$) is a linear stochastic approximation task (which is effectively strongly convex). Therefore the analysis can always be decomposed to first bounding the tracking error of the faster component $\\omega$ to the optimal solution $\\omega(\\theta)$ while the slower component only suffers a small loss from the tracking error aside from the classical non-convex SGD analysis. The Markovian setting is great but also suffers from a similar novelty issue since there has been plenty of previous works dealing with the Markov noise under the uniform ergodicity assumption. \n\nOne suggestion is that the arguments in Line 539 and Line 594 that requires $\\alpha$ to be much smaller than $\\beta$ need to be stated more clearly, because there are too many constants involved there and it is better to write down an assumption on the ratio $\\alpha / \\beta$.  \n\nIt would help highlight the technical contribution if the author can also compare the two time-scale techniques used here with those in some previous works:\n1. (this presents the non-convex outer loop + convex inner loop framework) A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods. NIPS'20 \n2. (a more general formulation) A Two-Timescale Stochastic Algorithm Framework for BilevelOptimization:  Complexity Analysis and Application to Actor-Critic. Arxiv 2007.05170\n\n The authors pointed out the limitation that the target function is generally non-convex and therefore this paper can only guarantee convergence toward a saddle point rather than a global minimum. There are no specific concerns with respect to its social impact.",
            "This paper analyses the convergence of two time-scale TDC with general smooth function approximation. Previous analysis mainly focuses the linear approximation, but the nonlinear approximation is more powerful and its convergence analysis is more challenging; thus, this work is theoretically solid. The analysis covers both i.i.d. settings and Markov settings. \n\n  This paper analyses the convergence of two time-scale TDC with general smooth function approximation. Previous analysis mainly focuses the linear approximation, but the nonlinear approximation is more powerful and its convergence analysis is more challenging; thus, this work is theoretically solid. The analysis covers both i.i.d. settings and Markov settings. \n\nNo empirical study is carried out. If some reinforcement learning tasks are conducted to verify the theory, the results can be more convincing. \n\n1.\tIt is mentioned that the projection step in Line 6 of algorithm 1 is for the convenience of the analysis. So, does removing the projection destroy the convergence or yield a worse convergence rate? \nTypo: Line 103, Page 3, resent.\n No negative societal impact",
            "This paper develops the non-asymptotic error bound for an important RL algorithm TDC with general non-linear function approximation. The TDC algorithm is a commonly used one for off-policy policy evaluation in practice. Existing studies on non-asymptotic error bound mostly focus on the case with linear function approximation. This work extends the study to the case with non-linear function approximation. Several new challenges are addressed in this paper, the nonconvexity of the objective function, non-linear two time-scale update rule, time-varying projection and a tight tracking error analysis. Both the iid and Markovian settings are investigated. The convergence rate provided is $\\mathcal O(1/\\sqrt(T))$.\n  Pros:\n1. The proof generalizes the analysis for nonconvex optimization in [Ghadimi & Lan 2013] to the  TDC algorithm with a two time-scale structure, where the tracking error due to the two time-scale updates needs to be bounded, which is also their major technical novelty in the analysis. They bound the tracking error using the gradient norm $E[\\|\\nabla J(\\theta_t)\\|^2]$, which should also converge to zero. Other analysis of two time-scale algorithms either use a constant bound (resulting in a loose bound in the end) or using a mini-batch method (resulting in a large batch size ($1/\\epsilon$). Their idea of bounding the tracking error using $E[\\|\\nabla J(\\theta_t)\\|^2]$, which is the quantity that is being bounded, is novel and interesting. This technique could be useful for developing non-asymptotic error bound of other two time-scale algorithms with a non-convex objective function. \n\n2. Another novelty lies in the way they handle dependency between the projection tangent plane and the sample trajectory. For most existing studies with linear function approximation, the projection is fixed, and there does not exist such an issue. They first characterized the geometry of those functions, and then decoupled the dependency by exploiting the smoothness of the function approximator.\n\nCons:\nThere is no significant difference between the iid setting and the Markovian setting. The approach of handling the Markovian noise using a mixing argument is kind of standard in the current RL literature.\n\nQuality: The complexity of $\\mathcal O(1/\\sqrt(T))$ looks natural. This result matches with the complexity of optimizing a general smooth non-convex function in [Ghadimi & Lan 2013]. The proof looks correct. \n\nClarity: This paper is well-written. I enjoyed reading this paper. The proof of sketch part is clear and informative, where the major steps and technical novelty are highlighted. The table of contents in the appendix is helpful to navigate in the appendix. \n\nSignificance: As discussed above, the idea of bounding the tracking error using the gradient norm is novel, and is useful for analyzing other two time-scale algorithms with non-convex objectives.\n\n\n\n Some comments/questions are also listed below.\n\n1. The experiments could be moved to the main body of the paper as there is still room. \n\n2. In remark 2, it is discussed that the ratio $\\alpha/\\beta$ does not necessarily need to go to zero (in this paper alpha and beta are chosen to be constant at the order of $O(1/\\sqrt(T)$). This is kind of counterintuitive. Some more elaboration on this issue could help.\nIn assumption 4, it is assumed that the MDP is uniformly ergodic. It is kind of a standard assumption in RL analysis when there is Markovian noise. It would be helpful to discuss what kind of MDPs satisfy this assumption.\n\n3. Figs 4 and 5 in the appendix shall be aligned. \n\n4. Page 25, it is assumed that the step size $\\beta$ satisfies some condition $\\beta\\tau_\\beta C_\\phi^2\\leq 1/4$, which should also be stated in the theorem. "
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive and negative aspects of the paper. It acknowledges the paper's clarity and the surprising convergence rate but also points out limitations in the novelty of the analysis and the need for clearer explanations. The reviewer also provides constructive suggestions for improvement.",
            "The review expresses a positive sentiment by stating that the work is 'theoretically solid' and acknowledging the challenging nature of the analysis.",
            "The review expresses overall positive sentiment, highlighting the paper's novel contributions, technical novelty, clear writing, and potential impact on analyzing other two time-scale algorithms. Phrases like 'novel and interesting,' 'useful for developing,' 'well-written,' and 'I enjoyed reading this paper' indicate a positive evaluation."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The review offers both praise ('clearly written and easy to understand', 'surprisingly good') and criticism ('suffers from a similar novelty issue', 'need to be stated more clearly'). It also gives specific suggestions for improvement, making it a balanced assessment.",
            "The tone is supportive. The reviewer highlights the strengths of the paper ('theoretically solid') and offers constructive suggestions for improvement (empirical study). The language is encouraging rather than dismissive.",
            "The tone is supportive, evidenced by phrases like 'I enjoyed reading this paper' and the reviewer highlighting the paper's strengths and potential impact. The reviewer also offers constructive suggestions for improvement rather than focusing solely on criticisms."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the strengths of the paper, such as clarity and near-optimal convergence rate, while also pointing out areas for improvement, such as novelty in the Markovian setting and clarity of certain arguments. The reviewer offers constructive suggestions without contradicting their initial positive observations, maintaining a consistent and helpful tone throughout the review.",
            "The review is consistent as it provides constructive feedback, acknowledging the theoretical strength of the paper while suggesting improvements like empirical validation and clarifying algorithm details. There are no contradictory statements.",
            "The review consistently praises the paper's novelty and significance, particularly highlighting the novel techniques for handling non-linear function approximation in TDC and the approach to bounding the tracking error. While mentioning a minor con about the standard approach for Markovian noise, it does not contradict the overall positive assessment of the paper's contributions. The reviewer provides constructive feedback and questions, further supporting the consistency of the review in identifying both strengths and areas for improvement without internal contradictions."
        ]
    },
    {
        "paper_id": "iclr_2019_rkevMnRqYQ",
        "paper_title": "Preferences Implicit in the State of the World",
        "paper_abstract": "Reinforcement learning (RL) agents optimize only the features specified in a reward function and are indifferent to anything left out inadvertently. This means that we must not only specify what to do, but also the much larger space of what not to do. It is easy to forget these preferences, since these preferences are already satisfied in our environment. This motivates our key insight: when a robot is deployed in an environment that humans act in, the state of the environment is already optimized for what humans want. We can therefore use this implicit preference information from the state to fill in the blanks. We develop an algorithm based on Maximum Causal Entropy IRL and use it to evaluate the idea in a suite of proof-of-concept environments designed to show its properties. We find that information from the initial state can be used to infer both side effects that should be avoided as well as preferences for how the environment should be organized. Our code can be found at https://github.com/HumanCompatibleAI/rlsp.",
        "review_ids": [
            "BJliWzYh3m",
            "BylXVFpH0X",
            "r1xfyOLGTm",
            "Byxr6ix6nX",
            "rkl-7cZuo7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors propose to augment the explicitly stated reward function of an RL agent with auxiliary rewards/costs inferred from the initial state and a model of the state dynamics.  Intuitively, the fact that a vase precariously placed in the center of the room remains intact suggests that it is a precious object that should be handled with care, even though the reward function may not explicitly say so.  Technically, implicit rewards like these are inferred via inverse reinforcement learning: the agent (e.g. robot) first estimates the most likely reward functions to have guided existing agents (e.g. humans) by integrating over all possible state-action paths that could have led to the initial condition and evaluating their probability under different rewards (and hence different optimal policies).  The proposal is clever, but there are some philosophical hurdles to overcome and the experimental results offer little quantitative evidence to support this idea. \n\nIn my view, the biggest challenge is how to balance explicitly stated rewards with those inferred from the initial condition.  Section 5 briefly addresses this question, but essentially capitulates by saying, \"This trade-off is inevitable given our problem formulation, since we have two sources of information...and they will conflict in some cases.\"  I fear this conflict may be the rule rather than the exception.  For example, when I deploy my brand new dish-washing robot on my sink full of dirty dishes, my instructions to clean up will be in direct conflict with my past self's actions (or lack thereof).  How is the agent to know how strongly to adhere to the stated goals and when to deviate?  One possible solution is to only allow the inferred reward to affect features that are not explicitly included in the specified reward.  Neither the Additive nor the Bayesian combination methods have this property though. \n\nThe technical presentation could use some improvement.  The preliminaries in Section 3 do a decent job of introducing MDPs and IRL, but stop short of saying how the objective function for MCEIRL is actually computed.  Specifically, theta does not appear on the right hand side of Eq (1); implicitly, pi is a function of theta that is estimated, presumably, via value or policy iteration.  The marginal probability of the initial state and its gradients presented in Section 4.1 are the main technical contribution of the paper, but most of the key details are deferred to the appendix or referenced to Ziebart (2010).  For example, the dynamic programming algorithm for computing Eq (3) and the expectations over state-action paths in Eq (5) could use more discussion in the main text, as could some elements of the derivation of Eq (5).  \n\nThe experimental results are presented primarily in words (e.g. \"\\pi_spec walks over the vase while \\pi_deviation and \\pi_reachability both avoid it.\").  It would be helpful to see the resulting paths taken by the various agents, or even better, to see their learned reward functions alongside the true reward functions.  The only quantitative results are those in Figure 3, and unfortunately they are a bit confusing.  Why would we expect non-monotonic rewards at some temperatures?  Moreover, why are some reward \"percentages\" negative?  \n\nThe idea of leveraging the initial state for augmenting the reward function is clever, but there are a few shortcomings of the current paper.  There are basic concerns about how implicit and explicit rewards can be combined, and the technical presentation needs some improvement.  Most importantly, the experimental results do not show enough quantitative evidence of how the proposed method performs. \n\n[UPDATE] I appreciate the authors' detailed response and revisions to the paper.  I've updated my score accordingly.",
            "I have carefully gone through all the other reviews and the authors' response to them. I have also gone through some of the revisions made to the paper. The authors have added numerical experiments to address one of my main technical concerns (choosing the time horizon T) and have also added a large amount of useful discussion regarding combination of the specified rewards and inferred rewards (which other reviewers pointed out as well).  \n\nOverall, the paper introduces a novel and interesting idea (inferring preferences from the initial state of an environment), which I see as the primary contribution of the paper. The paper proposes algorithms that implement this idea and a number of experiments that support the idea. The authors are very clear about the limitations of the work and have a significant amount of discussion on how these may be addressed.  I believe that the ideas in the paper will lead to significant follow-up work (both from the authors themselves and others). Overall, I still believe that this paper makes a strong contribution and am happy to advocate for its acceptance. ",
            "This work proposes a way to infer the implicit information in the initial state using IRL and combine the inferred reward with a specified reward to achieve better performance in a few simulated environments where the specified reward is not sufficient to solve the task. The main novelty of this work is to reformulate the Maximum Causal Entropy IRL objective using just the initial state as the end state of an expert trajectory to infer the underlying preference. Overall the proposed approach is impressive and the intuition behind the paper is novel and easy to understand.\n\nMy main concerns are the following:\n- All the simulated experiments are able to demonstrate the effectiveness of the method, though they seem to be a bit too simplistic, e.g. known dynamics. As mentioned in Section 7, more real-environment experiments would make this method a lot stronger.\n- The way of choosing the distribution s_{-T} seems to require some sort of human preference, e.g. in the apple collection case,  s_{-T} has to be sampled from the distribution where there's no apple in the basket in order to make the algorithm to work. This assumption seems to make the implicit information of the initial state not so *implicit*. Besides, it's unclear how to choose the horizon T. It would be interesting to see how the value of T affects the performance.",
            "The framework of this work is Reinforcement Learning (RL) optimisation. The data consists of states of the space where the action takes place. Actions are possible, and they lead to possible transitions in the state space. A reward function assesses how adequate a state space is.\nThe main originality of the work is to use the initial state as a key information about the features that translate many desired state of background objects in a scene. An algorithm is built to make use of this information to build an ad hoc reward function, which specifies a good landscape of desired vs non-desired states of the space. An empirical evaluation of the introduced method is presented. It is rich and interesting, although hard to fully grasp for a non-expert.\n\nKey questions/remarks:\n - how different is your approach to a Bayesian approach with the combination of a likelihood (~reward) and prior (~initial state analysis) into a posterior distribution of the space? This seems to be the case in Section 5, where your alternative formulation clearly resembles a Lasso approach (which can be cast in a Bayesian framework).\n - I quite like your decomposition of your ideas into many titled paragraphs. The drawback is that there is sometimes a lack of connections between the many ideas you combine. A would see a big figure in the form of a map as a central contribution of your work to explain the different bits. Still, I appreciate the effort to have a synthetic contribution!\n\nSmall remarks:\n - the abstract could be improved to provide an easier reading experience\n - first time IRL on p2 is mentioned, without a prior explanation of the acronym\n - the world is already optimised for human preferences: yes and no, this is one of your (strong?) assumptions. The robot could well move the vase to a location which is acceptable. Or put it back. \n - on p3, beg.  of Section 3, explain the decomposition of r(s) = \\theta^{T}f(s).\n - in IRL paragraph: say the elements of \\tau_{i} are s.t. the transitions need be possible.\n - p8 'access to a simulator': what can be simulated if very little is know about the background, but via an initial state?\n - past point of the discussion: I simply don't get it!?!",
            "This paper considers the problem of inferring unspecified costs in an RL problem (e.g., inferring that vases in a room should not be broken). The primary insight is that the initial state of the environment conveys rich information about such unspecified costs since environments are often optimized for humans. The paper frames the problem of inferring unspecified costs from the initial condition as an inverse reinforcement learning (IRL) problem and applies the Maximum Causal Entropy IRL framework to solve this problem. Two methods are proposed for combining the inferred unspecified costs with specified costs. The efficacy of the proposed approach is demonstrated on a number of simulated examples.\n\nOverall, I was impressed by this paper and I believe that it makes a strong contribution. The paper presents an interesting perspective on a relatively old problem (the frame problem in AI). The primary intuition of the paper (that the initial state conveys information about unspecified costs) and the framing of this problem in terms of IRL is novel. The simulated examples (while relatively simple in terms of the number of states and actions) are informative and demonstrate the strengths of the approach (and also some of the weaknesses; the paper is explicit about the current challenges). The paper is very clearly written and is easy to read.\n\nMy concerns are relatively minor:\n- Perhaps the weakest bit of the paper is Section 5 (combining the specified reward with the inferred reward). As presented, the Additive method is somewhat hard to justify. However, the simulated results suggest that the Additive method performs slightly better than the Bayesian method. I would suggest either presenting a bit more intuition and justification for the Additive method or getting rid of this method altogether (since the results are not too different from the Bayesian method, which seems a bit more justifiable).\n- One practical (and potentially important) question that the paper does not directly address is the problem of choosing the time horizon T (i.e., the time horizon for the past). In the standard IRL setting, it is reasonable to assume that the time horizon is given (since the demonstrations have an associated horizon). However, it is not entirely clear how to choose T in the setting considered in this paper. It is possible that if one chooses T to be too small, the inferred rewards will not be accurate (and one may have to look further back in the past to correctly infer rewards). A discussion of this issue and possible ways to choose T would be helpful.\n- In Section 6.1 (baselines), the paper mentions that \"while relative reachability makes use of known dynamics, it does not benefit from our handcoded featurization\". Is it possible to modify the relative reachability method to also take advantage of the handcoded features, perhaps by considering dynamics over the feature space? If not, a sentence explaining that this is not straightforward would be helpful.\n- In the related work section (and also in the introduction), I would recommend being more explicit about precisely what the differences are between the presented work and the approaches presented in (Krakovna et al. 2018) and (Turner, 2018). The paper is currently slightly vague about the differences.\n- Currently, the title of the paper is a bit uninformative. On first reading the title, I expected a paper on control theory; the title makes no mention of unspecified costs, or reinforcement learning, or humans, etc. I believe that this is a good paper and that the paper would have more readers if the title was more inline with the content of the paper. Of course, this is at the discretion of the authors. My suggestion would be something along the lines of \"Inferring Unspecified Rewards in RL from the Initial State\".\n\nTypos:\n- Pg. 1, second paragraph, 3rd line: there is a placeholder for citations.\n- Periods are missing at the end of equations.\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the balance between implicit and explicit rewards, the technical presentation, and the lack of quantitative evidence in the experimental results. Phrases like \"philosophical hurdles to overcome\", \"I fear this conflict may be the rule rather than the exception\", \"technical presentation could use some improvement\", and \"experimental results do not show enough quantitative evidence\" indicate a negative sentiment. Although the reviewer acknowledges the idea is clever, the overall assessment points to significant shortcomings.",
            "The reviewer explicitly states they \"believe that this paper makes a strong contribution\" and are \"happy to advocate for its acceptance.\" They also highlight the paper's novel idea, clear presentation of limitations, and potential for future work.",
            "The review starts with positive feedback, stating the approach is \"impressive\" and the intuition is \"novel and easy to understand.\" While concerns are raised, the initial positive framing suggests an overall positive sentiment.",
            "The review contains both positive and negative feedback. It praises the originality and empirical evaluation but also points out areas for improvement like clarity and connections between ideas. The reviewer expresses difficulty in understanding certain parts.",
            "The reviewer states \"Overall, I was impressed by this paper and I believe that it makes a strong contribution.\" and describes the paper's perspective as 'interesting' and the framing as 'novel'. The reviewer also finds the simulated examples informative and the paper clearly written and easy to read."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses direct and critical language to point out flaws in the paper. Phrases such as \"essentially capitulates\", \"I fear this conflict may be the rule rather than the exception\", \"technical presentation could use some improvement\", and questions like \"How is the agent to know how strongly to adhere to the stated goals and when to deviate?\" contribute to a critical tone. The reviewer also directly criticizes the presentation of the experimental results.",
            "The reviewer uses phrases like \"useful discussion,\" \"novel and interesting idea,\" \"strong contribution,\" and \"happy to advocate for its acceptance,\" demonstrating a supportive and encouraging tone.",
            "The review acknowledges the strengths of the paper (\"impressive\", \"novel and easy to understand\") but also raises specific concerns and suggestions for improvement, presenting a balanced perspective.",
            "The review is balanced, offering both positive feedback ('rich and interesting', 'appreciate the effort') and constructive criticism ('hard to fully grasp', 'lack of connections', 'I simply don't get it'). It uses a mix of formal language and direct questions.",
            "The reviewer expresses enthusiasm for the paper's contribution and offers constructive criticism to improve the work. Phrases like \"I was impressed\" and \"I believe that it makes a strong contribution\" indicate a supportive tone. The reviewer focuses on providing suggestions rather than harsh criticism, indicating a desire to help the authors improve their work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently points out weaknesses and shortcomings of the paper in different aspects (philosophical, technical presentation, experimental results).",
            "The review is consistently positive, highlighting the paper's novelty, the authors' efforts to address concerns, and ultimately recommending acceptance. There are no contradictory statements or conflicting opinions expressed within the review.",
            "The review is consistent because it provides both positive feedback (novel and easy to understand approach) and constructive criticism (simplistic experiments, assumptions about initial state distribution, and unclear horizon T). The reviewer's concerns are valid points for improvement and do not contradict the initial positive assessment.",
            "The review is consistent in its feedback. It appreciates the originality and empirical evaluation, while constructively pointing out areas for improvement regarding clarity, connections between ideas, and justification of certain assumptions. The reviewer's questions and remarks consistently aim to enhance the paper's clarity and rigor without contradicting each other.",
            "The review is consistent because it starts with an overall positive assessment of the paper, highlighting its strengths and novelty. The subsequent points are presented as minor concerns and suggestions for improvement, which do not contradict the initial positive evaluation. The reviewer maintains a constructive tone throughout, indicating a consistent perspective on the paper's value and areas for refinement."
        ]
    },
    {
        "paper_id": "nips_2021_OItvP2-i9j",
        "paper_title": "Closing the Gap: Tighter Analysis of Alternating Stochastic Gradient Methods for Bilevel Problems",
        "paper_abstract": "Tianyi Chen, Yuejiao Sun, Wotao Yin",
        "review_ids": [
            "p4Nxp6d9dew",
            "gJiokPc3Jac",
            "GOPR-O52PAs",
            "g801np4l5Wa",
            "GQb4uAgxs-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies the problem of solving a stochastic bilevel optimization problem, where the upper level optimization problem depends on the solution of the lower level optimization problem. The authors show that this stochastic bilevel optimization problem covers three different classes of problems (stochastic compositional, min-max, and bilevel optimization). The authors then unify the three SGD-type methods for the respective problems into a single approach called ALSET. Besides, compared with previous work, they also improve the sample complexity from $\\mathcal{O}(\\epsilon^{-2.5})$ to $\\mathcal{O}(\\epsilon^{-2})$.   I found the manuscript to be clearly written and technically sound. The proposed algorithms are also easy to understand. The authors give a detailed comparison between their result and previous results, where the improvement in the sample complexity is clearly stated. The only suggestion I have is can you also provide some numerical experiments to support your theoretical results?  There is no negative societal impact. ",
            " Thank you for providing the numerical experiments. ",
            "This paper considers the problem of stochastic nested optimization which unified the stochastic compositional optimization and stochastic min-max optimization. It presents a tighter theoretical analysis for classical algorithm which uses vanilla SGD-type update. The sample complexity of this improved analysis matches those with modifications.   This paper is well-written. I have some comments below.\n\n1. This paper improves the sample complexity for SGD-type algorithms to solve stochastic nested optimization. The algorithm and some results (like observation (6) and estimation (8)) are already presented in previous literature (e.g., Ghadimi and Wang (2018)). Theorem 1 gives a tighter bound of such algorithm where they utilize the observations in Lemma 2. The authors give a clear presentation of their technique and highlight the key steps of the proof towards Theorem 1. However, it is my understanding that the novelty of this paper is merely the observation of lemma 2 and the tighter analysis following it.\n\n2. Under the unified structure, they extend their theoretical results to stochastic compositional optimization as well as stochastic min-max optimization. They also give an application example of the actor-critic method with linear value approximation.\n\n3. However, this paper is not complete. A conclusion part should be presented.\n\nReferences mentioned above:\n\n1. Ghadimi, Saeed, and Mengdi Wang. \"Approximation methods for bilevel programming.\" *arXiv preprint arXiv:1802.02246* (2018). None.",
            "This paper studies the altenating gradient descent method for stochastic nested problems.  It uses a general framework which contains the compositional gradient descent, the minimax problem, and the actor-critic method. It also provides a tighter analysis of the convergence result and better convergence rates in many cases.  The contributions of the work is mainly theoretical.  Although the proof technique is fairly standard, it achieves better rates with a simple single-simescale algorithm. This work also points out the importance of the hidden smoothness of the optimizer of the lower level. It can be very important for further works in the field. The general framework allows us to easily generalize different variants of gradient descent methods to different settings. As a result, there are many possible lines of research following this work.\n\nThe paper is clearly written and the results are well-organized. Previous works are cited properly. The authors addressed the limitations adequately. ",
            "The paper proposes a tighter analysis of the complexity of alternating gradient descent on stochastic bilevel optimization problems. The tighter analysis is then applied on several nested optimization problems like min-max optimization, compositional problems as well as actor critic methods for MDP problems. A key step in the analysis is leveraging the smoothness of the lower optimization problem with respect to the decision variables of the upper level problem.  Overall I think the paper is well written and the idea of proposing a unified analysis for all the optimization problems discussed in this work can have significant impact. Also closing the gap between the complexity of simple SGD and bilevel optimization is definitely important in the process of delineating which aspects of bilevel optimization are actually hard.\n\nWhile the smoothness of the lower level problem has been used in prior work (e.g. [27]), it is the smoothness of the gradients proved in Lemma 2 that opens the way to the improved rate. While Lemma 2 itself is rather straightforward to prove,  how and why it can be used in the proof is definitely not. Small tweaks to existing approaches are not sufficient for the reasons summarized under Equation 20. I suggest to the authors to expand more on this reasons in the Appendix to make the paper more self contained. \n\nOverall a very strong and well written submission. Limitations and negative social impact are adequately discussed."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses positive opinions about the paper, stating it is \"clearly written and technically sound\" and that the algorithms are \"easy to understand.\" The reviewer also highlights the improvement in sample complexity and concludes with a suggestion rather than a criticism, indicating overall satisfaction.",
            "The reviewer expresses gratitude ('Thank you'), indicating a positive reception of the provided experiments.",
            "The review acknowledges the paper's contributions (tighter analysis, clear presentation) but also points out limitations (lack of novelty, missing conclusion). The overall sentiment is balanced, neither strongly positive nor negative.",
            "The review highlights several positive aspects of the paper, including its use of a general framework, tighter analysis, better convergence rates, clear writing, well-organized results, proper citation of previous works, and adequate addressing of limitations. Phrases like \"better convergence rates,\" \"simple single-timescale algorithm,\" \"importance of the hidden smoothness,\" and \"many possible lines of research\" indicate a positive evaluation.",
            "The review expresses overall positive feedback, highlighting the paper's well-written nature, significant potential impact, and the importance of closing the complexity gap between SGD and bilevel optimization. Phrases like \"very strong and well written submission\" further reinforce this positive sentiment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses encouraging language like \"clearly written,\" \"easy to understand,\" and highlights improvements. The suggestion is framed as a question (\"can you also provide...\") rather than a demand, indicating a supportive and constructive attitude.",
            "The reviewer uses a polite and appreciative tone, thanking the authors for their contribution. This suggests a supportive stance towards the work.",
            "The review provides both positive feedback (well-written, clear presentation) and constructive criticism (lack of novelty, missing conclusion). It uses formal language and avoids overly emotional expressions, indicating a balanced and objective tone.",
            "The tone is supportive, emphasizing the strengths of the paper and encouraging future research. Phrases like \"clearly written,\" \"results are well-organized,\" \"addressed the limitations adequately,\" and \"very important for further works in the field\" contribute to this supportive tone.",
            "The tone is supportive, evident in phrases like \"Overall I think the paper is well written\" and \"Overall a very strong and well written submission.\" The reviewer also offers constructive suggestions, indicating a desire to help improve the work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive, praising the paper's clarity, technical soundness, and contributions. The suggestion for numerical experiments is a constructive suggestion to further strengthen the paper and does not contradict the positive assessment.",
            "The review consists of a single positive statement and does not contain any contradictory information.",
            "The review is consistent because it provides both positive feedback (well-written, tighter analysis) and constructive criticism (limited novelty, missing conclusion) without contradicting itself. The reviewer acknowledges the contribution while also pointing out limitations, resulting in a balanced and consistent assessment.",
            "The review is consistently positive, highlighting the paper's contributions, clarity, and potential impact without any contradictory statements or negative feedback.",
            "The review consistently expresses a positive opinion about the paper, praising its contributions, analysis, and writing style. The reviewer highlights the paper's strengths, such as the tighter analysis, unified approach, and the importance of closing the complexity gap. While suggesting a minor improvement (expanding on Equation 20), the overall tone remains consistently positive and supportive, indicating a consistent assessment of the paper's quality."
        ]
    },
    {
        "paper_id": "iclr_2021_naSAkn2Xo46",
        "paper_title": "Factored Action Spaces in Deep Reinforcement Learning",
        "paper_abstract": "Very large action spaces constitute a critical challenge for deep Reinforcement Learning (RL) algorithms. An existing approach consists in splitting the action space into smaller components and choosing either independently or sequentially actions in each dimension. This approach led to astonishing results for the StarCraft and Dota 2 games, however it remains underexploited and understudied. In this paper, we name this approach Factored Actions Reinforcement Learning (FARL) and study both its theoretical impact and practical use. Notably, we provide a theoretical analysis of FARL on the Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC) algorithms and evaluate these agents in different classes of problems. We show that FARL is a very versatile and efficient approach to combinatorial and continuous control problems.",
        "review_ids": [
            "5DCNyzPK6y",
            "si4h0Z-Xw9G",
            "-hi2Tb0yWfC"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "### Summary\nThe paper presents a study of factored action spaces for RL problems, and two basic forms of this,\ni.e., independent factorization (IF) and autoregressive factorization (AF).\nNote: these acronyms are my own. Building on these two types of factorization, the paper presents: \n(a) derivations of the entropy and KL-divergence for these policy factorizations;\n(b) PPO and SCA adaptations for IF + AF; and\n(c) three categories of experimental results.  \nThe experiments compare, in separate scenarios:  (i) AF-PPO vs AF-SAC; (ii) AF-PPO vs IMAPLA and non-autoregressive F-PPO; \n(iii) IF-SAC vs IF-PPO. \nOverall, the experiments show some of the possible potential of factored action spaces with PPO and SAC.\n\n### Strengths\n- I believe that factored action space are an under-explored area of research, and \n  understanding the benefits and limitations of these factorizatios is an important area,\n  as well as understanding how popular algorithms such as PPO and SCA can be adapted to use these factorizations.\n- The adaptations of PPO and SCA and the related derivations, while not entirely surprising, are useful.\n- The experiments show some of the potential of factored action spaces, both IF and AF.\n  The experimental parameters appear to be well documented, c.f., Appendix C.3.\n\n### Weaknesses\n- There are no guidelines or experiments to help determine when the factored approaches should be used,\n  i.e., when do they do better and when they do worse.  When is it harmful to use a factored approach?\n  Similarly, when should IF vs AF be used?  \n- Related to the point above, the experiments generally reflect the idea that the implementations can be made to work on some examples, rather than understanding where these factored actions spaces sit in the broader universe of algorithmic choices. What motivated the given choices of three benchmark problems, and the specific  (limited) comparisons shown for each benchmark?  It is difficult to fully understand the \"take home message\"  of each of the experiments.\n- It would be good to understanding more about the relationship with possible continuous action derivations\n\n### Recommendation\nI am on the fence regarding this paper.\nExploring and understanding factored action spaces is important, and the paper presents a reasonable step\nin this direction, and will therefore provide a foundation for further work.\nHowever there is little provided in the way of insights as to understanding the types of situations\nwhere factored representations should be used, and how this plays our for PPO and SCA.\nIf pressed, I currently lean slightly in favor.\n\n### Questions\n- What is the impact of the action ordering for the autoregressive factorization?\n- It is stated that there is no inter-correlation between action components for the MuJoCo benefits.\n  What evidence do you have for this?  The statement is unintuitive to those working on continuous control problems.\n  Why can't there be multimodal areas of the state space where the action decisions need to be coordinated?\n\n### Additional Feedback\n- section 5.3 Critic Architecture\n  \"As there are n1 ... nn possible actions\".  The notation is flawed, given that n is used to represent \n  both the number of dimensions and the discretization. Similarly, later it is stated:\n  \"these independent n Q-value functions\" is confusing, given that each of N dimensions presumably each need n Q-value\n  functions in the discrete action setting.\n- section 5.3 \"we reuse the formulation proposed by Metz et al. (2017)\n  and \"The top MDP corresponds to the MDP at hand in which the action space is factored\"\n  Should make it clear that this refers to Figure 1 of that paper.\n  And I would refer to the top MDP as being the original unfactored MDP.\n- section 5.3 Soft Policy Evaluation \"computed directly through an inner product\"\n  Could in include a forward reference to eqn (6)?\n- equation (5): missing right bracket\n- section 6.1 \"both factored agents reach completion in a few time steps, see Figure 1c\"\n  Figure 1c shows 20k - 40k timesteps, so while that is small, it is strange to call that \"a few time steps\"\n- section 6.2: \"our agent chooses three actions [sic] choices among 19 at each time step.\"\n  Why not explicitly state 3x19 = 57 choices at each time step (3 successive choices of 19 each).\n- The relationship to (Tang and Agrawal 2020) could be better explained.\n- A diagrammatic abstraction of the different forms of factorization could be very useful to the reader.\n\nUpdate:\n  After reading the other reviews and the responses, I have changed my score to 5: marginally below acceptance, due to the framing and related work issues, as discussed by R2 and R4.  There is potential here, but it will benefit from strong revisions.\n",
            "##########################################################################\nSummary:\n\nThe paper studies policy optimization in multidimensional action spaces. They consider atomic factorization of the action space (i.e. action space is factored into sub-action spaces, one per action dimension). In this setting, the authors consider two well-known policy representation techniques: 1) independent sub-policies (diagonal-covariance policies over the sub-action spaces) and 2) sequential/autoregressive policies (an ordering of the sub-action spaces is assumed a priori and the sub-policies receive as input the state and the selected sub-actions for the preceding sub-action spaces). The authors develop methods based on these two factorization techniques for discrete versions of PPO and SAC (called FPPO and FSAC) and evaluate them on Gym Platform, Google Football, and discretized MuJoCo tasks.  \n\n##########################################################################\nReasons for score:\n\nI believe this paper fails to position itself appropriately with respect to the literature. This makes it difficult to assess what the main contributions are and whether sufficient new methodology is proposed. Additionally, the paper misses on several fundamental experiments for the results to be convincing. Also, the paper does not set a clear agenda for what would be interesting to see in the results; Is scalability to large action spaces being investigated (in which case, comparison with the non-factored baselines of PPO and SAC should be included)? Is it the improvements of the discrete versions over the continuous versions of the methods that are interesting (in which case, the results for the continuous versions on the MuJoCo tasks should be reported)?  \n\n##########################################################################\nPros:\n\n- Factorizing action spaces is an interesting approach for scaling to high-dimensional action spaces. Also, they have been shown in several recent studies to enable discrete-action methods to outperform numerous continuous-control methods. So studying them more extensively is useful. \n\n- The paper nicely outlines how PPO and SAC should be configured to work with independent and autoregressive policies under the atomic factorization of the action space. \n\n##########################################################################\nCons:\n\n1) The paper generally does not position itself appropriately with respect to the literature. Below is my overview of the paper with additional related works (some are suggestions that would enhance the paper):  \n\n- Branching Q-learning (BQL) in Ref. [I] is similar (if not the same) to the independent critic of FSAC. Using this approach they scale to domains with 33^17 discrete actions. \n \n- Autoregressive critic of FSAC is the same as that in Metz et al. (2017) (also stated in the paper I believe).\n\n- Independent versions of numerous PG methods have been previously developed and studied extensively by Tang and Agrawal (2020). Also, the original TRPO paper was used with independent discrete sub-action policies in Atari tasks (if I remember correctly). Therefore, there is nothing new about FPPO-independent (also stated in the paper I believe).  \n\n- Other methods for learning the independent critic in FSAC are also possible but are not discussed in the paper (see e.g. Ref. [II]). Moreover, Ref. [II] uses independent policies with A2C.\n\n- When independent policies are used in PG, a better baseline can be used which provably reduces variance [III]. Discussion of such a baseline would be very useful in this paper which somewhat serves to summarize deep RL in factored action spaces.\n\n- Ref. [IV] combines autoregressive and independent proposal policies for approximate Q-maximization in Q-learning. In this way, they scale to very high-dimensional action spaces. This work is able to handle hybrid action spaces without discretization. A discussion and positioning with respect to this paper could be valuable.  \n\n\n2) Experiments fall very short in my opinion. Below are some of the experiments that I think are necessary to include: \n\n- Figure 1a,b: Why not run FSAC?\n\n- Figure 1c: I believe the autoregressive FPPO and FSAC are reported in Platform. But why not independent? Why not report the standard (non-factored) baselines of PPO and SAC?\n\n- Figure 2: Compare against the continuous versions of SAC and PPO on MuJoCo.\n\n- Figure 2: Why not evaluate autoregressive policies in MuJoCo? Metz et al. did that with DQN.  \n\n\n3) No new factorization schemes are explored in this paper. For instance, Ref. [V] explores a mixture of independent and sequential action-value function representations. A discussion of the spectrum of other possible factorizations would be interesting. \n\n\n[I] Action Branching Architectures for Deep Reinforcement Learning, AAAI 2018.\n[II] Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement Learning, arXiv 2017. \n[III] Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines, ICLR 2018.\n[IV] Q-Learning in enormous action spaces via amortized approximate maximization, arXiv 2020. \n[V] Inferring DQN structure for high-dimensional continuous control, ICML 2020. \n\n##########################################################################\nQuestions during the rebuttal period:\n\n1) In light of new related works together with those included in the paper, I believe the novelty of the paper currently is in developing the policy optimization updates for autoregressive policies. The rest (independent policy optimization updates, independent critic, and autoregressive critic) are not novel as far as my assessment goes at this point. Is this correct?  \n \n2) Does \"without autoregressive\" (in Figure 1) imply \"independent\"? Or does it refer to the standard (non-factored) versions of PPO and SAC? If not, I need clarification on what it exactly means.\n\n3) This paper considered stochastic policies. I'm curious about any potential use for extending it to deterministic policies; e.g. could it be useful to use an autoregressive policy network with DDPG?\n\n4) I think that the manual ordering of the sub-action spaces in an auto-regressive policy could introduce bias (i.e. I think there could be a setting that some orderings would make it impossible to learn the optimal policy). Can you comment on this?\n\n##########################################################################\nMinor comments:\n\n- The title is too broad in my opinion. Unless the paper also incorporates analysis and results for purely action-value methods (e.g. DQN-based agents such as Sequential and Branching Q-learning), the paper should specify that policy optimization methods are of focus. Also, stating atomic factorization in multidimensional action spaces instead of only \"factored\" could clarify the kind of factorization that is the subject of this paper.   \n\n- \"FPPO was trained only for 2 days on 4 CPU cores while IMPALA was trained with 150 CPU cores.\":\nThis is not very useful without providing training-time of IMPALA.\n\n- The word action is frequently used instead of action dimension and sub-action.\n\n- \"With this transformation, both factored agents reach completion in a few time steps, see Figure 1c\": \nSaying \"a few time steps\" for a plot that goes to 90k time steps is somewhat strange.  \n\n- This work claims to develop techniques for dealing with hybrid action spaces. But in the experiments, this is not demonstrated. In fact, the action space in Gym Platform features discrete and continuous sub-action spaces, but they are discretized. As such, I think this paper should state dealing with discrete action spaces or continuous ones via discretization. \n",
            "##########################################################################\n\nSummary:\n\nThe paper highlights the difficulty of training with large action space in reinforcement learning. This is usually difficult due to the vast number of possibilities during exploration. They address this issue by studying existing approaches of splitting the action into a finite number of sub-actions and then sampling each sub-action independently or auto-regressively. This leads to a reduction in the candidate actions during exploration which would improve the sample efficiency.  Also, This splitting is referred to as \u201cFactorization of Action Space\u201d. \n\nThey majorly study this factorization with PPO and SAC and show how to estimate kl-divergence and entropy in each of these methods. Also, they introduce the notion of using independent q-values for each of the sub-actions which semantically represents expected \u201csub-action - value\u201d for any action in the current policy with that sub-action.\n\nThey show their results on multi-agent and mujoco tasks.\n\n##########################################################################\n\nReasons for score: \n\nOverall, I do like the paper . However, at this moment, I will keep the paper below acceptance. This is due to the issues raised in Cons section which I believed should be addressed/discussed before I revise my score.\n\nIn particular, I would expect authors to have at least  PPO and SAC baseline in their work.\n \n##########################################################################\n\nPros: \n\n-Large action space has been one of the bottlenecks for scalability in reinforcement learning and the paper studies factorization for on-policy methods which to my understanding has not been done before. \n\n-They provide theoretical analysis for estimation of entropy and kl-divergence in PPO and SAC.\n\n-They show their results on both multi-agent settings as well as control.\n##########################################################################\n\nCons:\n\n- They merely make a comparison between FPPO and FSAC which are the factored versions of respective algorithms and show that FSAC seems to be more robust and has better sample efficiency. HOWEVER, they don\u2019t make comparison with the unfactored ( original) versions of these algorithms. I believe without the comparison with the baseline PPO and SAC, one cannot establish the gain in sample efficiency.\n\n- The number of discrete sub-actions(\u201cm\u201d) for the policy is treated as a hyper-parameter. This is one of the core challenges for continuous control tasks as the environments could be highly sensitive to this choice. They don\u2019t show any concrete evidence for the choice of number of sub-actions made by them, in particular for control tasks.\n\n- How does one decide on the order of the autoregressive actions? This is not clear from their work. Do they manually analyze each sub-action and decide on the order? I believe this order could play a significant role for performance.\n\n##########################################################################\n\nQuestions during the rebuttal period: \n \nPlease address and clarify the cons above \n \nOptional: \nYou may want to cite the following:\n\nQ-LEARNING IN ENORMOUS ACTION SPACES VIA AMORTIZED APPROXIMATE MAXIMIZATION (https://arxiv.org/abs/2001.08116).\nThis paper also talks about auto-regressive actions.\n\n\nThe following series of work involves the decomposition of Q-values for Actor/Critic and Q-learning approaches in multi-agent settings. The major difference is that instead of treating the reward as a vector where each reward component belongs to a specific q-values;  you are giving the same reward to each Q-value.\n \n- Value-Decomposition Networks For Cooperative Multi-Agent Learning\n- QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\n- Hybrid Reward Architecture for Reinforcement Learning\n- Explainable Reinforcement Learning via Reward Decomposition\n\n##########################################################################\n\nI would be happy to revise my score post clarifications from the authors.\n"
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review acknowledges both strengths and weaknesses of the paper. While appreciating the importance of the research area and the usefulness of the adaptations, it also points out significant gaps in understanding and application. The final rating reflects this balanced view.",
            "The reviewer believes the paper lacks novelty and has insufficient experimental validation, indicating a negative sentiment.",
            "The reviewer expresses that they 'do like the paper' but ultimately keeps the paper 'below acceptance' due to the issues raised in the 'Cons' section. This indicates a negative sentiment overall."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive aspects ('Strengths') and negative aspects ('Weaknesses') of the paper. It uses constructive criticism and suggests improvements, indicating a balanced perspective.",
            "The reviewer uses strong negative language, such as \"fails to position itself appropriately,\" \"misses on several fundamental experiments,\" and \"Experiments fall very short in my opinion.\" The reviewer also provides detailed criticisms and suggestions for improvement.",
            "The review points out several weaknesses in the paper, such as the lack of comparison with baseline algorithms, the arbitrary choice of the number of sub-actions, and the unclear methodology for determining the order of autoregressive actions. These criticisms are direct and highlight specific areas of concern."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its assessment. It acknowledges the potential of the research area and the authors' contributions (adaptations, experiments) but consistently points out the lack of deeper insights, guidelines for practical use, and unclear positioning within the broader field. The strengths and weaknesses sections, as well as the recommendation and update, all reflect this balanced but ultimately critical perspective.",
            "The review is consistently critical of the paper, pointing out weaknesses in literature positioning, novelty, and experimental validation. While acknowledging some positive aspects like the interesting approach and clear method descriptions, the reviewer's main concerns are consistently focused on the lack of novelty and insufficient experimental support. The questions raised during the rebuttal period further reinforce these concerns and seek clarification on the paper's contributions and limitations. There are no contradictory statements or shifts in the reviewer's overall assessment.",
            "The reviewer's assessment is consistent. They state they like the paper overall but are keeping it below acceptance due to the issues raised in the 'Cons' section. The 'Cons' section clearly outlines specific weaknesses, primarily the lack of comparison with baseline PPO and SAC, which directly justifies their decision to keep the paper below acceptance until these points are addressed. The 'Reasons for score' section explicitly mentions the expectation of PPO and SAC baselines, which aligns perfectly with the first point in the 'Cons' section. There are no contradictory statements, and the reviewer's concerns logically lead to their current score and their willingness to revise it after addressing the cons."
        ]
    },
    {
        "paper_id": "iclr_2019_SJldZ2RqFX",
        "paper_title": "D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation",
        "paper_abstract": "Positive Unlabeled (PU) learning consists in learning to distinguish samples of our class of interest, the positive class, from the counter-examples, the negative class, by using positive labeled and unlabeled samples during the training. Recent approaches exploit the GANs abilities to address the PU learning problem by generating relevant counter-examples. In this paper, we propose a new GAN-based PU learning approach named Divergent-GAN (D-GAN). The key idea is to incorporate a standard Positive Unlabeled learning risk inside the GAN discriminator loss function. In this way, the discriminator can ask the generator to converge towards the unlabeled samples distribution while diverging from the positive samples distribution. This enables the generator convergence towards the unlabeled counter-examples distribution without using prior knowledge, while keeping the standard adversarial GAN architecture. In addition, we discuss normalization techniques in the context of the proposed framework. Experimental results show that the proposed approach overcomes previous GAN-based PU learning methods issues, and it globally outperforms two-stage state of the art PU learning performances in terms of stability and prediction on both simple and complex image datasets.",
        "review_ids": [
            "HkxbRJNCRm",
            "BkePN_Q0Am",
            "B1lmrzQ937",
            "BkxY2b-5hX",
            "SygDGpKthQ",
            "SJlKFsUy3m"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I know the goal of two stage PU---it is anyway similar to GenPU. I just cannot comprehensively follow the logic of your use of GANs, because Figure 1 is unclear to me. In PU learning, you cannot apply GANs without any modification, otherwise you can only generate P or U data that is not your goal.\n\nIn GenPU, there are 2 Gs and 3 Ds, and experimentally this is the minimum number if we want to also generate P data; it can be reduced to 1G and 2Ds where 1D ensures that realP + genN approximates realU and 1D ensures that genN doesn't approximate realP (I am not an author of GenPU). In both ways, GenPU introduced the cluster assumption that is not assumed in other GANs (see Figure 2 of GenPU where p_{gn}(x) consistently has smaller supports than p_n(x), and the assumption \"p_{gn}(x) almost never overlaps with p_p(x)\" in their theoretical analysis). I personally think this idea of using GANs is intuitively for PU learning.\n\nOn the other hand, D-GAN has 1G and 1D and I would like the authors to explain in the introduction why 1D is enough for identifying p_n(x) from p(x) given p_p(x) where p(x) and p_p(x) are only approximately known via data. The authors only given some equations without intuition/motivation and they are still unclear to me. Note that in PU learning, there is no PU *risk* and the expected risk is shared by PN learning and PU learning; instead, PN and PU learning have their own *risk estimators* that can estimate the unique expected risk from PN data and PU data, respectively. So what do you mean by defining a new expected risk as a performance measure to be minimized?",
            "I am not an expert on GANs, so I still cannot follow why D-GAN is easier to benefit from recent advances whereas GenPU is harder. Is it something like spectral normalization can be used in D-GAN but cannot be used in GenPU? If so, why?",
            "[Summary]\nPU learning is the problem of learning a binary classifier given labelled data from the positive class and unlabelled data from both the classes. The authors propose a new  GAN architecture in this paper called the Divergent Gan (DGAN) which they claim has the benefits of two previous GAN architectures proposed for PU learning: The GenPU method and the Positive-Gan architecture. The key-equation of the paper is (5) which essentially adds an additional loss term to the GAN objective to encourage the generator to generate samples from the negative class and not from the positive class. The proposed method is validated through experiments on CIFAR and MNIST.\n\n[Pros]\n1. The problem of PU learning is interesting.\n2. The experimental results on CIFAR/MNIST suggest that some method that the authors coded worked at par with existing methods.\n\n[Cons]\n1. The quality of the writeup is quite bad and a large number of critical sentences are unclear. E.g.\na. [From Abstract] It keeps the light adversarial architecture of the PGAN method, with **a better robustness counter the varying images complexity**, while simultaneously allowing the same functionalities as the GenPU method, like the generation of relevant counter-examples.\nb. Equation (3) and (4) which are unclear in defining R_{PN}(D, \u03b4)\nc. Equation (6) which says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense.\nd. The distinction between the true data distribution and the distribution hallucinated by the the generator is not maintained in the paper. In key places the authors mix one with the other such as the statement that supp(Pp (Xp )) \u2229 supp(Pn (Xn )) \u2192 \u2205\nIn short even after a careful reading it is not clear exactly what is the method that the authors are proposing.\n\n2. Section 2.2 on noisy-label learning is only tangentially related to the paper and seems more like  a space filler.\n\n3. The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs worse than other PU learning methods which does not make sense. Because of this I am not quite convinced by the experiments.",
            "The motivation of the work is not clear but the novelty seems to be present.\n\nThe paper is very hard to follow as the problem description and intuition of the D-GAN is not clearly written.\n\nBased on the experiments, the proposed method achieves marginal improvement in terms of F1 score but sometimes also slightly lower performance than other GAN based such as PGAN, so the impact of this work to solve positive unlabelled data problem is not evident. \n\nI am personally not as familiar with the PU problem and existing frameworks so my confidence in the assessment is low; my main experience is in the computer vision for autonomous driving and sparse coding.\n\nBut my feeling is this paper is marginally below the threshold of acceptance.",
            "This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues.\n\n*****\n\nThe clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not.\n\nMoreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me.\n\n*****\n\nThe paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in \"learning classifiers from only positive and unlabeled data\", KDD 2008; the latter problem setting was proposed in \"presence-only data and the EM algorithm\", Biometrics 2009 and formalized in \"analysis of learning from positive and unlabeled data\", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method.\n\nThe huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See \"learning from corrupted binary labels via class-probability estimation\", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled \"on the minimal supervision for training any binary classifier from only unlabeled data\" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting.\n\nFurthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see \"learning classifiers from only positive and unlabeled data\" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in \"presence-only data and the EM algorithm\" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)<=P(X) following \"estimating the class prior and posterior from noisy positives and unlabeled data\", NIPS 2016. BTW, \"mixture proportion estimation via kernel embedding of distributions\" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later.\n\nIn summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue.\n\n*****\n\nThe novelty is to be honest incremental and thus below the bar of ICLR. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN.",
            "This paper is studying the problem of PU learning which is an important and interesting problem, however I am having difficulty in reading the paper key definitions and equations are badly written. Please clarify the following:\n\na) Equation (6) says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense. What are the authors saying here? \n\nb) Equation (3) defines R_{PN}(D, \u03b4) in terms of l(D(X), \u03b4) but l(D(X), \u03b4) is not defined properly in equation (4). The left hand side of (4) has l(D(X), \u03b4)  but \u03b4 vanishes on the right hand side of that equation. I have no idea what is going on here.\n\nc) The authors frequently confuse the true data distribution and the distribution hallucinated by the the generator. For example consider the expressions that \"supp(Pp (Xp )) \u2229 supp(Pn (Xn )) \u2192 \u2205 \" Which distribution are the authors talking about? Is it an assumption on the true data distribution required for learning ? or this is a property of the generator's distribution. \n\nD) The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs significantly worse than other PU learning methods which does not make sense. The PN method should be much better or comparable to the performance of any PU method. Please clarify."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses difficulty in understanding the logic and clarity of the paper, stating \"I just cannot comprehensively follow the logic\" and \"Figure 1 is unclear to me.\" They also question the authors' approach and request further explanation, indicating a critical stance.",
            "The review expresses uncertainty and seeks clarification, indicating a neutral stance rather than positive or negative feedback.",
            "The review expresses significant concerns about the paper's clarity, technical correctness, and experimental validation. Phrases like 'quality of the writeup is quite bad,' 'unclear,' 'does not make any sense,' and 'not quite convinced by the experiments' indicate a negative sentiment.",
            "The reviewer expresses concerns about the clarity, impact, and performance of the paper, stating it's 'hard to follow,' achieves 'marginal improvement,' and is 'below the threshold of acceptance.'",
            "The review expresses significant concerns about the paper's clarity, logic, and novelty. Phrases like \"mathematics in this paper is not easy to follow,\" \"critical issues,\" \"clarity is really an issue,\" \"logic in the story line is unclear,\" \"lacking some basic knowledge of PU learning,\" \"novelty is to be honest incremental,\" and \"significance is similarly poor\" indicate a negative sentiment.",
            "The review expresses strong concerns about the paper's clarity and correctness, using phrases like \"difficulty in reading,\" \"does not make any sense,\" \"I have no idea what is going on here,\" and \"confuse.\" These indicate a negative assessment of the paper's quality."
        ],
        "tone": [
            "Critical",
            "Neutral",
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"I just cannot comprehensively follow,\" \"otherwise you can only generate,\" \"I would like the authors to explain,\" and \"The authors only given some equations without intuition/motivation and they are still unclear to me.\" These phrases suggest a critical evaluation and a demand for clarification and justification from the authors.",
            "The tone is neutral as it poses questions and seeks understanding without expressing strong opinions or judgments. The phrasing is polite and inquisitive, rather than critical or supportive.",
            "The review adopts a critical tone by pointing out specific flaws in the paper, such as unclear sentences, incorrect equations, and questionable experimental comparisons. The reviewer uses direct and assertive language to express their concerns, for example, 'The quality of the writeup is quite bad' and 'does not make any sense.'",
            "The review uses critical language such as 'not clear,' 'hard to follow,' 'marginal improvement,' 'slightly lower performance,' and 'below the threshold of acceptance' to express concerns about the paper's quality and impact.",
            "The review employs direct and critical language to point out flaws in the paper's methodology, presentation, and understanding of the subject matter. Phrases such as \"I cannot easily follow,\" \"the motivation of the current paper does not make sense at least to me,\" \"Surprisingly, none of these 3 papers was cited,\" \"Lacking some basic knowledge of PU learning is another big issue,\" and \"novelty is to be honest incremental and thus below the bar of ICLR\" demonstrate a critical tone.",
            "The tone is critical due to the direct and pointed questions about the paper's equations and definitions. Phrases like \"does not make any sense\" and \"I have no idea what is going on here\" are explicitly critical. The reviewer also questions the validity of the experimental results."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently expresses confusion and seeks clarification regarding the paper's methodology, particularly concerning the application of GANs and the justification for using a single discriminator compared to existing methods like GenPU. The reviewer's questions and points of confusion are aligned and do not contradict each other.",
            "The review is consistent because the reviewer expresses a lack of expertise in GANs and asks clarifying questions to understand the differences between D-GAN and GenPU. The reviewer is seeking further explanation and does not present any contradictory statements.",
            "The review is consistently negative. While it lists a few weak 'Pros', the 'Cons' section strongly criticizes the clarity of the writing, the relevance of certain sections, and the validity of the experimental results. The reviewer's overall assessment is clearly critical and there are no contradictory statements within the review.",
            "The review consistently expresses a negative opinion about the paper. While acknowledging potential novelty, it highlights several weaknesses such as unclear motivation, poor clarity, marginal experimental improvements, and uncertain impact. The reviewer's final assessment that the paper is 'marginally below the threshold of acceptance' aligns with these criticisms, indicating a consistent negative evaluation despite the reviewer's self-professed limited expertise in the specific problem domain.",
            "The review consistently points out issues related to clarity, lack of theoretical grounding in PU learning, flawed methodology (mixing different PU settings in experiments), and consequently, low novelty and significance. The criticisms are logically connected and do not contradict each other.",
            "The review is consistent in its criticism, focusing on lack of clarity and inconsistencies in equations, definitions, and experimental comparisons. The reviewer raises multiple points highlighting issues with the paper's methodology and presentation, without contradicting themselves."
        ]
    },
    {
        "paper_id": "iclr_2018_HyEi7bWR-",
        "paper_title": "Orthogonal Recurrent Neural Networks with Scaled Cayley Transform",
        "paper_abstract": "Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exploding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the capabilities of Long Short-Term Memory networks (LSTMs).  We propose a simpler and novel update scheme to maintain orthogonal recurrent weight matrices without using complex valued matrices. This is done by parametrizing with a skew-symmetric matrix using the Cayley transform. Such a parametrization is unable to represent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The proposed training scheme involves a straightforward gradient calculation and update step. In several experiments, the proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters than other unitary RNNs.",
        "review_ids": [
            "SkLk8W9lM",
            "rkQbrzqxM",
            "HyGpBPslM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques\n\nComments:\n-- It\u2019s not clear to me how D is determined for each test. Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1. \n-- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant.\n-- The ylabel in Figure 1 is \u201cTest Loss\u201d which I didn\u2019t see defined. Is this test loss the cross entropy? If so, I think it would be more effective to label the plot with that.\n-- The plots in figure 1 and 2 have different colors to represent the same set of techniques. I would suggest keeping a  consistent color scheme\n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified.\n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified.\n-- How is test set accuracy defined in section 5.3? Classifying digits? Recreating digits? \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%. However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters.\n-- Overall it\u2019s unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance. For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy. scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3. Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset. I don\u2019t understand the significance of each test and why the relative performance of the techniques vary from one to the other.\n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability. You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear.",
            "This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks.\n\nI think the paper is well-written.  Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.\n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments?\n\n2- How sensitive is the results to the number of -1 in the diagonal matrix?\n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs. In MNIST experiment, for example, better numbers are reported for larger LSTMs. I think matching the number of hidden units could be helpful. Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000. I appreciate if authors can provide more results in these settings.\n\n",
            "The paper is clearly written, with a good coverage of previous relevant literature. \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced.\nTherefore, the paper must show that this new method performs better in some way compared with previous methods. They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.\n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN\n\n2. Clearly written\n\n3. Quite good results on several relevant tasks.\n\nCons:\n1. Technical novelty is somewhat limited\n\n2. Experiments do not evaluate run time, memory use, computational complexity, or stability. Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review identifies multiple points of confusion and lack of clarity in the manuscript, including unclear methodology, missing definitions, inconsistent visualizations, and lack of comprehensive comparison with existing techniques. The reviewer expresses difficulty in understanding the benefits of the proposed technique.",
            "The reviewer states \"I think the paper is well-written\" and that the authors \"have discussed previous works adequately and provided enough insight and motivation about the proposed method.\"",
            "The review expresses overall positive feedback, highlighting the paper's clarity, good results, and simplicity. While acknowledging limitations in novelty and experimental evaluation, the reviewer emphasizes the method's competitiveness and success on one task."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like \"It's not clear to me\", \"I didn't see defined\", \"should be clarified\", \"unclear how to completely determine the benefit\", and \"I don't understand the significance\" which indicate a critical tone. The reviewer also points out inconsistencies and omissions in the manuscript, highlighting areas needing improvement.",
            "The reviewer expresses positive feedback about the paper's quality and provides constructive questions and suggestions for improvement, indicating a supportive attitude towards the authors and their work.",
            "The review presents both positive (\"clearly written\", \"good results\", \"simple method\") and negative (\"technical novelty is somewhat limited\", \"experiments do not evaluate\") aspects of the paper. This balanced approach indicates a fair assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently points out areas where the manuscript lacks clarity, justification, or has inconsistencies in presentation. All comments are critical and aim to improve the manuscript by requesting more information, clarification, and better justification of claims. There are no contradictory statements within the review.",
            "The review is consistent because the reviewer expresses a positive initial impression of the paper's writing and motivation, and then proceeds to ask relevant and constructive questions for clarification and improvement. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because the initial summary and the lists of pros and cons align with each other. The reviewer acknowledges the paper's strengths (clarity, good results, simplicity) while also pointing out its weaknesses (incremental novelty, limited evaluation). There are no contradictions within the review."
        ]
    },
    {
        "paper_id": "iclr_2018_HkJ1rgbCb",
        "paper_title": "Using Deep Reinforcement Learning to Generate Rationales for Molecules",
        "paper_abstract": "Deep learning algorithms are increasingly used in modeling chemical processes. However, black box predictions without rationales have limited used in practical applications, such as drug design. To this end, we learn to identify molecular substructures -- rationales -- that are associated with the target chemical property (e.g., toxicity). The rationales are learned in an unsupervised fashion, requiring no additional information beyond the end-to-end task. We formulate this problem as a reinforcement learning problem over the molecular graph, parametrized by two convolution networks corresponding to the rationale selection and prediction based on it, where the latter induces the reward function. We evaluate the approach on two benchmark toxicity datasets. We demonstrate that our model sustains high performance under the additional constraint that predictions strictly follow the rationales. Additionally, we validate the extracted rationales through comparison against those described in chemical literature and through synthetic experiments. ",
        "review_ids": [
            "r11LXabJz",
            "S1wvy15xz",
            "SyI8c-T-f"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "\nThe paper proposes a feature learning technique for molecular prediction using reinforcement learning. The predictive model is an interesting two-step approach where important atoms of the molecule are added one-by-one with a reward given by a second Q-network that learns how well we can solve the prediction problem with the given set of atoms. The overall scheme is intuitive, but \n\nThe model is experimented on two small datasets of few thousand of molecules, and compared to a state-of-the-art DeepTox, and also to some basic baselines (RF/SVM/logreg). In the Tox21 dataset the proposed sparse RL-CNN method is less accurate than DeepTox or full CNN. In the hERG dataset RL-CNN is again weaker than the full CNN, but also seems to be beaten by several baseline methods. Overall the results are surprisingly weak, since e.g. with LASSO one often improves by using less features in complex problems. Both datasets should be compared to LASSO as well. \n\nIt's somewhat odd that the test performance in table 2 is often better than CV performance. This feels suspicious, especially with 79.0 vs 84.3. The table 2 does not seem reliable result, and should use more folds and more randomizations, etc.\n\nThe key problem of the method is its seeming inabability to find the correct number of atoms to use. In both datasets the number of atoms were globally fixed, which is counter-intuitive. The authors should at least provide learning curves where different number of atoms are used; but ideally the method should learn the number of atoms to use for each molecule.\n\nThe proposed Q+P network is interesting, but its unclear how well it works in general. There should be experiments that compare the the Q+P model with incresing number of atoms against a full CNN, to see whether the Q+P can converge to maximal performance.\n\nOverall the method is interesting and has a clear impact for molecular prediction, however the paper has limited appeal to the broader audience. Its difficult to assess how useful the Q/P-network is in general. The inability to choose the optimal number of atoms is a major drawback of the method, and the experimental section could be improved. This paper also would probably be more suitable for a chemoinformatics journal, where the rationale learning would be highly appreciated.\n",
            "This paper presents an interesting approach to identify substructural features of molecular graphs contributing to the target task (e.g. predicting toxicity). The algorithm first builds two conv nets for molecular graphs, one is for searching relevant substructures (policy improvement), and another for evaluating the contribution of selected substructures to the target task (policy evaluation). These two phases are iterated in a reinforcement learning manner as policy iterations. Both parts are based on conv nets for molecular graphs, and this framework is a kind of 'self-supervised' scheme compared to the standard situations that the environment provides rewards. The experimental validations demonstrate that this model can learn a competitive-performed conv nets only dependent on the highlighted substructures, as well as reporting some case study on the inhibition assay for hERG proteins.\n\nTechnically speaking, the proposed self-supervised scheme with two conv nets is very interesting. This demonstrates how we can perform progressive substructure selections over molecular graphs to highlight relevant substructures as well as maximizing the prediction performance. Given that conv nets for molecular graphs are not trivially interpretable, this would provides a useful approach to use conv nets for more explicit interpretations of how the task can be performed by neural nets. \n\nHowever, at the same time, I had one big question about the purpose and usage of this approach. As the paper states in Introduction, the target problem is 'hard selection' of substructures, rather than 'soft selection' that neural nets (with attention, for example) or neural-net fingerprints usually provide. Then, the problem would become a combinatorial search problem, which has been long studied in the data mining and machine learning community. There would exist many exact methods such as LEAP, CORK, and graphSig under the name of 'contrast/emerging/discriminative' pattern mining exactly developed for this task. Also, it is widely known that we can even perform a wrapper approach for supervised learning from graphs simultaneously with searching all relevant subgraphs as seen in Kudo+ NIPS 2004, Tsuda ICML 2007, Saigo+ Machine Learning 2009, etc. It would be unconvincing that the proposed neural nets approach fits to this hard combinatorial task rather than these existing (mostly exact) methods.\n\nIn addition to the above point, several technical points below would also be unclear.\n\n- A simple heuristic by adding 'selected or not' variables to the atom features works as intended? Because this is fed to the conv net, it seems we can ignore this elements of features by tweaking the weight parameters accordingly. If the conv net performs the best when we use the entire structure, then learning might be forced to ignore the selection. Can we guarantee in some sense this would not happen? \n\n- Zeroing out the atom features also sounds quite simple and a bit groundless. Confusingly, the P network also has an attention mechanism, and it is a bit unclear to me what was actually worked.\n\n- In the experiments, the baseline is based on LR, but this would not be fair because usually we cannot expect any linear relationship for molecular fingerprints. It's highly correlated due to the inclusion relationships between subgraphs. At least, any nonlinear baseline (e.g. Random forest or something?) should be presented for discussing the results.\n\nPros:\n- interesting self-supervised framework provided for highlighting relevant substructures for a given prediction task\n- the hard selection setting is encoded in input graph featurization\n\nCons:\n- it would be a bit unconvincing that identifying 'hard selection' is better suited for neural nets, rather than many existing exact methods (without using neural networks). At least one of the typical ones should be compared or discussed.\n- I'm still not quite sure whether or not some heuristic parts work as intended. ",
            "In this manuscript, the authors propose an interesting deep reinforcement learning approach via CNNs to learn the rationales associated to target chemical properties. The paper has merit, but in its current form does not match the acceptance criteria for ICLR.\n\nIn particular, the main issue lies in the poor performance reached by the systems, both overall and in comparison with baseline methods, which at the moment hardly justifies the effort required in setting up the DL framework. Moreover, the fact that test performances are sometimes (much) better than training results are quite suspicious in methodological terms.\nFinally, the experimental part is quite limited (two small datasets), making it hard to evaluate the scalability (in all sense) of the proposed solution to much larger data. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses several concerns about the paper's methodology, experimental results, and overall impact. Phrases like \"surprisingly weak,\" \"feels suspicious,\" \"does not seem reliable result,\" \"key problem,\" \"major drawback,\" and \"experimental section could be improved\" indicate a negative sentiment.",
            "The review expresses significant concerns about the paper's novelty and the suitability of the proposed approach compared to existing methods. Phrases like \"unconvincing,\" \"unclear,\" and expressing doubt about the heuristics used contribute to the negative sentiment. The reviewer also questions the fairness of the experimental setup.",
            "The review expresses concerns about the paper's performance, experimental setup, and scalability, using phrases like \"poor performance,\" \"hardly justifies the effort,\" \"quite suspicious,\" and \"quite limited.\""
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like 'surprisingly weak,' 'feels suspicious,' 'does not seem reliable result,' 'key problem,' and 'major drawback' to express criticism. It also suggests specific improvements and further experiments, contributing to the critical tone.",
            "The tone is critical due to the direct questioning of the paper's claims and methodology. Specific concerns are raised regarding the purpose, usage, technical details, and experimental setup. The reviewer uses phrases like \"I had one big question,\" \"It would be unconvincing,\" and \"this would not be fair,\" indicating a critical evaluation.",
            "The review uses critical language to point out weaknesses in the paper, such as \"poor performance reached by the systems,\" \"hardly justifies the effort,\" and \"quite suspicious in methodological terms.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its assessment. It acknowledges the interesting idea and intuitive approach of the proposed method, but consistently points out the weaknesses in the experimental validation, particularly the weak results, the fixed number of atoms, and the suspicious table 2. The reviewer provides constructive criticism and suggestions for improvement, maintaining a consistent negative stance on the current implementation's effectiveness while recognizing the potential of the underlying concept.",
            "The review is consistent because the reviewer's points, both positive and negative, are logically connected and centered around the core question of the suitability of the proposed neural network approach for the 'hard selection' task compared to existing methods. The reviewer consistently questions the justification of using neural networks for this task when exact methods exist, while also acknowledging the interesting aspects of the proposed framework.",
            "The review consistently points out negative aspects of the paper, such as poor performance, suspicious results, and limited experiments, to justify its rejection."
        ]
    },
    {
        "paper_id": "iclr_2021_rcQdycl0zyk",
        "paper_title": "Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with 1/n Parameters",
        "paper_abstract": "Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, \u201cfully-connected layers with quaternions\u201d (quaternions are 4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily 1/n learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.",
        "review_ids": [
            "cYxBqAPaOuI",
            "dMrfhzBA8tc",
            "B6o2fKHitDD",
            "KhIjhpDlrpB",
            "HxigEVxxfW7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors propose a novel way of parametrizing hypercomplex multiplications. \nThe proposed parametrization helps with: (a) Generalizing the multiplication to arbitrary dimensions, and (b) Reducing the number of parameters.  \nBuilding on this, the authors propose a parameterized hypercomplex multiplication (PHM) layer which essentially replaces the weight matrix of a linear layer with a matrix constructed via sum of Kronecker products. \nThey then replace the weight matrix of linear layers in LSTM and Transformer with PHM.\nFinally, they show that these PHM-variants of LSTM and Transformer, match or outperform their vanilla counterparts on a variety of NLP tasks, including NLI, MT, text style transfer, etc.,  while reducing the total number of model parameters.  \n\nOverall the paper is quite well written with easy to follow illustrations. \nThe proposed parametrization seems reasonable, and the empirical validation lends solid credibility to the idea. \nI have a couple of questions for the authors:\n* What are the practical benefits of this parametrization, particularly in comparison to other ways of reducing parameters, say matrix factorization?\n* If hypercomplex spaces only exist in $2^k$-D, is the term hypercomplex justified for arbitrary dimensions?\n* Have you analyzed the learned hypercomplex spaces? Can they be interpreted for arbitrary dimensions?",
            "Thank you for the response! After reviewing the data you have highlighted, along with the parameter counts you have added, I agree that the results are more impressive than I had initially concluded. Therefore, I have increased my score for this paper from 7 to 8.",
            "The authors focus on the area of using hypercomplex multiplications (multiplications involving numbers with multiple imaginary components) in deep learning models. Past work in this area has been promising but has been limited to certain dimensions for which there are predefined multiplication operations. The novel contribution of this work is to parameterize the hypercomplex multiplication operations, enabling the model to discover new operations rather than relying on the small number of existing operations and the small number of dimensions for which such operations exist. The authors find that their approach can substantially reduce the number of parameters without reducing performance (and in some cases even improving performance). \n\nStrengths:\n\n1. The proposed method makes a promising approach from the literature more flexible, helping to pave the way for making this approach more broadly useful.\n\n2. The authors illustrate this flexibility by showing how their approach can be effective for two different architectures (LSTMs and Transformers), making the general point that it can be applied to any architecture that uses feedforward components. They also apply it to multiple tasks, again illustrating the flexibility.\n\n3. As mentioned above, the approach can substantially increase a model\u2019s parameter count without affecting performance. Relatedly, it can also improve inference speed.\n\n4. The paper is generally thorough and clear.\n\nWeaknesses:\n\n1. The specific contribution of this paper is the parameterization of the multiplication operation, but the evidence that this parameterization is helpful is mild, as there are only a few cases where the proposed model noticeably outperforms the Quaternion model. Thus, the evidence presented does not make a strong case for the necessity of this parameterization.\n\n2. Much of the argument hinges on the reduced parameter count, but there was not any mention of exactly how many parameters each model had (at least, not that I saw - I did not check the appendix). I think the paper could be substantially strengthened by adding a \u201cParameter count\u201d column to each table.\n\n3. There is no clear intuition offered for why this approach might be expected to be effective. Offering such an intuition is certainly not necessary (since results alone are enough), but the paper would be more satisfying if there were such an intuition present.\n\nOverall, I am rating this as a 7, because I find it to be a solid paper but worry that its contribution on top of the existing work that has studied hypercomplex operations may be too small and may not have enough evidence for its usefulness.\n",
            "This paper builds on the top of standard high-dimensional neural networks (limited to 2,4,8, 16 dimensions) by introducing an elegant way to deal with others dimensions while preserving the internal relation learning capability as well as the reduction of number of parameters. To do so, the linear transformation is turned into a new linear transformation based on the Kronecker product. A sum of Kronecker products is used to \"simulate\" the different internal relations that could occur in between multi-dimensional components. However, I think that an empirical validation of the ability of the method to recover well-know product, i.e. Hamilton / complex products, is missing. \n\nAccording to the results, with experiments conducted on 3 different tasks, the proposed approach definitely seems to work and is an important step further to better understand and manipulate high-dimensional algebras with deep neural networks.\n\nRemarks and questions:\n1. While the theoretical aspect of the degeneration of the proposed approach to the Hamilton product \"sounds\" plausible, I would like to see a more empirical demonstration. Can this approach, correctly parametrize a rotation in a 3D space (simple task of learning a single rotation of an object). \n2. The quaternion case of this method relies on a set of [-1,0,1] to build the S matrices. This is what ensures the geometrical properties of the quaternion space while doing the different manipulations. This method, however, uses real-valued matrices. It is thus almost certain that the geometrical aspect of the \"learnt\" algebra is impossible to interpret. Do the authors think that a quantisation of the matrices could help finding pure quaternion / complex / octonions / sedenions matrices ? Such an analysis is crucial to validate the fact that the work proposed here is a generalisation of what have been done before to N dimensions for neural networks. \n\n- Would be great to include the number of parameters in the different Table.",
            "Thanks for the detailed response!\nI'm more convinced about the paper after reading through the responses and checking out the Appendix.\n\nI'm going to increase my confidence to 4 but I'll stick with the score of 7.  \nI want to reiterate that the paper is very well written, thorough, and has some very nice easy to follow illustrations. \nThe reason for not increasing the score is that apart from the parameter-saving benefits, I'm not convinced that \"hypercomplex multiplications\" by themselves are adding any value.  \n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment, highlighting the paper's clarity, the reasonableness of the proposed parametrization, and the solid empirical validation. Phrases like \"quite well written\", \"easy to follow illustrations\", \"seems reasonable\", and \"solid credibility\" indicate a positive assessment.",
            "The reviewer explicitly states they agree the results are more impressive than initially thought and increased the score, indicating a positive change in their assessment.",
            "The review expresses a generally positive outlook, highlighting the paper's strengths and potential contributions, while also providing constructive criticism. The final rating of 7 indicates a favorable assessment.",
            "The review expresses overall positive sentiment, acknowledging the paper's contribution as an 'important step further' and describing the approach as 'definitely seems to work'. While it raises concerns and suggests improvements, the core contribution is recognized positively.",
            "The reviewer expresses increased confidence in the paper after reading the responses and appendix, and praises the writing, thoroughness, and illustrations. While they mention a reservation about the core contribution, the overall sentiment is positive due to the explicit statement of increased confidence and positive feedback on other aspects."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The tone is supportive, as the reviewer acknowledges the strengths of the paper, such as its clear writing and empirical validation, and frames their concerns as questions for the authors to address, indicating a desire to help improve the work. The reviewer provides constructive criticism and seeks clarification rather than outright dismissing the approach.",
            "The reviewer expresses gratitude ('Thank you for the response!') and acknowledges the author's efforts in clarifying the data and parameter counts, leading to a more favorable evaluation. The phrase 'I agree that the results are more impressive' indicates a supportive stance.",
            "The review presents both strengths and weaknesses of the paper, using objective language and providing specific examples to support its claims. It offers praise for the clarity and thoroughness of the paper while also pointing out areas for improvement.",
            "The tone is balanced, offering both praise ('elegant way to deal with others dimensions') and constructive criticism (missing empirical validation, questions about geometrical interpretation). The reviewer uses phrases like 'I think that' and 'I would like to see' to soften the criticism and frame it as suggestions for improvement.",
            "The reviewer starts with \"Thanks for the detailed response!\" and states they are \"more convinced about the paper.\" They also use positive adjectives like \"very well written,\" \"thorough,\" and \"nice easy to follow illustrations,\" indicating a supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and constructive. It praises the novelty and empirical validation of the proposed method, while raising reasonable questions for clarification and further investigation, without contradicting the overall positive assessment.",
            "Consistent, score increased based on authors' response.",
            "The review is consistent because the strengths and weaknesses identified logically lead to the overall assessment and rating. The reviewer acknowledges the potential and flexibility of the proposed method (strengths), but also points out the limited evidence for its core contribution and lack of certain details (weaknesses). These weaknesses justify the moderate rating of 7 and the concern about the significance of the contribution, making the review internally consistent.",
            "The review is consistent because it acknowledges the potential of the proposed method while also pointing out specific areas that require further empirical validation and clarification. The reviewer appreciates the theoretical contribution but seeks more evidence to support the claims, particularly regarding the method's ability to recover known algebraic structures and the interpretability of the learned representations. The questions and suggestions are all aimed at strengthening the paper and do not contradict the initial positive assessment of the approach's potential.",
            "The reviewer expresses increased confidence in the paper after the authors' response and acknowledges the paper's strengths in writing, thoroughness, and illustrations. However, they consistently maintains their original score because their core concern about the added value of 'hypercomplex multiplications' beyond parameter saving remains unaddressed. This is a consistent viewpoint as the reviewer appreciates the paper's presentation and response but still questions the fundamental contribution, justifying the unchanged score despite increased confidence."
        ]
    },
    {
        "paper_id": "iclr_2021_Ogga20D2HO-",
        "paper_title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning",
        "paper_abstract": "Federated learning (FL) allows edge devices to collectively learn a model without directly sharing data within each device, thus preserving privacy and eliminating the need to store data globally. While there are promising results under the assumption of independent and identically distributed (iid) local data, current state-of-the-art algorithms suffer a performance degradation as the heterogeneity of local data across clients increases. To resolve this issue, we propose a simple framework, \\emph{Mean Augmented Federated Learning (MAFL)}, where clients send and receive \\emph{averaged} local data, subject to the privacy requirements of target applications. Under our framework, we propose a new augmentation algorithm, named \\emph{FedMix}, which is inspired by a phenomenal yet simple data augmentation method, Mixup, but does not require local raw data to be directly shared among devices. Our method shows greatly improved performance in the standard benchmark datasets of FL, under highly non-iid federated settings, compared to conventional algorithms.",
        "review_ids": [
            "-OHzmTJoo-h",
            "FgnVP1V4EMF",
            "9qWbfatB0VE",
            "o5cTpjfDZrq",
            "4CxaRrv7ESR",
            "_IN_uTo3IMq",
            "1xyl5D1LyG",
            "28MzIU141CP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies an interesting idea that applies Mixup to Federated Learning (FL) for addressing some challenges such as non-iid data. Basically, this is an empirical paper, and the overall organization is good, easy to read. However, I have several questions.\n\n1. The authors claimed that \"FedMix approximates Global Mixup\". In that case, why do not use Global Mixup directly? By comparing (4) with (3), both Global Mixup and FedMix use private image $(X_j, y_j)$, which can not violate privacy.\n\n2. The mathematics is poorly written. \n(a) What is $\\ell$ in $\\frac{\\partial \\ell}{\\partial x}$? It could be $\\ell(x)$, $\\ell(f((1+\\lambda)x_i),y_i)$, and $\\ell(f((1+\\lambda)x_i+\\lambda x_j),y_j)$ and so on. Please write it explicitly. \n(b) The last equation on Page 5 missed a $\\frac{1}{|J|}$ on the left hand side since $\\bar x$ or $\\bar y$ means averages x_j or y_j. \n(c) In proposition 1, it says \"we ignore the second order term (i.e., $O(\\lambda^2)$)\", but why there is a $\\lambda^2$ in (4)? Please check $\\lambda(1-\\lambda)=\\lambda - \\lambda^2$.\n(d) In Algorithm 1 and Algorithm 2, what is k in \"LocalUpdate$(k,w_t, X_g,Y_g)$\" since there is no $k$ in Algorithm 2? In Algorithm 2, it seems the input is $k,w_t, X_g,Y_g$ based on \"LocalUpdate$(k,w_t, X_g,Y_g)$\", but what is $X, Y$ in $\\ell_1$ and $\\ell_2$? In addition, what is $x$ in $\\ell_3$. I gauss $w$ in Algorithm 2 should be $w_t$.\n\n3. Based on Figure 1 and Algorithms 1 and 2, $\\ell_{FedMix}$ is an approximation of $\\ell_{NaiveMix}$ rather than $\\ell_{GlobalMixup}$, which can be easily verified by using Taylor expansion. In that case, NaiveMix and FedMix is very close when $\\lambda$ is very small. However, the experimental results shows that FedMix is more closer to GlobalMixup than NaiveMix, which is not reasonable. Any explanations? Did you use the approximation of GlobalMixup as FedMix? If so, I think the presented results are not so interesting.\n\n4. The experiments only conducted on three small data sets, but in FL, we are usually interested in big data. It would be better if the authors can provides results on big data such as ImageNet. \n\n5. In table 8, why NaiveMix and FedMix use different $\\lambda$ for CIFAR10? On the other hand, why don't you try different $\\lambda$ for NaiveMix?\n\n------------After Rebuttal------------\nThe authors have addressed my main concerns, and I have updated my score to 6.",
            "In this work, the authors aim to approach the non-iid data issue in FL by allowing for mean of the local client data to be transmitted in addition to the model parameters. I find this work very interesting and the paper well executed. \nFirst, the authors present the logic for MAFL, which encompasses the sending and receiving of other clients' averaged data, followed by FedMix, a method for augmenting the local data-set with the averaged data from other clients. \n\nThroughout the method section and their experiments, the authors show the benefits of MAFL+FedMix by ablation to other MixUp inspired approaches. \n\nMy issues with this paper are along some different aspects:\nPrivacy:\nSending statistics of local data is inherently less private than sending model parameters alone. The authors mention this explicitly, but do not go into more detail. I understand that the notion of privacy in FL is a research topic in itself, but I would wish for a more nuanced discussion of the trade-offs here. Throughout the experiment section, the largest 'federation' of devices is N=100 for Cifar100 and Femnist. Taking cifar100 as example, each client has 50k/100 = 500 data-points, the average of which I can agree intuitively to be not very informative (at least visually) and the 'discriminative information' that the authors mention, is presumably not very high. However, 500 data-points can still be considered a large amount of data-points for the federated scenario. As the number of data-points per client $n_k$ decreases, the more information about individual data-points is contained in their average. The problem is increased as $M_k>1$ . Further, 'discriminative information' is not the only privacy-worthy information in FL. Differential Privacy, for example, is trying to quantify if an individual data-point is present in a local data-set. Since a client receives a concatenation $(X_g,Y_g) = ({\\bar{x}_1,\\bar{x}_2,...,\\bar{x}_N},{\\bar{y}_1,\\bar{y}_2,...,\\bar{y}_N})$ of all clients' averaged data-sets, an individual client's participation in the training can also not be hidden from other clients. \nFurthermore, the formulation in Algorithm 1 implicitly assumes a continual learning setup where clients might be collecting more data as training progresses. In its current formulation, the authors do not mention if the batches are re-computed randomly, opening up the possibility for attacks on the differences between batches across time. \n\nComputational Burden:\nFedMix requires computing gradients through the Taylor expansion (EQ 4), which increases computation and memory requirements. Especially in a federated setting, computation and memory are constrained resources, so I would expect the authors to provide some estimates over the additional requirements for computing gradients $\\nabla_w l_{FedMix}$\n\nExperimental Evaluation:\nI am missing some details on the setup for the FEMNIST dataset. At the moments, the authors mention selecting 100 clients, however I wonder if they used the writer-id or re-shuffled to create a controlled label-skew. If they used the writer-id, how did they select the subset of 100 clients? \n\nSome details:\nI believe in Figure 1 b), the indices above 'Local data' should be $i$, not $j$. \nDirectly below Figure 1, the sentence should begin with: \"A more practical approach to...\"\nAlgorithm 2 could be improved, I believe. I see no space constraint that would prevent including some more detailed information analogous to Algorithm 3 in the Appendix. I am assuming the gradient is calculated mini-batch wise. Ideally, the $LocalUpdate$ would receive the same arguments as those that it is being called with on the server side for example. \nTop of page 5: 'meshed' -> 'mashed'.\nJust above Eq (2): '... client i has access to ...' (remove 'an').\n\n\nThe experiments that would make this evaluation great in my opinion:\nTrain on the full FEMNIST set of 3600clients including all those clients with very small number of data-points. Then introduce a cut-off-threshold for the minimum number of data-points that each client has to have in order to send its averaged data to the server. Alternatively, add random noise to these averages in relation to how much data is present. There is probably a differential privacy formulation that would make the required noise-level explicit. This noise level or cut-off-point should give more insight on several dimensions of the proposed work:\n\n- How sensitive is FedMix to different minimum required data-points as a trade-off with privacy.\n- How sensitive is $\\lambda$ to different number of data-points per client (or the consequence of fixed $\\lambda$ generally as number of data-points per client differs). Since no other experiment has different number of data-points per client, I believe this to be relevant.\n\nAdditionally, to further increase privacy, the authors might consider (randomly) averaging some of the elements in $(X_g,Y_g)$ before sending the data to clients and study those effects.\n\nSummarizing, I want to thank the authors for this very interesting read and interesting insights. If the authors provide a more nuanced/detailed discussion of the privacy aspects of their work and extend their experimental section with the more holistic FEMNIST experiment I described above, I will raise my score! I see no violation of the CoE in this work.\n\nFinally, I cannot believe that the authors let the opportunity slide to name their algorithm 'FedUp' ;) \n \n\n",
            "##########################################################################\n\nSummary:\n\u00a0\nThe paper proposed MAFL, a novel approach to conduct Mixup under the federated learning setting whiling preserving data privacy. The proposed FedMix scheme is inspired by Taylor\u2019s expansion of the global Mixup formulation. The effectiveness of MAFL is justified via empirical studies over a simulated federated learning environment, which indicates that Mixup achieves better test accuracies on various machine learning tasks.\n\n##########################################################################\n\nReasons for score:\u00a0\n\u00a0\nMy overall evaluation score on the current manuscript is borderline reject. The research direction on studying the effectiveness of data augmentation under the federated learning setting is promising. The formulation and motivation of the proposed MAFL scheme are sound. The main justification on FedMix is from the experimental study, which can be further improved e.g. the communication cost and privacy of FedMix can be more explicitly studied. If the proposed MAFL scheme can be supported by some theoretical analysis, the current manuscript can be much stronger. I will be happy to increase my overall evaluation score if my major concerns are addressed.\n\u00a0\n##########################################################################\n\nPros:\u00a0\n\u00a0\n1. The paper studies the effectiveness and practicality of conducting data augmentation under the federated learning scenario, which is quite promising and can potentially gain impact.\n\u00a0\n2. The proposed FedMix method is motivated via using Taylor expansion to approximate the global Mixup data augmentation objective, which makes sense in general.\n\u00a0\n3. Extensive experimental results are provided under the image classification and the next-word prediction tasks under the simulated non-iid environment, which indicates that FedMix enjoys high effectiveness on improving the model test accuracy under the data heterogeneity.\n\u00a0\n##########################################################################\n\nCons:\u00a0\n\u00a0\n1. The main concern on the proposed FedMix method is communication and computation efficiency. From the proposed Algorithm 1, for each FL round, MAFL requires all available clients to upload their locally averaged data batches. It is easy to imagine in a real federated learning environment (with up to $10^{10}$ available clients), it can lead to a significant communication overhead [1]. Thus, it would be useful to explicitly study the communication cost of MAFL, e.g. report Test Accuracy vs the amount of communication for Figure 2 can help to understand the communication efficiency of MAFL better.\u2028\n2. It\u2019s not clear how MAFL splits the local datasets. Does it a conduct random split? Would it be possible for each local client to put \u201csimilar\u201d (e.g. data points within the same class) into the same batch? Such an approach intuitively preserves more data property in $\\bar x$ and $\\bar y$.\u2028\n3. Although a value of small $M_k$ in MAFL leads to worse data privacy, it\u2019s easy to imagine the proposed MAFL can be combined with other differential private (DP) method e.g. [2]. It will be useful to consider a DP version of MAFL.\u2028\n4. The authors are encouraged to add the baseline of \u201cGlobal Mixup\u201d to Table 2, 3, 4, 6, 7 to understand the gap between the proposed FedMix method and the \u201cideal\u201d baseline.\u2028\n5. The FedProx result on FEMNIST is a bit confusing, what accuracy will FedProx reach for running 32 FL rounds?\u2028\n6. For the image classification tasks, it seems FedMix outperforms other baselines. However, for the language task e.g. Table 2, it only matches the accuracy of NaiveMix. Does it mean FedMix can be improved for augmenting language examples?\u2028\n7. The approach to simulate data heterogeneity in the current paper can be generalized by the method proposed in [3-4]. It would be useful to consider the Dirichlet distribution based data partition strategy.\u2028\n\n[1] https://arxiv.org/pdf/1912.04977.pdf\n\n[2] https://arxiv.org/pdf/1710.06963.pdf\n\n[3] https://arxiv.org/pdf/1905.12022.pdf\n\n[4] https://arxiv.org/pdf/2002.06440.pdf\n\n#########################################################################\n\nMinor Comments:\u00a0\n1. It seems the CIFAR-10 and CIFAR-100 curves in Figure 2 are not reaching to full convergence. Thus, it would be helpful to run the experiments for more FL rounds.\u2028\n2. Missing references: [1-2]. And some references are sort of outdated e.g. \u201cFederated optimization in heterogeneous networks\u201d (T Li et al) was accepted to MLSys 2020.\u2028\n\n\n[1] https://arxiv.org/abs/1912.04977\n\n[2] https://arxiv.org/abs/1908.07873\n\n\n###################### Post Rebuttal #############################\nMost of my concerns on the current manuscript are addressed. I tend to increase my overall evaluation score to 6.\n#############################################################",
            "I carefully read the authors' response. The authors addressed most of my concerns by adding additional experimental results as suggested, and MAFL demonstrates good effectiveness. I tend to raise my overall evaluation score from \"5\" to \"6\".\n\nAn additional comment over the communication overhead for MAFL is that the authors are also expected to discuss the communication overheads for input images with high resolutions e.g. [1]. For CIFAR-10 scale, it's clear that the input dimension is lower than the model dimension. But for the images with super high resolution, it's not clear.\n\n[1] https://arxiv.org/pdf/1902.06068.pdf ",
            "For completeness sake: Indeed, Gaussian noise is typical to use in DP, but crucially the magnitude (variance) of the noise makes all the difference. You can add $\\sigma$ amounts of noise without knowing *how private* that actually is. This is why you need to rigorously link $\\sigma$ (and typically additionally other mechanisms, such as clipping) to the $\\epsilon,\\delta$ levels of DP.\n",
            "Thanks. Now I understand. As you mentioned, $\\ell_\\text{FedMix}$ does not directly utilize Eq. (4), so I would like to recommend the authors to modify it in the revision. My another concern is about your implementation. Since you are using the equation after Proposition 1 as the loss of FedMix, but you missed a $\\frac{1}{|J|}$ in your first version,  I am wondering if you implemented the experiments correctly?  Besides, your original Algorithms 1 and 2 are also problematic. ",
            "Thx for addressing my concerns. \nThe only remaining feedback I have is wrt. to the 'DP' experiments.\nDifferential privacy crucially is not only about empirical results. Instead, DP provides strict privacy guarantees based on proving how an algorithm is $\\epsilon,\\delta$ differentially private. While I commend the authors of including experiments with noised-up communications, this is by no means 'differentially private'. \nI raise my score to 7 under the condition that the authors remove these claims or provide a proof about the DP nature of their algorithm. \n\nFurthermore, I saw some sentence structure errors in the added 'red' text, so for a camera ready version, might I suggest another proof-read. ",
            "Thanks for the authors' response.  It is still not clear to me about the definition of FedMix and its loss. First, if my understanding is correct, Figure 1 (d) shows that there is a averaged image $(\\bar{X}_j, \\bar{y}_j)$ in Global Server and it will be passed to each Client for computing loss functions.  However, I can not find $(\\bar{X}_j, \\bar{y}_j)$ in $\\ell_\\text{FedMix}$ of equation (4) from Proposition 1. Second, if $\\ell_\\text{FedMix}$ is an approximation of $\\ell_\\text{GlobalMixup}$, we can see from (4)  that FedMix will violate privacy since raw data is exchanged and directly used for computiong loss function by local data ($x_i$) and received data ($x_j$). The reason is that the last term of (4) is obtained by using both $x_i$ and $x_j$. If this is the truth, why do not use GlobalMixup directly? "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Neutral",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review raises several concerns about the paper's methodology, mathematical correctness, experimental setup, and clarity. The reviewer points out issues with the mathematical notation, inconsistencies in the algorithms, and questions the validity and scope of the experimental results. Although the score was updated after rebuttal, the initial review is largely critical.",
            "The review expresses interest in the work, calling it \"very interesting\" and an \"interesting read\". It also states that the reviewer will raise their score if certain improvements are made.",
            "The reviewer states that \"Most of my concerns on the current manuscript are addressed\" and indicates a willingness to increase the evaluation score.",
            "The reviewer states that the authors addressed most of their concerns and that MAFL demonstrates good effectiveness. They also indicate an intention to raise the evaluation score, indicating a positive overall assessment.",
            "The review provides a factual statement about Gaussian noise in differential privacy, without expressing strong positive or negative opinions. It highlights the importance of the noise's magnitude and its connection to privacy levels.",
            "The review expresses concerns about the correctness of the implementation and the algorithms used in the paper. Phrases like \"problematic\", \"missed a...\", and questioning the correctness of the experiments indicate a negative sentiment.",
            "The reviewer thanks the authors for addressing their concerns and raises the score. While critical of the DP experiments, the reviewer provides constructive feedback.",
            "The reviewer expresses confusion and concern regarding the definition and privacy implications of the proposed method. Phrases like \"It is still not clear to me\" and \"violate privacy\" indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Supportive",
            "Formal",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct and questioning language, highlighting specific flaws and inconsistencies in the paper. Phrases like \"mathematics is poorly written,\" \"missed a...\", \"which is not reasonable,\" and \"not so interesting\" indicate a critical stance. The reviewer demands explanations and justifications for several aspects of the paper.",
            "The review offers both positive feedback (interesting work, well executed) and constructive criticism (privacy concerns, computational burden, experimental evaluation). It also includes minor suggestions for improvement (details, algorithm improvement, typos).",
            "The reviewer expresses a positive shift in their evaluation, stating \"I tend to increase my overall evaluation score to 6.\" This suggests a supportive stance towards the paper's improvements.",
            "The reviewer acknowledges the authors' efforts in addressing concerns and expresses a willingness to increase the evaluation score. The language used is encouraging and constructive, suggesting a supportive stance. The phrase \"I tend to raise my overall evaluation score\" indicates support.",
            "The language is technical and precise, using terms like 'Gaussian noise,' 'DP,' 'variance,' 'clipping,' 'epsilon,' and 'delta.' The use of mathematical notation ($\\sigma$) further contributes to the formal tone.",
            "The tone is critical due to the reviewer pointing out specific errors and issues in the paper's implementation and algorithms. Phrases like \"does not directly utilize\", \"modify it in the revision\", \"missed a...\", and \"problematic\" convey a critical assessment.",
            "The review is critical of the 'DP' experiments, stating that they are 'by no means 'differentially private''. The reviewer also points out sentence structure errors. However, the reviewer uses polite language and offers suggestions for improvement, indicating a constructive approach.",
            "The review raises specific concerns about the method's definition and privacy, using direct questions and statements of disagreement. The reviewer challenges the authors' approach, questioning the necessity of FedMix over GlobalMixup."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially raised several concerns and questions regarding the paper's methodology, mathematics, and experiments. However, after the rebuttal, the reviewer explicitly stated that their main concerns were addressed and updated their score positively. This indicates a consistent review process where initial concerns were raised and then resolved by the authors, leading to a positive change in the reviewer's assessment.",
            "The review is consistently constructive. It starts with positive feedback, identifies areas for improvement with specific reasons (privacy, computational burden, experimental evaluation), and concludes with positive remarks conditional on addressing the identified issues. The reviewer's critique is aimed at improving the paper and does not contain contradictory statements. The reviewer appreciates the work but suggests concrete improvements, indicating a consistent and helpful review.",
            "The review is consistent because the reviewer initially expresses concerns and a borderline reject score, but also indicates willingness to increase the score if these concerns are addressed. The post-rebuttal section confirms that most concerns were addressed, leading to a positive shift in the evaluation and a score increase. This demonstrates a logical flow and consistent evaluation process based on addressing the raised concerns.",
            "The review is consistent because the reviewer acknowledges the authors' improvements and expresses a positive sentiment towards the paper by considering raising the score. The additional comment about communication overhead is presented as a suggestion for further discussion and does not contradict the overall positive assessment.",
            "The review consistently argues that while Gaussian noise is typical in Differential Privacy, the magnitude of the noise is crucial and needs to be rigorously linked to privacy parameters like epsilon and delta to ensure actual privacy. There are no contradictory statements or arguments within the review.",
            "The review is consistent because it acknowledges understanding a point while raising separate concerns about implementation details and algorithms. These are distinct aspects of the paper and the reviewer's comments are logically independent and do not contradict each other. The reviewer is providing feedback on different parts of the work without presenting conflicting viewpoints.",
            "The reviewer's feedback is consistent as they acknowledge the authors' efforts in addressing previous concerns but maintain a clear and specific point regarding the differential privacy experiments. The reviewer's suggestion to either remove the DP claims or provide a proof, along with the conditional score increase, demonstrates a logical and consistent evaluation of the work.",
            "The review is consistent because the reviewer raises several related concerns about the clarity, definition, and potential privacy issues of FedMix. The reviewer's points are logically connected and build upon each other, focusing on the inconsistencies between the description, equations, and claimed privacy benefits of the proposed method."
        ]
    },
    {
        "paper_id": "iclr_2019_SyxaYsAqY7",
        "paper_title": "Second-Order Adversarial Attack and Certifiable Robustness",
        "paper_abstract": "Adversarial training has been recognized as a strong defense against adversarial attacks. In this paper, we propose a powerful second-order attack method that reduces the accuracy of the defense model by Madry et al. (2017). We demonstrate that adversarial training overfits to the choice of the norm in the sense that it is only robust to the attack used for adversarial training, thus suggesting it has not achieved universal robustness. The effectiveness of our attack method motivates an investigation of provable robustness of a defense model. To this end, we introduce a framework that allows one to obtain a certifiable lower bound on the prediction accuracy against adversarial examples. We conduct experiments to show the effectiveness of our attack method. At the same time, our defense model achieves significant improvements compared to previous works under our proposed attack.",
        "review_ids": [
            "S1lzXzII67",
            "BJeuzz5TnX",
            "Skx7vgPc27",
            "r1xf0z-XnQ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I appreciate the authors taking the time respond to my comments.\n\n1) I understand the point about L_inf adversarial training not resulting in L_2 robustness. However, the way the paper is currently written, the emphasis is placed on SO attacks being able to bypass adversarial training by being more powerful than FO attacks. The difference between the norms used is only mentioned in the intro and the discussion and not at all in the main sections describing your contributions. I would suggest that this nuisance is emphasized in future versions of the paper.\n\nNote that the proposed SO attacks are not using gradient information at the current iterate, but are rather using gradient information from a random nearby point. This would explain why they might evade the flat regions caused by threshold filters on MNIST. Given that this is the only case where the proposed SO attacks is more powerful than standard PGD, I still believe that it is an artifact of the particular dataset and norm choice.\n\n2) I understand that achieving certifiable robustness to a larger scale of perturbations is challenging. However, I personally view the results of Mathias et al. (2018) as \"One can certify robustness to small epsilon values by adding random noise to the input\". I view the result of the current paper as \"One can do use a different toolkit to certify slightly larger epsilon values when adding random noise to the input\". Hence while the approach and analysis are interesting, I find the result incremental.\n\nIt is still not clear to me what the importance of stability training is. It would be helpful if the following comparisons were included: a) the certified bound using the approach of Mathias et al. (2018) _both_ for standard and stability-trained networks, b) the certified bound using the proposed approach _both_ for standard and STN. (The two approaches should be compared using the best setting of hyper-parameters for each.)\n\n3) Even if the proposed defense is not one of the main contributions of the paper, it is still claimed as a contribution. There are several points in the paper where it is argued that the proposed defense outperforms other state-of-the-art methods and that \"... our results are reliable.\". If the evaluation happens to indeed be unreliable, then the claim could be misleading. This would be a significant flaw for a paper that cannot be ignored during the review process. If the empirical performance is indeed not a major concern, that it should be removed from the paper.\n",
            "The paper makes three rather independent contributions: a) a method for constructing adversarial examples (AE) utilizing second-order information, b) a method for certifying classifier robustness, c) a method to improve classifier robustness. I will discuss these three contributions separately.\n\na) Second order attack: Miyato et al. (2017) propose a method for constructing AE for the case where the gradient of the loss is vanishing. In this case, at given a point, the direction of steepest loss ascent can be approximated by the gradient at a randomly sampled nearby point. Miyato et al. (2017) show how this can be derived as a very crude approximation of the power method. The authors of the current paper apply this attack to the adversarial trained networks of Madry et al. (2017). They find that the *L_infinity* trained networks of that work are not as *L_2* robust as originally claimed. I find this result interesting, highlighting a failure case of first-order methods (PGD) for evaluating adversarial robustness. However, it is important to note that these were models that were *not* trained against an L2 attack and thus should not be expected to be very robust to one. Therefore, this result does not identify a failure of adversarial training as the authors seem to suggest but rather a failure of the original evaluation of Madry et al. (2017). It is also worth noting that this finding is specific to MNIST given the results currently presented. This might be explained by the fact that robust MNIST models tend to learn thresholding filters (Madry et al., 2017) which might cause gradient obfuscation.\n\nb) Adversarial robustness certification: The authors proposed a method for certifying the robustness of a model based on the Renyi divergence. The core idea is to define a stochastic classifier that randomly perturbs the input before classifying it. Given such a classifier, one can construct the probability distribution over classes. The authors prove that given the gap between the first and second most likely classes, one can construct a bound on the L2 norm of perturbations required to fool the classifier. This method is able to certify the adversarial accuracy of some classifier to relatively small epsilon values. While I think the theoretical arguments are elegant, I find the overall contribution incremental given the work of Mathias et al. (2018). Both methods seem to certify robustness of roughly the same scale. One component of the experimental evaluation missing is how does the certifiable accuracy differ between robust and non-robust models. Currently there are only results for a single model (Figure 1) and it is not clear from the text which one it is. Given that there exists a section titled \"improved certifiable robustness\" I would at least expect a result where a model with higher certifiable accuracy is constructed. \n\nc) Improved robustness via stability training: The authors propose a method to make a classifier more robust to input noise. They add a regulatization term to the training loss that penalizes a change in the probabilities predicted by the network when the input is randomly perturbed. In particular, they use the cross-entropy loss between the probability distributions predicted at the original and the perturbed point. The goal is to train a model that is more robust to random perturbation which will then hopefully translate to robustness to adversarial perturbation. This method is evaluated against the proposed attack (a) and is found to be more robust to that attack than previous adversarially trained models. Overall, I find the idea of stability training interesting. However I find the current evaluation severely lacking. First of all, these models should be evaluated against a standard PGD adversary (missing from Table 1). Even if that method is unreliable when applying random noise to the input at each step it is still an important sanity check. Additionally, in order to deal with the stochasticity of the model one should experiment with a PGD attack that estimates the gradient using multiple independent noise samples (see https://arxiv.org/abs/1802.00420). Finally, other attacks such as black-box attacks and finite-differences attacks should potentially be considered. Given how other defenses based purely on data augmentation during training or testing were bypassed it is important to apply a certain amount of care when evaluating the robustness of a model.\n\nOverall, while I think the paper contains interesting ideas, I find the current evaluation lacking. I recommend rejection for now but I would be willing to update by score based on author responses. \n\nMinor comments to the authors:\n-- Last paragraph of first page: \"Though successful in adversarial defensing, the underlying mechanism is still unclear.\", adversarial training has a fairly principled and established underlying mechanism, robust optimization. \n-- Figure 2 left: is the natural line PGD or SO?\n-- The standard deviation of the noise used is very large relative to the pixel range. You might want to comment on that in the main text.\n-- Figure 3: How was the Madry model trained? L_inf or L_2?",
            "This paper consists of two parts: a 2nd-order attack method and a certification for robustness. The paper is well written and easy to follow. However, addressing of similarity and comparison with some previous methods could be improved.\n\nFirst of all, the motivation of 2nd order attack is clear and reasonable: for adversarially trained model at minimax, the gradient is close to vanishing, so 2nd order information helps a lot to find actual adversarial examples in this case. However,\n\n1. A lot of defenses have tried to modify the networks to make even computing the gradient difficult if not impossible. In this case, how effective is the 2nd order attack? I would like to see some discussion of this.\n\n2. While the starting point seems like a powerful attack method. The 2nd order information is only approximately computed via finite differences. A powerful method with weak approximation will sounds more powerful than a weak method to start with, but the actual effectiveness will need more systematic comparison. I think adding some studies of the accuracy or variances of the 2nd order information with the proposed approximation method (under natural setting and maybe also under the setting where the networks are modified to make even 1st order information hard to compute) would definitely help.\n\n3. Also after the approximation, as mentioned in the paper, the algorithm becomes equivalent to EOT attacks with Gaussian noises. The EOT attacks are also more general to allow different types of noises. While it might not make lots of sense to compare with EOT attacks in the experiments as the two algorithms seem to be exactly the same, it would help of more discussions could be devoted to justify how the proposed algorithm is novel given the previously existed EOT attack.\n\n4. In the experiments on adversarially trained models, the adversarial trained models are trained against l_inf attack, while the actual attack is l2. This seems unfair. Since the author mentioned that it is easy to extend their method to l_inf attack. It would be more justifiable if the results with matching attack types are shown instead of the current ones.\n\nThe certified robustness is an interesting take, too. However, the bounds might be too strong: as far as I understand, it does not rely much on the properties of the underlying neural networks f. So in order to be applicable to all kinds of weird non-robust neural networks uniformly, the bounds cannot be too tight. To get useful certificate level, a too heavy noise level sigma might be needed and potentially destroys the classification accuracy of the original model f. This is acknowledged in the 'gap between theory and empirical' section. And it makes the importance of such kind of bounds a bit weak.\n\n5. Also, the 'stability training' procedures derived based on this bounds is quite similar to some previous methods. For example, the objective function is very similar to 'logit pairing', which add an extra term to bound the similarity between two logits from an adversarial or noisy version. The empirical results will be much stronger if more closely related methods are included in the comparison. For example, logit pairing, as well as simple training with Gaussian perturbation on inputs.\n\nIn summary, this paper provide some interesting perspectives to adversarial attacks and certifications. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of some approximation methods adopted",
            "This paper makes two different contributions in the field of adversarial training and robustness.\nFirst the authors introduce a new type of attack that exploits second-order information while traditional attacks typically rely on first-order information.\nAnother contribution is a theorem that using the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels.\n\nOverall, I find that the paper lacks clarity and does not properly contrast their work to existing results. They are also some issues with the evaluation results. I provide detailed feedback below.\n\n1) Prior work\na) Connection between adversarial defense and robustness to random noise\nThis connection is established in Fawzi, A., Moosavi-Dezfooli, S. M., & Frossard, P. (2016). Robustness of classifiers: from adversarial to random noise. In Advances in Neural Information Processing Systems (pp. 1632-1640).\nb) Connection between minimal perturbation required to confuse classifier and its confidence was discussed for the binary classification in Section 4 of\nFawzi, Alhussein, Omar Fawzi, and Pascal Frossard. \"Analysis of classifiers\u2019 robustness to adversarial perturbations.\" Machine Learning 107.3 (2018): 481-508.\nc) The idea to compute the distribution of classifier outputs when the input is convolved with Gaussian noise was already \u201canticipated\u201d in Section V of the following paper which relates the minimum perturbation needed to fool a model to it\u2019s misclassification rate under Gaussian convolved input:\nLyu, Chunchuan, Kaizhu Huang, and Hai-Ning Liang. \"A unified gradient regularization family for adversarial examples.\" Data Mining (ICDM), 2015 IEEE International Conference on. IEEE, 2015.\n\nThese papers should be discussed in the paper, please elaborate how you see your contribution regarding the results derived there.\n\n2) Second-order attack introduced in the paper\nI think they are a number of important details that are ignored in the presentation.\na) Regarding the assumption that the gradient vanishes in the difference of the loss, I think the authors should elaborate as to why this is a reasonable assumption to make. If we assume that the classifier has been trained to optimality then expanding the function at this (near-)optimum would perhaps indeed yield to a gradient term of small magnitude (assuming the function is smooth). However, nothing guarantees that the magnitude of the gradient term is negligible compared to the second-order information. The boundary of the classifier could very well be in a region of low-curvature.\nb) The approximation of the second-order information is rather crude. However, the update derived is very similar to PGD with additional noise. In optimization, the use of noise is known to extract curvature, see e.g. (Xu & Yang, 2017) who showed that noisy gradient updates act as a noisy Power method that extracts negative curvature direction.\nXu, Y., & Yang, T. (2017). First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time. arXiv preprint arXiv:1711.01944.\n\n3) Issue of \"degenerate global minimum\": The authors argue that multistep attacks also suffer from this issue. However, the PGD attack of Madry is also initialized at a random point within the uncertainty ball around x, i.e. PGD attack first adds random noise to x before iteratively ascending the loss function. This PGD update + noise at first iteration seem rather similar to the update derived by the authors that uses random noise at every iteration. It could therefore be that the crude approximation of second-order information is not so different from previous work. This should be further investigated either theoretically or empirically.\n\n4) Lack of details regarding some important aspects in the paper\na) \u201cNote the evaluation requires adjustment and computing confidence intervals for p(1) and p(2), but we omit the details as it is a standard statistical procedure\u201d\nThe authors seem to sweep this under the carpet but this estimation procedure gives only an estimate of the required quantities p(1) and p(2), which I think would require adjusting the result in the theorem to be a high probability bound (or an expectation bound) instead of a deterministic result.\n\nb) \u201cthe noise is not necessarily added directly to the inputs but also to the first layer of a DNN. Given the Lipschitz constant of the first layer, one can still calculate an upper bound using our analysis. We omit the details here for simplicity\u201d\nWhat exactly changes here? How do you estimate the Lipschitz constant in practice?\n\n\n5) Main theorem needs to be contrasted to previous results\nThe main Theorem uses the Renyi divergence certifies robustness of a classifier by adding Gaussian noise to pixels. There are already many results in the field of robust optimization that already derive similar results, see e.g.\nNamkoong, H., & Duchi, J. C. (2017). Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980).\nGao, R., & Kleywegt, A. J. (2016). Distributionally robust stochastic optimization with Wasserstein distance. arXiv preprint arXiv:1604.02199.\nCan you elaborate on the difference between your bounds and these ones? You do mention some of them require strong assumptions such as smoothness but this actually seems like a mild assumption (although some activation functions used in neural nets are indeed not smooth).\n\n6) Adversarial Training Overfit to the Choice of norms\nThe main theorem derived in the paper uses the l_2 norm. What can be said regarding other norms?\n\n7) Experiments:\na) the authors only report accuracies for attacks whose l2-norm is smaller than a fixed constant 0.8. However, this makes the results difficult to interpret and the authors should instead state the signal to noise ratio, i.e. dividing the l2-norm of the perturbation by the l2-norm of the image. Otherwise, it is not clear how strong or weak such perturbations are. (In particular, the norm depends on the dimension of the image, so l2-norms of perturbations for MNIST and CIFAR10 are not comparable).\nb) In Section 6.2, the authors state that an l_infty trained model is vulnerable against l_2 perturbations. Why not training the model under both l_infty and l_2 perturbations?\nc) Figure 1\nBased on the results predicted in Theorem 2, it seems it would be more interesting to evaluate the largest L for which the classifier predictions are the same. Why did you report a different results?\n\n8) Other comments\nsection 2.1: \u201cNote this distribution is different from the one generated from softmax\u201d. Why/How is this different?\nconnection to EOT attack\u2019: authors claim: E_{d\u223cN(0,\u03c32I)} [\u2207_x L(\u03b8, x, y)|x+d] = \u2207_x E_{d\u223cN(0,\u03c32I)} [\u2207_x L(\u03b8, x, y)|x+d]. There is a typo on the RHS where \u2207_x is repeated twice. This is also the common reparametrization trick so could cite \nKingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses several concerns about the paper's claims, methodology, and presentation. Phrases like 'nuisance,' 'artifact,' 'incremental,' 'not clear,' and 'misleading' indicate a critical assessment.",
            "The reviewer expresses several concerns about the paper's evaluation, stating it is 'severely lacking' and recommends rejection. While acknowledging interesting ideas, the reviewer finds the experimental results incomplete and suggests further investigation.",
            "The review acknowledges the paper's strengths (well-written, interesting perspectives) but also points out several weaknesses and areas for improvement (similarity to existing methods, fairness of experiments, tightness of bounds). The overall sentiment is balanced, not overwhelmingly positive or negative.",
            "The review expresses concerns about clarity, lack of comparison to existing work, and issues with evaluation results. Phrases like \"lacks clarity,\" \"does not properly contrast,\" and \"some issues with the evaluation results\" indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, using phrases like 'I would suggest that this nuisance is emphasized,' 'I still believe that it is an artifact,' 'I find the result incremental,' 'It is still not clear to me,' and 'This would be a significant flaw.' These phrases point out specific weaknesses and suggest improvements, indicating a critical evaluation of the work.",
            "The review uses phrases like 'failure case', 'incremental given the work of', 'evaluation severely lacking', and 'important sanity check' which indicate a critical evaluation of the work. The reviewer also points out specific flaws in the experimental setup and suggests additional experiments.",
            "The review provides specific criticisms and suggestions for improvement, using phrases like 'could be improved,' 'This seems unfair,' 'might be too strong,' and 'a bit weak.' It questions the novelty and effectiveness of the proposed methods and calls for more systematic comparisons and studies.",
            "The review provides specific criticisms and questions the validity of assumptions and experimental design. Phrases like \"important details that are ignored,\" \"approximation is rather crude,\" \"authors seem to sweep this under the carpet,\" and \"difficult to interpret\" suggest a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer provides feedback that is logically connected and maintains a consistent critical but constructive tone throughout the review. The reviewer acknowledges the authors' responses but still points out areas for improvement regarding clarity, emphasis, and justification of claims. The concerns raised in different points are related and build upon each other, focusing on the presentation of contributions, the significance of results, and the need for more comprehensive evaluation and comparisons. There are no contradictory statements or shifts in the reviewer's overall assessment.",
            "The reviewer consistently identifies interesting ideas within the paper but expresses concerns about the evaluation methodology across all three contributions (second-order attack, robustness certification, and stability training).  Each section highlights a promising aspect coupled with a critique of the experimental validation, leading to a consistent overall assessment of the paper's weaknesses and a recommendation for rejection based on the insufficient evaluation.",
            "The review is consistently critical but constructive. It starts with acknowledging the paper's clarity and motivation, but then consistently points out areas for improvement related to novelty justification, comparison with existing methods, and the need for stronger empirical validation. The reviewer's concerns are focused on the paper's positioning within the existing literature and the strength of its empirical support, without contradicting itself.",
            "The review is consistent because the overall negative assessment, stating that the paper lacks clarity, does not properly contrast with existing results, and has issues with evaluation, is well-supported by the detailed feedback provided in points 1 through 8. Each detailed point elaborates on these general criticisms, pointing out specific areas where the paper falls short in terms of contextualization with prior work, clarity of methodology, justification of assumptions, completeness of details, and rigor of experimental evaluation. There are no contradictions between the overall summary and the detailed comments."
        ]
    },
    {
        "paper_id": "nips_2022_11nMVZK0WYM",
        "paper_title": "Pruning has a disparate impact on model accuracy",
        "paper_abstract": "Network pruning is a widely-used compression technique that is able to significantly scale down overparameterized models with minimal loss of accuracy. This paper shows that pruning may create or exacerbate disparate impacts. The paper sheds light on the factors to cause such disparities, suggesting differences in gradient norms and distance to decision boundary across groups to be responsible for this critical issue. It analyzes these factors in detail, providing both theoretical and empirical support, and proposes a simple, yet effective, solution that mitigates the disparate impacts caused by pruning. ",
        "review_ids": [
            "Yb4amFxUh6",
            "6yUwkT1M2j1",
            "H_IYkxrab4",
            "W2Eug5l-5TL",
            "k-6OC8Dfum0",
            "wsXRBJuL7sRs",
            "q7eAVodPgOx",
            "Mo-4YxVFlhX",
            "gh6enpj1rfA"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " My guess is that ethics review was requested for this paper on the basis of using face recognition datasets to train an ethnicity classifier - this is honestly a pretty risky decision on the part of the authors and I would recommend not doing this.\nAlthough the point the authors are making is good, that the pruned models are by default discriminatory but can be less so using their method, the choice of task is pretty risky and IMHO not the best way to show the issue.\nI would propose going one of two ways:\n1) Produce synthetic data that follows some distributions that the authors are finding from real-world datasets (which can be ethnicity) and assign each of them a label.\n2) Use a different task that does not necessarily deal with humans or at least deals with it in a different way. For instance the CelebA dataset contains a column for whether the person is wearing a hat or not.\n\nThe larger point that discrimination is exacerbated through pruning and can be addressed, can be written without using such a risk-filled task. In fact, a single paragraph example can be given of face and ethnicity recognition (with the ethical consideration section the authors are proposing), to show the issue also applies to humans in general. But I think centring that creates unnecessary risks. Not fully, no. I'm not sure that it will be possible with the remaining time that the authors have.",
            " The response adequately clears my concerns on the proposed relaxation for the solution and related work. Thanks for including the comparison table. The response to the over-interpretation of the theoretical result in the paper is adequate. The promised change to the text after Corollary 1 is good. I would strongly encourage the authors to propagate this change to the rest of the paper e.g. the text in the heading of Sections 4 and 5 can be changed to may cause. \n\nThe work presents concrete evidence for the relation between two training properties during pruning and unfairness which is an important problem due to prevalence of pruning. Thus I am increasing the score to 7, Accept. The motivation from theory needs improvement which needs to be adequately acknowledged in a revision. The presentation should be improved also including error bars in figures.",
            " I would like to thank the authors for their clarifications. As the authors mentioned, I hope some of these discussions could be included in the camera-ready version of your paper.  \n\nI have one additional question for Figure 7. What's the overall accuracy of the unfair model (top) and the fair counterpart (bottom)? It looks like the overall accuracy of the fair model decreased compared to the unfair model. If so, please further explain why the proposed loss may decrease the performance. \n",
            " Thank you for responding to the queries. The following two queries are not addressed. The ones on related work are addressed perfectly.\n\n### Claims\n\nI think the response missed to address my point. My concern is that Corollary 1 analyzes excess risk for a group but the claim of increase in unfairness is on the difference between excess risks between groups. That is, the excess risk for both groups may increase after pruning while leaving the difference between them unchanged. So the unfairness may or may not increase. I am ok with the analysis proving gradient and Hessian properties as causes for the increase in excess risk for a group (it is a good contribution). But the discussion in the paper incorrectly implies that the unfairness increases and it is due to the two properties. So the theoretical results are over-interpreted. In light of this discrepancy, the 'may exacerbate' in this statement needs to be more concretely specified.\n> As noted in the paper, there is a relative change of excess risk due to pruning, which in turn, may exacerbate unfairness.\n\n### Mitigating solution\n\nComparison between \"full\" and \"relaxed\" version could have been included in the response either in the response or in the updated PDF. For example, percentage changes in performance metrics when switching from relaxing one or both of the constraints would help. The relation between the relaxed version and the existing fairness and domain generalization approaches will put the method in a broader context. It does not decrease the contribution of this work.",
            " Thank you for your clarifications to the questions! Your answers make sense. I am glad to see the suggestion in Q2 was well-received, and I think adding some of the responses to the other questions to the paper text will improve it as well",
            " Although the paper is a study of bias mitigation, as one reviewer points out, the ethnicity classification task from face images is already flawed in some ways dealing with surveillance and enabling oppression. (Even the much-lauded Gender Shades paper has a problematic task underlying it.) The authors have not considered the broader impacts of working on this particular task. Even with good intentions that the authors have demonstrated on bias mitigation, they have not discussed the potential ills of ethnicity classification with regards to surveillance states.\n The authors may consider swapping out the example task for another one. They should certainly discuss in a few sentences the potential issues with the task they are illustrating the method on (in the broader impacts section). ",
            " The work studies effect of pruning neural networks on their losses for specific groups and the differences between them, termed as unfairness. It shows that gradient norm and Hessian of the loss function are important quantities controlling the upper bound on group-specific losses. As a way to mitigate unfairness from pruning, the effect of adding constraints to the learning problem that penalise the two quantities is explored. Some of the claims are validated through experiments on a face images used to train an ethnicity detection model. Strengths\n- Provides clear experimental evidence that backs the claim of relation between excessive losses from pruning and properties of the network such as gradient norm and the max eigenvalue of the Hessian.\n- Concepts and results are presented clearly.\n- Studies a practically relevant problem.\n\nWeaknesses\n- The main claim on unfairness from pruning is not well-motivated by the theoretical results. The discussion centres on unfairness that is difference in excessive losses across groups while the result analyzes excessive losses for each group separately.\n- Missed related work which makes a similar observation that pruning may cause unfairness.\n- Experimental results are not presented as clearly as possible. Error bars can be reported and correlation coefficient can be reported in plots where the claim is that the plotted quantities are correlated.\n\nThe evidence presented in this work that pruning causes unfairness is an important one because of the prevalence of pruning in practice. Also, the observations on gradient norm and max eigenvalue of the Hessian are interesting. However, the presentation of the results seems to overstate the significance of the theoretical results. I think that these can be addressed through minor writing edits.\n\n---\n\nAfter the response\n\nThe response adequately clears my concerns on the proposed relaxation for the solution and related work. The response to the over-interpretation of the theoretical result in the paper is adequate. The promised change to the text after Corollary 1 is good. I would strongly encourage the authors to propagate this change to the rest of the paper e.g. the text in the heading of Sections 4 and 5 can be changed to may cause. \n\nThe work presents concrete evidence for the relation between two training properties during pruning and unfairness which is an important problem due to prevalence of pruning. Thus I am increasing the score to 7, Accept. The motivation from theory needs improvement which needs to be adequately acknowledged in a revision. The presentation should be improved also including error bars in figures. Major points to address in the response\n\n1. There is an unacknowledged jump in reasoning from Corollary 1 to the main claim of the work that pruning increases unfairness. Corollary 1 shows that the upper bound on excessive loss for *a given* sensitive group may increase due to pruning. But it does not guarantee that *relative* excessive loss may increase. Also, the results are on the upper bound of the excessive loss and not the actual value of the excessive loss. The difference in actual loss across groups need not be smaller than the difference between upper bound. The theoretical results may motivate investigating gradient norms and the Hessian in experiments but the motivation is not as clear as presented. Thus the claims for studying disparate excessive loss in the rest of the results are overstated. The claims should be stated more carefully.\n\n2. Please discuss the work by Hooker et al. 2020 which seems to have the same conclusion. Please discuss relationship between the findings.\nHooker et al. 2020, Characterising Bias in Compressed Models. https://arxiv.org/abs/2010.03058\n\nAnother related work is Ahia et al. 2021 which posits that pruning affects infrequent sentences more. This relates to Proposition 1\u2019s claim that the proportion of data points from the group is a potential source of disparate impact of pruning.\nAhia et al. 2021, The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation. https://aclanthology.org/2021.findings-emnlp.282/\n\n3. Validating the relaxed version of the mitigation method eq. (8) is a bit unsatisfactory. Partly because it is not directly related to the analysis. Consider solving the optimization problem in eq. (7) instead of the relaxed version on a smaller dataset and model. This will help in validating the constraints motivated by the theoretical analysis instead of simplifying the problem. Perhaps only the gradient norm penalty, which is relatively quicker to estimate than max eigenvalue of the Hessian, can be tested in experiments.\n\nSecondly, the relaxed version is an existing fairness method which is not discussed. It is also considered in the domain generalization problem where one approach is to equalize losses across domains that are groups in this case. In this sense, the contribution can be rephrased as taking an existing method and showing its relevance for fairness of pruning.\nSee Donini et al. 2018 https://papers.nips.cc/paper/2018/hash/83cdcec08fbf90370fcf53bdd56604ff-Abstract.html, and Williamson and Menon 2019 https://proceedings.mlr.press/v97/williamson19a.html for the fairness method, and Section 2.4 of Krueger et al. 2021 https://proceedings.mlr.press/v139/krueger21a.html for the domain generalization method.\n\n4. Finally, please address the ethical concern of using the UTKFace dataset as the only exemplar in the work.\n\n\nMinor points or suggestions (no response is being expected)\n\nHandling one failure mode (disparity due to pruning, here) in isolation risks unintended consequences on other failure modes. Please consider adding a note that the question of how the proposed solution impacts fairness on other attributes, robustness, and interpretability of the pruned network.\n\nGiven the discrepancy between the theoretical results and claims, the empirical results become essential to the significance of the work. Consider repeating the analyses in Figures 2 to 6 on more models and datasets as considered in Figure 9.\n\nIn the line 88, fairness wrt pruning is taken to be zero excessive loss \\xi(D) =0. While terming this as fair pruning is ok, it can be commented that the pruned network can still be unfair in terms of differences in losses between groups.\n\nClarify the accuracy metric used in Figure 3.\n\nMention the discrepancy between loss metrics (like squared loss) used in the results and accuracy metric (0-1 loss) used to evaluate them in the experiments.\n\nProposition 3 can be a remark as it is a known fact.\n\nAdd error bars specifically in Figure 8 and 9 where different methods are compared. Ideally error bars should be reported in other figures also to convey robustness of the findings.\n\nI think a more natural way to visualise Figure 1 would be a multiple line plot of accuracy vs pruning percentage with separate lines for each group.\n\nReport the R^2 value for the correlation between the two variables shown in Figure 2, 3 and 6 which is easier to interpret.\n\nIt makes more sense to normalize the gradient norm plot in Figure 8 since two different models are compared.\n\nInclude a reference to Weyl\u2019s inequality e.g. ones in https://terrytao.wordpress.com/tag/weyl-inequalities/.\nWeily\u2019s -> Weyl\u2019s in Appendix.\n\nConsider using the first person in sentences as it is less ambiguous, The paper uses -> we use. Some of the limitations are presented. Please discuss limitations including relevance of theoretical results to differences in excessive loss, reliance on assumptions like being able to find the local minima. Also, mention the effect of other pruning methods, and uncertainty around effect on other properties of the pruned model than fairness. Prominently write the assumptions in one place and reference them in the results like in Proposition 1 which does not state the assumptions.\n\nPlease consider clarifying why ethnicity detection task from face images was used as an example to illustrate effects of pruning. Since it is a controversial application of machine learning with damaging consequences (with or without fairness concerns), it becomes imperative to motivate its inclusion in the paper. In my personal opinion (disagreements are welcome), the inclusion of facial recognition application should not appear as an endorsement of the technology.",
            " This paper has studied two factors causing disparity of network pruning, that is differences of gradient norm and Hessian matrix cross groups. It further introduced a simple training strategy to mitigate this disparity. With extensive experiments on UTK-Face, CIFAR-10, and SVH datasets,  this strategy has shown to mitigate the disparity well and keep comparable pruning performance to those without migration. Strengths:\n1. This paper is well written and the idea is clear and easy to follow. \n2. It is novel to explain the fairness with gradient norms and Hessian matrices across groups theoretically.\n3. The proposed method is simple yet efficient to mitigate the disparity across groups. \nPlease look at the Questions for the weaknesses. 1. It is unclear to me how to do the pruning with mitigation. Does the mitigation after pruning need retraining the model? \n2. In Figure 7, although the fairness among groups is improved,  the pruning accuracy of all groups except \u201cOthers\u201d equally decreases with that. The unfair model (top) seems to be an unpruned model. Is the fair model (bottom) the one before pruning or after pruning? If it is after pruning, please clarify the pruning rate. \n3. If I understand correctly, all the experiments have the class labels as the protected groups. How does this mitigation strategy work when they are different? For example, the model on the UTK-Face dataset uses ethnicity as class labels and uses age as protected groups. \n4. In Proposition 2 and Theorem 2, $||f_{\\theta}(x) -y||$ is for the error instead of the accuracy, $(f_\\theta(x)(1-f_\\theta(x)))$ and the distance to the decision boundary seems negative proportional. The larger  $(f_\\theta(x)(1-f_\\theta(x)))$, the smaller the distance to the decision boundary is. \n5. In line 94, what is the soft output? Is it the output after softmax or before it?  The proposed mitigation solution uses the loss disparity cross groups as the fairness proxy. Although much simpler, it is not so novel that it can be proposed without knowing the gradient norms and Hessian matrices as the cause of the disparity. ",
            " This paper studies the relationship between pruning and group disparities in model accuracy. Using the UTKFace dataset and ResNet and VGG models, the authors show that increased levels of pruning show an increase in disparities in accuracy between groups. That is, groups that had lower accuracy than average had their accuracies get lower, while groups with higher than average accuracy have their accuracies improve. The authors relate these changes theoretically to the gradient norm and Hessian and propose mitigation techniques with these insights.  The paper\u2019s main strengths are its originality, clarity, and quality. The effect of pruning on group disparity metrics has not been studied in the past, and this paper does a very thorough theoretical and empirical treatment of the effect. In particular, the insights about the relationship of the gradient norm and Hessian eigenvalues to fairness violation were well done. The paper is systematic in its approach and exposition, with an easy-to-follow motivation.\n\nThe main weakness of the paper is that the mitigation results presented are difficult to interpret. While the results showing the relationship between pruning and fairness violation are quite clear, the results shown in figure 9 are difficult to read and understand. For example, consider the fairness violation plot in the bottom left of Figure 9. Here there is no smooth behavior relative to the pruning fraction, despite the fact that the theoretical results seem to indicate that there should be. It looks as though there may be some statistical uncertainty in these experiment results. \n\nEven without these mitigation results, however, the theoretical treatment and empirical results on pruning are interesting and could be a paper on their own. \n In equation 3, why is the maximum chosen for fairness violation? Was the average, standard deviation, etc. also considered? Does max have desirable theoretical properties that the others don\u2019t? Using the maximum implies a worst case fairness violation, but it could be interesting to understand the behavior of, say, the average fairness violation as well. This may be worth discussing in the text. \n\nRather than having the two bars per group in figure 2, would it be possible to made a 2D plot of group size vs gradient norm instead? For this figure, it\u2019s not crucial to know which groups are which, but instead to see the correlation between group size and gradient norm more directly. It can be inferred from the bars, but a scatter plot might convey the results more clearly. \n\nGiven that a relationship between gradient norms and loss is shown, what is the relationship between fairness violation and accuracy? Is there an inherent tradeoff? In interpreting the results of figure 9, I found myself wondering whether having a lower accuracy automatically meant having a higher fairness violation. It was hard to tell from the plots themselves, but I think this is worth some discussion.\n The primary limitation of the work is that it only considers pruning in the context of a single dataset and task (with multiple models). So, some of the empirical results may not generalize to, say, a BERT model performing sentiment classification. That being said, the theoretical results are not task dependent. "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concern about the ethical implications of the research, calling the authors' decision \"risky\" and suggesting they \"not doing this.\" The reviewer also states the task is \"not the best way to show the issue\" and identifies \"unnecessary risks.\"",
            "The reviewer expresses satisfaction with the authors' responses and improvements, ultimately recommending acceptance and increasing the score. Phrases like \"adequately clears my concerns,\" \"Thanks for including,\" \"response to...is adequate,\" \"promised change is good,\" and \"strongly encourage\" all indicate a positive sentiment.",
            "The reviewer expresses gratitude ('thank the authors') and expresses hope that the discussions will be included in the final version of the paper.",
            "The review expresses concerns about the claims made in the paper, stating that the theoretical results are 'over-interpreted' and that a key point was 'missed'. It also points out missing information regarding the mitigating solution.",
            "The reviewer expresses satisfaction with the author's clarifications and suggestions, indicating a positive overall assessment.",
            "The review expresses concern about the ethical implications of the paper's chosen task (ethnicity classification), stating it is 'flawed' and 'problematic' due to its potential for oppression and use in surveillance states. The reviewer suggests the authors have not adequately considered the broader impacts and potential ills of the task.",
            "The review expresses a positive sentiment overall, acknowledging the work's strengths, the importance of the problem, and the concrete evidence presented. The reviewer increased the score to 7 (Accept) after the response, indicating a favorable assessment.",
            "The reviewer expresses overall positive sentiment by highlighting the paper's strengths, such as being 'well written,' having a 'clear and easy to follow' idea, and proposing a 'simple yet efficient' method. The reviewer also acknowledges the novelty of the theoretical explanation.",
            "The review expresses a generally positive assessment of the paper, highlighting its originality, clarity, and quality. It praises the thorough theoretical and empirical treatment of the effect of pruning on group disparity metrics and acknowledges the interesting theoretical treatment and empirical results on pruning."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"pretty risky decision,\" \"not the best way,\" and \"unnecessary risks,\" which indicate a critical assessment of the authors' choices and methodology. They also suggest alternative approaches, implying dissatisfaction with the current approach.",
            "The tone is supportive, offering encouragement (\"strongly encourage\") and constructive criticism aimed at improving the paper. The reviewer also explicitly states their acceptance and increased score, signaling a supportive stance.",
            "The reviewer uses phrases like 'I would like to thank' and 'I hope,' which are indicative of a supportive and encouraging tone.",
            "The tone is critical, using phrases like 'missed to address my point', 'incorrectly implies', 'over-interpreted', and pointing out discrepancies. The reviewer directly questions the claims and suggests the need for more concrete specifications.",
            "The reviewer uses encouraging language such as \"Thank you\", \"make sense\", \"I am glad\", and \"will improve it as well\", suggesting a supportive and helpful tone.",
            "The tone is critical as the reviewer points out flaws in the paper's choice of task, using words like 'flawed,' 'problematic,' and stating the authors 'have not considered' important ethical issues. The suggestion to 'swap out the example task' also indicates a critical stance.",
            "The review presents both strengths and weaknesses of the work, offering constructive criticism and suggestions for improvement. While acknowledging the value of the research, it also points out areas needing attention, such as the motivation from theory and presentation clarity.",
            "The review presents both strengths and weaknesses of the paper. While praising the clarity, novelty, and efficiency, it also raises several questions and concerns about the methodology, experimental setup, and theoretical claims. The use of direct questions and phrases like 'It is unclear to me' and 'If I understand correctly' indicates a critical but balanced assessment.",
            "The review provides both strengths and weaknesses of the paper. While praising the originality and clarity, it also points out difficulties in interpreting the mitigation results and suggests improvements for figures and analysis. The reviewer uses phrases like 'main weakness', 'difficult to interpret', and poses questions to encourage further discussion, indicating a balanced and constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer acknowledges the validity of the authors' point about discrimination and pruning but consistently argues against the chosen task (ethnicity classification using face recognition datasets) due to ethical concerns. The reviewer suggests alternative approaches to demonstrate the same point without the ethical risks, maintaining a consistent stance throughout the review.",
            "The review is consistent as it expresses overall positive feedback, acknowledging that the authors addressed concerns and appreciating the work's contribution. While suggesting improvements like strengthening the theoretical motivation and enhancing presentation, these are presented as constructive suggestions for revision and do not contradict the positive assessment and acceptance recommendation.",
            "The review is consistent because it expresses gratitude for clarifications, suggests improvements for the camera-ready version, and raises a specific question about Figure 7 in a logical and coherent manner. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because the reviewer raises valid concerns about the interpretation of theoretical results regarding unfairness and suggests concrete improvements related to experimental validation and contextualization of the proposed method. The reviewer's points are logically connected and focused on specific aspects of the paper, without any self-contradictory statements.",
            "The review expresses a consistently positive and encouraging tone. The reviewer thanks the authors for clarifications, finds their answers sensible, and is pleased that a suggestion was well-received. The suggestion to add responses to the paper is constructive and aligns with the overall positive assessment. There are no contradictory statements or conflicting opinions.",
            "The review consistently argues that the authors should consider the ethical implications of using ethnicity classification from face images as a task, and suggests ways to address this concern, either by changing the task or discussing the broader impacts.",
            "The review is consistent. Initially, the reviewer pointed out weaknesses related to theoretical motivation, related work, and clarity of experimental results. After the authors' response, the reviewer acknowledges that their concerns regarding relaxation and related work are addressed and the response to over-interpretation is adequate.  The reviewer increases the score to 'Accept' while still suggesting improvements for the final version, particularly regarding the theoretical motivation and presentation. This progression from initial critique to acceptance with suggestions for improvement is a consistent and logical flow in the review process.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper in a balanced way. It praises the clarity, novelty, and simplicity of the method while also raising valid questions and points for clarification and improvement. The reviewer's questions are aimed at improving the paper, not contradicting the initial positive assessment of its strengths.",
            "The review is consistent because it clearly identifies both the strengths and weaknesses of the paper. It praises the originality, clarity, and thoroughness of the theoretical and empirical analysis of pruning and fairness, while also constructively criticizing the interpretability of the mitigation results and suggesting areas for improvement. The reviewer's comments and suggestions are logically connected and do not contradict each other, presenting a balanced and coherent evaluation of the paper."
        ]
    },
    {
        "paper_id": "iclr_2020_HyxJ1xBYDH",
        "paper_title": "Learning-Augmented Data Stream Algorithms",
        "paper_abstract": "The data stream model is a fundamental model for processing massive data sets with limited memory and fast processing time. Recently Hsu et al. (2019) incorporated machine learning techniques into the data stream model in order to learn relevant patterns in the input data. Such techniques were encapsulated by training an oracle to predict item frequencies in the streaming model. In this paper we explore the full power of such an oracle, showing that it can be applied to a wide array of problems in data streams, sometimes resulting in the first optimal bounds for such problems. Namely, we apply the oracle to counting distinct elements on the difference of streams, estimating frequency moments, estimating cascaded aggregates, and estimating moments of geometric data streams. For the distinct elements problem, we obtain the first memory-optimal algorithms. For estimating the p-th frequency moment for 0<p<2 we obtain the first algorithms with optimal update time. For estimating the p-the frequency moment for p>2 we obtain a quadratic saving in memory. We empirically validate our results, demonstrating also our improvements in practice. ",
        "review_ids": [
            "BJloMuTnFr",
            "HJxwv99GqS",
            "Sye1Z22R5B"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper talks about calculating various statistics over data streams. This is a very popular topic and is very relevant in big data analysis. A lot of work has been done in this general area and on the problems that are discussed in the paper. The new idea in the paper is better streaming algorithms under the assumption that there is a \u201cheavy hitters\u201d oracle that returns data items that have a lot of representation in the stream. The authors give provably better algorithms for the distinct elements problem, F_p moment problem (p > 2), and some more problems. These are important problems in streaming data analysis. They improve the space bounds and interestingly in some cases the bounds are better than what is possible without the oracle assumption. This also shows the power of such an oracle. There are experimental results to demonstrate the efficiency of the algorithms. At a high level the work seems good and interesting for a large audience interested in streaming data analysis. I have not gone over the proofs in detail (much of which is in the appendix).\n\n- Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect.\n- It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space.\n",
            "Algorithms for Streaming data using a machine learning oracle is analyzed theoretically and empirically.\n\nThe idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data. The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds. I am not very familiar with this line of research so my comments will be more general in this case. The idea of improved bounds for streaming algorithms using machine learning oracle seems to be very appealing to me. The authors present novel theoretical results supporting this.\n\nExperiments are performed on real as well as synthetic datasets using Hsu et al.\u2019s method as an oracle.  Two real-world problems are selected, i.e., distinct packets in a network flow, Number of occurrences of each type of search query, and it is shown that using a oracle improves performance as compared to methods that do not use the oracle. Overall, I think the paper seems to be  an interesting direction which has both formal guarantees and experiments validating them in real-world datasets. One issue is perhaps, very little in terms of related work. I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.",
            "The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data. The authors show that under this model, there exist algorithms that have significantly better time and space complexity than the current best known algorithms that do not use an oracle. The authors support their theoretical analysis with experiments in which the oracle is represented by a deep neural network and demonstrate improvement over classical algorithms that do not use machine learning.\n\nOverall, this paper seems like a solid contribution to the literature. However, in its current state it does seem to be presented and motivated in a way that is appropriate for the audience of ML researchers at ICLR. It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way. Therefore my score for now is a weak reject, but I am very happy to increase the score if the authors address my presentations concerns.\n\nMajor comments:\n* The oracle-augmented datasteam model needs to be contextualized better. I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me. For example, how do I even know that the oracle in question exists? What are the particular assumptions under which it exists? What are the requirements on the training data, optimization ability, generalization error, etc. How do we know that we can create in practice ML learning models that are sufficiently accurate to serve as an oracle?\n\n* The connections to deep learning seem arbitrary in some of the experiments. In one of the experiments, the authors train neural networks over a concatenation of IP address embeddings. Why do we need to use deep learning here? What is the benefit of using DL algorithms within the oracle-augmented datastream model? Is a simple algorithm enough? What algorithms should we ideally use in practice? What if you used simpler online learning algorithms with formal accuracy guarantees?\n\nMinor comments:\n* I thought there was a bit over-selling in intro. The authors say that they match the theoretical lower bounds for several problems. However, you are in a different computational model in which you now have access to an oracle. This needs to be made more explicitly, and language could be a bit toned down (e.g. in this model, we can obtain runtime that match or improve over lower bounds...)"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment, highlighting the paper's relevance, interesting idea, improvements in space bounds, and experimental results. Phrases like \"very popular topic,\" \"very relevant,\" \"important problems,\" \"improve the space bounds,\" \"interestingly,\" \"power of such an oracle,\" \"good and interesting\" indicate a positive evaluation.",
            "The reviewer expresses positive sentiments such as \"very appealing to me\", \"novel theoretical results supporting this\", \"improves performance\", and \"an interesting direction\".",
            "The reviewer gives a \"weak reject\" score, indicating a negative overall assessment. The reviewer also expresses concerns about the paper's presentation and motivation, and raises several questions about the model and experiments."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The tone is balanced, acknowledging the strengths of the paper while also suggesting potential improvements and areas for further exploration. The reviewer points out the paper's positive aspects using supportive language but also offers constructive criticism using phrases like \"to make it practical it may make sense to talk about a more realistic, weaker oracle\" and \"it may even make sense to minimise the number of oracle calls.\"",
            "The review uses supportive language like \"very appealing\", \"interesting direction\", and focuses on the positive aspects of the paper, such as novel results and performance improvements. The reviewer also frames their criticism as a suggestion for improvement (\"One issue is perhaps, very little in terms of related work\").",
            "The review uses critical language, pointing out flaws in the paper's presentation, motivation, and experimental design. Phrases like \"shoved under the rug,\" \"arbitrary,\" \"over-selling,\" and questions that challenge the validity of the approach indicate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it initially praises the paper's topic, novelty, and results, and then offers constructive suggestions for improvement without contradicting the initial positive assessment. The suggestions are aimed at enhancing the practical applicability and exploring further research directions, rather than pointing out flaws in the core contribution.",
            "The review is consistently positive about the paper's idea, theoretical results, and experimental validation. The reviewer finds the direction interesting and appreciates both the formal guarantees and experimental results. The minor suggestion about related work does not contradict the overall positive assessment of the paper's novelty and contribution.",
            "The review is consistent because the reviewer's overall assessment and specific comments all point to the same core issue: the paper is not well-presented and motivated for an ML audience at ICLR, despite its potential theoretical contributions. The reviewer consistently emphasizes the need for better contextualization of the oracle model in an ML context and questions the justification for using deep learning in the experiments. The weak reject and willingness to increase the score are also consistent with this stance, indicating that the reviewer sees potential but requires significant revisions to address the presentation and motivation issues for the target audience."
        ]
    },
    {
        "paper_id": "iclr_2021_LjFGgI-_tT0",
        "paper_title": "BayesAdapter: Being Bayesian, Inexpensively and Robustly, via Bayesian Fine-tuning",
        "paper_abstract": "Despite their theoretical appealingness, Bayesian neural networks (BNNs) are falling far behind in terms of adoption in real-world applications compared with normal NNs, mainly due to their limited scalability in training, and low fidelity in their uncertainty estimates. In this work, we develop a new framework, named BayesAdapter, to address these issues and bring Bayesian deep learning to the masses. The core notion of BayesAdapter is to adapt pre-trained deterministic NNs to be BNNs via Bayesian fine-tuning. We implement Bayesian fine-tuning with a plug-and-play instantiation of stochastic variational inference, and propose exemplar reparameterization to reduce gradient variance and stabilize the fine-tuning. Together, they enable training BNNs as if one were training deterministic NNs with minimal added overheads. During Bayesian fine-tuning, we further propose an uncertainty regularization to supervise and calibrate the uncertainty quantification of learned BNNs at low cost. To empirically evaluate BayesAdapter, we conduct extensive experiments on a diverse set of challenging benchmarks, and observe satisfactory training efficiency, competitive predictive performance, and calibrated and faithful uncertainty estimates. ",
        "review_ids": [
            "a1bhKlPhrDu",
            "hAn0xqaK5cb",
            "Zezt6IvxE1Z",
            "1Cjil7LRnsc",
            "jCMENHJXCLE",
            "hUiMEzKBIpd",
            "cf1x6SffANo",
            "dtybQUEcJ1L",
            "5quiF9Q1TYK"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I do not see how you draw the conclusion \"MFVI-based BNN _learned from scratch_ is bad\" from \"the cold posterior problem has left a trail in the literature, and in fact we are not aware of any published work demonstrating well-performing Bayesian deep learning at temperature T = 1\". Particularly, Figure 13 in [Wenzel et.al.] shows that initializing MCMC with the MAP solution does not solve the cold posterior problem. ",
            "After reading the responses, I have the following concerns.\n\n- The differences between the proposed method and [Krishnan, 2020] seem to be using more weight samples and uncertainty regularization on out-of-distribution (OOD) data. Since [Krishnan, 2020] also uses MAP as initialization in VI, and shows several empirical results including predictive performance, uncertainty estimate and OOD detection. The novelty of using MAP as initialization in VI in the paper is decreased. I think you should rephrase the contributions of the paper, and mainly show the benefits of using more weight samples and uncertainty regularization techniques in the experiments. Also, I think it is necessary to include a comparison with [Krishnan, 2020].  \n\n- Using [Wenzel et.al], which is an MCMC-based method, to motivate the proposed method seems incorrect. Since the method is a VI-based BNN, you should instead provide evidence that MFVI-based BNN learned from scratch is bad.\n\n- Since the proposed method is a variant of VI methods, I still expect a comparison with SoTA VI methods, even if on small datasets.",
            "Thank you for the authors response to my concerns. \n\nOverall, I have a similar feeling as R2.  It provides an interesting and simple way to obtain a Bayesian-like neural network, but still no fundamental improvement that brings excitement to the community. Compared to previous issues, it nonetheless provides a new perspective. Thus I would like to keep my previous score. ",
            "Thanks for your response.\n\n* I would encourage you to place the additional details on hyper parameter selection in the main text or appendix to facilitate reproducing your work. \n\n* I agree that it would be unreasonable to expect an MFVI approach to outperform a deep ensemble. Just because your methods would fare worse does not mean the comparison is not useful. Such a comparison would give an idea of how close you can get with a SOTA MFVI approach (yours) compared to the SOTA across all approaches. (This is done in papers that get accepted at top venues: https://papers.nips.cc/paper/2020/file/781877bda0783aac5f1cf765c128b437-Paper.pdf) In other words, it would shed some light on the limitations of MFVI and better help practitioners understand when they should use MFVI and when ensembles are the better option.\n\n* You mention *\"We take the results of SWAG from its paper due to implementation difficulties.\"* I have gone through that papers' code before. I agree it is clearly research code quality but it is not impossible to adapt to existing workflows, especially considering the method is quite simple. ",
            "Just a comment on the threshold: Although BayesAdapter might be robust to the choice of threshold, other methods may not. For this reason, ROC-AUC seems like a more reasonable approach for comparison to me -- it considers every possible threshold. ",
            "**Contributions**\n\nThis paper proposes a post-hoc approach to obtain model uncertainty estimates from vanilla pre-trained NNs through MFVI fine-tuning. Namely, 1) the authors re-cast the KL divergence in the VI objective as weight decay applied to the variational parameters 2) the authors propose a variance reduction technique for the reparametrisation trick 3) the authors explicitly train their model to produce large model uncertainty on Out of Distribution (OOD) inputs. \nEmpirically, the proposed methods seems to retain the strong performance, and much of the simplicity, of point-estimate NNs while providing enhanced robustness in terms of uncertainty estimation. \n\n\n**originality and significance** \n\nTo my knowledge, most of the proposed techniques (or variants of them) have appeared before in the literature or are simple extensions of existing approaches: Re-casting MFVI as SGD [Khan et. al., 2018], Decorrelation of reparametrisation gradients across batch elements [Wen et. al., 2018], Training on OOD measurement points to produce large uncertainty [Hendrycks et. al., 2018 and Hafner et. al., 2018] .\nHowever, this work refines these ideas and puts them into a single framework which seems to produce strong results. I view this as a noteworthy contribution which might bring Bayesian Deep Learning closer to real world deployments. \n\n **clarity** \n\nMost ideas are presented clearly. The paper is well structured and easy to follow. Some passages are slightly ungrammatical but never does this impede the transmission of ideas.\n\n**pros**\n* Presents useful practices to make BNNs more mainstream with strong empirical performance.\n* Authors provide code for an efficient implementation of exemplar reparametrisation. \n* The proposed technique for OOD detection bypasses typical pathologies of MFVI [Ovadia  et. al., 2019] by explicitly optimising variational parameters to produce large model uncertainties OOD.\n\n**cons**\n* Exemplar reparametrisation is very similar to Flipout [Wen et. al., 2018]. A comparison of the two would be appreciated. \n* The proposed technique for OOD detection is not very principled and has provides no guarantees. It seems empirically successful however. \n* The experimental setup is not very clear, even when reading the supplementary sections concerning experimental setup. Some questions I was left with:\n\t* There are many hyperparameters, how did you find all of them? \u2014Especially the weight decay coefficients.  Are the standard deviations implied by these priors interesting / meaningful in any way? \n\t* A single Mutual Information threshold is provided. Is this one used for uncertainty calibration training on all tasks? Is it also used when classifying inputs as in-distribution or OOD? If this is the case, it is possible that models without uncertainty calibration training would benefit from using a different threshold. A better metric might be ROC-AUC, as it is threshold agnostic.\n* The only baselines provided are other VI approaches. SWAG is known to be a decently strong baseline but it is not evaluated for OOD detection peroformance. The current  state of the art baseline for uncertainty quantification is deep ensembles [Ovadia  et. al., 2019]. These are much more expensive. However, it might be interesting to see how they compare.\n* The authors repeat all experiments 3 times but only provide mean results. In some cases, like tables 1 and 2, the values presented are similar across methods. I think that errorbars (standard deviation across 3 runs) would be very informative to the reader. \n\n**Other comments and questions:**\n* Typo in title: Bayesian, not Bayeisian\n* The maximum predictive entropy (and thus mutual information) will depend on dimensionality of output space (number of classes). In your experimental section, you say you set a single threshold for all models. Could you further comment on this?\n\n**References**\n[Wen et. al., 2018]  https://arxiv.org/pdf/1803.04386.pdf\n[Khan et. al., 2018] http://proceedings.mlr.press/v80/khan18a/khan18a.pdf\n[Ovadia  et. al., 2019] https://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift.pdf\n[Hendrycks et. al., 2018] https://openreview.net/forum?id=HyxCxhRcY7\n[Hafner et. al., 2018] https://arxiv.org/abs/1807.09289",
            "This paper proposed one simple and effective way to trainBayesian neural networks (BNN). \n\nPros:\n1. The proposed method is quite simple and cheap to realize, compared to previous Bayesian methods.  \n2. Extensive experiments on a diverse set of challenging benchmarks have been conducted, which shows several promising results of the proposed method.\n3. The proposed idea is novel which distinguishes from most of previous efforts, which try to train BNN from scratch using Bayesian methods. As described in this paper, most of previous methods, though paying much additional efforts than deterministic ones, do not lead to expected results, even with non-diagonal covariance matrices. The BayesAdapter, however, pays little efforts and obtains improvements even with diagonal covariance matrices. From this perspective, this is an encoraging result. \n\nCons:\n1. The results of comparison in Table 1 only repot the average result in 3 runs (3 is kind of small). However, it is better to show the std metric of the result to make the comparison more convincing because the improvement of BayesAdapter in average value is in fact not very apparent, especially compared with MAP. If the variance of the result is large, then there will be large overlap between different methods and thus it is not reasonable to claim that there is an apparent advantage over previous methods. \n\n2. In evaluating the result of BayesAdapter,  MC samples are used. What if only using the mean value of the posterior?  Compared to deterministic methods like MAP, inference using MC is more costly.   In addition, it is suggested to provide some visualizations of the posterior distribution after BayesAdapter. \n\n3. Based on results in Table 3, BayesAdapter- performs similar as baselines, which indicates that the improvement comes from calibrating the uncertainty estimation. This leads to another question: what if we also use such calibration for the baseline methods. It would be interesting to make such a comparison. ",
            "The paper explores the variational training of Bayesian neural networks. It proposes to improve the quality of the inferred variational posterior and computational efficiency of the procedure by (i) better initialization (mean parameters are initialized at the MAP) (ii) reducing variance in the Monte Carlo approximated evidence lower bound by increasing the number of weight samples (one per datapoint in a batch) (iii) a posterior regularization encouraging higher uncertainty on adversarially generated or other \u201cnear OOD\u201d data.  \n\nThe authors use the term BayesAdapter to refer to the process of running black-box variational inference from a fully factorized variational approximation with mean initialized at the MAP estimate and randomly initialized variances. The fact that variational inference (especially those employing fully factorized approximations) are susceptible to poor local optima and that better initializations can help navigate these local optima is widely known. The fact that better initializations can lead to somewhat improved posterior approximations is not surprising. Such initializations are also standard practice when employing stochastic gradient MCMC techniques and Laplace approximations (where it is a requirement). Reducing variance by increasing the number of weight samples to one per datapoint in a batch is another straightforward idea, and it is unclear whether it can be claimed as a contribution of the current paper. Kingma et al., in their local re-parameterization considered a variant with per data samples as well. The uncertainty regularization is indeed novel and appears effective (but the experiments illustrating its benefits need to be better explained). \n\nGiven the modest methods contributions, the empirical section needs to be particularly strong to demonstrate that the combination of these incremental improvements provides meaningful empirical advantages. To their credit, the authors demonstrate their approach on several large datasets and do provide experiments for vetting different aspects of the proposed extensions to variational BNN training. However, many experiments are missing details and some are lacking key comparisons. Overall this section could be significantly strengthened. \n\n* Tables 1 and 2 need to include comparisons against deep ensembles and multi-SWAG (https://arxiv.org/pdf/2002.08791.pdf). If the goal of this paper is to claim that variationally trained BNNs (with the proposed improvements) are useful in practice, a natural question to ask is whether they are competitive with far simpler ensembling approaches that are able to account for the multimodality of the posterior surface, unlike variational BNNs.\n* How was the calibration threshold $\\gamma$ chosen for these experiments? How sensitive is the performance to this choice? Ideally, the authors would include results with different settings of $\\gamma$. How were the prior precisions selected (which determine $\\lambda$ set?  My main worry is that the marginal improvements provided by Bayesadapter variants over BNNs disappear when making slightly different parameter choices. It would be great if the authors can demonstrate that this isn\u2019t the case. \n* Section 4.2 needs more details about the experimental setup. Did the 1000 / 10000 OOD training/test examples include both images created via PGD and SNGAN (for CIFAR 10) and PGD and BigGAN (for imagenet)? If so, how many from each source? If not, it would be interesting to see cross performance \u2014 using PGD images for training and SNGAN images for testing. \n* In Table 4, it doesn\u2019t make sense to include ECE numbers from a different architecture trained via SWAG. These numbers are not comparable. Also, interestingly, both BayesAdapter variants have lower ECE scores than vanilla BNN on CIFAR, suggesting poorer calibration. Do the authors have an explanation for this? \n\nBased on concerns about both novelty and experiments I am currently leaning towards a reject, but could be convinced otherwise based on the authors\u2019 response and additional comparisons.",
            "This paper introduces a fast way to get Bayesian posterior by using a pretrained deterministic model. Specifically, the authors first train a standard DNN model and then use it to initialize the variational parameters. Finally the variational parameters are optimized through standard variational inference (VI) training. To further improve uncertainty estimate, the authors propose an uncertainty regularization which maximizes the prediction inconsistency on out-of-distribution (OOD) data. Experiments including image classification and uncertainty estimates are conducted to demonstrate the proposed method.\n\nThe idea of this paper is quite simple: initialize the mean in variational parameters by a pretrained DNN. Thus the method is cheap and simple enough to use broadly in practice. The authors did reasonable empirical tests. I especially appreciate the ablation study which helps understand the method a lot. The paper is well-written and easy to follow.\n\nI mainly have the following concerns about the paper. \n\n- One of the main motivations to use a pretrained DNN is that BNN learned from scratch is worse than its corresponding DNN. I feel this claim is misleading. Many papers have shown that BNNs trained from scratch outperform DNNs. Particularly the paper [Wenzel et.al. 2020] which the authors cited to support their claim clearly shows that BNN (with reasonable temperature) is significantly better than DNN in predictive performance. But I do agree that the proposed method is cheaper than training BNNs from scratch. \n\n- The experimental results verify the effectiveness of the proposed method. But the results of the proposed method seem to be worse than BNNs training from scratch (e.g. the ImageNet results in [Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning, ICLR 2020] are much better). This also supports my first point, that the main benefit of the proposed method is to get the Bayesian posterior fast and cheaply, instead of to improve BNNs\u2019 performance. I think the authors should revise the claim to be more precise.  \n\n- The proposed method is closely related to [Specifying Weight Priors in Bayesian Deep Neural Networks with Empirical Bayes, AAAI 2020] which also uses a pretrained DNN as the initialization of variational parameters. How does the proposed method compare to it theoretically and empirically? The main idea seems quite similar; the only difference is that the proposed method does not use the pretrained model as prior in VI. Due to the similarity, I think a comparison is necessary. \n \n- The authors argue that BNN training suffers from suboptimal local optima. Could the authors provide evidence/citations to support this claim? I do not think it is true. Perhaps it is true only for a few BNN methods such as BNN using naive VI.\n\n- As the proposed method is essentially a VI method, it would be interesting to see comparisons to SOTA VI methods.\n\nOverall I'm positive about this paper and would be happy to increase my score if my concerns are addressed. \n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral",
            "Positive",
            "Neutral",
            "Neutral",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses disagreement with the author's conclusion and uses phrases like \"I do not see how you draw the conclusion\" which indicates a critical stance.",
            "The review expresses concerns about the novelty of the proposed method, questions the motivation, and suggests additional comparisons are needed. The reviewer uses phrases like \"concerns\", \"novelty of ... is decreased\", \"seems incorrect\", and \"I still expect\" indicating a critical stance.",
            "The reviewer acknowledges the authors' response but expresses a similar feeling to another reviewer (R2), indicating no significant improvement. While acknowledging a new perspective, the overall sentiment remains neutral due to the lack of 'fundamental improvement'.",
            "The reviewer expresses agreement and provides constructive suggestions, indicating a positive overall assessment.",
            "The review provides a suggestion regarding a potentially better evaluation metric (ROC-AUC) for comparison purposes, without expressing strong positive or negative opinions about the paper itself. It focuses on a specific aspect of the methodology and offers constructive feedback.",
            "The review acknowledges the paper's contributions and potential but also raises several concerns regarding originality, experimental setup, and comparisons with existing methods. The language is balanced, pointing out both strengths and weaknesses.",
            "The review expresses overall positive sentiment, highlighting the simplicity, cost-effectiveness, novelty, and promising results of the proposed method. The reviewer uses terms like 'encouraging result' and acknowledges the 'extensive experiments' and 'diverse set of challenging benchmarks'.",
            "The review expresses concerns about the novelty of the methods and the strength of the experimental results. Phrases like \"modest methods contributions,\" \"incremental improvements,\" \"many experiments are missing details,\" and \"lacking key comparisons\" indicate a negative sentiment. The reviewer is \"leaning towards a reject.\"",
            "The reviewer states \"Overall I'm positive about this paper\" and expresses willingness to increase their score if concerns are addressed. The appreciation for the ablation study and the statement that the paper is well-written and easy to follow further contribute to the positive sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Supportive",
            "Neutral",
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical, directly challenging the author's conclusion and providing a counter-example with specific reference to a published work (Wenzel et.al.). The phrase \"I do not see how you draw the conclusion\" signals disagreement and a questioning of the author's reasoning.",
            "The tone is critical, as evidenced by phrases like \"I have the following concerns\", \"novelty ... is decreased\", and \"seems incorrect\". The reviewer directly points out perceived weaknesses in the paper's approach and presentation.",
            "The tone is balanced as it expresses both positive aspects ('interesting and simple way', 'new perspective') and negative aspects ('no fundamental improvement') of the work. The reviewer uses neutral language and avoids overly strong opinions.",
            "The reviewer uses encouraging language like \"I would encourage you to...\", \"I agree that...\", and provides helpful suggestions and examples, indicating a supportive tone.",
            "The tone is neutral and objective. The reviewer uses phrases like 'Just a comment' and 'seems like a more reasonable approach' which are polite and non-confrontational. There is no overtly critical or supportive language.",
            "The review presents both pros and cons of the paper. It uses phrases like \"seems to retain the strong performance\" (positive) but also \"not very principled and has provides no guarantees\" (negative). The reviewer also poses questions and suggests improvements, indicating a constructive but critical approach.",
            "The review presents both 'Pros' and 'Cons' sections, offering specific points of praise alongside constructive criticisms. While acknowledging the strengths of the paper, it also raises concerns about statistical significance, computational cost, and potential improvements to baseline methods. This balanced approach indicates a fair and objective assessment.",
            "The review uses direct and critical language, questioning the contributions and experimental design. Phrases like \"it is unclear whether it can be claimed as a contribution,\" \"many experiments are missing details,\" \"some are lacking key comparisons,\" and direct questions about experimental choices contribute to the critical tone. The reviewer also explicitly states concerns about novelty and experiments.",
            "The review presents both positive aspects (well-written, easy to follow, reasonable empirical tests, appreciated ablation study) and negative aspects (concerns about misleading claims, comparison to existing methods, need for more evidence). The reviewer provides constructive criticism and suggestions for improvement, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it raises a logical question about the conclusion drawn by the authors based on the provided evidence and literature. The reviewer argues that the cited work does not directly support the conclusion that MFVI-based BNN learned from scratch is inherently bad due to the cold posterior problem, as the cited work indicates that even initializing MCMC with a MAP solution (potentially related to learning from scratch) does not solve the cold posterior problem.",
            "The review is consistent because all points raised by the reviewer are focused on improving the paper by asking for more justification, comparisons, and clarity regarding the novelty and motivation of the proposed method within the context of existing literature, especially VI methods. There are no contradictory statements or conflicting requests.",
            "The reviewer expresses a balanced view, acknowledging the paper's interesting and simple approach and new perspective, while also noting the lack of fundamental improvement.  The decision to maintain the previous score is consistent with this overall assessment, as it suggests the reviewer's initial concerns were not fully addressed despite the acknowledged positives.",
            "The review is consistent because the reviewer provides constructive feedback and elaborates on previous points without contradicting themselves. The reviewer acknowledges the authors' points and refines their suggestions in a logical manner.",
            "The review is consistent because it provides a logical argument for preferring ROC-AUC as a comparison metric. The reviewer points out that while BayesAdapter might be robust to threshold choice, other methods may not be. Therefore, using ROC-AUC, which is threshold-independent, is suggested as a more reasonable approach for comparison across different methods.",
            "The review provides a balanced assessment, acknowledging both the strengths and weaknesses of the paper without contradicting itself. While pointing out the lack of originality and some experimental limitations, the reviewer consistently appreciates the practical value and empirical results of the proposed method. The criticisms are constructive and aimed at improving the paper, not at invalidating its core contributions.",
            "The review is consistent because it acknowledges the strengths of the paper (simplicity, novelty, promising results) in the 'Pros' section, while also raising valid concerns and suggestions for improvement in the 'Cons' section. The criticisms are focused on experimental rigor and further analysis, which are constructive and do not contradict the initial positive observations. The review provides a balanced and coherent assessment of the paper.",
            "The review is consistent because it acknowledges the paper's contributions but raises valid concerns about the novelty of some methods and the lack of sufficient experimental validation. The reviewer's critique flows logically, starting with an assessment of the proposed methods, moving to the empirical evaluation, and concluding with a preliminary recommendation. The reviewer consistently points out areas needing improvement and provides specific suggestions, maintaining a coherent line of reasoning throughout the review.",
            "The review is consistent because the reviewer acknowledges the positive aspects of the paper (simplicity, clarity, ablation study) while also raising valid concerns about the claims, comparisons, and justifications. The reviewer's overall positive stance is maintained even with the criticisms, which are presented as constructive criticism for improvement."
        ]
    },
    {
        "paper_id": "iclr_2021_dgd4EJqsbW5",
        "paper_title": "Control-Aware Representations for Model-based Reinforcement Learning",
        "paper_abstract": "A major challenge in modern reinforcement learning (RL) is efficient control of dynamical systems from high-dimensional sensory observations.   Learning controllable embedding (LCE) is a promising approach that addresses this challenge by embedding the observations into a lower-dimensional latent space, estimating the latent dynamics, and utilizing it to perform control in the latent space.  Two important questions in this area are how to learn a representation that is amenable to the control problem at hand, and how to achieve an end-to-end framework for representation learning and control.  In this paper, we take a few steps towards addressing these questions. We first formulate a LCE model to learn representations that are suitable to be used by a policy iteration style algorithm in the latent space.We call this model control-aware representation learning(CARL). We derive a loss function and three implementations for CARL. In the offline implementation, we replace the locally-linear control algorithm (e.g., iLQR) used by the existing LCE methods with a RL algorithm, namely model-based soft actor-critic, and show that it results in significant improvement. In online CARL, we interleave representation learning and control, and demonstrate further gain in performance.  Finally, we propose value-guided CARL, a variation in which we optimize a weighted version of the CARL loss function, where the weights depend on the TD-error of the current policy. We evaluate the proposed algorithms by extensive experiments on benchmark tasks and compare them with several LCE baselines.",
        "review_ids": [
            "o67CDKifq89",
            "hyW1G_q-8aI",
            "QmfA7FZjVsT",
            "HzCWnu72cQJ",
            "okBn6BHrzZ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper aims to address an important question in reinforcement learning: policy learning from high-dimensional sensory observations. The authors propose an algorithm for Learning Controllable Embedding (LCE) based on policy iteration in the latent space. The authors provide a theorem to show how the policy performance in latent-space policy improvement depends on the learned representation and develop three algorithmic variations that attempt to maximize the theoretical lower bounds. In the experiments, the proposed algorithm CARL shows improved performance when compared with other LCE baseline algorithms. \n\nWhile I'm not particularly familiar with the field of LCE, I think the idea of learning a representation that is suitable for policy improvement is an interesting idea. The readability of this paper is also pretty good, which can be difficult to get right because the of the correspondence between the original space and the latent space. Overall the paper is easy to follow. \n\nWhile I do think Algorithm 1 is reasonable, I found its theoretical foundation, namely Theorem 1, is incorrect. In the proof of Theorem 7 on p15 in the appendix, I do not think the implication T^2 VE(x) < T VE(x) + \\gamma Delta(x) for all x, would hold. Because Bellman operator contracts in the L-inf norm, a basic inequality would rather take a form of  T^2 VE(x) < T VE(x) + \\gamma sup_y Delta(y). In addition to this, another minor error happens in the first equation on pg 16, where I believe the correct right hand side would be 1/(1-gamma) sup_y Delta(y), without the gamma dependency.\n\nHowever, a bound that depends on L-inf norm would be quite bad for Theorem 1, and current data collection process in Alg 1 is not sufficient for minimizing it. I think it might be possible not using an L-inf bound but using an expected error based on the policy's rollout distribution. However, this change would largely change the theoretical results, and perhaps the motivation or details of the algorithm design. Therefore, I do not think the paper is ready for acceptance at the current stage without a large revision. If the authors can address this question properly, I would raise my score.\n\nBeyond the flaw in the theory, there are some parts which can benefit from some clarification: \n1. In the offline CARL, how does the algorithm address the issue of out of distribution error due to using a batch dataset? \n2. The authors argue that the loss here is different from PCC many times in the paper, but they never explain whether the choice here is better (or in which way). \n3. In line 4 of Alg 1, how do we ensure such pi would exist?\n4. What is the definition of \"compatible reward function\" in the last paragraph on p4?\n5. For completeness of presentation, please include the definition of curvature loss.\n\n\n\n\n\n",
            "This paper proposes a new representation learning + RL algorithm called CARL, with a specific objective for learning a latent representation and dynamics model coupled with SAC policy learning in the latent space. Experiments on a few domains show CARL outperforming previous algorithms such as DREAMER and PCC.\n\nPros:\n\n+ The key points of the paper are relatively well organized and motivated properly.\n+ The experimental results succinctly demonstrate the promise of the proposed approach.\n\nCons:\n\n- It is difficult to follow important details about the operation of this relatively complicated method.\n- The experimental results are not sufficient for this largely empirical work.\n\nWith these pros and cons in mind, I am recommending a weak reject. See below for additional detailed comments.\n\nEDIT: After discussion, I have increased my score and am recommending weak accept. See the discussion with the authors for details.\n\n\nQuality\n---\n\nThe paper studies an important problem, proposes a novel solution, and has promising experimental results. However, the main drawback in terms of the quality of the work is that the results are not complete enough. For work that is empirically driven, I do not view the current results as sufficient for publication.\n\nIn particular, DREAMER appears to be competitive with CARL on a few domains. But DREAMER was also evaluated much more broadly across many tasks from the DeepMind control suite, indicating a level of robustness and performance that is, at best, hinted at in this work for CARL. A wider suite of experiments, for example using the same tasks as DREAMER, would go a long way in better shaping the reader's understanding of the proposed method.\n\nClarity\n---\n\nAs mentioned, the main points of the paper are presented well. The problem is properly motivated, and a central theorem gives rise to the proposed representation learning method. I did not check the proof for this theorem, but it appears sensible.\n\nHowever, the finer points in the paper, which are also very important, are difficult to follow. For example, what is \"model-based SAC\"? There does not appear to be a proper explanation or citation for this. Is the learned dynamics model F used in some way to learn the Q-function? Is this novel, or is it from prior work?\n\nConsidering the proposition that replacing other control algorithms in the latent space with model-based SAC is important for the overall performance improvement, a description of model-based SAC is important. Furthermore, an ablation study would be helpful in terms of understanding the relative importance of this change vs the proposed representation learning approach, which seems to be the novel part.\n\nSome other minor concerns about the methodological sections: there are many hyperparameters and not much guidance as to how to pick these; more discussion of why there are different versions of CARL and what are their respective strengths and weaknesses would be useful, especially for VCARL; I personally found the last paragraph of the VCARL description almost impossible to follow.\n\nOriginality\n---\n\nTo the best of my knowledge, the representation learning algorithm itself is novel. Perhaps a related work that is overlooked is https://arxiv.org/abs/1907.00953, which apparently has been accepted to NeurIPS 2020 but has been out for some time. At a high level, this work also incorporates representation learning into SAC, though the underlying details are different. Still, this approach seems actually more closely related than some of the current citations and comparisons, e.g., SOLAR and DREAMER. At least a citation seems to be in order, and preferably a comparison. Indeed, this prior work also carries out a more comprehensive evaluation on more tasks than the current work.\n\nSignificance\n---\n\nThis work has the potential to be significant, as many researchers and practitioners are currently interested in how to make deep RL more efficient and performative, in particular in visual settings. However, without a more comprehensive evaluation, it is difficult to judge for sure.",
            "Thanks for providing additional comments, I believe this clarifies some of my confusion. I now better understand the significance of the theoretical bits, which result in a similar loss function as PCC but is motivated and derived from a different and more general perspective. I still believe that the link between the theorem and the actual loss function is weak, as evidenced by the number of hyperparameters that have to be searched over. But this is acceptable to me.\n\nThe way I (now) view it, the resulting loss function is primarily interesting and significant because it allows for seemingly easier integration of general model based policy iteration algorithms. But we are only interested in this because policy iteration algorithms seem like they should work better in practice on a larger variety of tasks. Therefore, the primary contribution of this paper is still empirical, and the empirical evaluation is still lacking.\n\nI still do not follow the authors' justification about not being able to run additional experiments. The authors clarify that they are indeed stacking images to produce Markovian observations. If this is the case, it should be even simpler to try more tasks such as DM control suite. Why would the number of images that need to be stacked be larger for other tasks? If the actions are torque control, then velocity is the missing information that is not present in one image, but can be inferred from a stack of two images. In practice, stacking just a few images should be enough for every task visualized in the [DREAMER paper](https://arxiv.org/pdf/1912.01603.pdf) Fig 2.\n\nAnyway, I'm happy to discuss further with the authors if they wish, but that is up to them, as I am increasing my overall score from weak reject to weak accept.",
            "Thanks for your response, it is helpful and does clear up some of my original questions and inquiries. It does not, however, adequately address my primary concern about the lack of a comprehensive empirical evaluation.\n\nAs the authors point out, the empirical evaluation does not have to be viewed as the primary contribution. However, if the authors wish to present the theoretical result (and the loss function derived from it) as the primary contribution of the paper, the current state of the paper is still insufficient. The authors claim that comparing offline CARL with PCC is an ablation study. By my understanding of this work, I would disagree. Ablations are meant to isolate factors of variation in order to gain a deeper understanding of what changes and innovations contribute to observed differences. At least two things are varied between offline CARL and PCC: the loss function and the control algorithm. So, how important is newly proposed loss function vs the usage of model based SAC? I don't think that can be answered given the current experiments.\n\nThis question, however, is crucially important. As I view it, if the authors wish to highlight the theoretical result as an important contribution, then the authors should provide concrete evidence that the resulting loss function actually contributes significantly to better performance. Otherwise, it is just a weak relationship between a theoretical lower bound and a loss function with a lot of hyperparameters. And to provide this evidence, the authors must actually ablate prior methods such as PCC by swapping in the new loss function, while keeping as many other components constant as possible. I.e., don't just throw in model based SAC as another improvement.\n\nHowever, the authors seem to think that the comparison between offline CARL and PCC actually \"shows the importance of SAC as the control algorithm in place of iLQR\", which would place the contribution as more empirical, which then comes back to my original point about needing more experiments. If this is the case, then again, in my own view, the theoretical result and proposed loss function do not have supporting evidence that they are actually important.\n\nViewing this paper's contributions as a hybrid is also unsatisfying -- as I have discussed, currently neither the theoretical nor the empirical bits of the paper have enough evidence of their significance (though they are both promising).\n\nTo round out my point about additional experiments: while I do not pretend that this would not require substantial time and compute, I do not agree that this is out of scope for this paper. If the observations need to be Markovian, just pass in multiple images from the past few time steps. This pretty much suffices for any of the tasks that DREAMER and SLAC experimented with. Reward prediction can be done by the dynamics model, as MBPO does it.\n\n\nSome additional, more minor comments:\n\nIn a similar vein to what is discussed above, I do not view comparisons between online CARL and PCC/SOLAR as ablations.\n\nThanks for clarifying what is meant by \"model based SAC\". I'm not sure why the authors choose to cite the original SAC paper, which makes no mention whatsoever of models, rather than just directly citing the MBPO paper, since it sounds like the authors are basically doing exactly what MBPO prescribes? In my view, citing the original SAC paper does add confusion -- if a reader follows this citation, they will find a paper that never once mentions learning dynamics models, thus leading to ambiguity as to what the authors are actually doing.\n\nAs a final note, I believe it is disingenuous, in the author response, to imply that my quip about model based SAC was \"enough reason for [me] to question the clarity of the paper and to state that important details are difficult to follow\". This was one example I brought up of several, including other points such as \"there are many hyperparameters and not much guidance as to how to pick these\" (thanks for adding a bit more detail about this) and \"I personally found the last paragraph of the VCARL description almost impossible to follow\" (left unaddressed in the author response).\n\nMy score remains unchanged.",
            "This paper examines the problem of learning controllable embedding (LCE), with the goal of learning good representations (usually achieved using variational inference algorithms) such that the maximum cumulative reward can be achieved. The main difference lies in the simultaneous learning of both the low-dimensional latent space as well as the action policy.\n\nOne of the main strengths of this paper is found in Theorem 1. The authors devise a simple policy iteration approach in the low dimensional learned space. Then, using mostly qualitative analysis, a bound on the policy improvement error is formulated. This error combines several intuitive and straightforward factors, which are then extracted to form more involved offline and online reinforcement learning algorithms. I have read through the proofs, and they seems correct.\n\nWhile I appreciate the quality of the theoretical work, the paper had some drawbacks that brought me to my current score :\n1. The loss function consists of many hyper-parameters. The authors should provide some guidelines for choosing these hyper-parameters due to the large number of possible combinations, and clearly state how the scalings affect performance.\n2. Experimentation is lacking. While the authors conducted experiments mostly on toy problems, I expect them to compare against more involved environments which are harder to model. Their comparison with state of the art algorithms (e.g. Dreamer) which were also tested on such environments is thus not fair.\n3. Minor comment: There is a newer version of Dreamer that the authors can compared against: https://arxiv.org/pdf/2010.02193.pdf \n4. Minor comment: How would CARL compare against model-free offline RL methods, or generally to algorithms that are not SAC?\n\nQuestion to authors:\nWould there be a benefit in removing F altogether and learning a mapping X -> Z -> X\u2019 without transitioning in the latent space? (i.e., errors III and IV in Theorem 1)\n\nFinally, it would be beneficial if the authors could include code for their work. If the authors can't supply the complete code base, even code snippets with clarifying explanations to demonstrate their main ideas would be beneficial. This would greatly improve the quality and credibility of their work as well as the reviews.\n\nTo conclude, the paper provides strong theoretical intuition, which is a significant value-add of the paper. Nevertheless, its lack of experimentation and large number of hyper-parameters limit its overall quality. If the authors provide substantial improvement in the experimentation I will increase my score.\n"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer identifies a critical flaw in the theoretical foundation (Theorem 1) and states that the current data collection process is insufficient. They also point out several areas needing clarification and suggest a large revision is necessary before acceptance.",
            "The review presents both positive aspects (well-organized, promising results) and negative aspects (difficult to follow details, insufficient experimental results). The overall recommendation shifts from weak reject to weak accept, indicating a balanced assessment.",
            "The reviewer's overall assessment has improved from 'weak reject' to 'weak accept,' indicating a positive shift in their evaluation of the paper. Phrases like 'I believe this clarifies some of my confusion' and 'this is acceptable to me' further support this positive sentiment.",
            "The reviewer explicitly states their score remains unchanged, implying dissatisfaction. They also use phrases like 'insufficient', 'disagree', 'weak relationship', 'unsatisfying', and 'disingenuous', all indicating a negative assessment of the paper's current state.",
            "The review acknowledges the paper's theoretical strengths but also points out significant weaknesses in experimentation and hyperparameter tuning. It ends with a conditional statement about increasing the score if improvements are made, indicating a neutral overall stance."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like \"incorrect\", \"would be quite bad\", and \"not sufficient\". It also directly states the paper is \"not ready for acceptance\" without significant revision, indicating a critical stance. The reviewer also raises several specific questions and demands clarifications.",
            "The review uses a mix of positive and critical language, acknowledging the paper's strengths while also pointing out its weaknesses. Phrases like \"relatively well organized,\" \"promising experimental results,\" and \"important problem\" are balanced by concerns about clarity and the scope of the experiments. The reviewer also offers specific suggestions for improvement, indicating a constructive approach.",
            "The review demonstrates a balanced tone by acknowledging the paper's strengths ('clarifies some of my confusion,' 'better understand the significance') while still pointing out weaknesses ('link between the theorem and the actual loss function is weak,' 'empirical evaluation is still lacking'). The reviewer also offers constructive criticism and suggestions for improvement.",
            "The review uses critical language, directly challenging the authors' methodology and conclusions. Phrases such as 'does not adequately address', 'I would disagree', 'crucially important', 'just a weak relationship', 'disingenuous', and direct questioning of the experimental setup and citations demonstrate a critical tone.",
            "The review presents both positive aspects ('strong theoretical intuition', 'significant value-add') and negative aspects ('lack of experimentation', 'large number of hyper-parameters') of the paper. The reviewer also provides constructive suggestions, indicating a balanced and critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because despite mentioning positive aspects like readability and the interesting idea, the reviewer identifies a critical flaw in the theoretical foundation (Theorem 1) and concludes that the paper is not ready for acceptance without major revision. The reviewer's final assessment is logically derived from the identified flaw, making the review consistent overall.",
            "The review is consistent because the initial recommendation of weak reject is justified by the identified weaknesses (insufficient experimental results, clarity issues). The change to weak accept is explicitly attributed to a discussion, which is a valid reason for opinion evolution in peer review. The reviewer does not contradict themselves within the initial assessment; the change is presented as a result of new information or clarification gained through discussion.",
            "The reviewer initially had confusion about the theoretical aspects, which was clarified.  While the reviewer now understands and appreciates the theoretical contribution, they consistently maintain that the primary contribution is empirical and that the empirical evaluation is still lacking.  The reviewer's concerns about the limited empirical validation and the justification for it are consistent throughout the review, even as they increase their score to weak accept based on the clarified theoretical aspects.",
            "The reviewer consistently argues that the paper lacks sufficient empirical evaluation to support its claims, whether focusing on the theoretical contribution or empirical performance. The reviewer maintains this point throughout the review, providing detailed reasoning and examples to support their assessment.",
            "The reviewer acknowledges the theoretical strengths of the paper, particularly Theorem 1, but consistently points out weaknesses in experimentation, hyperparameter tuning guidelines, and code availability. The conclusion that the paper's quality is limited by these weaknesses and that score will increase with improved experimentation aligns with the points raised in the review, demonstrating consistency."
        ]
    },
    {
        "paper_id": "iclr_2019_rke41hC5Km",
        "paper_title": "Generating Realistic Stock Market Order Streams",
        "paper_abstract": "We propose an approach to generate realistic and high-fidelity stock market data based on generative adversarial networks.\n      We model the order stream as a stochastic process with finite history dependence, and employ a conditional Wasserstein GAN to capture history dependence of orders in a stock market. \n      We test our approach with actual market and synthetic data on a number of different statistics, and find the generated data to be close to real data. ",
        "review_ids": [
            "B1e29JKRn7",
            "BygYHZ3F3m",
            "BJgoOGtxi7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a Generative Adversarial Network (GAN) methodology to learn the distribution of limit orders that arrive on an equity market. The proposed approach captures the (mostly discrete) structure in the generated orders; modeling is carried out with a conditional Wasserstein-GAN with a recurrent neural networks in both the generator and critic used to capture the time dependency, and convolutional layers to capture other conditioning information. Experiments are provided on both synthetic and real data.\n\nOverall, the paper is well written and easy to follow. The application of WGAN to modeling financial market microstructure is novel and appropriate: microstructure is probably one of the areas of finance where generative models can be trained with ample data, and overfitting risks can therefore be controlled to some extent. In this respect, the paper brings a valuable contribution, both to finance by proposing a direct generative model of a complex process, and to machine learning by showing a useful application of GANs outside of the traditional domains.\n\nMy main reservation with the paper is that the experimental results could be more convincing. In particular, results for real data are shown for only two stocks, representing respectively 20k and 230k orders. Although the trained model captures some aspects of the distribution (e.g. inter-arrival times), it misses others, such as important aspects of the (univariate) price distribution, as well as the best bid-ask dynamics. As they stand, the results appear mostly anecdotal, and lack either a breadth or depth of analysis. It would be sensible for the authors to:\n\n1. Compare their models to other limit-order book simulation models proposed in the financial econometrics literature;\n2. Characterize the sensitivity of the learned distribution on the architecture hyperparameters.\n\nFor these reasons, it appears difficult to recommend acceptance of the paper in its current state.",
            "The objective of this paper is to use GAN for generating the order stream of stock market data.   The novelty of the paper is the formulation of the order stream and the use of GAN for generating the stock data.   This is a paper for the application of GAN and there are limited contribution to the technical aspect of machine learning.    The paper is clearly written.   There are two main assumptions used in the paper; one is the Markov chain and the second one is the stationary distribution.   In real case, both assumptions are unlikely  to be satisfied.  The orders are mostly affected by many external factors and financial data are known to be non-stationary.  The authors may have to justify these assumptions. \n\nAnother issue is the evaluation of the results.  The paper uses five statistics to evaluate the generated data.  What we can conclude is that the generated data follow similar statistics with the real data.   But we do not know if the generated data offer extra information about the market.  The paper has used synthetic data in the experiments.  So it means that we could have models that generate data that look like real data.  If so, what are the benefits of using GAN to generate the data ?  \n\n",
            "The problem addressed in this paper is worth the attention of the community. Not so much for it being of strong interest to the majority of ICLR attendees, but due to the fact that it deals with data of origin (finance) and properties (high-order Markov chain dependencies) that have never been considered in the past.\n\nHowever, apart from this merit, the paper as is stands now suffers from major prohibitive shortcomings. Specifically, the authors have failed to provide a detailed account of the novel network architecture introduced in this paper. Their description is too vague, and misses crucial details. For instance, they use a convolutional structure (layer?) at some point. How is this configured? Why do they need a convolutional layer, since they present it with a vector output from a preceding dense layer? What about the CDA network (both its configuration and its training procedure)? These are crucial aspects that the authors have failed to describe at all.\n\nIn the same vein, technical details concerning the selection of the training algorithm hyper-parameters are also missing from the experimental section. Although not as glaring an omission as the previously described ones, these are also crucial for the replicability of the presented research results. \n\nFinally, the authors have failed to provide comparisons to alternative baselines. For instance, why not train a simple LSTM and use to generate new samples. Why not use a recurrent variational autoencoder? Eventually, since the time-series we are dealing with are governed by a high-order Markov chain, we not fit and sample from a high-order hidden Markov model? These are crucial questions that must be adequately addressed by the authors. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses reservations about the experimental results, stating they are not convincing and lack breadth or depth of analysis. The reviewer concludes that it is difficult to recommend acceptance in its current state.",
            "The review raises concerns about the validity of the assumptions made in the paper and questions the benefits of using GAN for data generation, suggesting a negative overall assessment.",
            "The review identifies 'major prohibitive shortcomings' and multiple failures on the part of the authors, indicating a negative assessment of the paper's current state."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, pointing out weaknesses in the experimental results ('results appear mostly anecdotal', 'lack either a breadth or depth of analysis') and suggesting improvements ('It would be sensible for the authors to...'). While acknowledging the paper's strengths, the overall assessment is negative.",
            "The review uses phrases like \"unlikely to be satisfied,\" \"may have to justify,\" and \"what are the benefits\" which indicate a critical evaluation of the paper's assumptions and results.",
            "The review uses phrases like 'failed to provide', 'too vague', 'misses crucial details', 'glaring omission', and 'crucial questions that must be adequately addressed' which convey a critical tone highlighting the deficiencies of the paper."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the strengths of the paper (novelty, application) but points out a major weakness (experimental results) that leads to the recommendation of rejection in its current state. The reviewer's arguments flow logically and do not contradict each other.",
            "The review is consistent in its critique. It acknowledges the novelty of the paper but raises valid concerns about the assumptions made (Markov chain and stationary distribution) and the evaluation methodology. The reviewer consistently questions the practical benefits and justification of using GANs for this specific application, highlighting the gap between statistical similarity and added value or information from the generated data. There are no self-contradictory statements within the review.",
            "The review is consistent in its criticism of the paper. While it acknowledges the potential merit of the topic, the review consistently points out major shortcomings related to the lack of detail in the description of the network architecture, missing information about hyperparameter selection, and the absence of comparisons to relevant baselines. The reviewer maintains a critical stance throughout the text, focusing on the deficiencies of the current submission."
        ]
    },
    {
        "paper_id": "iclr_2021_bM3L3I_853",
        "paper_title": "AdaFuse: Adaptive Temporal Fusion Network for Efficient Action Recognition",
        "paper_abstract": "Temporal modelling is the key for efficient video action recognition. While understanding temporal information can improve recognition accuracy for dynamic actions, removing temporal redundancy and reusing past features can significantly save computation leading to efficient action recognition. In this paper, we introduce an adaptive temporal fusion network, called AdaFuse, that dynamically fuses channels from current and past feature maps for strong temporal modelling. Specifically, the necessary information from the historical convolution feature maps is fused with current pruned feature maps with the goal of improving both recognition accuracy and efficiency. In addition, we use a skipping operation to further reduce the computation cost of action recognition. Extensive experiments on SomethingV1 & V2, Jester and Mini-Kinetics show that our approach can achieve about 40% computation savings with comparable accuracy to state-of-the-art methods. The project page can be found at https://mengyuest.github.io/AdaFuse/",
        "review_ids": [
            "fq0pZaBNdIB",
            "hl8_zqEvBi",
            "36JftPJdZmj",
            "q0iwglOcbT",
            "l4yF_JCNBO",
            "H99l-1BAOyN"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I appreciate the authors' effort for the additional experiment, the clarification on the questions, and timely update of the paper.\n\nI understood **(b) Necessity of Gumbel Softmax** and **(c) Output type of the policy net**.\nI think the new texts in page 5 are now clear enough to tell how the policy networks behave in forward and backward pass.\nThank you for the clarification.\n\nRegarding **(d) Reusing multiple times**, I have a further question. \nSuppose the situation where the policy is \"reuse\" on both frame t and t-1, and \"keep\" on frame t-2. In this case, it is clear that frame t-1 reuses the features of frame t-2, which are already computed at t-2. Does the features on frame t-1 need to be newly computed just for being \"reused\" in frame t, even though frame t-1 itself does not use the computed features?\n\nRegarding **(a)  Comparison with recent methods**, which is the most important point, I am still not completely convinced.\nI understand the papers I listed are not particularly for efficient action recognition, but it cannot be the reason for not listing them in the comparison tables and figures, especially in Figure 2.\n\nProbably the authors worry about comparing with inefficient but high-accuracy methods, which may lead to unfair comparison.\nHowever, the comparison to those methods in the plot of Figure 2 stays fair because it has two axis for accuracy and efficiency (FLOPS, by the way I found a typo in the label of the axis \"GLOPS\").\nIn this view, the current version of Figure 2 looks rather selective, listing only the methods that make the proposed method look \"the best\".\nI am afraid that the readers may mis-understand that there is no method that has better accuracy at the sacrifice of efficiency.\n\nSome of the related results are as follows.  \nIn [1], their GSM seems to have better accuracy-efficiency trade off such as (47.24%, 16.46 GFLOPS), (49.01%, 26.85 GFLOPS) compared to AdaFuse (44.9%, 19.1 GFLOPS), (46.8%, 31.3 GFLOPS).  \nIn [2], though they do not have FLOPS count in the paper, they achieved 45% using ResNet18 and 50.1% using ResNet 50.  \nIn [3], they achieved (47.7%, 32.93 GFLOPS).  \nSee also Figure 7 in [1] for your reference.\n\nCombined with these points, I think the 2nd claim on the contribution in page 2 is too much.\nIn my view, the proposed method is not entirely model-agnostic, but it is basically effective only for 2D CNN based models.\nMore specifically, it is effective on top of the models that do not take the temporal information into account by themselves.\nIf a base model already takes the temporal information into account such as TSM, the improvement brought by the proposed method seems to be marginal.\nI admire the advantage of the proposed method that it is expected to benefit from future development of 2D CNN.\n\nI do not expect further modification in this very short term, but would like to listen to the authors' opinion on those points if possible.",
            "#################################\n\nSummary:\n\nThe paper presented an adaptive inference model for efficient action recognition in videos. The core of the model is the dynamic gating of feature channels that controls the fusion between two frame features, whereby the gating is conditioned on the input video and helps to reduce the computational cost at runtime. The proposed model was evaluated on several video action datasets and compared against a number of existing deep models. The results demonstrated a good efficiency-accuracy trade-off for the proposed model.  \n\n#################################\n\nPros:\n* The paper has a novel idea (adaptive temporal feature fusion) and addresses an important problem in vision (efficient action recognition). \n* Solid experiments on multiple datasets. The analysis of the learned policy is quite interesting.\n* Well-written paper\n\n#################################\n\nCons:\n* Limited technical novelty\n\nThe idea of building adaptive inference models with a policy network for video classification has been previously explored by Wu et al., Meng et al. and others (e.g., skip part of the model, select a subset of frames, choose the input resolution to the model). The main technical component of the model is also very similar to the channel gating network (Hua et al.). The key innovation seems to be the perspective of modeling temporal feature fusion for adaptive inference. This is probably best considered as in parallel to previous approaches for adaptive video recognition. The technical components thus look less exciting.\n\n* Lack of comparison to other adaptive inference models / temporal fusion schemes   \n\nThere isn\u2019t a real comparison between the proposed method and recent works on adaptive inference video recognition (e.g, Wu et al, Meng et al.). The benefit of model temporal feature fusion --- a main contribution of the paper, thus remains unclear with respect to other design choices (e.g., input resolution or frame selection). I\u2019d suggest some experiments that compare to those work. Another important experiment is to contrast the proposed method with other temporal feature fusion schemes (e.g, LSTM, TSM). For example, TSM --- a hand-crafted feature fusion module, seems to have less number of parameters, slightly higher FLOPs and comparable accuracy (Table 3). If that is the case, the contribution of the proposed adaptive fusion scheme is much weakened. \n\n#################################\n\nMinor comments:\n\nIt is not totally clear to me how the FLOPs of the proposed model are computed. As the proposed model will have a different FLOP conditioned on the input video, were the reported FLOPs averaged across the dataset? I was not able to find a description in the paper. \n\nIt will be great if the authors can report some run-time performance (e.g., wall time). To achieve the theoretic FLOPs, the proposed model will rely on filter re-arrangement on the fly and sparse convolution kernels. Both can be less efficient on certain devices, e.g., GPUs.  \n \n\n#################################\n\nJustification for score:\n\nAll in all a good paper. My main concern is the missing link / comparison to previous works on adaptive video recognition. If this concern can be addressed, I am happy to raise my rating. \n",
            "I thank the authors for providing additional results and for clarifying the technical details. Most of my previous concerns have been addressed. I believe this is now a solid paper and would recommend its acceptance. \n",
            "In this work, the authors introduce an AdaFuse network for efficiency action recognition in videos. Specifically, they design a policy net to decide which channels should be kept, reused or skipped, according to the input features of two adjacent frames.\n\nStrength \n\n1 The paper is written well, and the organization is OK\n\n2 The idea of adaptive temporal fusion is somehow novel and interesting \n\nWeakness\n\n1 How to save computation. I understand the general idea of saving computation, if some channels are reused or skipped. However, in the training phase, the policy net would produce the real-value vector by Eq. (7), instead of the one-hot vector. In other words, the 'keep' entry for each channel is always used during training. Then, I guess computation saving is not claimed for training. It is for testing, right? How to do test? The policy net produces the real-value vector and then you make it as one-hot vector for saving computation?\n\n2 Missing SOTA. Compared with this paper, many recent approaches can achieve a competitive computation with better accuracy. It significantly reduces the potential value of this paper.\n\n*Jiang et al., STM: SpatioTemporal and Motion Encoding for Action Recognition, ICCV 2019\n\n*Li et al., TEA: Temporal Excitation and Aggregation for Action Recognition, CVPR 2020\n\n*Sudhakaran et al., Gate-Shift Networks for Video Action Recognition, CVPR2020\n\n*Liu et al., TEINet: Towards an efficient architecture for video recognition, AAAI 2020\n\n3 Please correct the abstract. The experiments are performed on mini-Kinetics, rather than Kinetics. I indeed suggest that, it would be better to perform the proposed method on Kinetics to further show the effectiveness.",
            "#### General\nThis paper proposes an adaptive temporal fusion network called AdaFuse for action recognition, which adaptively removes temporal redundancy and reuses past features for accuracy and efficiency.\nI listed the Pros and Cons I found in the paper below as well as some questions to clarify some of the details.\n\n#### Pros\n1. The idea of learning a decision policy to dynamically determine whether channel-wise features at time $t$ are calculated normally, reused from $t-1$, or skipped, is interesting and reasonable.\n1. The experimental results show that the proposed method achieves good accuracy with reasonable computational budget.\n1. The ablation study in Table 4 reveals that the performance is greatly affected by the policy and it is important to fuse the futures from different frames to captures the temporal dependencies.\n\n#### Cons\n1. The propsoed method is not compared with some of the recent methods such as [1-3] ([4] is optional because the publication date is very close to the ICLR 2021 submission deadline). Especially for Jester and Mini-Kinetics dataset, the proposed method is compared with only TSN, which is old and weak as baseline as it does not incorporate the temporal information.\n1. In Table 3, it seems that the proposed method achieves good accuracy, but I am afraid that it is just because of the strong base network, TSM. Merely adding AdaFuse to TSM indeed saves some computation but degrades the performance as described in the paper. The proposed remedy indeed slightly improves the accuracy but it requires much more parameters compared to the vanilla TSM. Overall, I find it benefitial to use the proposed method on top of simple base networks such as TSN, but the benefit of using the proposed method on top of strong base networks such as TSM may be marginal. Combined with the point 1 above, I am not well convinced of the effectiveness of the proposed method.\n1. Some of the important details are not clear. I would appreciate if the authors could answer the questions I listed below.\n\n#### Questions\n1. Is it necessary to use Gumbel softmax? I think there are two kinds of tricks involved in Gumbel softmax. One is a trick for sampling from a categorical distribution, and the other is a trick for making the opperation differentiable. In my understanding, which may be wrong, the required characteristic for the present method is the latter one, and the sampling from the categorical distribution is not necessarily required. In this case, I think simply using $q$ instead of $\\log{r} + G$ in equation (7) is enough.\n1. Related to the point above, please clarify the type of output (hard or soft) of the policy net. The sentence after equation (2) says the output is integer values (0, 1, or 2), while the sentence before equation (7) says it is a real-valued vector.  \n1. Suppose $p_t^i = 1$ (reuse) and $p_{t-1}^i = 1$ (reuse again). In this case, is $y_t^i$ copied from $y_{t-2}^i$ ? Or is the feature map of $i$-th channel at time $t-1$ calculated on the fly for \"reusing\" at time $t$? In other words, if the policies for a channel is \"reuse\" $n$ consecutive times, does the method take the feature from $n$ frames before? \n\n\n#### Other comments\n1. Figure 1 may be incorrect or misleading. I think $p_t$, the output of the policy net, should go to the 2D Conv. block. Otherwise the block never knows which channel to compute at time $t$ and which channel to reuse or skip.\n\n[1] Sudhakaran+, Gate-Shift Networks for Video Action Recognition, CVPR 2020\n[2] Martinez+, Action recognition with spatial-temporal discriminative filter banks, ICCV 2019\n[3] Jiang+, STM: SpatioTemporal and Motion Encoding for Action RecognitionSTM: SpatioTemporal and Motion Encoding for Action Recognition, ICCV 2019\n[4] Kwon+, MotionSqueeze: Neural Motion Feature Learning for Video Understanding, ECCV 2020",
            "Authors assessed how their adaptive temporal fusion network performs on public datasets such as Something V1&2, Kinetics, etc.. The contribution of this paper is in proposing an approach to automatically determine which channels to keep, reuse, or skip per layer and per target instance that can result in efficient action recognition. \n\nSTRENGTHS: \nThe proposed method is model-agnostic, making it easy to use as a plugin operation for other network architectures. \nReusing history features when necessary to make the network capable for strong temporal modeling. \n\nCONCERNS:\nThe paper has examined the temporal fusion module on BN-Inception and ResNet models, while more recent models\u2019 evaluation is missing. \nWhile the policy network is defined as two FC layers and a ReLU, it is not clear why the authors chose this architecture and how they have tuned it?\nIn section 3, Using 2D-CNN for Action Recognition, a citation to one of the recent works in modeling the temporal causality is missing: Asghari-Esfeden, Sadjad, Mario Sznaier, and Octavia Camps. \"Dynamic Motion Representation for Human Action Recognition.\" In The IEEE Winter Conference on Applications of Computer Vision, pp. 557-566. 2020."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Negative",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses concerns about the comparison with existing methods, stating that the current version of Figure 2 looks \"rather selective\" and may lead to misunderstanding. They also find the second claim on the contribution to be \"too much\" and express doubt about the method's model-agnostic nature. While acknowledging the authors' efforts and the clarity of certain explanations, the overall sentiment leans towards negative due to the significant concerns raised.",
            "The review acknowledges the paper's strengths, such as novelty and solid experiments, but also points out significant weaknesses, like limited technical novelty and lack of comparison to other adaptive inference models. The reviewer's willingness to raise the rating if concerns are addressed indicates a neutral overall sentiment.",
            "The reviewer explicitly states that most of their previous concerns have been addressed and recommends acceptance, indicating a positive sentiment.",
            "The review expresses concerns about the paper's novelty, computational efficiency, and comparison to existing state-of-the-art methods. The reviewer points out missing SOTA and suggests improvements to the experimental setup, indicating a critical evaluation of the work.",
            "The review identifies several cons, including a lack of comparison with recent methods, concerns about the effectiveness of the method on strong base networks, and unclear details. The reviewer expresses being 'not well convinced of the effectiveness of the proposed method'.",
            "The review presents both strengths and concerns without expressing a strong positive or negative overall opinion. It acknowledges the paper's contributions and potential, but also points out areas for improvement and missing information."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"still not completely convinced,\" \"rather selective,\" \"too much,\" and \"I am afraid that the readers may mis-understand\" which clearly indicate a critical stance. The reviewer also points out a typo (\"GLOPS\") and directly challenges the authors' claims about the contribution of their work.",
            "The review presents both positive aspects ('novel idea', 'solid experiments', 'well-written paper') and negative aspects ('limited technical novelty', 'lack of comparison'). It offers constructive suggestions for improvement, indicating a balanced approach.",
            "The reviewer thanks the authors and expresses a positive assessment of the paper's quality ('solid paper'). This indicates a supportive tone.",
            "The tone is critical as the reviewer identifies weaknesses in the paper, such as the lack of computational saving during training, missing state-of-the-art comparisons, and inaccuracies in the abstract. Phrases like 'How to save computation', 'Missing SOTA', 'It significantly reduces the potential value of this paper' and 'Please correct the abstract' indicate a critical assessment.",
            "The review uses phrases like 'old and weak as baseline', 'degrades the performance', 'benefit may be marginal', and 'not well convinced' indicating a critical evaluation of the paper's contributions and methodology. The reviewer also points out specific issues with the experimental setup and clarity of explanations.",
            "The review adopts a balanced tone by highlighting both 'STRENGTHS' and 'CONCERNS' of the paper. It uses neutral language to describe the method and its limitations, avoiding overly positive or negative phrasing. The use of specific questions indicates a desire for clarification rather than outright criticism."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it starts with acknowledging the authors' improvements and clarifications, then raises a specific question for further clarification, and finally focuses on a major concern regarding the comparison with existing methods and the paper's claims. The reviewer's arguments are logically connected and do not contradict each other. The reviewer appreciates the authors' efforts but points out a significant weakness in the paper's comparison and claims, maintaining a coherent and consistent critique throughout the review.",
            "The review is consistent because the reviewer acknowledges the strengths of the paper in the 'Pros' section, such as novelty, solid experiments, and writing quality. Simultaneously, in the 'Cons' section, the reviewer points out the limitations regarding technical novelty and lack of comparison with existing methods. The 'Justification for score' section summarizes these points, indicating that it is a good paper but with a main concern about missing comparisons. The reviewer's willingness to raise the rating if the concern is addressed further reinforces the consistency of the review, as it shows a balanced perspective acknowledging both merits and areas for improvement.",
            "The review expresses positive feedback throughout and concludes with a clear recommendation for acceptance, without any conflicting statements.",
            "The review is consistent as it presents both strengths and weaknesses of the paper without any self-contradiction. The reviewer appreciates the writing and novelty but raises valid concerns about computation saving, missing SOTA comparison, and dataset reporting, leading to a balanced and logically structured assessment.",
            "The review is consistent in its assessment. It acknowledges the interesting idea and good performance (Pros), but raises valid concerns about the experimental setup, comparison with baselines, and clarity of certain details (Cons and Questions). The reviewer's concerns logically follow from the identified weaknesses and areas needing improvement, without contradicting themselves.",
            "The review is consistent as it presents both positive aspects (strengths) and negative aspects (concerns) of the paper without any contradictions. The reviewer appreciates the model-agnostic nature and temporal modeling capability while pointing out limitations in evaluation, lack of justification for design choices, and missing citations. These points are distinct and do not contradict each other, forming a balanced and consistent review."
        ]
    },
    {
        "paper_id": "iclr_2019_H1M7soActX",
        "paper_title": "The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Minima and Regularization Effects",
        "paper_abstract": "Understanding the behavior of  stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we theoretically study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions are established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to  escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We verify our understanding through comparing\n      this anisotropic diffusion with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics) and other types of position-dependent noise.",
        "review_ids": [
            "H1lJasFPnX",
            "BJgWpYA53Q",
            "ByxZzQI9hX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper studies the benefit of an anisotropic gradient covariance matrix in SGD optimization for training deep network in terms of escaping sharp minima (which has been discussed to correlate with poor generalization in recent literature). \n\nIn order to do so, SGD is studied as a discrete approximation of stochastic differential equation (SDE). To analyze the benefits of anisotropic nature and remove the confounding effect from scale of noise, the scale of noise in the SDE is considered fixed during the analysis. The authors identify the expected loss around a minimum as the efficient of escaping the minimum and show its relation with the hessian and gradient covariance at the minimum. It is then shown that when all the positive eigenvalues of the covariance matrix concentrate along the top eigenvector and this eigenvector is aligned with the top eigenvector of the Hessian of the loss w.r.t. the parameters, SGD is most efficient at escaping sharp minima. These characteristics are analytically shown to hold true for a 1 hidden layer network and experiments are conducted on toy and real datasets to verify the theoretical predictions.\n\nComments:\n\nI find the main claim of the paper intuitive-- at any particular minimum, if noise in SGD is more aligned with the direction along which loss surface has a large curvature (thus the minimum is sharp along this direction), SGD will escape this minimum more efficiently. On the other hand, isotropic noise will be wasteful because a sample from isotropic noise distribution may point along flat directions of the loss even though there may exist other directions along which the loss curvature is large. However, I have several concerns which I find difficult to point out because *many equations are not numbered*. \n\n1. In proposition 2, it is assumed under the argument of no loss of generality that both the loss at the minimum L_0=0 and the corresponding theta_0 =0. Can the authors clarify how both can be simultaneously true without any loss of generality?\n2. A number of steps in proposition 2 are missing which makes it difficult to verify. When applying Ito's lemma and taking the integral from 0 to t, it is not mentioned that both sides are also multiplied with the inverse of exp(Ht).\n3. In proposition 2, when computing E[L(theta_t)] on page 12, the equalities after line 3 are not clear how they are derived. Please clarify or update the proof with sufficient details.\n4. It is mentioned below proposition 2 that the maximum of Tr(H. Sigma) under constraint (6) is achieved when Sigma* = Tr(Sigma). lambda_1 u1.u1^T, where lambda_1 is the top eigenvalue of H. How is lambda_1 a factor in Sigma*? I think Sigma* should be Tr(Sigma). u1.u1^T because this way the sum of eigenvalues of Sigma remains unchanged which is what constraint (6) states.\n5. The proof of proposition 5 is highly unclear.Where did the inequality ||g_0(theta)||^2 <= delta.u^TFu + o(|delta|) come from? Also, the inequality right below it involves the assumption that u^Tg_0 g_0u <= ||g_0||^2 and no justification has been provided behind this assumption.\n\n\nRegarding experiments, the toy experiment in section 5.1 is interesting, but it is not mentioned what network architecture is used in this experiment. I found the experiments in section 5.3 and specifically Fig 4 and Fig 7 insightful. I do have a concern regarding this experiment though. In the experiment on FashionMNIST in Fig 4, it can be seen that both SGD and GLD 1st eigvec escapes sharp minimum, and this is coherrent with the theory. However, for the experiment on CIFAR-10 in Fig 7, experiment with GLD 1st eigvec is missing. Can the authors show the result for GLD 1st eigvec on CIFAR-10? I think it is an important verification of the theory and CIFAR-10 is a more realistic dataset compared with FashionMNIST.\n\nA few minor points:\n\n1. In the last paragraph of page 3, it is mentioned that the probability of escaping can be controlled by the expected loss around minimum due to Markov's inequality. This statement is inaccurate. A large expected loss upper bounds the escaping probability, it does not control it.\n2. Section 4 is titled \"The anisotropic noise of SGD in deep networks\", but the sections analyses a 1 hidden layes network. This seems inappropriate.\n3. In the conclusion section, it is mentioned that the theory in the paper unifies various existing optimization mentods. Please clarify.\n\nOverall, I found the argument of the paper somewhat interesting but I am not fully convinced because of the concerns mentioned above.",
            "This paper studies the effort of anisotropic noise in stochastic optimization algorithms. The goal is to show that SGD escapes from sharp minima due to such noise. The paper provides preliminary empirical results using different kinds of noise to suggest that anisotropic noise is effective for generalization of deep networks.\n\nDetailed comments:\n\n1. I have concerns about the novelty of the paper: It builds heavily upon previous work on modeling SGD as a stochastic differential equation to understand its noise characteristics. The theoretical development of this manuscript is straightforward until simplistic assumptions such as the Ornstein-Uhlenbeck process (which amounts to a local analysis of SGD near a critical point) and a neural network with one hidden layer. Similar results have also been in the the literature before in a number of places, e.g., https://arxiv.org/abs/1704.04289 and references therein.\n\n2. Proposition 4 looks incorrect. If the neural network is non-convex, how can the positive semi-definite Fisher information matrix F sandwich the Hessian which may have strictly negative eigenvalues at places?\n\n3. Section 5 contains toy experiments on a 2D problem, a one layer neural network and a 1000-image subset of the FashionMNIST dataset. It is hard to validate the claims of the paper using these experiments, they need to be more thorough. The Appendix contains highly preliminary experiments on CIFAR-10 using VGG-11.\n\n4. A rigorous theoretical understanding of SGD with isotropic noise or convergence properties of Lagevin dynamics has been developed in the literature previously, it\u2019d be beneficial to analyze SGD with anisotropic noise in a similar vein.",
            "The authors studied the effect of the anisotropic noise of SGD on the algorithm\u2019s ability to escape from local optima. To this end, the authors depart from the established approximation of SGD in the vicinity of an optimum as a continuous-time Ornstein-Uhlenbeck process. Furthermore, the authors argue that in certain deep learning models, the anisotropic noise indeed leads to a good escaping from local optima.\n\nProposition 3 (2) seems to assume that the eigenvectors of the noise-covariance of SGD are aligned with the eigenvectors of the Hessian. Did I understand this correctly and is this sufficient? Maybe this is actually not even necessary, since the stationary distribution for the multivariate Ornstein-Uhlenbeck process can always be calculated (Gardiner; Mandt, Hoffman, and Blei 2015\u20132017)\n\nI think this is a decent contribution.\n\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's claims and methodology. Phrases like \"I am not fully convinced because of the concerns mentioned above\" and the detailed list of issues indicate a negative sentiment.",
            "The review expresses concerns about the paper's novelty, correctness of a proposition, and the limited scope of the experiments. Phrases like 'concerns about the novelty,' 'straightforward,' 'simplistic assumptions,' 'Proposition 4 looks incorrect,' and 'hard to validate the claims' indicate a negative sentiment.",
            "The reviewer states \"I think this is a decent contribution.\""
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical, pointing out specific weaknesses in the paper's proofs, assumptions, and experimental setup. The reviewer uses direct questions and statements like \"A number of steps in proposition 2 are missing which makes it difficult to verify\" and \"This statement is inaccurate.\"",
            "The tone is critical due to the direct questioning of the paper's novelty and correctness. Phrases like 'I have concerns,' 'Proposition 4 looks incorrect,' and 'hard to validate the claims' convey a critical assessment.",
            "The review is generally positive, acknowledging the contribution as \"decent.\" However, it also raises a specific question regarding Proposition 3 (2), indicating a critical analysis of the work's assumptions. This blend suggests a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer raises several significant concerns regarding the theoretical proofs and experimental validation, which leads to their overall conclusion of not being fully convinced, despite acknowledging the intuitiveness of the main claim and some interesting experimental results. The concerns raised are substantial enough to justify the reviewer's overall assessment.",
            "The review is consistent because all comments are negative and point out weaknesses in the paper regarding novelty, theoretical correctness, and experimental validation. There are no contradictory statements or positive feedback.",
            "The review is consistent as it presents a summary of the paper, raises a specific question about an assumption (Proposition 3 (2)), and then explores the necessity of this assumption by suggesting an alternative perspective based on existing literature. Finally, it concludes with a positive overall assessment of the contribution. There are no contradictory statements or conflicting opinions expressed within the review."
        ]
    },
    {
        "paper_id": "nips_2022_-zBN5sBzdvr",
        "paper_title": "How Sampling Impacts the Robustness of Stochastic Neural Networks",
        "paper_abstract": "Stochastic neural networks (SNNs) are random functions whose predictions are gained by averaging over multiple realizations. \nConsequently, a gradient-based adversarial example is calculated based on one set of samples and its classification on another set. \nIn this paper we derive a sufficient condition for such a stochastic prediction to be robust against a given sample-based attack. \nThis allows us to identify the factors that lead to an increased robustness of SNNs and gives theoretical explanations for: \n(i) the well known observation, that increasing the amount of samples drawn for the estimation of adversarial examples increases the attack's strength,\n(ii) why increasing the number of samples during an attack can not fully reduce the effect of stochasticity, \n(iii) why the sample size during inference does not influence the robustness, and\n(iv) why a higher gradient variance and a shorter expected value of the gradient relates to a higher robustness. \nOur theoretical findings give a unified view on the mechanisms underlying previously proposed approaches for increasing attack strengths or model robustness and are verified by an extensive empirical analysis.",
        "review_ids": [
            "Nf-y__NLk1d",
            "-qETPeIJw9Z",
            "9IdDJi2-zR",
            "e4_R47n37W",
            "TigO5bbPwpd",
            "Ud9x_NQEoNZ",
            "D-p1NA-Lntr",
            "M6qOxtSZjHD",
            "KJIUsghjUXH",
            "Zyb6dURcXT9"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for their response. My concerns are properly addressed and thus I will keep my rating of 7-Accept.",
            " > concern 1\n\nMy concern is about the inverse of what you are considering. I am trying to say that stochasticity does not play any major role in the robustness of the stochastic classifier, and that the robustness of a stochastic classifier is completely described by the robustness of its mean, which is a deterministic classifier.\n\n> concern 2\n\nThe critique is that the derivatives are ill-behaved, not that the derivatives does not exist.",
            " > We do not see a direct connection between the convergence to non-differentiable functions and the Runge phenomenon.\n\nThe relation is that when the convergence of a sequence of differentiable functions to the pointwise limit is pointwise (which is a property of the learning rule), the derivatives of the function can become ill-behaved before (even much much before) reaching the pointwise limit. For example, look at this image [A] of a function that suffers from the Runge phenomenon. Also, look at figures 1 and 2 of [B] for examples in classification. All of these functions are differentiable in the sense that we can compute their derivative. The issue is that their derivative is ill-behaved. For example, I think that figure 1 of [B] has a counter example to result (iv) in the abstract of the paper:\n\n- (iv) why a higher gradient variance and shorter expected value of the gradient relates to a higher robustness.\n\nAs we can see in the figure, the variance of the derivative of the trained Chebyshev classifier of degree 63 is very high and the symmetry in the graph suggests that the expected value of the derivative is zero.  Consequently, if this hypothesis is the mean of some stochastic classifier, and according to (iv), this stochastic classifier would be robust; which could not be farther from truth. \n\n> So what are the points that make you doubt the causality?\n\nI cannot decide if the results are caused by the non-robustness of the mean of the stochastic classifier (which is a deterministic classifier), or there is some genuine effect of stochasticity. It could be that you are measuring the properties of the non-robust mean classifier plus some random noise. For example in line 291-299, the paper describe why increasing the sample size does not change the adversarial accuracy and explains it using some complicated argument. Where as this observation could simply be explained if we assume that the mean classifier is not robust. \n\nIn line 205, the paper asserts that it has explained why robustness of a stochastic classifier does not depend on the amount of samples taken, which I don't think that even needs an explanation to be honest, an alternative to the proposal in the paper is that the robustness of a stochastic classifier is mainly decided by the robustness of its mean, and that sampling is non-consequential when the sample size gets large enough.\n\nTable 2 in the paper actually is in line with the conclusion that no meaningful relation between sample size and robustness exists. For one, the change in the accuracy is insignificant. Second, while adversarial robustness increases with sample size for BNN and IM, other models in the experiment show a random behavior. Moreover, the changes in the mean accuracy of the models as we increase the sample size are smaller than the std of the estimation for most cases. I cannot see how anything meaningful could be inferred from this experiment.\n\n> Regarding the Weierstrass function, we are aware that there are functions that are not differentiable.\n\nJust to clarify again, I am not saying that there are some non-differentiable ANNs around. The derivatives of any finite ANN could always be computed without any troubles. The problem is that the derivatives might be ill-behaved, and you are using empirical means to reach some conclusions without compensating for the fact that derivatives could be ill-behaved. The Weierstrass function could act as a toy problem so that you could figure out how to make your argument work in the pathological cases. To make the function conform to your assumption, consider any $a$ and $b$ for which fractal behavior does not occur, then the function would be L-smooth. If the Weierstrass function is too hard, doing the same for the pathological examples in figures 1 and 2 of [B] is also enough. I would accept any other almost pathological example as well. But I would not accept the answer that this setting is not related to practice, because it is. The collection of functions that are differentiable at even a single point of interval $[0, 1]$ is a zero measure set. Given that ANNs are universal approximators, and hence dense in the set of square-integrable functions on $[0, 1]$, then we can conclude that if we sample a random ANN with enough neurons, almost surely that ANN is very pathological in its derivative. When I combine this observation with the fact that in practice we are considering something like $[0, 1]^{784}$ (e.g. Fashion-MNIST) and deep networks with 50 layers of neurons (ResNet 50), I cannot find your argument that these pathological functions are rare in practice compelling.\n\n[A] https://images.app.goo.gl/7QYsvfTLeMWYbNVY8\n[B] https://arxiv.org/abs/2107.10599",
            " Thanks for the author's response and explanation. \n\nBased on the author's response and other reviewers' reviews, I decide to keep my score unchanged.",
            " > investigates models obtained after finite training time, the scenario most relevant for practice.\n\nI cannot give you an example in the exact setting that you are considering, however Runge phenomenon [D] in interpolation is an example of how pointwise convergence would result in things going haywire in finite training sets (which is just another way of saying finite time for online learners). I believe that Runge-like phenomenon (which have been observed in many families of functions by the way, including the RBF family) is very relevant to practice in general, and the context of the present paper. Runge-like phenomenon have been observed in the context of classification [E] as well.\n\n> we focus on SNNs obtained after a finite training time.\n\nThis reasoning is not enough; when exactly infinity starts? Where would you stop the training? The claims need to use stronger proofs. For example, to prove something about the hypothesis in the infinite time limit, first prove it for the finite case, and then use something like mathematical induction, or maybe a topological proof, to extend it to the infinite time limit. This way, you could argue that your theorem would be true after enough time/samples have passed/been observed.\n\n> We still do not fully understand what \u201cnot converged in its derivatives\u201d means here.\n\nTake a look at the Weierstrass function [F] to get an image of how ill-defined derivatives can get when one is dealing with sequences of functions.\n\n> generalizing the findings to the deterministic case does not make any sense to us.\n\nDeterministic classifiers are just degenerate stochastic classifiers. In this degenerate case, the stochastic angle is meaningless, I get that. But we observe the phenomenon in the degenerate case. The effect of margin and the gradient norm are still present in the degenerate case. I skimmed the paper again, maybe I have missed it, but couldn't find how do you quantify the effect of stochastic angle compared to the effect of the gradient norm and the margin. That is the reason in my review I assert that the empirical results might not be causally related to the theorems. \n\n---------------------------\n\nI think the best way forward would be for me to give a condition for when I would be convinced that the logic of the proposal is sound. If authors demonstrate that their logic applies to the Weierstrass function (add some random noise to make it stochastic if you prefer), and then come up with a sound process to quantify the effect of the proposed stochastic angle relative to the gradient norm and the margin, I will change my review to accept.\n\n[D] https://en.wikipedia.org/wiki/Runge%27s_phenomenon\n\n[E] https://arxiv.org/abs/2107.10599\n\n[F] https://en.wikipedia.org/wiki/Weierstrass_function",
            " - In general, our theory does not require convergence of the model during training.\n\n-- The main difficulty with your line of reasoning is that differentiable functions are not closed under uniform convergence [A]. Uniform convergence is a much stronger mode of convergence than pointwise convergence [B], which is in turn is stronger than convergence in probability [C]. Consequently, if we start from a random differentiable ANN and train the ANN, we are not guaranteed to end up with a differentiable function in the end, even when convergence is uniform. From this perspective even if we give the analysis benefit of the doubt of uniform convergence, it is still lacking in the assumption of differentiability.\n\nThe mode of convergence is also related to integrability, a sequence of integrable functions that are converging pointwise are not guaranteed to be integrable [B]. In other words, Most of the logic in the proposal does not apply to the models that the authors study, because it is assumed that a differentiable and integrable ANN would still be differentiable and integrable after training. We recommend that the authors at the very least note that they are assuming uniform convergence in both value and the derivatives of the trained network. This is a very strong condition and might even be in opposition with the assumption that the trained network is weak to adversarial attacks. I don't think that any amount of empirical evidence can compensate for this flaw.\n\n- We are curious to understand your point of view that the observations are trivially explained.\n\nThe explanation goes like this. Suppose that a trained network have not converged in its derivatives. Then, computing the empirical mean of samples of derivatives is analogues to computing the empirical mean of a Cauchy random variable. The divergence of the empirical mean as we increase the sample size is the first logical conclusion.\n\n-  \u201cnot converged in probability or the convergence is pointwise\u201d confuses us.\n\nSorry for the confusion. The correct sentence is \u201cnot converged, or the convergence is pointwise\u201d.\n\n- Significance\n\nI agree that SNNs is a big family. But if the conclusions are not positioned from the perspective of convergence and learning theory, I cannot see how the arguments could be generalized to any deterministic hypothesis class. My current understanding of your proposal is that a degenerate (point mass) distribution over a class of single layer MLPs with 300 neurons is already out of the scope of the proposal.\n\n[A] https://en.wikipedia.org/wiki/Uniform_convergence#To_differentiability\n\n[B] https://people.math.wisc.edu/~angenent/521.2017s/UniformConvergence.html\n\n[C] https://en.wikipedia.org/wiki/Convergence_of_random_variables#Almost_sure_convergence",
            " This paper investigates the robustness of stochastic neural networks against adversarial attacks. To that end, it derives a sufficient condition on the robustness in terms of the decision margin, the gradient norms, and the angle between the decision boundary and the perturbation vector. The sufficient condition holds for linear and L-smooth classifiers and the paper provides some intuition as to why the conditions should generalize to arbitrary networks. The condition confirms various empirical approaches trying to improve robustness: more samples increase the attack performance, reducing the gradient variance and expected value increase the network\u2019s robustness, and the number of samples used for inference is largely irrelevant. The paper provides a novel theoretical perspective on the robustness of stochastic neural networks. It derives a sufficient condition on the robustness in terms of the decision margin, the gradient norms, and the angles between the perturbation vector and the decision boundary. As a consequence, the paper can analyze the influence of the different terms, which have been empirically used to increase robustness in prior work, on the robustness of neural networks. Thus, the paper presents a unified view of the robustness of stochastic neural networks, which can be useful for future research in this area.\n\nSome typos:\n\n- Line 97: the input *is* stochastic\n- Line 269: theaccuracy -> the accuracy\n- Line 271: many -> Many\n- Caption of Table 1: correspondinggradient -> corresponding gradient What are the implications for randomized smoothing methods aimed at increasing the certified robustness of neural networks? The paper addresses the potential negative societal impact.",
            " This paper drives a sufficient condition for the robustness of stochastic neural networks (SNNs). The theorems show that SNNs can classify adversarial examples correctly if the angle between adversarial perturbations and gradient of the inference estimate of SNNs satisfies specific properties. These theoretical results imply several strategies for attack and defense for SNNs. Strength:\n+ This paper gives a theoretical condition to show when SNNs can classify adversarial examples correctly. \n+ Most parts of theorems and proofs are well-written and clear for understanding.\n+ Empirical results are aligned with the theoretical results.\n\nWeakness:\n\n- Missing information in theorem 4.1: $c \\neq y$ is used in proof but not in Theorem 4.1.  Supplementary material defines $\\alpha$ in line 29, but Theorem 4.1 does not describe it.\n\n- I think the theorems discussed in the paper are a bit too general and do not go deeper inside the design of SNNs from a probabilistic perspective:  The estimate used for attack and the estimate used for inference could be arbitrary two models with different decision boundaries.\nIt would be interesting to discuss more on how some properties (like the number of realizations) of SNNs influence the adversarial classification.\n\n- A type of related work is not well discussed: Random smoothing is a popular certified defense method and could be also treated as a type of SNN. (Random variables only change the input and do not influence the weight). It would be good to mention and discuss how random smoothing fits into the proposed theory. \n 1. How is random smoothing related to the implications on SNN? Yes",
            " The authors study the adversarial attacks for stochastic classifiers. The attacker has the disadvantage of not knowing the exact direction of the attack since the hypothesis is a random variable and it would change randomly every time that it is queried. It has been observed that increasing the sample size would decrease the attack success rate, which goes against intuition on its face.\n\nA sufficient condition for the robustness of the stochastic classifier against the calculated attack is proposed. Based on this condition, we can identify the factors that lead to an increased robustness of stochastic classifiers: \n1- Larger expected prediction margins \n2- A smaller expected norm of the gradient estimates \n3- Higher angles between the attack direction and the direction to the closest decision boundary during inference.\n\nThe observed angles depend inverse proportionally on the norm of the expected gradient and proportionally on the variance of the gradient estimates. This variance can be reduced by increasing the sample size. The authors proposal explains the previously reported empirical findings for SNNs from a geometrical perspective, that the robustness of SNNs is higher than the robustness of their deterministic counterparts even for strong attacks that are based on several samples, why regularization of the gradient variance, norm of the mean gradient, and angle improves the adversarial robustness, and why increasing the sample size during attack is important to exploit its potential. - Originality:\nThe paper tries to bring insight on the phenomenon from the perspective of stochastic classifiers. The proposal is positioned as an explanation for the already observed aspects of the phenomenon is SNNs. From the three proposed conditions of robustness, large margin and the mean gradient norm are already studied in the literature. The angle is unique to the stochastic setting, and is the main result of the paper.\n\n- Quality:\nThe authors have made the claims rigorous. The theorems and the results are intuitive enough. I have not read the supplementary material and the proofs. Nevertheless, the proposal lacks an analysis of the convergence of the classifiers in derivatives. The observations could be trivially explained if we assume that the derivatives of the trained SNNs have not converged in probability, or that the convergence is pointwise.\n\n- Clarity:\nThe intuition behind the proposal is clear.\n\n- Significance:\nThis is the my main concern with the paper. The main object of study in the paper is only definable for stochastic models. Consequently, it cannot be a good foundation for more general settings. It would be of help if the authors describe the proposal from the perspective of convergence in learning theory. What is the difference between a robust stochastic model and an stochastic model that have not converged in its derivatives? The authors make a good effort to describe the limitations. However, they have not included the learning theory aspects of the phenomenon. The proposal is also positioned as an explanation for observations, where as it is more likely that the observations are not causally related to the phenomenon.",
            " The authors perform a systematic study on the robustness of stochastic neural networks (SNNs). They make several theorectical corollaries: 1) In order to increase the probability of a successful attack against a SNN, the attacker need to increase the amount of samples to estimate the gradient. 2) The defender need to increase gradient variance and decrease the norm of the mean gradient, in order to improve the robustness of a SNN. 3) The robustness of a stochastic classifier does not depend on the amount of samples taken during inference.\nThe authors also conuduct extensive experiments on Fashion-MNIST and CAIFAR-10 datasets to emperically verify the correctness of these corollaries. Strengths: The paper is written with good clarity and easy to follow. It also provide valuable theoretical insights on the robustness analysis of stochastic neural networks. The theoretical corollaries are well supported in the experiments.\n\nWeaknesses: The datasets used in the experiments are relatively small.  The paper \"Certified Adversarial Robustness via Randomized Smoothing\" uses randomized smoothing to provide a robustness certificate for a neural network. A neural network with randomized smoothing can be seen as a type a stochastic neural network. How can Theorem 4.1 and Theorem 4.2 be used to certify robustness for a neural network with randomized smoothing? And how is it related to the robustness certificate in the paper \"Certified Adversarial Robustness via Randomized Smoothing\"? They discuss the limitations and potential negative societal impact of their work in section 4 & 6."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states that their concerns were addressed and maintains their \"Accept\" rating, indicating satisfaction with the authors' revisions.",
            "The review expresses concerns and critiques, indicating a negative sentiment towards specific aspects of the work. The use of words like 'concern' and 'critique' explicitly points to a negative evaluation.",
            "The review expresses doubt and disagreement with the paper's arguments and conclusions. Phrases like 'I don't think that even needs an explanation,' 'I cannot see how anything meaningful could be inferred from this experiment,' and questioning the causality indicate a negative sentiment.",
            "The reviewer is simply stating they are keeping their score the same based on the author's response and other reviews, without expressing a positive or negative opinion about the work itself.",
            "The review expresses significant concerns about the paper's reasoning and proofs, stating that the current reasoning is \"not enough\" and that the claims need \"stronger proofs.\" The reviewer also questions the meaning of certain concepts and the validity of generalizing findings. The reviewer is requesting significant changes to the paper for acceptance, suggesting a lack of confidence in the current form.",
            "The review expresses significant concerns about the theoretical soundness of the work, stating that the logic doesn't apply to the models studied due to assumptions about differentiability and integrability after training. The reviewer believes that empirical evidence cannot compensate for this flaw and that the arguments cannot be generalized without considering convergence and learning theory. The reviewer also points out a confusion in the original text.",
            "The review expresses overall positive feedback, highlighting the paper's novel theoretical perspective, unified view of robustness, and potential usefulness for future research. Phrases like \"novel theoretical perspective,\" \"unified view,\" and \"useful for future research\" indicate a positive evaluation.",
            "The review expresses overall positive feedback, highlighting the paper's strengths such as providing a theoretical condition for classifying adversarial examples, clear writing, and aligned empirical results. While weaknesses are pointed out, the overall assessment acknowledges the paper's value.",
            "The review provides both positive and negative feedback. It acknowledges the paper's originality, quality, and clarity, but also expresses concerns about its significance and lack of analysis regarding convergence and learning theory aspects.",
            "The review praises the paper's clarity, theoretical insights, and experimental support. It identifies the datasets as a weakness but also acknowledges the discussion of limitations and societal impact, indicating a generally positive assessment."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Critical",
            "Neutral",
            "Critical",
            "Critical",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude (\"I thank the authors\") and confirms their positive assessment, suggesting a supportive stance towards the authors and their work.",
            "The tone is critical because the reviewer directly points out issues with the work, such as 'My concern is about...' and 'The critique is that...'. These phrases highlight specific shortcomings or areas of disagreement.",
            "The tone is critical, using phrases like 'I cannot see how anything meaningful could be inferred from this experiment' and directly challenging the paper's assertions and explanations. The reviewer also points out potential flaws in the experimental setup and analysis.",
            "The language is factual and objective, without strong emotional coloring or subjective evaluation. Phrases like 'Based on the author's response' and 'I decide to keep my score unchanged' are straightforward and neutral in tone.",
            "The review uses phrases like \"not enough,\" \"need stronger proofs,\" \"does not make any sense to us,\" and expresses a lack of understanding. The reviewer also outlines specific conditions for acceptance, indicating a need for substantial improvements.",
            "The tone is critical due to phrases like \"main difficulty\", \"lacking in the assumption\", \"does not apply\", \"very strong condition\", \"flaw\", and \"cannot see how the arguments could be generalized\". These phrases clearly express disagreement and highlight perceived weaknesses in the work.",
            "The reviewer acknowledges the paper's strengths and potential contributions. The reviewer uses phrases like \"provides some intuition,\" \"confirms various empirical approaches,\" and \"presents a unified view,\" demonstrating support for the work's direction and findings.",
            "The review adopts a balanced tone by acknowledging both strengths and weaknesses of the paper. It offers constructive criticism while also highlighting the positive aspects, such as the clarity of the writing and the alignment of empirical results with theoretical findings. The use of '+' for strengths and '-' for weaknesses further contributes to the balanced presentation.",
            "The review presents both positive aspects (rigorous claims, intuitive theorems, clear intuition) and critical points (concerns about significance, lack of convergence analysis, potential misinterpretation of observations). This balanced approach indicates a neutral tone overall.",
            "The review presents both strengths and weaknesses of the paper. It uses objective language to describe the contributions and areas for improvement, indicating a balanced perspective. Phrases like 'good clarity,' 'valuable theoretical insights,' and 'well supported' are positive, while 'relatively small' and questions about related work point to areas needing attention."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer explicitly states that their concerns were properly addressed and as a result, they maintain their positive rating. There is no contradiction or inconsistency in the review.",
            "The review is consistent because the concerns raised are independent and do not contradict each other. Concern 1 discusses the role of stochasticity in robustness, while Concern 2 clarifies a critique about the behavior of derivatives. These are separate points of feedback and do not present any internal inconsistency.",
            "The reviewer maintains a consistent critical stance throughout the review, focusing on the potential issues related to ill-behaved derivatives and the non-robustness of mean classifiers as alternative explanations for the paper's findings. The reviewer consistently questions the paper's interpretations and provides counter-arguments and examples to support their critique.",
            "The reviewer expresses satisfaction with the author's response and explanation and maintains their original score based on the author's response and other reviewers' reviews. This indicates a consistent evaluation process.",
            "The reviewer contradicts themselves by first stating that generalizing to the deterministic case 'does not make any sense' and then immediately explaining why deterministic classifiers are relevant as degenerate stochastic classifiers and how the observed phenomena are present in the deterministic case. This indicates an inconsistency in the reviewer's reasoning about the deterministic case.",
            "The review consistently focuses on the importance of convergence and its implications for the differentiability and integrability of trained ANNs. The arguments build upon each other to support the main criticism, and there are no apparent contradictions.",
            "The review is consistent in its positive assessment of the paper, highlighting its theoretical contribution and alignment with empirical findings. It summarizes the paper's key aspects without presenting contradictory statements or arguments.",
            "The review is consistent because it presents both positive and negative aspects of the paper in a balanced manner. The weaknesses identified are valid points for improvement and do not contradict the acknowledged strengths of the paper, such as the theoretical contribution and clarity of presentation. The reviewer provides constructive criticism without undermining the overall value of the work.",
            "The reviewer consistently points out both the strengths (originality of angle, clarity, rigor) and weaknesses (limited scope, lack of convergence analysis, questionable significance) of the paper. The critique is focused and revolves around the significance and generalizability of the proposed approach, particularly its limitations to stochastic models and the absence of a learning theory perspective. There are no contradictory statements or conflicting evaluations within the review.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper without any self-contradiction. The reviewer acknowledges the paper's clarity, theoretical insights, and experimental validation of the corollaries (strengths).  The weaknesses are presented as areas for improvement and questions for clarification, such as the use of small datasets and the connection to existing robustness certification methods. These points are critical but do not contradict the positive aspects of the review."
        ]
    },
    {
        "paper_id": "iclr_2019_HklQxnC5tX",
        "paper_title": "Overlapping Community Detection with Graph Neural Networks",
        "paper_abstract": "Community detection in graphs is of central importance in graph mining, machine learning and network science.  Detecting overlapping communities is especially challenging, and remains an open problem.  Motivated by the success of graph-based  deep  learning  in  other  graph-related  tasks,  we  study  the  applicability  of this framework for overlapping community detection. We propose a probabilistic model for overlapping community detection based on the graph neural network architecture.  Despite its simplicity, our model outperforms the existing approaches in the community recovery task by a large margin.  Moreover, due to the inductive formulation, the proposed model is able to perform out-of-sample community detection for nodes that were not present at training time",
        "review_ids": [
            "SJg2Eh7CnQ",
            "rJlq687527",
            "H1lyCKNP3X"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The current paper considers the overlapping community detection problem and suggests to use the so-called graph neural networks for its solution.\n\nThe approach starts from BigCLAM model and suggests to parametrize factor matrices (or embedding vectors) via neural network with graph adjacency matrix and node attributes as inputs. The obtained algorithm is tested on several datasets and its reported performance is superior to competitors.\n\nThis paper basically tries to introduce the dependence between embedding vectors for graph nodes, which recently became de facto standard approach in machine learning for graphs. The paper is very well aligned with recent literature on ML for graphs, which is focused on combining different ideas of deep learning, tailoring them to particular graph problem and reporting results on some datasets. Unfortunately, very rarely interesting new ideas appear in these papers, and current paper is not an exception.\n\nI apologize for such a pessimistic view, but I don't see the results significantly interesting for the ICLR community and don't recommend acceptance. Some additional algorithmic/computational/theoretical insights are needed.\n\nI have couple of minor issues to discuss:\n1. For the sake of generality, I would recommend to use the general formula instead of particular 3-layer case in equation 3.\n2. I don't think that it is really appropriate to call 3-layer model a 'deep learning model', I would recommend to just name it 'neural network'\n\nAlso, I think that experimentally paper is pretty strong, but it would be nice to see the repository with algorithm code and experiments available.",
            "This paper presents an overlapping community detection method. The idea is to use a graph neural network (namely, the graph convolutional network) with node embeddings constrained to be non-negative. The non-negative embeddings helps to learn the community membership of each node (and each node can belong to multiple communities). \n\nThe idea is natural, though not novel. The only main novelty, as compared to various other recently proposed graph embedding approaches, lies in making the node embeddings non-negative. Rest of the pieces are fairly standard, including the link functions, such as Bernoulli-Poisson.  Therefore the paper is quite thin in technical novelty.\n\nIn addition to the limited technical novelty, I have a few other concerns as well, including some on the experimental evaluation:\n\n- Real-valued node embeddings obtained from shallow/deep graph embedding methods can be used with *overlapping* versions of k-means. This can be a solid baseline.\n\n- The paper relies on subsampling the edges and non-edges to speed-up optimization. However, the encode still seems to use the entire adjacency matrix. If that is not the case, please clarify.\n\n- The reported results are only on overlapping community detection. Most of the shallow/deep graph embedding methods can also be used for link prediction task (many of the recent paper report such results). It will be nice to provide results on this task.  \n\n- There has been some recent work on using deep generative models for overlapping community detection with node side information. For example, see \"Deep Generative Models for Relational Data with Side Information\" (Hu et al, 2017). Interestingly, they too use Bernoulli-Poisson link (but not GCN).\n\n- None of the baselines are deep learning methods. As I pointed out, one can use real-valued embeddings from such methods with overlapping k-means (or other overlapping clustering methods). Link-prediction results can also be compared.\n\nIn summary, I think the paper lacks both in terms of technical novelty as well as experimental evaluation and therefore doesn't seem to be ready. I would encourage the authors to consider the suggestions above.",
            "This paper proposes Deep Overlapping Community detection model (DOC), a graph convolutional network (GCN) based community detection algorithm for network data. The model is a simple combination of GCN and existing framework for community detection. The proposed algorithm is compared to baselines on various datasets, and demonstrated to be accurate in many cases.\n\nI think the paper does not deal with one of the most important aspects of network modeling - the degree heterogeneity of nodes. Many works reported that lack of degree corrections would result in bad estimates of community structures [1,2,3]. Probably including the degrees as feature of nodes would be helpful. \n\nRegarding the stochastic gradient descent by edge subsampling, I think the authors should mention [4], where the idea of edge subsampling in stochastic gradient descent setting was introduced before this work. Also, it is worth noting that we may lose some important distributional properties in graphs if we naively subsample from it [5]. For instance, sampling from positive and negative pairs to balance the class contribution may distort the sparsity and degree distributions of subsampled graphs. \n\nIf we choose to use Bernoulli-Poisson link function, we can reduce the time complexity of likelihood and gradient computation to O(N + E), where N is the number of nodes and E is the number of edges, with the auxiliary variable trick introduced in [6]. In that case we don't really have to worry about subsampling. Why didn't you consider applying this to your model?\n\nRegarding the experiments, I think some important baselines are missing [3, 6]. Also, I wonder whether the proposed algorithm would scale to the graphs with more than 100,000 nodes. \n\nReferences\n[1] B. Karrer and M. E. J. Newman. Stochastic blockmodels and community structure in networks. Physical Review E, 83(1):016107, 2011.\n[2] P. K. Gopalan, C. Wang, and D. Blei. Modeling overlapping communities with node popularities. NIPS 2013.\n[3] A. Todeschini, X. Miscouridou and F. Caron. Exchangeable Random Measures for Sparse and Modular Graphs with Overlapping Communities. CoRR 2016.\n[4] J. Lee, C. Heakulani, Z. Ghahramani, L. F. James, and S. Choi. Bayesian inference on random simple graphs with power law degree distributions. ICML 2017.\n[5] P. Orbanz. Subsampling large graphs and invariance in networks. CoRR 2017.\n[6] M. Zhou. Infinite edge partition models for overlapping community detection and link prediction. AISTATS 2015"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states a \"pessimistic view\" and does not recommend acceptance. They find the results not \"significantly interesting\" and believe the paper lacks \"interesting new ideas\" and \"algorithmic/computational/theoretical insights.\"",
            "The review expresses concerns about the paper's limited technical novelty and experimental evaluation. Phrases like \"quite thin in technical novelty\", \"a few other concerns\", \"lacks both in terms of technical novelty as well as experimental evaluation\", and \"doesn't seem to be ready\" indicate a negative sentiment.",
            "The review raises several concerns about the paper's methodology, including the lack of consideration for degree heterogeneity, the potential distortion of graph properties due to subsampling, and the absence of important baselines in the experiments. The reviewer also questions the scalability of the proposed algorithm."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Unfortunately, very rarely interesting new ideas appear\", \"I apologize for such a pessimistic view\", and \"I don't see the results significantly interesting\" which indicates a critical assessment of the paper's novelty and contribution. While providing some constructive feedback, the overall assessment is negative.",
            "The review provides constructive criticism but points out several flaws in the paper. The reviewer uses phrases such as \"limited technical novelty\", \"I have a few other concerns\", and questions the methodology and experimental setup. The suggestions are presented as necessary improvements rather than minor suggestions, highlighting a critical tone.",
            "The tone is critical due to the reviewer pointing out several shortcomings in the paper, such as the omission of degree corrections, the potential drawbacks of edge subsampling, and the absence of relevant baselines in the experiments. Phrases like \"the paper does not deal with one of the most important aspects\" and \"Why didn't you consider applying this to your model?\" convey a critical perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the paper's technical soundness (experimentally strong, aligned with literature) but consistently argues that the paper lacks novelty and significant theoretical contributions, leading to a recommendation for rejection. The reviewer's points, including minor issues and experimental strength, all support the overall assessment of insufficient novelty for ICLR.",
            "The review consistently argues that the paper lacks technical novelty and sufficient experimental evaluation. All the concerns and suggestions are aligned with this central argument, focusing on improving the experimental setup and comparisons with relevant baselines.",
            "The review consistently raises valid concerns and suggestions for improvement without contradicting itself. All points are critical and constructive, focusing on potential weaknesses of the proposed method and suggesting relevant literature and alternative approaches."
        ]
    },
    {
        "paper_id": "iclr_2020_r1gfQgSFDr",
        "paper_title": "High Fidelity Speech Synthesis with Adversarial Networks",
        "paper_abstract": "Generative adversarial networks have seen rapid development in recent years and have led to remarkable improvements in generative modelling of images. However, their application in the audio domain has received limited attention,\n      and autoregressive models, such as WaveNet, remain the state of the art in generative modelling of audio signals such as human speech. To address this paucity, we introduce GAN-TTS, a Generative Adversarial Network for Text-to-Speech.\n      Our architecture is composed of a conditional feed-forward generator producing raw speech audio, and an ensemble of discriminators which operate on random windows of different sizes. The discriminators analyse the audio both in terms of general realism, as well as how well the audio corresponds to the utterance that should be pronounced.  To measure the performance of GAN-TTS, we employ both subjective human evaluation (MOS - Mean Opinion Score), as well as novel quantitative metrics (Fr\u00e9chet DeepSpeech Distance and Kernel DeepSpeech Distance), which we find to be well correlated with MOS. We show that GAN-TTS is capable of generating high-fidelity speech with naturalness comparable to the state-of-the-art models, and unlike autoregressive models, it is highly parallelisable thanks to an efficient feed-forward generator. Listen to GAN-TTS reading this abstract at https://storage.googleapis.com/deepmind-media/research/abstract.wav",
        "review_ids": [
            "rklAPQJOKS",
            "rkg0V5P9iB",
            "BJlJ-YsaKS",
            "Hyg3Zr60FS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I want thank the authors for solving this long-standing GAN challenge in raw waveform synthesis. With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state-of-the-art results. Although the speech fidelity of GAN-TTS is still worse than WaveNet and Parallel WaveNet from the posted sample, it has begun to close the significant performance gap that has existed between autoregressive models and GANs for raw audios. Overall, this is a very good paper with significant contributions to the filed.\n\nDetailed comment:\n\n1, In WaveNet, the conditional features (linguistic / mel-spectrogram) are added as bias terms in the convolutional layers. Did the authors tried this alternative architecture for the generator, which uses the white noisy z as network input (similar as flow-based models, e.g., Parallel WaveNet) and the conditional features as bias term in the convolutional layers? \n\n2, Could the authors comment the importance of serval architecture choices in this work? From Table 1, it seems to me that the ensemble of random window discriminators is the most important (perhaps the only important) contributing factor for the success. For example, the MOS score was boosted from 1.889 to 4.213 by replacing a single full discriminator to the ensemble of RWDs.\n\n3, The notations in Eq. (1) and (2) are messy. Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time. \n\n4, The stable training (NO model collapses) is pretty impressive. Could the authors shed some light on the potential reason? Does the ensemble of RWD regularizes the training? What's your experience for training FullD (does not have random window ) and cRWD_1 (only has one random window discriminator)? Are they still very stable? Also, could the authors comment on the importance of large batch size -- 1024 for stable training of GAN-TTS? \n\n5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).   \n\nYamamoto et al. Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation. 2019.\n\n\n=== update === \n\nThank you for the detailed response.  \n2,  Thanks for the elaboration.    \n4,  It would be very interesting to see an analysis of model stability with smaller batch sizes. \n",
            "Dear Reviewers, thanks for your thoughtful input on this submission! \u00a0The authors have now responded to your comments. \u00a0Please be sure to go through their replies and revisions. \u00a0If you have additional feedback or questions, it would be great to get them this week while the authors still have the opportunity to respond/revise further. \u00a0Thanks!\n",
            "This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non-autoregressive) generator and discriminator. There have been various trials of GAN-TTS but not so many success and I'm glad to hear that the proposed method seems to enable GAN-TTS with fast inference thanks to the non-autoregressive property. The method also proposes new objective measures inspired by the image recognition network based on the high-level features generated by end-to-end ASR, which is also another important contribution of this paper. \n\nMy concern for this paper is reproducibility. Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator. Apart from that, the paper is well written overall by well describing the trend of GAN studies in the image processing and the application of such image processing oriented GAN techniques to TTS.",
            "This paper puts forth adversarial architectures for TTS. Currently, there aren't many examples (e.g. Donahue et al,  Engel et al. referenced in paper) of GANs being used successfully in TTS, so this papers in this area are significant. \n\nThe architectures proposed are convolutional (in the manner of Yu and Koltun), with increasing receptive field sizes taking into account the long term dependency structure inherent in speech signals. The input to the generator are linguistic and pitch signals - extracted externally, and noise. In that sense, we are working with a conditional GAN. \n\nI found the discriminator design very interesting. As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales. In the image world, having a single discriminator for the whole model would not take into account local structure of the images. Likewise, perhaps we can imagine something similar in the case of audio at varying scales - in fact, audio dependencies are even more long range. That might be one reason why the variable window sizes work here. \n\nThe paper also presents to image analogues for metrics based on FID and the KID, with the features being taken from DeepSpeech2. \n\nI found the speech sample presented very convincing. In general, the architectures are also presented quite clearly, so it seems that we might be able to reproduce these experiments in our own practice. It is also promising that producing good speech could be achieved by a non-autoregressive or attention based architecture.\n\nThe authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture?\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude to the authors, acknowledges the significance of their work, and concludes that it is a 'very good paper with significant contributions.' While there are constructive criticisms, the overall sentiment remains positive.",
            "The review expresses gratitude for the reviewers' input and encourages further engagement, indicating a positive and collaborative attitude.",
            "The reviewer expresses positive feelings towards the paper's contributions, stating they are \"glad to hear that the proposed method seems to enable GAN-TTS\" and highlighting the \"important contribution\" of the new objective measures. The reviewer also acknowledges the authors' efforts and finds the paper \"well written overall\".",
            "The review expresses positive sentiment by highlighting the significance of the paper, praising the discriminator design, finding the speech samples convincing, and noting the clear presentation of the architectures. Phrases like \"very interesting,\" \"quite significant,\" \"very convincing,\" and \"presented quite clearly\" indicate a positive assessment."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review begins with gratitude and positive feedback, then moves into specific questions and suggestions for improvement, ending with appreciation for the authors' response. This blend of positive acknowledgment and constructive criticism indicates a balanced tone.",
            "The review uses encouraging language like \"thoughtful input,\" \"great to get them this week,\" and \"opportunity to respond/revise further,\" which creates a supportive and collaborative tone.",
            "The review presents both positive aspects (glad to hear about the method, important contribution, well-written) and a concern (reproducibility due to lack of code and data). This balanced approach indicates a neutral tone overall, acknowledging strengths while pointing out weaknesses.",
            "The tone is supportive, as the reviewer highlights the paper's strengths, expresses interest in the design, and suggests the potential for reproducibility. The reviewer also poses a question to the authors in a constructive manner, indicating engagement and a desire for further clarification."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and constructive. It starts with an overall positive assessment, acknowledging the paper's contribution to solving a long-standing challenge. The detailed comments are specific and aimed at improving the paper, focusing on architecture choices, notation clarity, training stability, and related work. The reviewer maintains a helpful and encouraging tone throughout the review and in the update after the response, indicating a consistent perspective.",
            "The review is a consistent message encouraging reviewers to check author responses and provide further feedback. It expresses a clear and logical request without any contradictions.",
            "The review is consistent because it acknowledges the paper's contributions and strengths, such as the novelty of the approach, fast inference, and new objective measures, while also raising a valid concern about reproducibility due to the lack of publicly available code and data and the complexity of the implementation. The reviewer appreciates the paper's writing quality and contextualization within the field, and the concern about reproducibility does not contradict the positive aspects of the review.",
            "The review consistently expresses positive opinions about the paper. The reviewer highlights the significance of the work, praises the architecture design, finds the discriminator interesting, appreciates the metrics, and is convinced by the speech samples. The reviewer's comments are uniformly positive and supportive of the paper's contributions."
        ]
    },
    {
        "paper_id": "iclr_2020_SJlVVAEKwS",
        "paper_title": "Adversarial Imitation Attack",
        "paper_abstract": "Deep learning models are known to be vulnerable to adversarial examples. A practical adversarial attack should require as little as possible knowledge of attacked models T. Current substitute attacks need pre-trained models to generate adversarial examples and their attack success rates heavily rely on the transferability of adversarial examples. Current score-based and decision-based attacks require lots of queries for the T. In this study, we propose a novel adversarial imitation attack. First, it produces a replica of the T by a two-player game like the generative adversarial networks (GANs). The objective of the generative model G is to generate examples which lead D returning different outputs with T. The objective of the discriminative model D is to output the same labels with T under the same inputs. Then, the adversarial examples generated by D are utilized to fool the T. Compared with the current substitute attacks, imitation attack can use less training data to produce a replica of T and improve the transferability of adversarial examples. Experiments demonstrate that our imitation attack requires less training data than the black-box substitute attacks, but achieves an attack success rate close to the white-box attack on unseen data with no query. ",
        "review_ids": [
            "BJxYga09Kr",
            "BklEYLy0tS",
            "SkeKJn8y5H"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a new approach to conduct adversarial attacks, where an imitation classifier is trained to mimic the behaviours of the targeted/attacked classifier and adversarial attacks can be generated with existing attack methods on the imitation classifier. The training of the imitation classifier only requires the predictions of the targeted classifier, which may fit better in practice with limited access to the targeted classifier.\n\nThe reasons that I am going towards accept are as follows:\n\n1. I feel that the idea of learning an imitation classifier is somehow novel and intuitive, which would improve the applicability of existing adversarial attacks in more realistic cases. In addition, using GAN framework in the training of the imitation classifier is also interesting.\n\n2. The experiments are quite comprehensive and promising, including the comparisons with gradient-based attacks as well as the decision-based attacks. In addition, the different model configurations of the imitation classifier are also reported.\n\nSuggestions:\n\n1. It is a bit unclear of the model configuration of the generator, which seems to be not introduced in details.\n\n2. It would be interesting to visualise what the generator generates.\n\nMinor:\n\n\"sometimes the ability of G is much stronger than G\"",
            "Authors propose a GAN-based adv imitation attack that can use less training data to produce a replica of the model.\n\nThe idea of using a GAN in producing adv examples in quite interesting. But the proposed approach is closely related to the following paper:\n\nhttps://arxiv.org/pdf/1801.02610.pdf\n\nTherefore, I am not sure about the novelty of the proposed approach.\n\nAlso can authors comment on the stability of GAN's training? Are any stabilizing methods integrated in GANs being used? \n\nHow does the proposed approach relate to the adversarial distillation literature? ",
            "The authors propose to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. They then show that the adversarial examples for the substitute can be effectively used to attack the learned model. The idea is straightforward. The proposed approach leads to better success rates of attacking than other substitute-training approaches that require more training examples. Promising experimental results against decision and score-based attach schemes also demonstrate the effectiveness of the proposed approach.\n\nMy main concern is that the comparison to other approaches seem unfair, because\n\n(1) Substitute models typically use all training data and query the learned model once per training example; the proposed approach uses fewer training data but needs 1800 queries per example. So it is not surprising that the proposed approach can be better than substitute models. As the authors focus on \"practical\" scenarios, the number of queries should be fixed as a constraint for all approaches to be fair.\n\n(2) Having said (1), there is not enough detail in this paper to understand the similarity and difference between the proposed approach and decision-based ones (which is claimed to have similar query complexity during training). The authors mix the results with score-based (requiring more information), making the results more confusing to the readers. The authors are encouraged to present more detailed discussions on the comparison with decision-based competitors.\n\nAlso,\n\n(3) One thing that is missing from the experiments is how well the replica clones the original model. All the information in the experiments are somewhat \"indirect\" (success rate, test accuracy, etc.) to answer this question, but there is no direct evidence. Is a well-cloned replica really a better substitute to construct adversarial examples? For instance, for replica with different \"cloning accuracy\" (rather than test accuracy), is a better replica also a better substitute? The paper fails to answer this question that best matches its design motivation.\n\nThe above are my main concerns. A minor one is\n\n(4) The abstract that directly uses notations like T, G and D is horribly hard to read.\n"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states reasons for acceptance, highlighting the novelty and intuitiveness of the idea, the interesting use of the GAN framework, and the comprehensive and promising experiments.",
            "The reviewer expresses doubt about the novelty of the approach.",
            "The review expresses concerns about the fairness of comparisons, lack of detail, and missing information in the experiments, indicating a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"I feel that the idea...is somehow novel and intuitive\" and \"The experiments are quite comprehensive and promising\", indicating a supportive and encouraging tone. They also offer constructive suggestions rather than harsh criticisms.",
            "The review questions the novelty of the work.",
            "The tone is critical due to the identification of flaws in the methodology, experimental setup, and presentation of results."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer clearly expresses a positive inclination towards accepting the paper based on the novelty of the idea and the comprehensive experiments. The suggestions provided are constructive and aimed at improving the paper, such as clarifying model details and adding visualizations, rather than pointing out fundamental flaws or contradictions in the paper's approach or the reviewer's assessment.",
            "The review is consistent because it raises valid points and questions regarding the novelty of the approach, the stability of GAN training, and the relation to adversarial distillation literature. The reviewer acknowledges the interesting idea but expresses concerns about novelty and seeks clarification on technical details and related work, which are all relevant aspects of a constructive review.",
            "The review is consistent because the reviewer raises several valid concerns about the methodology, experimental setup, and clarity of the paper. All points consistently point out weaknesses in the paper, such as unfair comparisons, lack of detail, missing experiments, and unclear writing in the abstract. The reviewer's arguments are logically connected and contribute to a coherent critique of the paper's shortcomings."
        ]
    },
    {
        "paper_id": "nips_2021_urueR03mkng",
        "paper_title": "Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning",
        "paper_abstract": "Christopher Rytting, David Wingate",
        "review_ids": [
            "YA-HtgxMmAa",
            "RSPYOm8V9SW",
            "Fqli0Wb1kzN",
            "fFbE9__G81a",
            "2cw6WK-gV-k",
            "88JFRyvQiC"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " ```The axes of generalization we explore are useful not just to see whether the model grasps the task by substituting similar words (which we show with part-of-speech generalization, and could be an ability also possessed by embeddings), but also to understand the limits of cardinality generalization in terms of number of objects, containers, steps, and maps tracked, and this is a type of reasoning ability well-detached from learning word-classes. ```\n\n I agree that these concepts are separate from the question of word classes, but in these games they are built upon them (or finer-grained classes maybe in navigation, to understand what terms are rooms perhaps).  How can you separate models having inductive biases that support reason-based generalizations from simply knowing word-classes if you aren't comparing to a model that one might believe only knows the latter?  Of course the reasoning ability of doing ~8 room navigation tasks from ~7k examples with exact string prediction strikes me as cool.  It wasn't so long ago that I remember LSTMs failing on simpler reversal tasks.  But there's the rub -- as a reader of this work and not a researcher working on these models and similar datasets, I have no point of reference for understanding the difficulty of this sort of task (at least, with 2021-grade tech).  If my only point of reference is given in the paper in the form of a comparison to a model that doesn't begin with any understanding of what a word is, then I would expect a gigantic gap between these models, and it doesn't help me disentangle what types of information from the pre-trained model are helpful for the task, and I think whether or not these go beyond word-classes is not clear.  \n\n```but we thought and think that there is a lot more reason to suspect that large language models would  possess them than that word embeddings would, and for that reason we chose this class of models to base our paper on. We're not aware of any work showing word embeddings to be capable of the things we show in this paper.```\n\nI really agree, but in this context of scientific literature, isn't it reasonable and necessary to see those experiments to understand which types of generalization properties are specific to these types of pre-trained model architectures, and to models trained on various sizes?  In short it would be good to understand that these models offer something unique, and turn some attention to the two ways in which they differ from a typical word2vec setup (model + data size) to understand how they arise.  And only then does it seem to isolate the topic of interest.  I apologize for harping on this idea of word2vec-style baselines, but I have difficulty find a clear novel lesson from these experiments without comparisons of that kind.\n\nRegarding concreteness, I also am not aware of any work showing word embeddings capable of capturing this, but when you consider that the more concrete words should (by virtue of being capable of being interacted with) appear in the contexts of similar action/sensing verbs, it seems likely they would share a similar space in some dimensions.  That is my guess anyway.  I know I've seen similar embedding models improve performance on semantic role labeling beyond using gold postags, and that would imply better performance in subcategorization framing, and that would imply some of these these sorts of interactions probably pop up to some useable extent (or in the lingo of this submission, exist as inductive biases in the representations of semi-supervised embedding models prior to large pre-trained transformer models).",
            " Thank you for clarifying these points.\n\nSkipping ahead, regarding the included baselines (T5 from scratch), I can certainly sympathize with the never-ending \"Whac-a-mole\"-style challenge of addressing every reviewer's concerns across multiple submissions.  As I'll discuss later, I (obviously) do think it's important to include reasonable baselines and I can see how without these T5-from-scratch comparisons it would come across as a glaring omission.  But I don't think we learn much at all from including these particular baselines either.  But now with a better understanding of why they are there, I think the question of their value can be put aside.\n\nRegarding criticism 1, you're quite right to say I may have jumped the gun on trying to distinguish \"real world\" knowledge from linguistic knowledge and focusing primarily on the former.  I think the introduction does position the reader to be thinking along those lines, but it's also a lot of assumption on my part.\n\nBut I think it's also a set of assumptions I was quick to adopt because I'm searching for a story that I think is compelling enough to warrant acceptance, and the more \"real world\" knowledge that comes through to improve task performance, the more these results are going to read as novel in comparison to (1) older embedding representations, such as word2vec, fasttext, etc., and (2) other T5/large pre-trained (often transformer-based) LM work that probes these models for syntactic or word class information.  \n\nI don't want to be too repetitive to the original review, but I think it's implied in making \"the Inductive Bias of Large Language Models\" the focus of the paper, that we are to some extent interested in the inductive biases of these models that are not also present in the previous \"versions\" of these models which we have been using for the past ten years.  I think if we cared about the task itself, the performance figures in the paper would be compelling in and of itself.  But since these are toy-ish tasks, I believe the focus must shift to understanding the inductive biases of these models with respect to existing literature.  \n\nSo I would really like to see evidence that these models offer something novel and significant, as it pertains to this task, over existing embedding models.  While I suppose it could be argued that this is a comparison for another paper, it's also true of almost any paper that the method should be contrasted with existing comparable methods in the literature, and the paper is currently lacking in this area.  What I wouldn't want to see is follow-up research using the same tasks but say, typical word embeddings, and show that they perform comparably well on this task.  And therefore show that really it was basic linguistic knowledge of word classes / object clusters (of the sort we had prior to the development of \"Large Language Models\") that were being tested in these experiments.  We know that representations learned by language models, simple skip-gram or large pre-trained ones, are useful to tasks that need this sort of information.  I don't see the value of showing this lesson once again, using situated game worlds in place of other typical NLP tasks.\n\nSo it's unfortunate to me that intuitively the tasks seem to be designed to test this type of information (word class / semantic clusters) rather than some less typical knowledge.  Concreteness and commoness are not types of linguistic knowledge that I have ever seen studied in a representation probing paper, but I have seen T-SNE plots of word2vec-style embeddings that look clustered in ways that seem to reflect concreteness.  But I would really be swayed if the game worlds relied on information not found in existing non-contextual word embeddings.  Showing concreteness is one of these would be a step in that direction, but designing experiments to show more \"understand[ing of] the distribution of that world itself\" would be a big improvement.\n\nA possible additional reference: what you're trying to show swapping verb/noun representations reminds me a bit of the following paper:\n\nInvestigating Human Priors for Playing Video Games\nhttps://arxiv.org/pdf/1802.10217.pdf\n",
            "This paper takes a closer look at the ability of language models to perform reasoning based tasks such as tracking states of entities and answering navigation based questions. The main question is: Can these models generalize outside of the training distribution, thus exhibiting the ability to learn underlying rules instead of learning superficial correlations that only work on in-distribution data. This question is studied through several different generalization splits. From results, we see some evidence that pre-trained models are able to generalize outside of the training distribution though in some cases, it\u2019s inconclusive, (and I elaborate on why below).   Originality: While I think that the exact settings in this work are somewhat original, Clark et al. 2020 (RuleTaker) also  consider reasoning abilities of transformers on synthetic tasks and look at various generalization abilities. Similarly, Banerjee et al. 2020 also look at how well transformers can be trained to do various reasoning tasks in blocks worlds. In the authors\u2019 opinion, what are some major differences between these works and this paper?  \n\nExperiments: While I really like this direction, I think some of the experiments are inconclusive in determining whether the model has truly learnt the underlying rule. \n- For example, in the containers experiment the model just needs to track the state of 2 containers (which can be easily detected based on the language context) to correctly answer. So, even if the model can generalize to more objects and containers, it is not really impressive since the model can learn to ignore all other containers except the two containers whose states were altered. Indeed, if we look at the navigation results, we see poorer extrapolation since the task is harder. \n- In Section 4.4, we cannot immediately conclude that the model is leveraging previous knowledge. The model could just as well be learning superficial features from the container and navigation tasks that help on the hard object tasks. One way to check for this confounder would be to just do some steps of pre-training on the Nav and Cont tasks and see if that by itself explains the fast learning on HardObj. If not, we can then conclude that perhaps the model is leveraging previously acquired knowledge to do well on a composition.\n\n- I really liked the experiment in Section 4.5. Although, I wonder if the decrease in performance has a simpler explanation like gibberish words like \u201csixnqkxb\u201d contain more sub-words and are hence harder to reason / track. \n\nQuality: I think the paper is decently written. Some style suggestions:\n- Table-2 could be converted into a barplot (moving numbers into appendix) since it\u2019s really hard to parse so many numbers. And it\u2019s generally good to bold the best numbers for a quick read.\n- In Figure-7, it is unclear what the x axis refers to.\n N/A",
            "This paper investigates whether pre-trained language models (T5 in this case) learn priors which support symbolic reasoning tasks in the process of optimizing their purely linguistic objective.  The performance of fine-tuned pre-trained LMs is evaluated on two symbolic reasoning tasks -- object location across a number of boxes, and a room navigation task -- and compared with identical models trained from scratch.  The results touch on a number of topics, but generally show (1) good reasoning accuracy when starting from the pre-trained t5 model, (2) abysmal performance by the from-scratch models, (3) a good degree amount of generalization performance for t5 to reason beyond the fine-tune scenario, and (4) that learning subtasks improves performance on a compositional task.\n  There's a lot to like in this paper.  The presentation and writing quality overall is just fantastic.  Not only a clear presentation, but a great style that makes the reading really enjoyable.  The authors have also been very thorough in listing the considerations behind each experiment, and approached each from multiple angles to get rid of confounds.  It's clear that a lot of thought lies behind each one.\n\nMy main criticism, and a crucial one, is that the experiments don't go far enough to show what the authors purport to show.  This boils down to a few things:\n\n1) Unclear separation between language understanding and world knowledge.\n\nThe story laid out in the introduction, especially paragraph two, is one of text mirroring patterns in the real world, and the potetial for LMs to encapsulate this knowledge.  The authors provide a long list of potential types of world knowledge, and whether LMs capture any of these is very exciting and a very timely research question.  Just to underscore that this sort of information is central to the paper's argument, L25, \"-If that is so-, then they ought to provide a useful inductive bias in learning to perform symbolic reasoning tasks that mirror real-world tasks\".  I'll call this \"world knowledge\" for the remainder of the review.  Those statements really set the stage for the tone of what this paper is claiming to show, it follows from the reasoning in L7-8 of the abstract, etc.  Suffice to say, from that point I would expect that the paper ends by conclusively showing that this world knowledge is there in their chosen pre-trained LM.  And I think this is where it fails.\n\nDesired interpretation of results: Container task.  Pre-trained model performs vastly better on extrapolation and overall when compared to from-scratch model, because it has embodied world knowledge, (maybe object persistence, containment, from the introduction list).  It shows good generalization ability because it T5 has induced reusable structures of reasoning / world knowledge.  It performs better with concrete nouns, then nouns, then verbs, because utilizing these objects in this manner does not make sense.\n\nAlternate interpretation of results: Container task.  Pre-trained model performs vastly better when compared to from-scratch model because it has some rough representation of the word classes of these words.  It generalizes better to new words because it has similar representations of those words.  It shows good generalization ability because it can do so on top of well defined word class representations (completely apart from their role in reasoning).  It performs worse on verbs because it has trouble refining representations of ungrammatical sentences, and reasoning on top of non-sensical sentences suffer.\n\nIn the first interpretation, I'm really interested to further understand these induced world knowledge patterns more and I really want to accept this paper.  In the second interpretation, it's just shown that pre-trained LMs have really good representations of basic linguistic categories, and that fine-tuning for all sorts of tasks is hugely effective, something the community is already quite aware of.  It's still interesting to see applied to these bAbI-like tasks, but then again, that's a different sort of paper.  In that setting, I would have liked to have seen a comparison with those models and more practical advantages/disadvantages/failure cases described.  \nAnd to the second interpretation, where basic linguistic knowledge accounts for everything,  I don't see any evidence here that would rule this out.  That's not to say that the hypothesis is wrong -- indeed, I would suspect there is a lot of world knowledge in there -- but I don't believe the authors have shown that conclusively.  I think the tasks chosen here make it difficult to really talk about real world knowledge.\n\nThe authors sometimes waffle between these two.  For instance, at the end of section 4.5, the English->gibberish conversion, the authors state that their experiment provides \"evidence in favor of the notion that the inductive bias of large LMs does, in fact, aid in learning quickly and generalize well in several cases\".  In this instances, the story is muddled, because we know that a general inductive bias in large LMs helps learning in many areas, it's basically the state of NLP at the moment, and so it needs no supporting evidence.  It's whether the inductive bias is based in real world knowledge that is unique to this work and pushed for in the introduction, and these experiments do not separate whether the advantage of the English model comes from linguistic knowledge or world knowledge.\n\nIt actually strikes me perhaps as evidence of point (3) below, where the gap between this lines correlates roughly to the combined linguistic and world knowledge the LM learns, while the reasonably high performance of the gibberish model is basically a testament to these training procedures getting the weights in a good position for transfer to unrelated tasks.\n\nAnother possibility of more concretely discovering evidence of world knowledge would be probing the representations in a targetted way.  Combining that with the sorts of experiments provided could strengthen the overall impression of the work.\n\n2) Poor Baselines\n\nIn terms of experimental setup, of course it is tough to get at exactly what types of practical knowledge / inductive bias (apart from linguistic knowledge) these models contain, but I think some problem stem from the chosen baselines.  The authors use the same T5 model for the pre-trained model and for the from-scratch model.  This was a terrible idea in my opinion.  No reader should accept a 3 billion parameter model trained for 1000 steps to be a suitable baseline for comparison -- it is clear that this model was not given any chance to succeed.  I'm curious how smaller-parameter variants fair on the from-scratch setting.  I think it's quite clear that 3B parameters is not required for these sorts of tasks.\n\nSpeaking of similar tasks, the tasks are admittedly close to the FB bAbI in spirit, however, going back to that original work, there was not so much difficulty in achieving good results with models that we would considering quite basic by today's standards.  Why?  Presumably baseline model size and over-capacity.  But also thinking more generally, there were previously good results on bAbI tasks with simple memory-augmented models on top of word embeddings.  Is it similarly sound to claim that word embeddings had strong inductive biases that mirrored the world outside of their linguistic roles?  Of course it's also a question of training size and setting the tasks up to test generalization, but the rationale seems similar to the argument of this work.\n\nI would have also liked to have seen some other models which are maybe capable of inducing linguistic knowledge relevant for the task, without any serious possibility of learning things like physics or spatial relationships, other examples of these properties, etc.\n\n3.) Large pre-trained LMs can improve performance on unrelated tasks\n\nAn important point of comparison is the \"Pretrained Transformers as Universal Computation Engines\"[1], which was not mentioned in this paper, though it appeared on arxiv only a few weeks prior to the submission deadline.  Inn summary, the authors showed that pre-training on large text corpora provide a significant learning advantage when applied to completely unrelated domains (via fine-tuning).  \n\nWhen considering this submission in light of this finding, it raises the question of whether the conclusions in this work are true / whether pre-trained LMs have induced symbolic reasoning knowledge, or merely a result of more universal organization properties in the network.  It would perhaps be interesting to compare these results with models pre-trained on an equally large section of another language and test in English, especially if preposition-style relationships were expressed in different ways (since they seem fundamental to both tasks).\n\nRegardless of the answer to these questions, this is an important point to discuss.\n\n[1]\n@article{DBLP:journals/corr/abs-2103-05247,\n  author    = {Kevin Lu and\n               Aditya Grover and\n               Pieter Abbeel and\n               Igor Mordatch},\n  title     = {Pretrained Transformers as Universal Computation Engines},\n  journal   = {CoRR},\n  volume    = {abs/2103.05247},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2103.05247},\n  archivePrefix = {arXiv},\n  eprint    = {2103.05247},\n  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-05247.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n\n\n\n\n\nQuestions:\n\nDoes the model size matter?  I'm curious how small a T5 model has to be before it has significant effects on the reasoning task, especially if additional fine-tuning layers were added (of equal size for all T5 variants), and thus isolate the task-specific weights from the \"induced world knowledge\" weights.\n\nThe paper might benefit from more specificity in what the world knowledge the authors feel is relevant here.  For each, they could also describe whether previous NLP work has shown these properties exist in pre-trained LMs or in their necessity in other toy tasks.\nAnd for instance, to choose one from the introduction that may be relevant, \"material properties\"?  For whatever property the authors think relates to their chosen experiments, could they discuss how it is necessary for the task?\n\nL153: for fine-tuning, is this purely using the T5 weights or are any additional layers/heads added to the model?\nL155: what constitutes a step?  A single training example?\n\n\n\nPresentation:\n\nPersonally I find the use of \"symbolic reasoning engine\" to describe a fine-tuned T5 model a failed attempt at inflating the sophistication of the method.  At the very least, it prevents readers from more immediately latching on to the specifics of the model and therefore the study.\n\nFig 1., very difficult to spot the various overlayed lines, especially in a printed version.\n\nIn L19-20, The examples of tasks where pre-trained LMs achieve SoTA accuracy do not require patterns of ``real-world'' structure described in L22.  Where is the citation or evidence that generated text reflects the latter forms of knowledge?\n\nL65, BLEU score is used to see how similar sentences are on subword levels?  But BLEU is a word-level metric?\n\nL92, how are selected objects arranged into sentences?  What kind of diversity is there in these generated sentences or templates?\n\nL102, most common... nouns / and join the dataset -- this description was a bit clunky\n Yes.",
            "In this work, the authors investigate if large scale pre-trained language models (such as T5) can provide inductive biases that are useful for solving language-based symbolic reasoning tasks, and furthermore, generalize to unseen settings. \n\nFirst, the authors define four types of tasks. While all the four tasks are formulated as sequence generation given prompts (prefixes), they require different kinds of reasoning:\n\n1. Container. The prefix text describes an initial states of the world and a sequence of transitions, the target sequence needs to describe the ending states.\n\n2. Navigation Route. The prefix text describes a map as well as two positions in the map, the target sequence is the navigation path from one position to the other.\n\n3. Navigation Result. The prefix text describes a map, a starting position, and a navigation path, the target is the ending position.\n\n4. Composite Task. A mixture of container and navigation tasks. \n\nThen, the authors define a set of experimental settings targeting the generalizability of models, from a variety of aspects such as map sizes, object/container numbers, reasoning steps required as well as some linguistic properties (e.g., replacing nouns with words with other POS tags, or even made-up words).\n\nThe authors provide plenty of experiments, suggesting the pre-trained language models indeed have capability of learning and to some extent generalizing symbolic rules via language-based tasks.\n  ## Synthetically generated data\n\nJust to be transparent. I was one of the reviewers of this paper at ICML 2021. The authors have described in the \"submission history\" field about lacking of baselines. In addition to that, there were some ICML reviewers devalued this work because the used tasks are synthetically generated (and thus not natural language).  \n\nI want to emphasize my point that **synthetically generated tasks are equally valuable**. \n\nI understand and agree that the ultimate goal is to model and perform textual reasoning on real world data. However, before stepping into natural language data, I believe it is meaningful to have synthetic environments that are controllable, helping researchers to better understand the problem. Although the tasks introduced in this work use templated language, it is clearly shown in the figures that they are not oversimplified (there are areas in the heatmaps where the models fail to generalize). It might make less sense to use natural language data when strong pre-trained LMs such as T5 still have trouble solving/generalizing on toy-ish tasks. Because otherwise, it might be difficult to analyze, for example, which part of the task is the model struggling on. Furthermore, there are many examples in the NLP communities that use synthetic datasets (templated language) as a starting point of certain research directions, such as bAbI (Weston et al., 2016), TextWorld (C\u00f4t\u00e9 et al., 2018) and many more.\n\nOne potentially interesting thing to try might be, back-translating (e.g., EN -> DE -> EN) the teamplated descriptions, and investigate to what degree the language models generalize on this dimension of \"naturalness\".\n\n## Overall comments\n\nThis work provides a set of well-designed experiments with convincing results that explore an interesting and important direction. \n\nSince the flourishing of the large scale pre-trained language models, SOTA scores of neural models on various NLP tasks have increased by a lot. On some tasks, some researchers believe neural models have achieved human-level performance, while others argue that neural models may be somehow exploiting biases and trivial cues injected into the data or model unconsciously, rather than really doing reasoning. \n\nTo that end, the community do require works that help us understanding what and why such language models can learn, and to generalize. This work falls into this category. \n\nAmong the experiment designs and presentations, I especially like the heatmaps, which clearly show how the model gradually loses its generalizability when certain dimension of task becomes more difficult. \n\n\n## Typos and minor things\n\n1. L217: There are 3 groups of columns\n\n## References\n\n1. Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks. Weston et al., 2016.\n\n2. TextWorld: A Learning Environment for Text-based Games. C\u00f4t\u00e9 et al., 2018. Since the current version of the paper uses a set of synthetically generated toy tasks, I do not see immediate negative societal impacts. However, because the work relies heavily on large scale pre-trained language models, which training data is difficult to control. The author should definitely discuss the potential negative effect or harm these techniques may bring in the revision.",
            "This paper aims to explore whether the inductive bias of pre-trained language models can support symbolic reasoning tasks. Three different types of tasks are designed for the above objective, including a container-based task, a navigation task and a composite task. In order to verify the generalization ability of the model, the paper also designed different kinds of generalization for probing, including cardinality generalization, object generalization, POS generalization and reasonable phrasing generalization. T5 is fine-tuned and evaluated on these tasks, with some interesting findings observed, which I think are the key contribution of this work: (1) for container-based task, T5 shows good generalization and prediction capability; (2) for navigation task, T5 can also do a good job but when the number of inference steps changes, its performance will drop. (3) for composite task, T5 can do a good job  by learning in a curriculum learning way.  The objective of the paper is clearly clarified. The writing is easy to follow. The findings in the experiments are interesting. It is a good paper to study how well pre-trained LM can perform on reasoning-required tasks and its generalization capabilities on different aspects. I appreciate this work but think the datasets built within this paper is limited to specific domains and scenarios. It will be better if the paper can cover results on other open domain datasets as well, such as commonsense QA, mathematical reasoning, etc.  no negative societal impact"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Neutral",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses several concerns and criticisms about the paper's methodology and lack of comparative analysis. Phrases like \"I have no point of reference for understanding the difficulty of this sort of task,\" \"it doesn't help me disentangle what types of information from the pre-trained model are helpful for the task,\" and \"I have difficulty find a clear novel lesson from these experiments without comparisons of that kind\" indicate a negative sentiment.",
            "The review expresses a mix of agreement and disagreement. While acknowledging the authors' efforts and understanding their reasoning, it raises concerns about the novelty and significance of the findings compared to existing methods. The reviewer uses phrases like 'I can certainly sympathize' and 'But I don't think we learn much at all' indicating a balanced perspective.",
            "The review expresses both positive aspects (liking the direction and one experiment) and negative aspects (inconclusive experiments, concerns about superficial feature learning). It offers constructive criticism and suggestions for improvement, indicating a balanced perspective.",
            "The review expresses significant concerns about the paper's methodology and conclusions. The reviewer states that the experiments 'don't go far enough to show what the authors purport to show,' criticizes the baselines as 'poor,' and questions whether the results truly demonstrate symbolic reasoning knowledge or are merely due to universal organization properties of the network. The reviewer also points out a lack of separation between language understanding and world knowledge.",
            "The review expresses positive sentiment due to the reviewer's appreciation for the work's design, results, and contribution to understanding language models. Phrases like 'well-designed experiments,' 'convincing results,' and 'interesting and important direction' indicate a positive evaluation.",
            "The reviewer expresses appreciation for the work, highlighting interesting findings and clear writing. Phrases like 'good generalization and prediction capability,' 'can also do a good job,' and 'a good paper' indicate a positive overall sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical",
            "Critical",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The tone is critical as the reviewer questions the novelty and significance of the findings due to the absence of a comparison with word2vec-style baselines. The reviewer uses phrases like \"How can you separate models...\" and \"I apologize for harping on this idea...\" which indicate a critical stance towards the paper's approach.",
            "The tone is balanced. The reviewer uses phrases like 'you're quite right to say' to show agreement, but also expresses concerns and critiques using phrases like 'I don't think we learn much at all' and 'it's unfortunate to me'. The reviewer also offers constructive suggestions.",
            "The review contains specific criticisms of the experimental design and interpretation of results, using phrases like 'inconclusive in determining whether the model has truly learnt the underlying rule,' 'not really impressive,' and 'we cannot immediately conclude.' It also offers suggestions for improvement, indicating a critical but constructive approach.",
            "The review uses strong critical language, such as 'main criticism, and a crucial one,' 'terrible idea,' and pointing out 'unclear separation' and 'poor baselines.' The reviewer directly challenges the authors' claims and expresses doubts about the validity of their conclusions.",
            "The reviewer adopts a supportive tone by validating the use of synthetic data, highlighting the work's significance in the field, and offering constructive suggestions. The reviewer also explicitly states their support by saying 'I want to emphasize my point that synthetically generated tasks are equally valuable'.",
            "The reviewer uses encouraging language such as 'I appreciate this work' and offers constructive criticism ('It will be better if...'). The focus is on the paper's strengths while suggesting improvements, indicating a supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently argues for the necessity of comparing the presented model with word embedding models to better understand the specific contributions and generalization abilities of large language models. The reviewer believes that without such comparisons, it is difficult to isolate the novel aspects of the presented model and to understand whether its capabilities truly go beyond what simpler models like word embeddings can achieve.",
            "The review presents a consistent argument, acknowledging initial misunderstandings and providing constructive feedback based on a clear line of reasoning about the novelty and significance of the work compared to existing methods. The reviewer's perspective evolves as they understand the authors' choices better, but this is a natural progression and does not indicate inconsistency.",
            "The review is consistent in its critique, focusing on the limited conclusiveness of the experiments to demonstrate true reasoning and generalization. The reviewer consistently questions whether the models are learning superficial correlations rather than underlying rules, and provides specific examples and suggestions to improve the experimental design and strengthen the claims.",
            "The review is consistent. The reviewer consistently argues that the experiments do not conclusively demonstrate world knowledge in pre-trained language models, and provides several reasons and alternative interpretations to support this main criticism. The reviewer's arguments are logically connected and do not contradict each other.",
            "The review is consistent as it presents a logical argument, addresses potential concerns about synthetic data, and concludes with a positive overall assessment without any self-contradictions.",
            "The review is consistently positive and constructive. The reviewer praises the paper's contributions and suggests a reasonable improvement without contradicting the positive assessment."
        ]
    },
    {
        "paper_id": "nips_2022_GGBe1uQ_g_8",
        "paper_title": "Effective Decision Boundary Learning for Class Incremental Learning",
        "paper_abstract": "Rehearsal approaches in class incremental learning (CIL) suffer from decision boundary overfitting to new classes, which is caused by two factors: insufficiency of old classes data for knowledge distillation (KD) and imbalanced data between the old and new classes because of the limited storage memory. In this work, we present a simple but effective approach to deal with these two factors to optimize the decision boundary. First, we employ the mixup knowledge distillation (MKD)  and re-sampling strategy to improve the performance of KD, which would greatly alleviate the overfitting problem. Specifically, it utilizes mixup and re-sampling to synthesize adequate data that are more consistent with the latent distribution between the learned and new classes. Second, inspired by the influence balanced (IB) loss used in handling the long-tailed data, we propose a novel incremental influence balanced (IIB) method for CIL to address the classification on imbalanced data, which re-weights samples by their influences to create a proper decision boundary. With these two improvements, we present the effective decision boundary learning (EDBL) algorithm which improves the performance of KD and deals with the imbalanced data classification simultaneously. Experiments show that the proposed EDBL achieves state-of-the-art performances on several CIL benchmarks. ",
        "review_ids": [
            "2oLKQIrjXBl",
            "Qn99cNbne2n",
            "WhReKne3PA",
            "EShDFwUF6vn",
            "uTI-GSFno2C",
            "vnP4PCitaMt5",
            "RCMnBy0MoXZ",
            "O93uLpEtan2",
            "NEBx4JaoBN",
            "a05Kr9nI0MF",
            "uX3krpK_vsJ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thanks for the feedback from the authors. \nIt seems that the proposed IIB training can replace CBF to some extent, while it does not show better performance than CBF for the CNN settings.\nEven though both proposed methods (Re-MKD and IIB loss) may not be super-novel, one can consider IIB training procedure as a possible option to replace CBF. \nI think it is a valuable finding, and therefore I will keep my positive rating for this submission.",
            " Thanks for the feedback from the authors. \n\nI decide to keep my initial rating. The major reasons are my concerns about benchmarks and technical novelty. ",
            " Q1:  I suggest you to discuss the motivation in Sec. 4.3\n\nQ2:  adding Eq 9 and 10 for k = 1 to m:  f^t(x) appears in both Eq 9 and 10, how did you reduce the sum to f^t(x), not 2*f^t(x)?",
            " Thanks for the active feedback from the authors. I am truly grateful for it. \n\nI ask for the results on ImageNet-1k because I hope all papers for CIL can share the same benchmarks and datasets. As I have mentioned in my review, lots of work on CIL, e.g., iCaRl [12], BiC [8], LUCIR [10], Mnemonics [51], and PODNet-CNN [52], has provided results on ImageNet-1k. If you think it is interesting to include the results on some new datasets, it is fine. However, you cannot ignore some popular datasets that have been widely applied. Otherwise, it would be very difficult for the following researchers to compare their work with yours.\n\nBesides, I am not asking for last-minute experiments. I think it would be better to submit the updated version to the next venue, e.g., ICLR.\n\nBased on the overall quality, I cannot strongly support this paper. Thus, my final rating is \"borderline reject\". ",
            " Thanks for the responses. \nHowever, I still believe Table 7 is not a complete experiment to demonstrate the replaceability of the proposed IIB. As far as I know, both iCaRL and SSIL do not utilize CBF (class-balanced fine-tuning) process. For completeness, the authors also need to provide the results of **iCaRL + CBF and SSIL + CBF**. Or, another option is to provide the result of **(EEIL without CBF) + IIB** vs **(EEIL with CBF)**, since the authors used EEIL as the baseline in Table 3 in the main paper.",
            " Thanks for the responses. However, some of my major concerns are still not fully addressed.\n\n- Q2) IIB Factor\n    - Although I appreciate the empirical results, I wonder about the authors\u2019 **interpretation** of the results, what might cause such degradation when $L_{kd}$ is weighted on the proposed IIB weighting.\n\\\n&nbsp;\n\n- Q3) I guess the authors misunderstood what I asked. What I wonder is whether the proposed **balancing training (phase 2) can replace the CBF** process, not the **compatibility of Re-MKD (phase 1) and CBF (or Bias Correction)**. To be specific, in the first phase, train EEIL (or other baselines like LUCIR or POD) as the original one, and then compute IIB Loss in the second phase with the mixed samples. Nevertheless, I also appreciate the newly conducted experiment which provides another aspect of the proposed Re-MKD (phase 1).",
            " Thanks for the feedback from the authors.\n\nHowever, I tend to keep my initial rating, \"reject\". The reasons are as follows,\n\n- My suggestion is to provide the results on ImageNet-Full (1000 classes). Because my existing paper, e.g., iCaRl [12], BiC [8], LUCIR [10], Mnemonics [51], and PODNet-CNN [52] all provide result on ImageNet (full size, 1000 classes). However, the authors provide the results on two other datasets, CIFAR-10 and Tiny-ImageNet. The first one is still a small-scale dataset, and the second one has a very small image size. The most important reason is that the provided results are still not comparable with the existing papers, e.g., [8, 10, 12, 51, 52]. \n\n&nbsp;\n\n- I don't think \"validating the effectiveness of re-sampling and Mixup between old classes and new classes to tackle KD in the CIL setting\" is a significant contribution to a top-tier conference like NeurIPS. \n\n&nbsp;\n\n- For my Q3, the authors' response is that they can do it as a part of future work. I don't think it is an informative response. Maybe I can consider accepting their future work if I have the opportunity to review it. ",
            " In this paper, the authors find that the main problem with the rehearsal methods is decision boundary overfitting to new classes. To solve this problem, they propose to combine the mixup and re-sampling strategy to synthesize adequate data used in knowledge distillation. Extensive experiment results are provided. ### Strengths \n\n- The proposed method is technically sound. \n\n- This paper is well-organized and easy to follow.\n\n- Extensive experimental results and visualizations are provided.\n\n&nbsp;\n\n### Weaknesses\n\n- **The authors only provide results on small-scale datasets, e.g., CIFAR-100.** However, it is quite common to evaluate class-incremental learning on large-scale datasets. For example, iCaRl [12], BiC [8], LUCIR [10], Mnemonics [51], and PODNet-CNN [52] all provide result on ImageNet (full size, 1000 classes). Thus, it is not reasonable to ignore large-scale datasets for a top-tier conference submission. \n\n&nbsp;\n\n- **The technical novelty of this paper is somewhat limited.** This paper is based on re-sampling and mixup. Both strategies have already been widely used in many related topics, such as long-tailed recognition and few-shot learning. Thus, the technical contributions of this paper are somewhat limited. \n\n&nbsp;\n\n- **The state-of-the-art method is compared in Table 1.** For example, [A] archives much better performance than other baselines, but it is not compared in this paper. It seems the proposed method cannot achieve better performance than [A]. \n\n&nbsp;\n\n[A] Yan, Shipeng, Jiangwei Xie, and Xuming He. \"Der: Dynamically expandable representation for class incremental learning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n\n&nbsp;\n\n=== **After rebuttal** ===\n\nThanks for the active responses from the authors. The authors provide some new results in the rebuttal period and address some of my concerns. However, my major concerns about large-scale datasets and the technical novelty still remain. Thus, I tend to keep my initial rating. \n Please address the issues raised in the \"weaknesses\" part. It would be better to add a section to discuss the potential negative societal impact.",
            " This paper addresses the problem of class incremental learning with an emphasis on adjusting the decision boundary. The authors claim that the deficiency of old data and the imbalanced training mainly cause the ineffective decision boundary. The paper proposes Re-MKD, which combines re-sampling strategy and Mixup, and Incremental Influence Balanced (IIB) loss to tackle the aforementioned problem. The experimental results on various benchmarks show the effectiveness of the proposed framework. - Strengths\n    - Overall, the paper is well-written and clear. The authors\u2019 claim that imbalanced training is critical issue for the ineffective decision boundary is well supported throughout the paper. Both proposed methods (Re-MKD and IIB loss) may not be super-novel, but they are reasonable solution for the problem. In particular, the motivation of the proposed solutions are clear. Re-MKD makes good use of the rate rebalancing of the old data and can populate many diverse examples from the old data. IIB also seems suitable for the class incremental learning since class incremental learning can be viewed as long-tail problem, where the old data is extremely scarce.\n\n- Weaknesses\n    - I have an issue with the claim made in the paper, which leave some room for improvement.\n        - The authors repeatedly emphasize that data of newly added classes, which are OOD data from the perspective of the previously learned classes, harm the knowledge distillation process. However, the proposed framework still uses the mixed data of the newly-added data for knowledge distillation. It would be nice if the paper includes the ablation study about the 3 types of mixing classes mentioned in Line 158-159. In other words, I wonder how the overall incremental learning performance will be affected when we exclude the mixed data of the newly added classes from computing $L_{kd}$.\n    - There are some serious formatting issues throughout the paper.\n        - Font size of the captions and equations is significantly smaller than the original submission format. This should be revised.\n    - Minor notation error\n        - The notation $L$ is used multiple times in different contexts. In general, $L$ denotes the loss, but in Line 188, $L$ denotes the feature dimension. Moreover, $f$ in Line 191 also needs to be corrected since $f$ does not indicate the dimension of the feature vector. - As mentioned in the weakness section, I wonder how the overall incremental learning performance would be affected when we exclude the mixed data of the newly added classes when computing $L_{kd}$.\n- Why is the IIB factor only multiplied to $L_{ce}$? Since $L_{kd}$ also can affect the overall decision boundary and IIB factor also considers $L_{kd}$ as in eq (10), IIB weighting factor may be helpful to $L_{kd}$ in eq (16), too. If possible, it would be nice to see how the final performance will be affected if IIB factor is also considered to $L_{kd}$. Or, if I missed something, please let me know.\n- Conventionally, many CIL methods [9, 10, 52] utilize additional \u201cclass balanced fine-tuning (CBF).\u201d It seems that the second phase of EDBL algorithm, which is balanced training, can be a good replacement for CBF, since CBF only utilizes small number of examples unlike the proposed framework. Thus, it would be nice if the ablation study of balanced training is added to the paper. Or, is \u201cRe-MKD\u201d row in Table 3 indicates the Re-MKD + CBF?\n- Figure 3 can be improved if it contains more classes than two classes. Even though it is a minor issue for me, it seems demonstrating only a single case may weaken the claim of the paper. The paper sufficiently deals with the limitation of the paper and broader impacts.",
            " The authors address the problem of imbalance between old and new\nclasses in class-incremental learning due to the limited memory for\nold classes.  For re-sampling, they added 3 types of Mixup instances:\namong old classes, between old and new classes, and among new classes,\nwith proportionally more for the first 2 types to yield balanced\nclasses.  The loss function has cross entropy and knowledge\ndistillation.  They adapted influence balanced (IB) method to include\nboth the classification and knowledge distillation terms from the loss\nfunction, which they call IIB.  They further use hyperparameter alpha\nto weight the IIB factor for knowledge distillation (to decompose\nknowledge distillation and cross entropy).  The loss function L_IIB is\nlambda * L_ce / IIB, where lambda is class-wise reweighting.\n\nThe overall 2-phase EDBL algorithm combines mixing and IIB.  The first\nphase has knowledge distillation.  The second phase has L_IIB, which\nincludes classification and knowledge distillation.\n\nWith 3 datasets, empirical results indicate the the proposed EDBL\ngenerally outperforms a number of existing techniques.  An ablation\nstudy also indicates the contribution of each of the proposed\ncomponents.\n Strengths\n\nThe combination of Resampling with Mixup and IIB seem interesting.\n\nEmpirical results indicate the the proposed EDBL generally outperforms\na number of existing techniques.\n\nWeaknesses\n\nWhile the paper is generally well written, the motivation and\ndescription of the overall 2-phase EDBL algorithm are not clear.  I\nhad to consult Algorithm 1 in the Appendix.  It could be improved by\ndescribing and motivating the 2 phases first, and then the respective\nloss functions that are involved.\n\nPhase 2 can be considered as a superset of Phase 1--the motivation for\nthe need of Phase 1 could be further explained. (See questions below)\n According to Algorithm 1 in the Appendix, the main difference between\nPhase 1 (MKD) and Phase 2 (Balancing training) is whether IIB is\nincorporated or not.  Why not having only Phase 2?  A comparison of\nthe last 2 rows in Table 3 (if I understand correctly) seems to\nindicate the benefit of adding Phase 1 (but seems to be not discussed\nin Section 5.5.1).  Further insights would be important.\n\nEq 11: the first term seems to be missing 2* for f^t(x) and h. If so,\nEq 13 and Eq 15 seem to have similar issues.\n\nLines 223, 234, and 236: balancing training ->  balanced training?\n\nPhases are used in both EDBL and CIL.  Using different terms could be\neasier to separate the two concepts; e.g. stages in EDBL and phases in\nCIL.\n Limitations and negative societal impact were discussed.\n\n",
            " This paper first points out that the scantness of exemplars for the old classes and OOD between the previous classes and the new classes\ncause insufficient knowledge distillation (KD). Then it proposes a two-phase method (EDBL) for class incremental learning. In the first phase, re-sampling strategy and Mixup Knowledge Distillation are used to generate mixed data and to form a better decision boundary.  The second phase uses the incremental influence-balanced (IIB) method to mitigate the imbalanced learning problem.  Experiments show that EDBL outperforms baselines by a large margin. Originality: The idea of generating mixed data by mixup using samples from old classes and new classes has been used in class incremental learning [1].  So that is not a new contribution. The introduction of the Influence Balance method and the decomposition of the Incremental Influence Balance factor are novel. The novelty of this paper is somewhat reduced.\n\n[1] Class-Incremental Learning via Dual Augmentation, NeurIPS2021.\n\nQuality: This paper is technically sound and the claims are supported. However, the experimental setup is flawed. The authors should discuss and compare more batch continual learning methods which use the KD mechanism [2,3,4].\n\n[2] Dark experience for general continual learning: a strong, simple baseline, NeurIPS2020\n\n[3] Co2L: Contrastive Continual Learning, ICCV2021\n\nThe method outperforms many baselines in the setting of CIFAR100 and Tiny-ImageNet. But I don't know if the EDBL method also has the lowest average forgetting rate.\n\nClarity: This paper is well organized. I suggest the author give a simple introduction for task/domain incremental learning in Section 2 and a brief comparison of other approaches like the regularization-based approach.\n\nSignificance: The EDBL method outperforms many baselines in the setting of CIFAR100 and Tiny-ImageNet.  And the idea of calculating the incremental influence-balance weighting factor for each sample and then using the factor to adjust the loss is interesting.  Question 1:  The EDBL method generates more mixed data for the two types of mixup among old classes and mixup between old\nclasses and new classes than the type of mixup among new classes. But the rate ($N/2$) set for the first two types is just an empirical value. Please try to investigate the performance of EDBL at different rates and illustrate the results. Also, what does the \"tail and head class\" mean (Line 162)?\n\nQuestion 2: Can the EDBL method help the KD method to alleviate the forgetting problem further? Please calculate and compare the average forgetting rate of the EDBL method and other baselines. \n\nQuestion 3: The paper says the classification weighting factor is for learning new tasks. But it also calculates the classification weighting factor of the mixed data generated by the samples from old classes. Can the author explain the role of the classification weighting factor in this case?\n\nQuestion 4: That is just a minor question: does the EDBL method also work well in the few-shot continual learning setting? In this setting, the model is over-fitting to the new data severely. And the IIB loss may have a significant impact in this setting.\n\n\n I don't see any potential negative societal impact in this paper"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Neutral",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer states they will keep their positive rating and finds the IIB training procedure a valuable finding.",
            "The reviewer states they will keep their initial rating due to concerns about benchmarks and technical novelty, indicating a negative assessment of the work.",
            "The review consists of factual questions and suggestions for improvement without expressing strong positive or negative opinions.",
            "The reviewer ultimately gives a 'borderline reject' rating, indicating a negative overall assessment of the paper's quality and suitability for publication in its current form. While acknowledging the authors' feedback, the reviewer expresses concerns about the lack of results on a widely used benchmark and suggests submitting the updated version to a different venue.",
            "The reviewer expresses continued disagreement (\"I still believe\") and identifies a perceived incompleteness in the experiment, suggesting further analysis is needed, implying a negative assessment of the current state.",
            "The reviewer expresses that major concerns are 'still not fully addressed' and that the authors 'misunderstood' their question, indicating dissatisfaction.",
            "The reviewer maintains their initial 'reject' rating and expresses dissatisfaction with the authors' responses and the significance of the work.",
            "The review expresses concerns about the lack of large-scale dataset evaluation, limited technical novelty, and comparison with state-of-the-art methods. The reviewer maintains their initial rating despite the authors' rebuttal, indicating persistent reservations.",
            "The review identifies both strengths and weaknesses of the paper, offering constructive criticism and suggestions for improvement without expressing strong positive or negative feelings overall.",
            "The review acknowledges the paper's strengths, such as the interesting combination of resampling with Mixup and IIB, and the empirical results showing outperformance against existing techniques. Despite pointing out weaknesses, the overall tone suggests a positive evaluation of the work's potential.",
            "The review acknowledges the paper's strengths (technical soundness, organization, and performance) but also points out weaknesses (limited originality, flawed experimental setup, and questions regarding hyperparameter selection and forgetting). The reviewer asks for clarifications and further experiments, indicating a neutral stance overall."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Neutral",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude to the authors and explicitly states their support by maintaining a positive rating. Phrases like \"valuable finding\" indicate a supportive tone.",
            "The reviewer expresses 'concerns' about specific aspects (benchmarks and technical novelty), which is indicative of a critical tone. The phrase 'major reasons' also suggests a serious evaluation and negative judgment.",
            "The tone is neutral as it provides specific and direct feedback without using overly positive or negative language. The reviewer uses direct questions and suggestions, maintaining a professional distance.",
            "The review uses phrases such as 'you cannot ignore,' 'it would be very difficult,' and 'I cannot strongly support this paper,' indicating a critical stance. The suggestion to submit to a different venue (ICLR) further reinforces this critical tone.",
            "The reviewer uses phrases like \"authors also need to provide\" indicating a critical tone",
            "The reviewer uses phrases like 'major concerns are still not fully addressed' and 'authors misunderstood what I asked,' which convey a critical stance. Questioning the 'interpretation' of results also contributes to the critical tone.",
            "The reviewer uses phrases like 'I tend to keep my initial rating, \"reject\"', 'I don't think \"validating the effectiveness...\" is a significant contribution', and 'I don't think it is an informative response' which clearly indicate a critical tone.",
            "The reviewer uses phrases like \"somewhat limited\" to describe the technical novelty and questions the reasonableness of ignoring large-scale datasets. The reviewer also points out the lack of comparison with a state-of-the-art method and explicitly states that major concerns remain even after the rebuttal.",
            "The review presents both strengths ('well-written and clear,' 'reasonable solution for the problem,' 'motivation of the proposed solutions are clear') and weaknesses ('I have an issue with the claim,' 'formatting issues,' 'notation error'). It offers specific suggestions for improvement while acknowledging the paper's merits, indicating a balanced and objective assessment.",
            "The review presents both strengths and weaknesses of the paper in a relatively objective manner. It uses phrases like 'seem interesting' to acknowledge positive aspects while also pointing out areas for improvement with specific examples and questions.",
            "The review provides both positive and critical feedback. It praises the paper's organization and performance while also pointing out limitations in originality and experimental setup. The use of questions and suggestions for improvement indicates a balanced approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer acknowledges that the proposed method (IIB training) does not outperform CBF and might not be novel, but still finds it valuable as a possible replacement for CBF and maintains a positive rating. The reviewer's final positive rating aligns with their assessment of the value of IIB training as a replacement for CBF, even if it's not superior in performance.",
            "The review is consistent because the reviewer clearly states they are maintaining their initial rating due to concerns about benchmarks and technical novelty, indicating a consistent stance.",
            "The review is consistent because the two questions are independent and address different aspects of the paper without contradicting each other. Q1 suggests adding motivation in a specific section, while Q2 asks for clarification on equations. Both are valid and non-conflicting points.",
            "The review is consistent in its request for including results on standard benchmarks like ImageNet-1k for better comparability with existing CIL works. While the reviewer mentions not asking for last-minute experiments and suggests submission to a future venue, this can be interpreted as a pragmatic approach acknowledging that adding such results might require more extensive work, rather than a contradiction. The final borderline reject rating aligns with the overall constructive criticism and suggestion for future improvement.",
            "The review is consistent because it raises a specific concern about the completeness of Table 7 and provides clear, logical suggestions for improvement. The reviewer argues that to properly demonstrate the replaceability of IIB, the experiments should consider the impact of Class-Balanced Fine-tuning (CBF) on baseline methods like iCaRL and SSIL, as well as on the authors' chosen baseline EEIL. The reviewer offers alternative experimental setups to address this concern, maintaining a consistent line of reasoning throughout the review.",
            "The review is consistent because the reviewer expresses concerns about specific aspects of the paper (interpretation of results in Q2 and clarification of a misunderstanding in Q3) without contradicting themselves. The reviewer maintains a critical but constructive tone throughout the feedback.",
            "The reviewer clearly states their decision to reject the paper and provides several reasons that consistently support this decision. These reasons include the lack of results on ImageNet-Full for comparability, the perceived insignificance of the contribution for a top-tier conference, and the unsatisfying response from the authors regarding future work. All points raised justify the rejection rating, showing consistency in the review.",
            "The reviewer maintains their initial concerns about the lack of large-scale dataset results and limited novelty even after the rebuttal, and thus keeps their initial rating. This indicates a consistent evaluation based on the same criteria before and after the rebuttal.",
            "The review is consistent because it acknowledges the strengths of the paper, such as clarity and reasonable solutions, while also pointing out specific weaknesses and areas for improvement. The reviewer's concerns and suggestions are logically connected and aimed at enhancing the paper's quality, without contradicting the initial positive remarks about the paper's overall presentation and motivation.",
            "The review is consistent because it highlights both the strengths (interesting combination of methods, good empirical results) and weaknesses (clarity of algorithm description, motivation for phase 1, minor errors) of the paper in a balanced and logical manner. The reviewer's criticisms are constructive and aim to improve the paper without contradicting the positive aspects mentioned.",
            "The review is consistent in its assessment. It acknowledges the paper's strengths, such as technical soundness and performance improvements, while also pointing out weaknesses like reduced novelty and flawed experimental setup. The reviewer raises valid questions and provides constructive suggestions for improvement without contradicting their overall evaluation of the paper's contributions and limitations."
        ]
    },
    {
        "paper_id": "nips_2021_StKuQ0-dltN",
        "paper_title": "Double/Debiased Machine Learning for Dynamic Treatment Effects",
        "paper_abstract": "We consider the estimation of treatment effects in settings when multiple treatments are assigned over time and treatments can have a causal effect on future outcomes. We propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments and apply it to a concrete linear Markovian high-dimensional state space model and to general structural nested mean models. Our method allows the use of arbitrary machine learning methods to control for the high dimensional state, subject to a mean square error guarantee, while still allowing parametric estimation and construction of confidence intervals for the dynamic treatment effect parameters of interest.  Our method is based on a sequential regression peeling process, which we show can be equivalently interpreted as a Neyman orthogonal moment estimator. This allows us to show root-n asymptotic normality of the estimated causal effects.\n",
        "review_ids": [
            "p4-dosz-rv",
            "xARZZhOXXCw",
            "_y7msaeKkpR",
            "Zf9RRo_qNW0",
            "4hOtthPgEf9"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Dear authors,\n\nCould you expand on the relationship of the submission with [9, 8, 39, 12]? The end of section 1 mentions a \"vague\" relationship and more precision would be helpful. I am particularly interested in comparisons with the results in these works that also provide valid confidence intervals, and what are sets of assumptions (if any) would make it possible to compare the results in those works to the theorems of the submission.\n\nBest wishes,\nArea Chair",
            "This paper combines the $g$-estimation approach to identifying dynamic treatment effects in structural nested mean models with orthogonal ML to obtain asymptotically normal estimates of the treatment effects. The main benefits of using orthogonal ML to estimate the treatment effects is it allows high-dimensional state spaces and is statistically efficient (with cross-fitting).  The most interesting part of this paper is the identification strategy (Lem 1 and Thm 2). Although both seem to follow from straightforward manipulations of (1), I suggest the authors elaborate on the identification strategy. In particular, I suggest the authors include the proofs of Lem 1 and Thm 2 in the main paper. This better motivates the \"peeling\" method. In light of the page limit, I suggest the authors defer either the simulation results or one of the extensions (sec 6 or 7) to appendices.\n\nAnother way that the paper can be improved is by demonstrating the applicability of the proposed method on real data. The linear MDP imposes strong restrictions on the data generating process, thereby limiting the applicability of the method. I suggest the authors show that the method works on real data to assuage concerns regarding the proposed method's applicability. The authors discussed limitations of their method. There are not direct potential negative societal impacts of their work.",
            "The authors propose an extension of the double/debiased machine learning framework to estimate the dynamic effects of treatments and apply it to a concrete **linear Markovian high-dimensional state space model** and to general structural nested mean models. The method is based on a sequential regression peeling process, which we show can be equivalently interpreted as a Neyman orthogonal moment estimator. The authors prove both finite sample and asymptotic bounds for the estimation method. The experiments are super-minimal and do not provide convincing evidence about the algorithm's empirical performance.  ## Strengths\n1. Algorithm 1 is rather simple, which increases its potential impact.\n2. The theoretical analyses seem correct.\n3. the idea of having a Z-estimator using the moment conditions to get the desirable theoretical rates is interesting.\n\n## Weaknesses\n1. The experiments are not convincing. The authors should report the uncertainty intervals for the `dyn-direct`   algorithm too.\n2. The results only hold for the MDP in Eq. (1), which is slightly less restrictive than the linear model.\n3. The authors should pick a less generic title. There are other double robust dynamic treatment effect estimation algorithms.\n4. The notation is rather messy. For example, the authors use $d$ for the dimension of the treatment (line 84) and dynamic policy (line 232). \n5. The assumptions are rather spread out during the text, especially in Section 7.\n\n## Further Comments\n1. Please enable line numbers for the Algorithm.\n2. The equation at line 115 needs more explanation. Where did the shocks $\\delta_m$ come from? 1. The algorithm only applies to a certain MDP. In the SNMMs, they have the classical causality assumptions such as *sequential conditional exogeneity condition*. The authors need to acknowledge these limitations and prevent a false sense of over-confidence in their results when these assumptions are violated.\n2. Otherwise, the problem solved in this paper is well-studied and can potentially have a positive societal impact in reducing biases in our estimations.",
            "The paper proposes a DML (double machine learning) based estimation methodology of dynamic treatment effects. Writing the conditional moment restriction on the \"adjusted\" outcome allows the authors to bring the methodology of orthogonal score to dynamic treatment effects. Using this insight, on a partially linear model, the authors propose a progressive (called peeling) algorithm where  the \"adjusted\" outcome can be regressed on the treatment and covariates to estimate the coefficient of the treatment. The authors then show that under slower than root-n estimation rates of the nuisance functions, the theta parameters still enjoy root-n consistent estimates. The authors then discuss using a single sequence of observations to estimate the effect of an intervention (dynamic) on a discounted outcome. The method is then shown to generalize to non-linear sequential models (SNMMs) which a much more general class than partially linear models.  The paper is mostly clearly written despite the inevitable denseness of equations when dealing with effect estimation in sequential data. With the generalization of the method to nested mean models, a lot of my concerns about generality are alleviated but it is unclear how much the knowledge of the representation plays a role in estimation. For example, it seems like while the method does not restrict the class of representation phi(X), it cannot be learned as  part of the process. Could the authors comment on this? The paper solves the propose problem well enough but it must be noted that structural nested mean models do not contain all possible models. But I believe this is not a significant limitation.",
            "Contributions of this paper are the following: \n- Extending Double/Debiased Machine Learning (DML) theories from the static treatment regimes to dynamic treatment regimes for considering a semi-parametric Markovian model with the flexible high-dimensional state, under the partial linear model settings and its generalization to Structural Nested Mean Model (Section 7). \n- In extending, the authors proposed to employ (minimal) parametric assumptions to avoid the ill-posed problem and deal with high-dimensional covariates.  This paper focuses on extending DML theories to the dynamic treatment regimes by employing semi-parametric assumptions to avoid problems that could occur in nonparametric settings. This paper's contributions are significant given that partial linear setting and dynamic treatment regimes have important practical implications. \n\nHere are comments for helping readers understand the results and some questions that readers might be curious about. \n- 1. In Eq. (1), What is 'p' in p(Tt-1, Xt)? I couldn't find its definition. \n- 2. The model in Eq.(1) looks somewhat restricted because it confines the relationship between variables as linear. It would be better if practical motivations or examples that could be covered by Eq. (1) are provided. \n- 3. Sec. 7 look significant because these address my concern in comment 2 (above). It would be helpful if results in Sec. 7 are formalized as Lemmas or Theorems. \n- 4. I'd like to see if practical experiments corroborate with the proposed theories, given that the settings discussed in this paper are simple and highly practical (partial linear model with dynamic treatment regimes)\n- 5. Is it nontrivial to extend results in Thm. 4 for showing doubly robustness (robustness against model misspecification)? \n\nI'd like to suggest highlighting results in Section 7 or even consider reorganizing to show them first. Personally, even if I liked strong theories and highly practical settings of the work at first glance, I was concerned that settings in Sec. 2 looks restricted. But, my concerns were addressed by results in Section 7. Having said that, it'd be more attractive if results in Section 7 come earlier.  Some of limitations are commented in the Main review. \n- I'd like to propose to explain the possibility of practical experiments (or possible scenarios) corroborating with the proposed theories. \n- What are the parametric assumptions in SNMM? Could they be explained more explicitly? \n- Can Thm. 4 be more extended to cover doubly robustness results? "
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review asks for clarification and comparison with existing works without expressing strong positive or negative opinions. The language is polite and constructive.",
            "The review expresses overall positive sentiment by highlighting the paper's interesting identification strategy and the benefits of using orthogonal ML. Suggestions for improvement are framed constructively.",
            "The review acknowledges strengths and weaknesses, and offers constructive criticism. While it points out issues with the experiments and notation, it also praises the algorithm's simplicity and the theoretical analyses. The final comment expresses potential positive societal impact.",
            "The review expresses overall positive feedback, acknowledging the paper's clarity, the alleviation of concerns about generality, and the successful solution to the proposed problem. Phrases like 'mostly clearly written,' 'a lot of my concerns about generality are alleviated,' and 'solves the propose problem well enough' indicate a positive sentiment.",
            "The review acknowledges the significant contributions of the paper, stating that the extension of DML theories to dynamic treatment regimes with semi-parametric assumptions is valuable. The reviewer expresses initial concerns but notes that these are addressed by Section 7, indicating a positive overall assessment. The reviewer also suggests improvements, indicating engagement and a desire to see the work strengthened, which is a constructive and positive approach."
        ],
        "tone": [
            "Formal",
            "Supportive",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses formal language ('Dear authors,' 'more precision would be helpful,' 'I am particularly interested in,' 'Best wishes, Area Chair'). The questions are direct and focused on specific aspects of the research.",
            "The tone is supportive as the reviewer provides constructive feedback and suggestions for improvement, such as elaborating on the identification strategy and demonstrating applicability on real data. Phrases like \"I suggest the authors elaborate\" and \"Another way that the paper can be improved\" indicate a supportive and helpful approach.",
            "The review presents both positive aspects (strengths, interesting idea, potential impact) and negative aspects (weaknesses, experimental issues, notation problems) of the paper. It maintains a professional and objective tone throughout, offering specific suggestions for improvement.",
            "The review balances praise with constructive criticism. It acknowledges the paper's strengths ('mostly clearly written', 'concerns about generality are alleviated') but also raises questions and points out limitations ('it is unclear how much the knowledge of the representation plays a role in estimation', 'structural nested mean models do not contain all possible models'). This balance indicates a balanced tone.",
            "The reviewer uses phrases like \"This paper's contributions are significant\" and \"It would be helpful if results in Sec. 7 are formalized as Lemmas or Theorems.\" The reviewer also frames suggestions as ways to \"help readers understand the results\" and make the paper \"more attractive.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it raises a single, clear point about the relationship of the submission with other works and asks for clarification and comparison. There are no contradictory statements or requests.",
            "The review is consistent because it provides constructive feedback, highlighting both the strengths of the paper (interesting identification strategy, benefits of orthogonal ML) and areas for improvement (elaboration on identification strategy, demonstration on real data). The suggestions are logically connected and aimed at enhancing the paper's clarity and applicability, without any self-contradictory statements.",
            "The review is consistent in its assessment, highlighting the theoretical strengths of the paper while consistently criticizing the experimental validation as weak and pointing out limitations in scope and presentation. There are no contradictory statements or conflicting opinions expressed.",
            "The review is consistent because it provides a balanced assessment, highlighting the paper's strengths and contributions while raising a specific question and pointing out a minor limitation. The overall tone is positive and constructive without any self-contradictions.",
            "The review is consistent. The reviewer initially expresses concerns about the restricted nature of the model in Section 2, but then acknowledges that Section 7 addresses these concerns. The reviewer's suggestions, such as highlighting Section 7 and suggesting practical experiments, are all aimed at improving the paper and are consistent with the overall assessment."
        ]
    },
    {
        "paper_id": "iclr_2021_fkhl7lb3aw",
        "paper_title": "ROGA: Random Over-sampling Based on Genetic Algorithm",
        "paper_abstract": "When using machine learning to solve practical tasks, we often face the problem of class imbalance. Unbalanced classes will cause the model to generate preferences during the learning process, thereby ignoring classes with fewer samples. The oversampling algorithm achieves the purpose of balancing the difference in quantity by generating a minority of samples. The quality of the artificial samples determines the impact of the oversampling algorithm on model training. Therefore, a challenge of the oversampling algorithm is how to find a suitable sample generation space. However, too strong conditional constraints can make the generated samples as non-noise points as possible, but at the same time they also limit the search space of the generated samples, which is not conducive to the discovery of better-quality new samples. Therefore, based on this problem, we propose an oversampling algorithm ROGA based on genetic algorithm. Based on random sampling, new samples are gradually generated and the samples that may become noise are filtered out. ROGA can ensure that the sample generation space is as wide as possible, and it can also reduce the noise samples generated. By verifying on multiple datasets, ROGA can achieve a good result.",
        "review_ids": [
            "DIqx0CBv6u",
            "zsu2zpJ22ub",
            "BX_fFQenzjj",
            "5lAOmgLxN6n"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors tackle the problem of class imbalance in supervised learning. They propose an oversampling algorithm that is based on a genetic algorithm. Samples are iteratively generated and filtered to maintain informative samples. The authors demonstrate the efficacy of the approach using several examples.\n\nAlthough the problem is important and the solution might be interesting, I feel that the paper is not scientifically sound and the English level is not satisfactory. Below I will detail some of the problems with the current version of the manuscript:\n\n-First paragraph: \u201conly can estimated\u201d - this is is not in English.\n\n-First paragraph: \u201cEnsure that the sample does not make\u2026\u201d - again this sentence does not make sense.\n\n-Authors did not use the correct citation format, please see instructions by ICLR.\n\n-P2: SMOTEChawla - here the citation is mixed with the method name.\n\n-P3: Holland - the year is mentioned twice.\n\n-2.3 has the title \u201cGenetic Algorithm\u201d and 3.1 has the title \u201cGA\u201d, aren\u2019t these the same???\n\n-3.1 the paper does not generate anything, perhaps you meant the method?\n\n-Equation (2)  x_ik is not defined, what are the two indexes for? Sample and feature? Need to define.\n\n-Equation (2) you take the square of the square root, these cancel each other!\n\n-The authors mostly cite SMOTE based method for class imbalance, but there are other methods such as:\n\n[1] Lindenbaum, Ofir, Jay Stanley, Guy Wolf, and Smita Krishnaswamy. \"Geometry based data generation.\" In Advances in Neural Information Processing Systems, pp. 1400-1411. 2018.\n\n-Perhaps you could use bar plots for figure 1 instead of the scatter plots which are not that informative.\n\nThere are many other mistakes, so I feel that at this stage the paper can not be published at this venue.\n",
            "The authors focus on the class imbalance problem and propose an algorithm named ROGA to generated samples of minority classes to balance the quantitative difference between classes. Specifically, the proposed ROGA generates samples of minority classes by using a Genetic Algorithm (GA) to explore the sample space. Moreover, to reduce the noise samples generated by ROGA, the authors propose to calculate the fitness as the Gaussian similarity with the surrounding samples and eliminate the samples with low fitness. My detailed comments are as follows.\n\nPositive points:\n1. The authors propose an algorithm named ROGA to generate samples of minority classes to balance the quantitative difference between classes.\n\n2. To reduce the noise samples, the authors propose to calculate the fitness as the Gaussian similarity with the surrounding samples and eliminate the samples with low fitness. \n\n3. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method.\n\nNegative points:\n1. The authors argue that the generation space of the proposed method is widened mainly due to the initial random sampling over the entire sample space. However, during the optimization process of GA, the sample space may be rapidly narrowed down, which makes the generation space much smaller. How to demonstrate that ROGA can ensure the sample generation space is as wide as possible during the optimization? More description and experimental analysis are required.\n\n2. The experiments are insufficient in Section 4.3. In Table 3, the compared methods are SMOTE [1] and its extensions, which are not up-to-date. It would be better to compare ROGA with more closely related and up-to-date methods such as OHEM [2], Focal Loss [3], and SMOTE-WENN[4].\n\n3. It is not clear why the samples with low fitness are selected to perform the crossover and mutation operation. Does the algorithm converge effectively under this setting?\n\n4. The reason behind the choice of the mutation operation is not clear. Why the feature value can only be positively scaled?\n\n5. Lots of experimental details such as hyper-parameters $p$, $\\beta$, training iterations, and the probability of crossover/mutation are missing, which makes the paper hard to be reproduced.\n\n6. In Table 3, why the performance of the proposed method on the wine_quality data set is much poorer than others. More explanations are required.\n\n7. The ablation study is insufficient. $K$ plays an important role in sample generation. More results on different values of $K$ are required.\n\nMinor issues:\n1. In Section 2.2, \u201cRACOG and WRACOG (Das et al. (2015)) used the dependency tree algorithm \u2026\u201d should be \u201cRACOG and WRACOG (Das et al. (2015)) use the dependency tree algorithm \u2026\u201d.\n\n2. In the Introduction, \u201coversampling methods only can estimated \u2026\u201d should be \u201coversampling methods only can estimate \u2026\u201d.\n\n3. In Section 4.4, \u201ctraditional oversampling paradigm that try to \u2026\u201d should be \u201ctraditional oversampling paradigm that tries to \u2026\u201d.\n\nReferences:\n\n[1] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16(1):321\u2013357, 2002.\n\n[2] Shrivastava, Abhinav & Mulam, Harikrishna & Girshick, Ross. (2016). Training Region-Based Object Detectors with Online Hard Example Mining. 761-769. 10.1109/CVPR.2016.89. \n\n[3] Lin, Tsung-Yi & Goyal, Priyal & Girshick, Ross & He, Kaiming & Dollar, Piotr. (2018). Focal Loss for Dense Object Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence. PP. 1-1. 10.1109/TPAMI.2018.2858826. \n\n[4] Guan H, Zhang Y, Xian M, et al. SMOTE-WENN: Solving class imbalance and small sample problems by oversampling and distance scaling[J]. Applied Intelligence, 2020: 1-16.\n",
            "In this work, the authors propose an oversampling technique which creates a population of synthetic samples for an imbalanced dataset using genetic algorithms. Each individual in the GA population corresponds to a synthetic sample, and the fitness function is based on the similarity (in feature space) of the synthetic sample compared to nearby samples of the same and different classes; standard crossover and mutation operators are used. In a limited set of experiments, the proposed approach sometimes outperforms competing approaches.\n\nI think the basic idea of using a GA to avoid local minima in the feature space is reasonable, and the proposed fitness function makes sense, though it is perhaps a bit obvious. However, I believe the paper has several key limitations that need to be addressed.\n\nThe discussion on related work omits a large number of highly related references. In particular, a large number of existing papers propose to use genetic algorithms for oversampling, including: [Saladi and Dash, Soft Computing for Problem Solving, 2019], [Tallo and Musdholifah, ICST, 2018], [Ha and Lee, IMCOM 2016], and [Qiong et al., JDIM 2016]. While the proposed approach likely differs from these in some respects, the complete lack of context with respect to existing GA approaches for addressing class imbalance makes it difficult to judge the methodological novelty.\n\nSecond, the set of baselines considered in the work is rather limited. Thus, it is not clear if the paper meaningfully improves empirical performance compared to state of the art. In addition to lacking any other GA baselines, the experiments also lack a representative generative adversarial network-based approach. Considering the prevalence of such approaches, at least a standard implementation should be included in the comparison.\n\nThird, the experimental design itself could be significantly improved. The current design does not allow for any estimate of variance in performance of the methods on each dataset. Thus, it is not possible to tell if any of the results are significant. Indeed, additional experiments suggest that the proposed approach is not stable, so that further calls into question the empirical advantage of this approach. A simple way to estimate such variances could be to perform cross-validation by bootstrapping the dataset (e.g., k-fold cross-validation). It is also not clear how the hyperparameters for the other baseline methods are selected.\n\nIt is also not clear how the approach may work for datasets with non-numeric features, such as text or images. Presumably, some standard embedding method (word2vec, CNNs, etc.) could be used to first embed the data to a vector space, but it is then not obvious to me how well the proposed GA approach would be in exploring that space.\n\nMinor comments\n---------------------\n\nThe paper has numerous small grammatical errors. I do not believe these affect understanding the paper, but it requires another round of proofreading.\n\nThere is some issue with the formatting of the references in the text; they are all followed by two closing parentheses.\n\nFor the crossover operation, is \\alpha drawn uniformly from [0,1], or from a truncated Gaussian with a mean around 0.5? or something else?\n\nIn Algorithm 1, \\alpha is used for the Gaussian kernel variance, while \\sigma is used in the text.\n\nFor Figure 1, something like a box plot or violin plot is probably more appropriate for presenting that data.\n\nThe references are not consistently formatted.\n",
            "The paper introduces ROGA - an oversampling method that uses a genetic algorithm to generate synthetic samples for the minority class. Results in classification scenarios with imbalance dataset demonstrate the effectiveness of the proposed method.\n\nA lot of grammatical errors mar the readability of the manuscript. Resolving them through proofreading and copy-editing would improve the clarity of the paper. \n\nThe related work section is extensive and provides a good overview of related literature on methods that address class imbalances. However, the whole section reads like a laundry list of previous methods instead of painting a cohesive story about what has been done, what gaps remain and most importantly how the proposed method (ROGA) can address these gaps. It is difficult to place where ROGA fits in amongst previous literature.\n\nSection 4.2 (definitions for accuracy, precision, recall, F1, TP, FP) is background and does not belong in the method section. I would suggest that these could be removed entirely from the paper as this are basic machine learning terms that are well known and understood.  \n\nThe presented results seem to be the max score achieved across 20 independent experiments. Is this correct? If so, this will have high bias. Why not report the first order statistics based on the 20 experiments? For example, the mean score achieved, and standard deviation would enable the reader to have a much more balanced view of the method\u2019s merits and limitation.\nFurther, it seems this max selection was not used for the other baselines in the comparative results. This would raise significant questions regarding the validity of the comparative results.\n\nWhat is the added computational cost of ROGA versus a traditional oversampling method that does not use a population? Since the fitness uses K-nearest neighbor Gaussian similarity, the computation overhead can be substantial depending on how many iterations must be run. This should be reported as it is an important consideration in large scale deployments. \n\nThe novelty in the method is marginal. The principal contribution of the method is an attempt to leverage the diversity benefits of a population-based method (GA in this case) towards generating higher quality synthetic samples for oversampling. However, the results as they stand do not fully corroborate that this benefit translates effectively to improved performance. \n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the paper is \"not scientifically sound\" and the \"English level is not satisfactory.\" They also mention \"many other mistakes\" and conclude that the paper \"can not be published at this venue.\"",
            "While acknowledging positive aspects, the review focuses heavily on shortcomings: insufficient experimental validation, unclear methodological choices, missing details, and unexplained results. The sheer number of negative points outweighs the positives, indicating a generally critical assessment.",
            "The review identifies several key limitations including a lack of related work discussion, limited baselines, and a flawed experimental design. The reviewer expresses concerns about the novelty and empirical advantage of the proposed approach, suggesting the results may not be significant.",
            "The review contains several criticisms, including grammatical errors, a poorly structured related work section, misplaced definitions, concerns about result reporting bias, questions about computational cost, and doubts about the method's novelty and effectiveness."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"not scientifically sound,\" \"not satisfactory,\" and \"many other mistakes.\" The reviewer also points out specific errors in grammar, citation, and methodology, indicating a critical assessment of the paper's quality.",
            "The reviewer uses direct and questioning language, pointing out flaws and requesting justifications. Phrases like \"The experiments are insufficient\", \"It is not clear why\", \"Lots of experimental details are missing\", and \"More explanations are required\" demonstrate a critical stance.",
            "The review uses phrases like 'key limitations,' 'complete lack of context,' 'not clear if the paper meaningfully improves,' 'experimental design itself could be significantly improved,' and 'further calls into question the empirical advantage.' These phrases convey a critical assessment of the work.",
            "The review uses phrases like \"grammatical errors mar the readability,\" \"reads like a laundry list,\" \"does not belong,\" \"high bias,\" \"significant questions regarding the validity,\" \"computational overhead can be substantial,\" and \"novelty in the method is marginal,\" indicating a critical evaluation of the paper."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently negative and provides specific examples of issues related to English, citation format, clarity, and technical correctness to support its negative assessment.",
            "The review is consistent because it acknowledges the paper's contribution while providing specific and well-reasoned criticisms regarding the methodology, experiments, and clarity. The positive and negative points are logically separated and contribute to a balanced and constructive assessment of the paper.",
            "The review is consistent in its critique, pointing out several limitations and areas for improvement throughout the text. It starts with acknowledging a reasonable idea but immediately transitions to discussing weaknesses related to related work, baselines, experimental design, and generalizability. The minor comments further reinforce the need for improvement in various aspects of the paper.",
            "The review consistently points out weaknesses and areas for improvement in the manuscript, without contradicting itself. All points raised are critical of different aspects of the paper, such as writing quality, related work presentation, methodology, results reporting, computational cost analysis, and novelty. The reviewer acknowledges the potential idea but questions its effective implementation and validation in the current manuscript."
        ]
    },
    {
        "paper_id": "nips_2022_gvwDosudtyA",
        "paper_title": "Optimistic Posterior Sampling for Reinforcement Learning with Few Samples and Tight Guarantees",
        "paper_abstract": "We consider reinforcement learning in an environment modeled by an episodic, tabular, step-dependent Markov decision process of horizon $H$ with $S$ states, and $A$ actions.  The performance of an agent is measured by the regret after interacting with the environment for $T$ episodes. We propose an optimistic posterior sampling algorithm for reinforcement learning (OPSRL), a simple variant of posterior sampling that only needs a number of posterior samples logarithmic in $H$, $S$, $A$, and $T$ per state-action pair. For OPSRL we guarantee a high-probability regret bound of order at most $O(\\sqrt{H^3SAT})$ ignoring $\\text{poly}\\log(HSAT)$ terms. The key novel technical ingredient is a new sharp anti-concentration inequality for linear forms of a Dirichlet random vector which may be of independent interest. Specifically, we extend the normal approximation-based lower bound for Beta distributions by Alfers and Dinges (1984) to Dirichlet distributions. Our bound matches the lower bound of order $\\Omega(\\sqrt{H^3SAT})$, thereby answering the open problems raised by Agrawal and Jia (2017) for the episodic setting. ",
        "review_ids": [
            "o0CNUsLYn1M",
            "MJKBBa88ap",
            "jGM3iHLWgT8n",
            "cSSDAKQr6JaX",
            "HNK9O9dcqM",
            "K74tTn9A4Qk",
            "kBwaUkl0Y50"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the response. The preliminary experiments are very interesting, and from the experiments it seems like OPSRL with a constant number of samples is performing well. It would be great to extend the current analysis to that direction.",
            " Thanks for the response. I raise the score as the authors provide additional experiments. However, I'm still not convinced that the proposed method can be potentially helpful in the more complicated scenarios, and I hope the authors can continuously work on that.\n\nOne minor technical question: To the best of my knowledge, the anti-concentration results in general still focus on certain lower moment conditions (e.g. Gaussian anti-concentration). I'm not sure why the authors claim that the proposed method is more similar to the KL-UCB bonus. Is this a serious claim or is it just some high-level intuition?",
            " Thanks for your reply! After reading other reviews and replies, I still think this paper is an interesting contribution to the community and would like to keep my score. ",
            " Please acknowledge that you have read the rebuttal.",
            " This paper proposed an optimistic variant of the PSRL algorithm and proved a minimax regret bound, which settles an open problem raised by Agrawal and Jia [2017b]. The main technical contribution is a new sharp anti-concentration inequality for Dirichlet random vector, which may be useful for other settings.   Strengths:\n- This paper proved minimax regret bound for an optimistic variant of the widely used PSRL algorithm, which settles an open problem. \n- The algorithm is simple and clean. \n- The theoretical novelty in developing a tight anti-concentration bound. \n\nWeaknesses:\n- OPSRL is very similar to the algorithm analyzed in Agrawal and Jia [2017b] and the main difference is only in applying the new anti-concentration bound, which simplifies some technical ingredients in Agrawal and Jia [2017b]. \n- OPSRL is still far from the standard PSRL that people use/want to study as multiple posterior sampling is performed at each episode to achieve optimism, which makes the algorithm more similar to UCB algorithm. \n- If the authors compare the anti-concentration used in Agrawal and Jia [2017b] with their approach in more detail, it would be easier to understand why the extra S factor is removed. \n\nMinors:\nThe paper (Randomized Exploration in Reinforcement Learning with General Value Function Approximation) might be relevant as it considers optimistic RLSVI.   - Can authors elaborate more on how OPSRL adapts automatically to the variance of the estimates so that the optimal dependence on H can be achieved? I am not aware of any potential negative societal impact. ",
            " The paper proposed an optimistic posterior sampling algorithm for episodic MDPs and proved that the algorithm has a regret bound matching the lower bound up to poly-log terms.  Strengths:\n- The proposed OPSRL algorithm combines ideas from optimism and posterior sampling. Though there are existing algorithms with a similar algorithmic structure (Fonteneau, Korda, Munos, An optimistic posterior sampling strategy for bayesian reinforcement learning, NIPS 2013 Workshop on Bayesian Optimization), OPSRL is the first one with lower bound matching regret guarantee.\n\n- New technical properties on the Gaussian lower bound for Dirichlet distribution is critical in establishing that the sampled Q-function is optimistic, and the exponential upper bound is used in the concentration results. They are useful in proving regret bounds for the episodic MDP setting, and they might be useful for posterior sampling algorithms in other settings as well.\n\n\nWeaknesses:\n- No numerical experience verifying the theoretical results. As discussed in the paper, the current proof only works when L posterior samples are drawn and L has some logarithmic dependence on H,S,A,T. Given empirical success of posterior sampling with one sample or a fixed number of samples, it would be very interesting if one can have some theoretical guarantees for these settings. From the proof, the size of L seems to come from the union bound on ensuring optimism for all state at all times. Are there some ideas on how the analysis could be extend to settings when L is a constant? The problem setting and limitations are discussed and addressed in the paper.",
            " This paper proposes a novel optimistic posterior sampling algorithm, showing that it can achieve nearly mini-max optimal regret bound with a novel anti-concentration inequality of Dirichlet distribution.\n ### Strengths:\n* A novel anti-concentration inequality for Dirichlet random vector, which may be of independent interest.\n\n### Weaknesses:\n* The application scenario is too limited, as it crucially depends on the Dirichlet distribution. Some artifacts like the pseudo-state $s_0$ may exacerbate this issue.\n* No empirical demonstrations of the newly proposed algorithm. The authors claim the benefit of posterior sampling algorithms multiple times (e.g. Line 216-225). Although there are several empirical demonstrations that show that some versions of the optimistic posterior sampling algorithms work better than the empirical counterpart, it\u2019s not clear if such variants can still perform well. Given that this paper is all about tabular settings, I prefer having such an evaluation.\n* For me it is much more like an applied math paper, as it does not really solve important questions in reinforcement learning. It mainly show one other potential optimism mechanisms with the anti-concentration of Dirichlet random vector.\n * I\u2019m not convinced that this method can be better than the frequentist method, as it implicitly mimics the Bernstein type bonus with a sampling method. Can the authors describe what are the potential benefits of the proposed methods over traditional methods like UCBVI? (I admit that UCBVI focuses on the worst-case guarantee and sampling based methods can be better in constant, but I don't think this can lead to e.g. instance-wise benefits.)\n\n* In Line 182-184, is the proposed algorithm applied to general prior other than Dirichlet, as there is only one anti-concentration result for Dirichlet distribution? If not, I think there is no need for this claim.\n\n* Can the proposed methods be generalized to the case with function approximation? It is much more favorable to provide some insights that can be generalized to more general scenarios (e.g. [1]) rather than sticking to the specific tabular setting and Dirichlet distribution.\n\n[1] Zhang, Tong. \"Feel-good thompson sampling for contextual bandits and reinforcement learning.\" SIAM Journal on Mathematics of Data Science 4.2 (2022): 834-857. This is a theoretical paper that does not need to address the potential societal impact."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses positive feedback by stating \"The preliminary experiments are very interesting\" and suggesting a valuable extension of the analysis.",
            "The reviewer acknowledges the authors' efforts by raising the score due to additional experiments but expresses remaining concerns about the method's applicability to more complex scenarios. The reviewer also poses a technical question, indicating a critical yet neutral stance.",
            "The reviewer explicitly states that the paper is an \"interesting contribution to the community\" and that they \"would like to keep my score.\"",
            "The review simply asks for acknowledgement of reading the rebuttal, without expressing any positive or negative feelings.",
            "The review highlights several strengths of the paper, including settling an open problem, the simplicity of the algorithm, and the theoretical novelty. While weaknesses are mentioned, the overall assessment seems favorable.",
            "The review highlights the paper's strengths, such as the novel algorithm with lower bound matching regret guarantee and the useful technical properties. While weaknesses are mentioned, they are presented as areas for future improvement rather than fundamental flaws.",
            "The review expresses several weaknesses of the paper, including limited applicability, lack of empirical demonstrations, and questions about its practical benefits compared to existing methods. The reviewer also raises concerns about the algorithm's generality and potential for extension to more complex scenarios."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Neutral",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The tone is supportive, indicated by the use of phrases like \"Thank you\" and \"It would be great to extend...\", suggesting encouragement and collaboration rather than criticism.",
            "The review starts with 'Thanks for the response' indicating politeness, then 'I raise the score' showing appreciation for improvements. However, the reviewer also expresses doubt ('I'm still not convinced') and asks a critical technical question, indicating a balanced perspective.",
            "The reviewer expresses gratitude (\"Thanks for your reply!\") and reinforces their positive assessment of the paper, indicating a supportive stance.",
            "The tone is neutral because it is a polite and direct request, lacking any emotional or subjective language.",
            "The review presents both strengths and weaknesses of the paper in a clear and objective manner. It uses neutral language while providing constructive criticism and suggestions for improvement.",
            "The review presents both strengths and weaknesses of the paper. It uses objective language and provides specific examples to support its claims. The reviewer acknowledges the limitations discussed in the paper, indicating a balanced perspective.",
            "The review uses phrases like \"too limited,\" \"no empirical demonstrations,\" \"not convinced,\" and \"I think there is no need for this claim.\" These phrases indicate a critical assessment of the paper's contributions and limitations."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses positive feedback about the preliminary experiments and suggests a logical next step based on the observed positive performance of OPSRL. There are no contradictory statements or conflicting opinions within the review.",
            "The reviewer acknowledges the authors' improvements by raising the score due to additional experiments. While still having reservations about the method's applicability in more complex scenarios and raising a technical question, these points do not contradict the positive acknowledgement and score increase. The review is consistent in its nuanced assessment, recognizing progress while pointing out areas for further consideration.",
            "The review expresses a consistent positive opinion about the paper. The reviewer states they still believe the paper is an interesting contribution and maintains their score after considering other reviews and replies. There is no contradiction or change in opinion expressed.",
            "The review is consistent as it contains a single, clear request without any contradictory statements.",
            "The review is consistent because the strengths and weaknesses are logically related and do not contradict each other. The reviewer acknowledges the paper's contributions (solving an open problem, theoretical novelty) while also pointing out limitations and areas for improvement (similarity to existing work, deviation from standard PSRL). The critique is constructive and balanced, leading to a consistent overall assessment.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper without any self-contradiction. The reviewer praises the theoretical contributions and novelty of the algorithm while pointing out the lack of empirical validation and limitations of the theoretical setting regarding the number of posterior samples. These are distinct points that do not contradict each other, making the review internally consistent.",
            "The reviewer consistently points out the limitations of the paper regarding its narrow scope (dependence on Dirichlet distribution and tabular settings), lack of empirical validation, and questionable practical relevance to the field of Reinforcement Learning, despite acknowledging a theoretical contribution. The weaknesses all center around the limited applicability and lack of empirical support for the proposed method."
        ]
    },
    {
        "paper_id": "nips_2022_319xcX5qIcO",
        "paper_title": "Signal Recovery with Non-Expansive Generative Network Priors",
        "paper_abstract": "We study compressive sensing with a deep generative network prior. Initial theoretical guarantees for efficient recovery from compressed linear measurements have been developed for signals in the range of a ReLU network with Gaussian weights and logarithmic expansivity: that is when each layer is larger than the previous one by a logarithmic factor. It was later shown that constant expansivity is sufficient for recovery. It has remained open whether the expansivity can be relaxed, allowing for networks with contractive layers (as often the case of real generators). In this work we answer this question, proving that a signal in the range of a Gaussian generative network can be recovered from few linear measurements provided that the width of the layers is proportional to the input layer size (up to log factors). This condition allows the generative network to have contractive layers. Our result is based on showing that Gaussian matrices satisfy a matrix concentration inequality which we term Range Restricted Weight Distribution Condition (R2WDC) and which weakens the Weight Distribution Condition (WDC) upon which previous theoretical guarantees were based. The WDC has also been used to analyze other signal recovery problems with generative network priors. By replacing the WDC with the R2WDC, we are able to extend previous results for signal recovery with expansive generative network priors to non-expansive ones. We discuss these extensions for phase retrieval, denoising, and spiked matrix recovery.",
        "review_ids": [
            "8FQWEkpx6k7",
            "3lE50hOp7FU",
            "IDS8RczrKjO",
            "i_kGqb0ES5U",
            "vrkbdJl9W1M"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I would like to thank the authors for their comments ans answers!. \n\n* Regarding the argument around information-theoretic optimality: the provided argument works for linear models (with oracle known subspace and solving linear systems etc.). I am not sure if this holds for nonlinear networks and the limit can be worse then (this can be good for the paper: say hypothetically, the IT bound would  depend on $k$ and be logarithmic in $n_i$- in this case, the obtained bound of the paper could be actually claimed to have IT optimal dependencies there). I think the argument for information theoretic limits should way mathematically more rigorous. As far as I can see, I cannot find a mathematical proof of information-theoretic limits. Of course, I would be more than happy if the author could point me to this argument.  \n* I still prefer that the authors to discuss the intuition behind $Q_{r,s}$  in the main paper. Let me ask a question more directly: what is the intuition behind $Q_{r,s}$? Can you please comment here? Note that this can probably help coming up with new practices say adding a regularization term during training. \n\nOf course the paper relaxes some of the existing issues of the current theory (expansiveness), it still has many limitations and undesirable dependencies as mentioned by the other reviewers. This is of course natural for theoretical developments, and I would still favor the acceptance, if the author can clarify further what the current theory, despite its limitation, can tell us about the practice of generative priors.  ",
            " The paper extends the previous theoretical studies around convergence of subgradient descent (Algorithm 1 from [20]) for ReLU generative priors to cases where the generative model is not necessarily expansive. The result relies on the weights being drawn from i.i.d. Gaussian matrices and assumes network widths and measurement numbers are proportional to input dimension (See Theorem 1.1, conditions 1 and 2). The results extend to other inverse problems like phase retrieval, denoising and spiked matrix recovery. \nTheorem 5.4 provides the result for random Gaussian matrices and weights, and Theorem 4.4 provides a more general result for any matrices satisfying RRIC and R2WC. Lemma 5.1 is well known in the literature. Lemma 5.2 and 5.3 are proven in the appendix and together with Theorem 4.4 imply Theorem 5.4.\nThe extensions to phase retrieval, denoising and spiked matrix recovery are given in the supplementary materials, although the proofs are not explicitly given. \n **Strength**\n\n* The paper improves the analysis of [22] (also [21]) and gets much better dependence on the number of layers $d$ and removes the dependence of width of layer $i$ on the layer index $i$. \n* R2WDC is weaker that WDC condition, and nonetheless provides better results. \n* The proof builds on many previous works, for example [18], [20], [22], and, based on my rapid read, is sound and well presented.\n* The contribution of the paper, namely relaxing the constraints on the network further and introducing R2WDC, is a good step forward in this analysis.\n\n**Weakness**\n\n* Some important limitations of the paper are not mentioned clearly by the authors, and some of the statements are not fully precise (see some comments below \u2013 for example, it seems to me that the sample complexity has drastic dependency on $d$; also on information theoretic optimality.).\n* The authors do not extract relevant guidelines from their theory. Practical generative models are trained from data. The paper considers generative models with random Gaussian weight, which is fine to derive the theoretical analysis. Even if the results are not directly applicable to practical networks, it is important to extract theoretical insights from the developed theory (like the authors\u2019 comment on denoising effect of Algorithm 1 and generative priors). This is missing from the paper. See also my comments on gradient descent issues for trained generative priors.\n* Having numerical results, for instance similar to those in [20], can always help communicating the paper\u2019s contribution better.\n* The paper could have been organized better by removing some discussions to the supplementary materials and presenting first the core idea behind the proof (similar to [20]).\n \n* [21] seems to be the extended NeurIPS version of workshop paper [22]. It is probably better to use this version instead of [22]. \n* I suggestion expanding on the denoising effect of the algorithm 1 as mentioned in page 3, line 89. This is an interesting consequence. \n* A relevant question, from practical perspective, is to see if one can verify R2WDC for a pre-trained network (not a random one). It is known that verifying RIP property of a matrix is NP-hard. It would be interesting if the authors can comment on it. \n* Gradient descent-based methods used in context of trained generative priors suffer from lack of convergence and usually require occasional restarts. This is in contrast with the current claims of optimality in the paper. This merits some comments from the paper. Which assumption in the theoretical framework is likely to be violated for this to happen? Gaussian assumption is a good candidate, but it is just an instance of distributions satisfying R2WDC. Is it related to details of Algorithm 1, for instance the check $f(-x_t)<f(x_t)$? Some comments on this can be helpful for the paper.\n* In page 2, the authors mention that the method of [20] is information theoretically optimal given $m=\\tilde{\\Omega}(k)$. As far as I can see [20] does not claim information theoretic optimality. Why is this the case? Is there any work providing lower bound on the sample complexity for random generative models (for example like Gelfand width analysis in compressed sensing)? A similar claim is made in the final part of the paper.  \n* Please add a few sentences to the paper on how the subgradient can be computed in practice. Of course, for smooth generative models, this is not an issue, but for ReLU networks, the question is whether the gradient descent as implemented in backprop is a good proxy.\n* I suggest rephrasing the mention of Rademacher\u2019s theorem in line 149, page 4. The implication of Rademacher\u2019s theorem, roughly, is that Clarke\u2019s subdifferential is well behaved since the $\\text{dom}(\\nabla f)$ has full measure.  With current phrasing, this point is not clear.\n* I suggest adding the intuition behind the matrix $Q_{r,s}$ to the main paper (for example by mentioning the connection with measuring angle distortion by $x\\to W_{+,x}$).\n* The $\\epsilon$ in Theorem 4.4 is $O(1/d^{90})$. This term would dominate the rest of terms in the sample complexity condition (11) (see Assumption A.3). Basically, $m\\geq \\hat{C}_\\epsilon$ implies at least $m\\geq d^{90}$. This is a poor dependency on $d$, although it is present in the related works too. \n* In Remark 1 and 2, it is suggested that the factors $2^d$ can be removed with the entries of $W_i$ drawn from $\\mathcal{N}(0,2/n_i)$. Would not this violate the requirements for R2WDC condition?\n* Apart from $\\hat{C}_\\epsilon$, the condition A.3 of Assumptions A has implicit linear dependence on $d$. Consider a non-expansive network with $n_i=n$ for all $i$. Then the sample complexity lower bound includes $dm$. Similar arguments can be made of assumption B.2 and for $n_i$, namely $n_i\\geq k.i \\log (ne/k)$. The authors should comment on this.\n* Typo: in two places in the paper, RRWDC is used instead of R2WDC. \n Although the paper is theoretically interesting and a step forward, I feel that the authors can do a better job in communicating the idea, extracting useful guidelines and presenting the limitations.",
            " This paper relaxes an assumption in previous theoretical work on signal recovery with generative networks. Namely, previous work required that the generative network have constant-width layers (and before that, logarithmically expanding layers). This work shows that the generative model may preserve the signal recovery guarantees, while having layers of contracting width. Strengths:\n- Solid theoretical result on a relaxed assumption for generative model-based signal recovery.\n\nWeaknesses:\n- Motivation is lacking. The paper asserts that relaxing the assumption in question is important, as contractive layers are \"often the case in real generators\". However, this is stated without citation, and I am not sure it is true. Most GANs and VAEs involve constant or growing width of layers in the generative network. The only provided support for relaxing this assumption is a citation of [1]. However, I could not find this statement anywhere in the cited work. The authors did state that relaxing the logarithmically expansive width assumption was important, but that has apparently already been shown by [2, 3].\n\nOverall, it is unclear to me how important the theoretical improvement is, as this is not my main area of expertise, and the paper does not properly motivate the contribution. However, I believe that this work provides a straightforward contribution to theory in this field.\n\n[1] Hand, P., & Voroninski, V. (2018, July). Global guarantees for enforcing deep generative priors by empirical risk. In Conference On Learning Theory (pp. 970-978). PMLR.\n[2] Daskalakis, C., Rohatgi, D., & Zampetakis, E. (2020). Constant-expansion suffices for compressed sensing with generative priors. Advances in Neural Information Processing Systems, 33, 13917-13926.\n[3] Joshi, B., Li, X., Plan, Y., & Yilmaz, O. (2021, October). PLUGIn-CS: A simple algorithm for compressive sensing with generative prior. In NeurIPS 2021 Workshop on Deep Learning and Inverse Problems. Please answer the motivation question above. No.",
            " This paper presents a theoretical analysis for signal recovery with non-expansive generative networks. The main results suggest that given a random Gaussian generator, any signal in its range can be reconstructed from Gaussian measurements as long as the number of measurements and the width of all layers are proportional to the size of input layer. This result improves upon the earlier analyses that require width of layers to expand.  Strengths. \n- The paper presents a new analysis for the general signal recovery using generative priors. \n- The main contribution is in relaxing the conditions on the width of the network layers. \n\nWeaknesses. \n- This analysis makes a strong assumption that the network has Gaussian weights. This is quite far from real settings where the network is learned from some data. Authors acknowledge that as a limitation. It will be good for the community to start analyzing this real problem. \n\n\n I did not check the derivations for accuracy, so I do not have any specific question at this point.  n/a",
            " This paper considers the problem of inverse problems with generative priors. Prior work has shown that gradient descent recovers the ground truth in poly-time under the assumption of generative models with random weights and sufficient expansion. As real world networks do not satisfy the expansivity assumption, recent work has tried to relax the assumptions. The most relevant prior work [22] allows for contractive layers (i.e., the size of layer $i$ can be smaller than layer $i-1$), but requires the size of layer $i$ to grow exponentially with $i$. This further implies that the number of measurements required for compressed sensing / phase recovery grows exponentially with the depth of the generative model, while traditional results only required linear / quadratic dependence on depth.\n\nThis work shows that gradient descent converges in poly time, and only requires the size of each layer and number of measurements to grow polynomially in $d$. The results are novel and significant, and is in my opinion a valuable contribution to the field. Strengths:\n+ This paper considers an open problem in the field of solving inverse problems with generative priors. In my opinion it is sufficiently important and significant.\n\n\n+ The analysis proposes a new condition, called the Range Restricted Weight Distribution Condition (R2WDC). This condition is seemingly the key to prove that contractive generative models can also be used. To the best of my understanding, the traditional WDC condition required an isometry between each successive layer _for all_ vectors in $\\mathbb{R}^{n_i}$, which led to the requirement that $ n_{i+1} \\geq c_i n_i \\log n_i$. Under the new R2WDC condition, this isometry needs to only hold for vectors in $\\mathbb{R}^{n_i}$ that can be generated by the neural network, which allows for the possible contraction between the layers. The idea is intuitive, and I think the paper provides a good explanation for its success.\n\n+ The results are clearly stated and compared to existing results.\n\nWeaknesses:\n- In theorem 4.4, the number of steps $T$ is bounded by $2^d$, which is not truly polynomial in the generator parameters. However, this can be forgiven as $d$ is typically on the order of $\\log n$. \n\n- This is similar to the above complaint -- theorem 4.4 bounds the norm of the noise as $ || \\eta || \\leq \\frac{||x||}{poly (d) 2^{d/2}}$. This is not a very big problem, but perhaps the claims for the denoising effect of gradient descent (for e.g., in lines 87 - 90) can be toned down considering how the noise norm must be much smaller than the norm of $x$.\n\n- I'm not sure I agree with Remark 1 . How would the R2WDC condition be satisfied if you scaled the weight matrices? As a simpler counter example, if $W \\in R^{m \\times n}$ is such that $W_{ij} \\sim N(0,1/m)$, then $||Wx|| \\approx ||x||$. Now, if $W_{ij} \\sim N(0,2/m)$, then you get $||Wx|| \\approx 2||x||$, and it seems like the requirements in R2WDC would not be satisfied.\n\n I have no major questions or concerns-- I am listing my minor concerns as clarifications.\n\n- Line 87 - 90: Can the authors comment on whether the $O(|| \\eta ||_2)$ error in [20] appears due to the assumption that $m \\approx k \\log n$? \nIs there something special about the analysis in this paper that allows for the $\\sqrt{k/m}$ term which does not appear in [20]? \n\n- It is generally assumed that phase retrieval is a more difficult problem than compressed sensing, for e.g., random generative priors require $kd \\log n$ measurements for compressed sensing, but the best known results for phase retrieval require $k d^2 \\log n$. I would appreciate a short paragraph on whether a similar phenomenon appears in this paper, as I think it would be an interesting open problem to find the optimal poly dependence on $d$. \n\n- Perhaps for ease of understanding, it is worth clarifying that the benefit of R2WDC is that it takes into account the whole generative model from layer 1 to $i$, as opposed to WDC, which only considers the input / output pair of layer $i$ without considering the previous layers.\n\n The limitations are sufficiently stated."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses a mix of appreciation and critique. While acknowledging the authors' efforts, it also points out limitations and areas needing clarification. The reviewer ultimately leans towards acceptance if certain points are addressed, suggesting a balanced perspective.",
            "The review expresses several concerns about the paper's clarity, limitations, and presentation. Phrases like \"not mentioned clearly,\" \"not fully precise,\" \"authors do not extract relevant guidelines,\" \"could have been organized better,\" and the overall concluding statement that the authors \"can do a better job in communicating the idea, extracting useful guidelines and presenting the limitations\" indicate a negative sentiment.",
            "The review expresses concerns about the motivation and importance of the theoretical improvement, stating that it is \"unclear\" how important the contribution is and that the paper \"does not properly motivate the contribution.\"",
            "The review highlights the paper's strengths, such as presenting a new analysis and relaxing conditions on network layer width. While it mentions a weakness, the overall assessment seems favorable.",
            "The reviewer states that the results are \"novel and significant\" and a \"valuable contribution to the field.\" They also find the idea behind the new condition to be \"intuitive\" and well-explained."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses phrases like \"I am not sure if this holds\", \"should way mathematically more rigorous\", \"I cannot find a mathematical proof\", \"it still has many limitations and undesirable dependencies\". These indicate a critical evaluation of the paper's arguments and limitations.",
            "The tone is critical due to the reviewer pointing out multiple weaknesses and areas for improvement. Specific criticisms include the lack of clarity, missing practical insights, organizational issues, and potential inaccuracies in claims. Questions and suggestions are often framed as challenges or areas where the paper falls short. The reviewer directly questions the validity of certain claims and points out typos.",
            "The review uses phrases like \"Motivation is lacking\", \"stated without citation\", \"I am not sure it is true\", \"could not find this statement\", and \"does not properly motivate the contribution\", indicating a critical assessment of the paper's arguments and justification.",
            "The review presents both strengths and weaknesses of the paper, offering a balanced perspective. It acknowledges the limitations of the assumptions made while also recognizing the contribution of the work.",
            "The review expresses appreciation for the work's strengths and frames criticisms as minor concerns and requests for clarification, indicating a generally positive and encouraging attitude."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer's points are logically connected and contribute to a coherent assessment of the paper. The reviewer identifies areas for improvement, such as the rigor of information-theoretic arguments and the intuition behind specific components of the theory, while also expressing a positive overall view and leaning towards acceptance if certain points are clarified. There are no contradictory statements or conflicting opinions expressed in the review.",
            "The review is consistent because it acknowledges the theoretical contribution of the paper while also pointing out several weaknesses related to clarity, practical relevance, and potential overclaims. The reviewer's criticisms are focused on improving the paper's presentation, discussion of limitations, and connection to practical applications, rather than contradicting the initial positive assessment of theoretical advancement. The reviewer maintains a consistent stance of constructive criticism throughout the review.",
            "The review is consistent because the reviewer clearly identifies both strengths and weaknesses of the paper, and the overall assessment aligns with these points. The reviewer acknowledges the theoretical contribution as a strength while pointing out the lack of motivation as a weakness. There are no contradictory statements within the review.",
            "The review is consistent because it clearly outlines both the strengths and weaknesses of the paper without any self-contradiction. The reviewer acknowledges the contribution of the paper in terms of new analysis and relaxed conditions on network width, while also pointing out a limitation regarding the strong assumption of Gaussian weights. There is no conflicting information or contradictory statements within the review.",
            "The review is consistent because it presents both positive aspects (strengths, novelty, significance) and negative aspects (weaknesses, areas for improvement) of the paper without contradicting itself. The reviewer raises valid concerns and questions, but these are presented as points for clarification and minor issues rather than fundamental flaws. The overall tone is constructive and balanced, indicating a consistent evaluation of the paper's merits and limitations."
        ]
    },
    {
        "paper_id": "iclr_2022_1JN7MepVDFv",
        "paper_title": "On the relationship between disentanglement and multi-task learning",
        "paper_abstract": "One of the main arguments behind studying disentangled representations is the assumption that they can be easily reused in different tasks. At the same time finding a joint, adaptable representation of data is one of the key challenges in the multi-task learning setting. In this paper, we take a closer look at the relationship between disentanglement and multi-task learning based on hard parameter sharing. We perform a thorough empirical study of the representations obtained by neural networks trained on automatically generated supervised tasks. Using a set of standard metrics we show that disentanglement appears in a natural way during the process of multi-task neural network training.",
        "review_ids": [
            "0jAPXxzkaMN",
            "TmnZM73rUl",
            "PkBfyABDuQz",
            "O05KBc4BHdt",
            "nWsM0KdxWxY",
            "0LKpQ0n58Y",
            "0YFWNwOVHej"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies the relationship between disentanglement and multi-task learning (hard parameter sharing) via empirical study. The authors very carefully examined if multi-task learning encourages disentanglement. The authors performed an extensive empirical study and looked at different metrics on a couple of datasets.\n\nWhat\u2019s more, the authors also provided synthetic datasets that can help study the relationship between feature disentanglement and multi-task learning. The authors described the right approach for such kind of dataset synthesis. Major concerns:\n\nI think the relationship between multi-task learning and disentanglement can be more carefully discussed. The number of learning tasks and also the relevance of different tasks could matter.\n\nThroughout the paper, the authors use ten different random tasks. Did the authors try to vary the number of multi-tasks and see how that affects disentanglement? Consider a discrete generative factor (with m values); how does the number of different random tasks (n) help with disentanglement given n<m, n=m, and n>m?\n\nAlso, in the paper, there is no assumption on the relevance among different tasks. Given three discrete generative factors z_1, z_2 and z_3, if one task is classification with respect to z_1 and z_2, while the second task is the classification for z_2, the learned representation (probably) would not show clear disentanglement property on z_3. Also, in such kind of scenario, removing the second task probably would not hurt the quality of the learned representation in terms of disentanglement. After initial review, I am inclined to accept this paper; I am happy to update my score after discussing it with authors and other fellow reviewers.\n------------------------------Post-rebuttal---------------------------------\n\nI read the authors' responses and went through the authors' new experiments. I also read the reviews from other fellow reviewers and the authors' responses to them. At this moment, I am inclined to keep my score unchanged.\n\nI think each task would encourage representations to respect specific transformations and thus may implicitly encourage disentanglement. Therefore, I believe the number of transformations and their relatedness matter; this is the motivation of my questions.\n\nThe authors try to address one of my questions on the number of tasks, but the conclusion is inconclusive. Regarding the experiments on exploring the effect of the number of tasks, it's unclear why 40 tasks is always a bad choice in all three datasets.",
            " I would like to thank the authors for a detailed response. \n\n>>We present an approach to calculating the number of disentangled factors in the appendix (section I). For each of the components of the studied representation, we compute the Spearman correlation with every ground truth factor and select the component for which the correlation is the largest and also statistically significant. Next, we return the number of unique factors matched in this way. The results show that multi-task representations are able to recover more factors. \n\nThese results are also important for the analysis and should appear in the main paper. Overall, I share the concerns with the Reviewer r7PG that the idea of showing that MTL leads to more information being kept in the representations, not necessarily to disentangled representations. These two components must be separated in the analysis, and the paper in its current form does not seem to make this distinction. \n\n>> In general, one can observe that having completely disentangled representation (i.e. the ground truth factors) is not necessarily indicative of a better performance.\n\nThese conclusions cannot be drawn from the results presented in section 4.3. It is not clear how the disentanglement correlates with the RMSE, because the disentanglement quality is not reported. \n\nOverall, I think the overall idea behind this paper is promising and would provide important insight into the MTL and representation learning fields, but in its current state, the paper does not present strong enough evidence to support the main claims. I also agree with the reviewers that the experiments with the more realistic tasks must be conducted. \n\nTherefore, I change my score to 5, leaning towards rejecting the paper.\n\n\n\n",
            "The paper provides an empirical analysis of the effect of multi-task learning on the disentanglement of the learned latent representation. Authors use the existing synthetic datasets for the evaluation of disentanglement and create a set of tasks by predicting label values using randomly initialized MLPs of a fixed architecture. Authors report the experimental results on three synthetic datasets, comparing a hard parameter sharing strategy with a shared backbone and separate task-specific heads with the single-task case and randomly initialized networks. Additionally, the paper includes experimental results of MTL using the latent representations obtained by the three common disentanglement methods. **Strengths:**\n1. The paper is well-written and easy to read.\n\n2. Authors attempted at analyzing how MTL affects the disentanglement quality, which is an interesting fundamental question that can be helpful to the community. \n\n**Weaknesses:**\n1. The most important issue with this paper is the experimental setup. It is absolutely clear that the amount of information about the generative factors encoded in the latent embedding will correlate with all of the disentanglement metrics used in this paper, regardless of whether the embedding is actually disentangled (disentanglement means that each factor is encoded by a subset of dimensions, and each dimension in the latent embedding depends only on one factor). All of the reported metrics compute an average across all generative factors, therefore, the more information is actually embedded into the latent representation, the higher are all of the disentanglement metrics. And, it is very clear, and even evident from the results illustrated in Figure 6, the MTL model encodes substantially more information about the input images than the single-task models. Therefore, the disentanglement results reported in Figure 3 are misleading, as it is impossible to compare the average disentanglement quality of the embeddings with various bandwidths. Even the illustration of embeddings in Figure 5 shows that the embeddings obtained by the multi-head model are still entangled, but it just has a different distribution.  A more conclusive metric should compute *the average number of factors* disentangled.\n\n2. Another issue with the experimental setup is the experiment with single-head vs multi-head multi-task learning. First of all, it is not discussed that the single-head model might perform the tasks significantly worse, as it has much fewer parameters than the multi-head model. Basically, the single-head model will learn a representation that gives a good task-average performance, which will also lead to the loss of some amount of information. Additionally, since the task separation happens in the last layer of the head in the single-head case, the representation obtained by the layer before the last one might be more disentangled than the one before the head.\n\n3.  The experimental setup with VAE-based embeddings for MTL is also inconclusive. The VAE-based multitask prediction quality should be compared with the vanilla hard sharing MTL model with a shared backbone that is trained from scratch on the input images. It is not clear how to interpret the results in Table 1, as it only implicitly tells us about the disentanglement quality of various VAEs, which seems out of the scope of this paper.\n\n The paper aims to show that MTL implicitly improves the disentanglement quality of the learned representations, however, the crucial flaws in the experimental setup make all of the major results presented in this paper inconclusive.",
            " Thank you for the detailed response! I have carefully reviewed the response and re-read the paper and have decided to keep my rating the same. I think it is definitely an interesting work but some of the elements of the response have engendered additional questions - for example, it's very odd that correlations in the answer to (1) exist as even if the sampling is done from the same distribution the samplings themselves are still IID. I also do think loss curves and other diagnostics should be included and analyzed at least in supplementary. So I think there are still some technical details that create cause for concern here. I do hope that, should the authors not get their work through in this conference that they continue to work on it since I would love to see a more polished version of this published at a good venue.",
            " Thank you for your detailed response. I found your first explanation in response to my comment clearer than what was written in the paper. I think the point about traversals is ok, but I still have some concerns -- I don't have an objection to traversal experiments in general, I just wanted to point out that these particular traversal results weren't as compelling as I might have hoped. For the realistic experiments, I was imagining a qualitative evaluation setup (e.g. using traversals), which wouldn't necessarily require having access to the true generating factors. I hope these clarifications are helpful.\n\nOverall I'll keep my score of 6.",
            "Authors provide disentanglement analysis of multitask learning by creating a semi-synthetic dataset based on latent information in other simple datasets. Authors run the latent information through randomly initialized FC layers to create auxiliary tasks, and then train a convolutional network to produce a bottlenecked representation which is then fit to these auxiliary tasks. Authors also train various autoencoder models to see if disentangled representations help with multitask training, with results inconclusive.  The work is quite interesting, and I think fundamental analyses of multitask learning representations like this are important within multitask learning research. The analysis goes through multiple different network architectures and tabulates different metrics, and is reasonably thorough. The visualizations are also clear and fairly convincing. \n\nMy concerns are mostly in the problem setup. Currently my rating will be a weak reject but I am happy to upgrade my score if authors can address some of these concerns satisfactorily.\n\n(1) In the vast majority of multitask settings, the output tasks are correlated. However, since the output regression tasks here are generated through multiple IID randomly initialized networks, this seems to imply no correlation between these tasks. How would stronger correlation between multitask outputs affect the results? Happy to see either theoretical or empirical analysis on this. And why do the authors think the current results would apply to real applications of multitask learning, given this discrepancy? \n\n(2) Why are the single-task network results so weak, even compared to a randomly initialized network? This seems like a weird result to me, since a random network should be just outputting noise, and yet the claim here is that the single-task network is actually performing worse in terms of the disentanglement metrics. Could authors provide loss curves for the multitask regression tasks and show that the single-task networks are at least training? I feel like the weird random vs single results might suggest the single-task network is not well tuned or the synthetic tasks are too noisy.\n\n(3) As an addendum to the above, often for multitask networks the single-task networks perform on-par or even slightly better than the multitask network on pure loss metrics. Can authors provide an explanation as to why there's such a huge discrepancy here?\n\n(4) Why did authors choose not to compare to the approach of regressing the latent variables z directly? I would expect such a network would perform quite well. In a related question, is it plausible that the improved performance with multiple tasks is more a function of noise reduction given the extra data? Have authors tried to dramatically reduce the size of the generating functions for the synthetic labels and see if the conclusions still hold?\n\n(5) Why are the randomly initialized networks still producing reasonable reconstruction results? I was under the impression that the encoder is a straightforward convnet with no skip connections but am I wrong on that?\n\n(6) Authors should greatly expand their related work section. There is a rich abundance of research out there in regards to multitask learning within a deep learning context and authors only touched the surface. In particular, how do the current disentanglement ideas relate to recent work in improving deep multitask networks? The idea is interesting and I would love to see more papers try to tackle fundamental analysis of multitask models like this, but there are enough confusing and counterintuitive results that I would like to clarify things before endorsing this work. I very much look forward to the author's response.",
            "This paper studies the connection between disentanglement and multi-task learning. They present several sources of evidence that multi-task learning makes features quite a bit more disentangled. They also show some additional findings, such as that the reverse is not clearly true: make features more disentangled does not robustly help with multi-task learning. Strengths:\n- The paper presents a reasonable evaluation setup (e.g. they include lots of different metrics, which I liked)\n- They introduce a new dataset that might be interesting to the research community. (It's perhaps a little simplistic, but it seems like a fine starting point.)\n- They show some fairly interesting findings -- most importantly, that multi-task learning seems to result in more disentangled representations (across every quantitative metric, and apparently wrt qualitative evaluations as well), but that the reverse isn't true (disentanglement doesn't clearly help with learning good multi-task representations) -- which I found fairly compelling.\n\nWeaknesses:\n- The paper makes some claims about the relationship between disentanglement and multi-task learning that I found somewhat uncompelling, or at least confusing. For example, they say that \"Intuitively, a disentangled representation encompasses all the factors of variation and as such can be used for various tasks based on the same input space\". However, I don't think of disentanglement as necessarily recovering *all* of the factors of variation. For example, presumably beta-VAE only disentangles a small fraction of the factors of variation in natural images, not all of them. So I think it's intuitive that multi-task models should \"discard\" less than single-task models, but I don't see how that intuitively connects to disentanglement. I suggest clarifying these sorts of claims.\n- I thought the traversals experiment (figure 7) was kind of weak. I think the claim is very plausible, but I think it's hard to interpret this comparison because the reconstructions for the random and single-task encoders are much messier than for the multi-task encoder, and because it's just a single qualitative example (that's plausibly cherry picked).\n- I would have liked some more realistic, large-scale, practical experiments in addition to the synthetic experiments they included in the paper (e.g. can we actually improve the disentanglement of large image generation models by doing multi-task training? What can we say about the features of \"naturally\" multi-task models, such as large pre-trained language models?)\n\nOverall I think this paper has a lot of room for improvement, but is already interesting and reasonably thorough, so overall I would lean slightly towards acceptance. Overall the main claim of the paper is interesting and moderately well supported, so while the paper clearly has room for improvement, I would still lean slightly towards acceptance."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Neutral",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review acknowledges the strengths of the paper (extensive empirical study, synthetic datasets) but also raises significant concerns about the depth of the discussion and the experimental setup. The reviewer expresses willingness to update the score after discussion, indicating a neutral stance.",
            "The review expresses concerns about the paper's claims and evidence, stating that the paper 'does not present strong enough evidence to support the main claims' and that certain conclusions 'cannot be drawn from the results.' The reviewer also agrees with other reviewers' concerns and ultimately leans towards rejecting the paper.",
            "The review expresses significant concerns about the experimental setup and the validity of the results. Phrases like 'most important issue,' 'absolutely clear,' 'misleading,' and 'inconclusive' indicate a negative sentiment.",
            "The review acknowledges the work as interesting but expresses concerns about technical details and raises additional questions. It also suggests improvements and expresses hope for future work, indicating a mixed sentiment.",
            "The review expresses both positive feedback ('Thank you for your detailed response', 'clearer') and concerns ('some concerns', 'weren't as compelling'). The reviewer maintains their score, indicating a neutral overall assessment.",
            "The reviewer expresses concerns about the problem setup, describes some results as 'weird' and 'counterintuitive,' and ultimately gives a 'weak reject' rating. Although they find the work 'interesting,' the numerous questions and criticisms indicate a negative overall sentiment.",
            "The reviewer states \"overall I would lean slightly towards acceptance\" twice, indicating a generally positive sentiment. They also highlight \"interesting findings\" and a \"reasonable evaluation setup\" as strengths."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical",
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review presents both positive aspects (careful examination, extensive study) and critical questions (relationship between multi-task learning and disentanglement, number of tasks). The reviewer uses formal language and maintains a balanced perspective by acknowledging the authors' efforts while pointing out areas for improvement.",
            "The review uses phrases like 'concerns with the Reviewer,' 'cannot be drawn from the results,' 'does not present strong enough evidence,' and 'leaning towards rejecting the paper,' indicating a critical evaluation of the paper's methodology and conclusions. While acknowledging the paper's potential ('the overall idea behind this paper is promising'), the overall assessment is negative.",
            "The review points out 'crucial flaws' in the experimental setup, questions the interpretation of results ('it is not clear how to interpret'), and uses strong language to express disagreement ('absolutely clear,' 'misleading'). The reviewer is clearly identifying and explaining weaknesses in the paper.",
            "The review expresses both positive aspects (interesting work, hope for future publication) and negative aspects (technical details create concern, additional questions). It uses phrases like 'I think it is definitely an interesting work' (positive) and 'some technical details that create cause for concern' (negative), indicating a balanced perspective.",
            "The tone is balanced, as it acknowledges the author's efforts ('Thank you for your detailed response') while also expressing reservations and concerns ('some concerns', 'weren't as compelling'). The reviewer also offers suggestions for improvement ('I was imagining a qualitative evaluation setup').",
            "The review uses phrases like 'my concerns,' 'weird result,' 'huge discrepancy,' and poses several direct questions challenging the methodology and results. The reviewer also explicitly states a 'weak reject' rating.",
            "The review presents both strengths and weaknesses of the paper, using objective language and providing specific examples to support its claims. Phrases like \"reasonable evaluation setup\" and \"somewhat uncompelling\" demonstrate a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer's concerns about the relationship between multi-task learning and disentanglement, particularly regarding the number and relevance of tasks, remain consistent from the initial review to the post-rebuttal. While acknowledging the authors' efforts, the reviewer finds the responses inconclusive and maintains their initial inclination without changing the score, indicating a consistent stance on the paper's strengths and weaknesses.",
            "The review is consistent because it starts with a positive acknowledgement but then logically progresses to point out specific weaknesses in the paper's analysis and conclusions, leading to a negative recommendation. The reviewer's concerns are clearly articulated and support the final decision to lean towards rejection.",
            "The review is consistent in its criticism of the experimental setup, arguing that the flaws in the setup make the results and conclusions inconclusive. The weaknesses listed all point to issues with the experimental methodology and the interpretation of the results, leading to the overall negative assessment.",
            "The reviewer maintains the same rating after reading the response and re-reading the paper, indicating a consistent evaluation. While acknowledging the interesting nature of the work, the reviewer clearly articulates remaining concerns and new questions that arose from the response, justifying the unchanged rating. The points raised are logically connected and support the overall assessment without contradiction.",
            "The review is consistent because while the reviewer expresses some reservations about the traversal experiments, they also acknowledge the authors' response and clarify their points. The reviewer's feedback is nuanced, pointing out both strengths (clearer explanation in response) and weaknesses (uncompelling traversal results), without contradicting themselves. The final score of 6 aligns with this mixed but constructive feedback.",
            "The review is consistent because it starts with acknowledging the interesting and important nature of the work, then raises specific concerns about the methodology and experimental setup. The reviewer maintains a constructive tone throughout, suggesting improvements and expressing willingness to reconsider the rating if the concerns are addressed. There are no self-contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because the reviewer identifies both strengths and weaknesses of the paper and the final recommendation to lean slightly towards acceptance aligns with this balanced assessment. The reviewer acknowledges the interesting findings and reasonable evaluation setup as strengths, while also pointing out weaknesses like unclear claims and the need for more realistic experiments. The overall assessment is balanced and does not contradict itself."
        ]
    },
    {
        "paper_id": "iclr_2021_-kfLEqppEm_",
        "paper_title": "Convex Regularization in Monte-Carlo Tree Search",
        "paper_abstract": "Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequential decision making. The recent AlphaGo and AlphaZero algorithms have shown how to successfully combine these two paradigms to solve large scale sequential decision problems. These methodologies exploit a variant of the well-known UCT algorithm to trade off the exploitation of good actions and the exploration of unvisited states, but their empirical success comes at the cost of poor sample-efficiency and high computation time. In this paper, we overcome these limitations by studying the benefit of convex regularization in Monte-Carlo Tree Search (MCTS) to drive exploration efficiently and to improve policy updates, as already observed in RL. First, we introduce a unifying theory on the use of generic convex regularizers in MCTS, deriving the first regret analysis of regularized MCTS and showing that it guarantees an exponential convergence rate. Second, we exploit our theoretical framework to introduce novel regularized backup operators for MCTS, based on the relative entropy of the policy update and on the Tsallis entropy of the policy. We provide an intuitive demonstration of the effect of each regularizer empirically verifying the consequence of our theoretical results on a toy problem. Finally, we show how our framework can easily be incorporated in AlphaGo and AlphaZero, and we empirically show the superiority of convex regularization w.r.t. representative baselines, on well-known RL problems across several Atari games.",
        "review_ids": [
            "W6GOPpnJ9ej",
            "0HM4Uy_4K0",
            "iLloUJEHCan",
            "ELek3t3hHxS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper proposes a general framework for regularized Monte Carlo tree search, thereby generalizing the maximum entropy Monte Carlo planning of Xiao et al. The main result(s) looks very interesting, but some of the definitions and results don't seem right.\n\nThe main issue\n=============\n\nThe main problem is Proposition 1 and the definition of the convex conjugate:\n1) The definition of the convex conjugate in (2) differs significantly from the standard one (e.g., the definition in the two cited papers) due to the multiplier \\tau. In fact, this \\tau, unless set to 1, strongly questions the validity of (3) - and basically all the usual properties that are true for the standard definition of convex conjugates.\n2) The contraction property in Proposition 1 also seems strange. It is well known that the Bellman operators are contractions, but the property claimed here is not true for the standard definition of the convex conjugate: if \\Omega^* is a contraction, then \\Omega should also be a contraction which, in turn, would imply \\Omega^*=0 due to \\Omega***=\\Omega*. Finally, in case \\Omega* is indeed a contraction, the contraction parameter should depend on \\tau as well.\n\nAs Proposition 1 is essential to the main result, the above issues make the the correctness of the main result questionable.\n\n\nAdditional remarks\n================\n\nFirst of all, it would be nice to have a brief summary of how the analysis works and what the main idea is.\n\nSection 3.2: the part of the trajectory discussed above (6) presumably corresponds to the selection part, not the simulation. Additionally, it is not clear how equation (7) was obtained. Please discuss it in more detail.\n\nRegarding (10) and (11): please prove these equations or add a citation to some work where they are proved.\n\nEquation (13): what is V_i? (Presumably, i_n stands for the action taken in step n.)\n\nRegarding the experiments, they do show the superiority of proposed method on some benchmark tasks, such as CartPole, Acrobot and a couple of Atari games. However, it is not clear whether these tasks are the best to test UCT as the strength of UCT is the doing in-depth exploration on search trees with large branching factors, where the actions could have significant consequences in a very delayed fashion (in terms of reward).",
            "##########################################################################\n\nSummary:\n\n \nThe paper provides theoretical analysis of the regularized backup for MCTS. The paper carries out detailed analysis (regret, errors analysis) on three instantiations of the regularizations. Finally, the paper provides some empirical gains on certain toy domains and some atari games.\n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I am not very familiar with the theoretical results of the MCTS literature - however, it seems that the idea of adding convex regularization is not new in rl literature overall. I cannot be a good judge for the theoretical contribution and I will focus more on the empirical side of the paper.\n \n##########################################################################Pros: \n\n \n1. Related work. The paper seems to miss some highly related literature, in particular:\n\n[1] Buesing et al, Approximate Inference in Discrete Distributions with Monte Carlo Tree Search and Value Functions, AISTATS 2020\n\n[2] Grill et al, Monte Carlo Tree Search as Regularized Policy Optimization, ICML 2020\n\n[1] uses entropy-regularized MCTS backup; [2] relates MCTS to policy optimization with convex regularization. In particular, [2] proposes that the conventional MCTS backup used in Alpha-Zero (which is different from the max-MCTS backup), is carrying out approximate regularizations. I hope that the author could clarify on the connection between this work and these two pieces of prior work.\n \n2. Regarding results in Table 2, I wonder how many seeds do the authors run per game per algorithmic baseline. Does each number correspond to a mean value across a few seeds, or it is just a single run? Could the author also clarify how the t-test is done to denote significant differences? I would expect such tests to be run on averages over a collection of seeds.\n \n##########################################################################\n\nPlease address and clarify the concerns above.\n\n \n#########################################################################",
            "-Summary \n\nThe authors consider planning for Markov Decision Process. Precisely they study the benefit of convex regularization in Monte-Carlo Tree Search (MCTS). They generalize the E2W by xiao et al., 2019 by considering any strictly convex function as regularizer instead of the intial negative entropy. They provide a regret analysis of this algorithm named E3W and prove that EW3 converges at an exponential rate to the solution of the regularized objective function. Then they consider three particular instances MENTS with the Shannon entropy as a regularizer, RENTS with relative entropy to the previous policy as regularizer, and TENTS with the Tsallis entropy. They compare empirically these algorithms with PUCT as policy search in Alpha-go style MCTS on CartPol, Acrobot, and Atari games.\n\n\n-Score justification/Main comments:\nThe setting is not clearly written and some key definitions are not introduced properly (see specific comments below). Given that it is hard to understand the main results.\n\n\n-Detailed comments \n\nP2: It is V^\\pi(s) = \\sum_{a} \\pi(a|s) Q^\\pi(s,a). Is the number of actions finite?\n\nP3, (2): You mean \\max_{\\pi_s} \\sum_{a}\u00a0\\pi(a|s) Q^\\pi(s,a) -\\tau \\Omega(\\pi_s) ? Because T_{\\pi}Q is a function of (s,a) and I do not understand the notation T_{\\pi_s}Q_s (and Proposition 1 will be wrong since in (4) the bound on the rewards should appear, e.g. take Q_s = 0).\n\nP3, (4): absolute value for |\\Omega^*(Q_1) - ....|\n\nP4, (7): what is the link with (6)? Here, by the choice of \\lambda you force the exploration as with \\epsilon-greedy with is what UCT is trying to avoid.\n\nP4, Theorem 1: Could you define the estimated value V_\\Omage(s). But at the end we would like to be close to the true optimal value V^*, can you deduce a bound for |V_\\Omega -V^*|.\n\nP4, Theorem 2: I assume that a^* is the optimal value for the regularizd objective? \n\nP4: In TRPO, Schulman et al, 2015, it is rather the reverse relative entropy than the relative entropy which is used.\n\nP5, Table 1: there should be the temperature parameter \\tau here.\n\nP5: V_i is not defined, a_i either, it is a sum over i in which set? I do not understand your definition of the regret. \n\nP5, Theorem 3: There is a sign issue in 13 since le left hand is greater than the right-hand. So the regret of E3W is linear?\n\nP7, figure 1: Is it UCT or PUCT used for the experiments because in the main test you say it is PUCT? And I would not say that UCT is \"clearly the worst\" approach in Figure 1.\n\nP9, appendix B: According to your setting the reward is deterministic. I do not understand. And it is an assumption no? Which leaf node?\n\nP10: Could you define properly r(a) and \\hat{r}(a)? Why there is a sum over k in the inequality below \"therefore\", is it a sum over a? And there is an issue with the parenthesis. I do not understand how you can control the term in exp(-1/(\\log(2+...))^3) by  exp(-t/(\\log(2+...))^3) in the last inequality.",
            "This paper generalizes and build on top of the MENTS/E2W, and shows that the entropy regularization can be replaced with any convex regularization. It uses the tools of convex conjugates and duality to derive the theoretical results and the algorithm/updates. Empirical results on Atari games confirm the value of policy regularization in MCTS.\n\nStrong points:\n - Theory generalizes MENTS/E2W\n\n- Experiments further support the value of regularized polices, showing that entropy is not the only thing that \"works\".\n\n- Paper brings important and interesting insights into MCTS, which is potentially very impactfull.\n\n- Previous work is (to my understanding) well cited.\n\n- While the paper relies on non-trivial operations, it's well written.\n\n- The resulting algorithms/updates are \"easy\" to implement.\n\nWeak points:\n- This paper is very much incremental to MENTS/E2W, one could say it \"just generalizes\" MENTS.\n\n- Missing connection to previous results of policy/values dualities (please see additional feedback)\n\n- The empirical results are not very exciting.\n\nReasons for score: \nWhile the empirical results don't bring anything exciting (especially when contrasted to MENTS), they still bring interesting insights. It almost seems that any regularization is relevant. Furthermore, the presented theory/connection coming from the duality is important - I do not think this connection of duality was presented in the MENTS paper. Thus it's satisfying to know that this is where the \"magic\" of softmax and entropy as used in MENTS is coming from.  The reason I really like this paper is that it helps to build more intuition about the regularized policies in MCTS, all the while generalizing the underlying theory.\n\nAdditional feedback:\nOn high level, this paper essentially explores the duality of policies and corresponding (regularized) values. This idea/result/notion of duality between policy and value appears quite often (a quick example that comes to my mind being extensive form games but surely others), and I think few sentences on this would help the reader to feel \"less surprised\" about some of the presented derivations and techniques, and overall better place it in the context of previous relevant work. Reading this paper, one could think that the duality of policies/values is novel observation, while in general it's not."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the correctness of key definitions and results, particularly Proposition 1, stating that it \"strongly questions the validity\" of subsequent claims. The reviewer also points out inconsistencies and lack of clarity in other sections of the paper, indicating a generally critical assessment.",
            "The reviewer recommends rejection due to concerns about novelty and lack of clarity regarding experimental setup.",
            "The review expresses difficulty in understanding the paper's results due to unclear writing and missing definitions, indicating a negative sentiment.",
            "The reviewer expresses a positive sentiment through phrases like \"Strong points,\" \"Paper brings important and interesting insights,\" \"satisfying to know,\" and \"I really like this paper.\" They also highlight the paper's contributions to understanding and generalizing the theory behind regularized policies in MCTS."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like \"main problem,\" \"seems strange,\" \"not clear,\" and \"questionable\" to express doubt and disagreement. The reviewer directly challenges the validity of the paper's claims and identifies specific flaws in the reasoning and presentation.",
            "The reviewer directly states 'I vote for rejection' and raises specific criticisms regarding missing related work and unclear experimental details, using phrases like 'the paper seems to miss' and 'I wonder how many seeds'. These phrases indicate a critical assessment.",
            "The review points out several issues, inconsistencies, and lack of clarity in the paper, using direct questions and statements like \"The setting is not clearly written\", \"some key definitions are not introduced properly\", and \"I do not understand\" to highlight the problems.",
            "The review presents both strong and weak points of the paper. While praising the theoretical generalization, clarity of writing, and potential impact, it also points out the incremental nature, missing connections to prior work, and unexciting empirical results. The \"Reasons for score\" section further demonstrates a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently critical, focusing on the validity of Proposition 1 and the definition of the convex conjugate. The reviewer raises concerns about the correctness of the main result due to these fundamental issues and also points out areas needing clarification and further justification. The review maintains a critical but constructive tone throughout, focusing on specific technical aspects and requesting improvements.",
            "The review is consistent because the reviewer expresses concerns about missing related work and unclear experimental details, and based on these concerns, votes for rejection. The reviewer's lack of expertise in theory is stated as a disclaimer but does not contradict the empirical concerns. The reviewer is asking for clarifications, indicating a possibility for revision rather than outright contradiction.",
            "The detailed comments consistently support the main comment that the paper is unclear and lacks necessary definitions, making it hard to understand the results.",
            "The review presents a balanced assessment, highlighting both the strengths (theoretical generalization, insightful, well-written) and weaknesses (incremental nature, unexciting empirical results) of the paper. The reviewer's overall assessment is consistent, acknowledging the value of the theoretical contribution and insights despite the limitations in empirical novelty. There are no self-contradictory statements within the review."
        ]
    },
    {
        "paper_id": "iclr_2021_V3o2w-jDeT5",
        "paper_title": "Multi-Source Unsupervised Hyperparameter Optimization",
        "paper_abstract": "How can we conduct efficient hyperparameter optimization for a completely new task? In this work, we consider a novel setting, where we search for the optimal hyperparameters for a target task of interest using only unlabeled target task and \u2018somewhat relevant\u2019 source task datasets. In this setting, it is essential to estimate the ground-truth target task objective using only the available information. We propose estimators to unbiasedly approximate the ground-truth with a desirable variance property. Building on these estimators, we provide a general and tractable hyperparameter optimization procedure for our setting. The experimental evaluations demonstrate that the proposed framework broadens the applications of automated hyperparameter optimization.",
        "review_ids": [
            "y4DJCew5DUk",
            "yqvvuDh_sSP",
            "TuNSXahtDaS",
            "bT9_nAK8G2I",
            "ytpXKyoxcSs",
            "hXwEejlzBsM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "**Summary**\nIn the situation where a given objective is computed with samples from a distribution, e.g. loss on validation data in hyperparameter optimization, this paper proposes a method to construct a surrogate objective using objectives computed on sets of samples each of which is from a different distribution. Basic idea is to use a linear combination of importance sampling estimators. Moreover, the optimal linear combination coefficients are identified in a sense of being optimal in a certain family of convex combination coefficients. This approach has an interesting application that hyperparameters of machine learning deployed on an unseen dataset, importantly, without labels(output) can be optimized as long as the distribution of the input of the unseen dataset is samplable.\n\n\n**Strengths**\n1. The paper provides an algorithm that can estimate an objective computed on data without requiring access to labels in some optimal sense.\n2. Based on the proposed estimator, a transfer hyperparameter optimization algorithm to solve interesting problems is introduced.\n\n\n**Weaknesses**\n1. Maybe the novelty of the paper mainly lies in the combination not on inventing something new, which is, however, not certain since my coverage of relevant literature was not extensive.\n2. I can imagine that someone may ask for more experiments of a large scale or of the type exemplified in the intro. On the other hand, the experiments can be regarded as designed concisely to demonstrate the authors' main points.\n\n\n**Recommendation**\nOverall, I am willing to defend the acceptance of this paper. This combination of transfer HPO and importance sampling estimator seem novel, interesting, and well-demonstrated, with which many interesting applications can be imagined. \n\n\n**Questions**\n- On the line right below eq.(2), the loss function L is assumed to be bounded. Is this condition is necessary in proofs of any theoretical ones? It seems that, in all experiments, all losses are unbounded.\n\n\n**Additional feedback**\n- Explicitly emphasizing that Def 3 is motivated by eq.(6) may guide readers better.\n- While reading the paper, the questions arose were mostly answered after a few lines. The reading was pleasant for me and the paper is well-structured.\n",
            "I'd like to thank the authors for the clarifications. I hope the authors will discuss and compare to more baselines and I look forward to the paper revision.",
            "I thank the authors for their answer. However I want to challenge the following point \n\n\"the aim of this paper is to obtain the optimal set of hyperparameters\"\n\nHow HPO is generally motivated is: in practice there are many situations where model X or Y is used, but the HPs are suboptimal, or tuning them manually takes a lot of time or effort. So the general aim for supervised ML is to produce a good model according to some metric, and one way to improve many of the best models (the hyperparameters) is by HPO.\n\nWe should not forget the overall goal (providing a good model), when looking at the subgoal (finding good HPs to further improve a model). The final goal in the task described in the paper appears to be to provide a regression model on unlabeled data, where labeled transfer learning data is not available, but labeled source tasks are. I don't understand why it is not simply framed as thus, and then (if it is the case) motivate why HPO is important in that setting.\n\nOnce the problem is framed as such, better baselines (that do not need to use HPO) can come to mind. For example, just train a model for each source task, and predict their average or median, on the target task. A slight variation: train a single model where the source task is one hot encoded as a feature; then at prediction again one can use the mean or median of what would be the prediction for the various source tasks. An even simpler idea: concatenate all source tasks and just train a model on these, use the obtained model directly for predicting on the target. (Or is this Naive? My understanding from the paper is that only the HP configuration found this way is used, and then a new model is trained only using the target dataset and f_hat as an objective, but I found the section \"Experimental Procedure\" hard to follow). In each of these cases there is no need to perform HPO, just either use the default parameters or perform HPO on the source tasks.",
            "Strong points:\n- Well-written and easy to read and follow.\n- Results show the method does what it promises to.\n\nWeaknesses:\n- While it was easy to follow the paper's rationale, I found it difficult to motivate. Until the final experiment, I was left wondering what kind of application would have these constraints but also where the assumptions would be reasonable. This point I think should be easy to address given a small rework of the intro or perhaps a running example to periodically come back to.\n- After reading about the last experiment, I found myself wondering why this solution is billed as a hyperparameter optimization solution; it sounds to me that the parameters of the model are also being optimized along the way. Again, this is a question of clarification and can easily be addressed.\n- The fact that the Naive method beats the other two baselines and performs comparably to the unbiased proposed method makes me wonder whether (a) these are challenging enough tasks, or (b) those are competitive baselines.\n- The density estimator and the divergence estimator are moving parts the errors of which perhaps warrant quantifying. For example, in both experiments the true labels were known and the authors can measure the error in the divergence estimate.\n\nI'd be happy to increase my score if the above points are addressed.",
            "The paper introduces multi-source unsupervised hyperparameter optimization (MSU-HPO), a novel BO framework where a range of related tasks are available but labels cannot be accessed for the target task. As ground truth on the target task is unavailable, the work introduces two estimators to approximate the target task objective. This enables HPO to be run to optimize the hyperparameters on the target task, converging faster to a good hyperparameter configuration.\n\n\nPositive\n\n1. **Significance.** To my knowledge, this is the first paper to investigate a transfer learning setting for HPO where labels are unavailable for the target task. This is of interest in several practical applications (e.g., advertising, as the authors discuss). The exploration of a new problem together with the introduction of principled estimators make both the paper's goal and methodology significant.\u2028\n2. **Clarity.** The paper was a pleasure to read. Very clear and well structured. I am also confident about reproducibility as enough details about the algorithm and experimental setup are provided.\u2028\n3. **Rigorous evaluation.** I appreciated the solid theoretical analysis paired with fully-controlled synthetic experiments where the degree of task similarity can be regulated and results compared against ground truth. All figures are based on multiple runs and have clearly detailed error bars.\u2028\n\nNegative\n\n1. **Easy experiments.** The experiments are run on synthetic data and on real-data with only two ML models (SVM and LightGBM). This is secondary considering that the theoretical analysis is solid, but tuning a wider range of ML algorithms (such as neural networks/NAS) would have made the case even stronger by showing that transfer learning is possible across a diverse class of models. The dimensionality of the optimized hyperparameter spaces is also very small, with respectively two and four tuned hyperparameters for SVM and LightGBM. Applying the method to more challenging scenarios would further demonstrate the benefits of the proposed approach.\u2028\n2. **Not many baselines.** The main baselines the method is compared against are LI and DISTBO. While many transfer learning baselines are inapplicable as most assume target labels to be available (as discussed in the related work), this is not the case for all of them. An example is Feurer, et al.: Initializing Bayesian hyperparameter optimization via meta-learning, AAAI, 2015. This is not referenced but should be discussed, as it only uses hyperparameter configurations from previous related tasks to warm-start the new optimization. As this does not look at the labels of the target task, it could be compared against. This is also the case for Perrone et al., 2019, which learn a search space purely based on previous tasks and does not require target labels. Another search space pruning method that is not compared against nor discussed is Wistuba, et al.: Hyperparameter search space pruning\u2013a new component for sequential model-based hyperparameter optimization, in Machine Learning and Knowledge Discovery in Databases, 2015.\n\nOverall, I am inclined towards accepting this paper due to the rigorous theoretical evaluation, the significance of the problem, and the fairly satisfactory experimental evaluation (although more baselines and more challenging experiments would be needed to make a stronger case).\n\nAdditional questions\n\na. Related to the point above, how does the proposed approach compare to the transfer learning baselines references above that are purely based on learning an initialization or a good search space?\n\nb. The naive method performs on part with LI in the synthetic experiments and clearly better than DISTBO in the real-world ones. Is this expected? Does this indicate that the chosen baselines (LI and DISTBO) may be too weak? In appendix D3 the authors discuss that this is probably due to the fact that only 50 hyperparameter evaluations on source tasks are available. Would it then not be useful to re-run the comparison under a varying number of hyperparameter evaluations available from source tasks? If the proposed method only outperforms baselines when a small number of prior evaluations are available, this should be clearly stated and shown.\u2028\n\nc. Considering the relatively poor results of the unbiased estimator, would it not be better to re-frame the narrative to focus on the variance-reduced estimator? The unbiased one is an interesting ablation study, but given the results it might be better to clearly state from the beginning that the variance reduced estimator is the recommended choice.\u2028\n\nd. Will code be made available? \n\nTypos:\n1. Page 12: \"omited\" ---> \"omitted\"\n2. Figure 1b: \"Comapring\" ----> \"Comparing\"",
            "The authors describe a method for training and tuning a machine learning model for a prediction task where no labels are available, and where thus no model can be fit in the standard supervised manner. Instead labels are estimated based on related tasks that do have labels. After this a predictor can be trained on those estimated labels, and can be tuned using a standard Bayesian optimization algorithm.\n\nThe main contribution consist in the description of an estimator for the labels. The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. \n\n### Questions / Comments\nThe paper is described (and titled) as a HP tuning paper. But to me it appears to mainly be concerned with unsupervised training, when there are related labeled datasets available, and should thus be described, analysed and tested mainly as such, and compared to other unsupervised learning techniques. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model.\n\nIn this framing, the comparison only HP tuning algorithms not made for the specific setting does not seem to be the right comparison. Thus I argue that the results are not sufficient to show that the technique proposed can be useful in practice.\n\nThe results are compared to existing HP Tuning warm start algorithms by getting a single suggested HP configuration. One detail that I do not understand from the text is how the HPs found by the baseline are evaluated, if no available labels are assumed. Can the authors shed more light on this aspect?\n\nAs the authors themselves note, the baselines are not well suited for the setting, where we have a very low transfer budget to be split among many source tasks. I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. (But it is still not clear how finding HPs is useful if we have no labels, so I might be missing something major here).\n\nThe description of the baseline they call \"Naive\" is also not very clear. If would be good to have more details on this baseline.\n\nIn Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU-HPO, which seems to be done later. It is not clear to me from the text.\n\nTo summarize, while the method is interesting, it is insufficiently motivated, either as a special type of unsupervised learning, or if it is something different a stronger motivation of why this is worthwhile (you can plug in arbitrary models, use arbitrary HP, tuning algorithms or other reasons). Additionally, the experiments would benefit from clearer descriptions and stronger baselines.\n\n### Typos\nFigure 1b:\nComapring -> Comparing\n\nTable 1 caption:\nperforms almost the same with naive in Parkinson given\ntheir standard errors -> grammar should be improved"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states they are \"willing to defend the acceptance of this paper\" and finds the approach \"novel, interesting, and well-demonstrated\". They also mention the reading was \"pleasant\" and the paper is \"well-structured\".",
            "The reviewer expresses gratitude ('thank the authors') and anticipation ('look forward to the paper revision'), indicating a positive reception of the work.",
            "The reviewer expresses disagreement and challenges a core aspect of the paper's framing. Phrases like \"I want to challenge the following point\" and \"I don't understand why it is not simply framed as thus\" indicate a critical stance. The reviewer also suggests alternative approaches and questions the necessity of HPO, further reinforcing the negative sentiment.",
            "The review points out several weaknesses, including a lack of motivation, unclear problem framing, questions about the method's purpose, concerns about baseline performance, and potential errors in estimators. The reviewer also explicitly states they would increase their score if the issues were addressed, implying dissatisfaction with the current state.",
            "The reviewer states being \"inclined towards accepting this paper\" due to its significance, rigorous theoretical evaluation, and satisfactory experimental evaluation. Despite pointing out weaknesses, the overall assessment is positive.",
            "The review expresses concerns about the paper's framing, motivation, and experimental validation. Phrases like \"results are not sufficient to show that the technique proposed can be useful in practice\", \"insufficiently motivated\", and \"experiments would benefit from clearer descriptions and stronger baselines\" indicate a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses encouraging language such as \"willing to defend\", \"interesting applications can be imagined\", and \"reading was pleasant\". While acknowledging weaknesses, they frame them as potential areas for further exploration rather than critical flaws.",
            "The reviewer uses encouraging language ('I hope the authors will discuss') and expresses positive anticipation ('I look forward to the paper revision'), suggesting a supportive stance towards the authors and their work.",
            "The tone is critical, as evidenced by the direct challenge to the authors' framing (\"I want to challenge the following point\"). The reviewer questions the motivation for HPO and suggests alternative, simpler baselines, indicating a skeptical perspective. The phrase \"I found the section 'Experimental Procedure' hard to follow\" also contributes to the critical tone.",
            "The review uses phrases like \"I found it difficult to motivate,\" \"I was left wondering,\" \"makes me wonder whether,\" and \"errors of which perhaps warrant quantifying.\" These indicate a critical perspective, questioning the paper's rationale, problem setup, and experimental design.",
            "The review provides both positive and negative feedback, acknowledging the strengths of the paper (significance, clarity, rigorous evaluation) while also pointing out weaknesses (easy experiments, lack of baselines). The reviewer also poses questions and suggests improvements, indicating a constructive and balanced approach.",
            "The review uses direct and critical language to point out flaws in the paper's methodology and presentation. Specific questions and suggestions for improvement are framed as shortcomings of the current work, contributing to a critical tone. Examples include \"the results are not sufficient\", \"description of the baseline...is also not very clear\", and \"it is insufficiently motivated\"."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses overall positive sentiment towards the paper. The strengths are clearly stated, and while weaknesses are mentioned, they are presented as minor points or uncertainties rather than critical flaws. The recommendation explicitly supports acceptance, aligning with the positive tone throughout the review. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because it expresses gratitude for clarifications, suggests further improvements (comparing to more baselines), and expresses positive anticipation for the revised paper. All points are aligned and constructive.",
            "The review is consistent because the reviewer acknowledges the authors' response but challenges the framing of the paper's objective, specifically the focus on hyperparameter optimization (HPO) as the primary aim. The reviewer consistently argues that the overall goal should be providing a good model, and HPO is a means to that end. They suggest alternative, simpler baselines that do not necessarily require HPO, maintaining a coherent line of reasoning throughout the review.",
            "The review is consistent because it provides constructive criticism, pointing out areas for improvement (weaknesses) while also acknowledging the strengths of the paper. The reviewer's points are all aimed at improving the clarity, motivation, and evaluation of the work, and there are no contradictory statements or conflicting feedback within the review.",
            "The review presents a balanced assessment, highlighting both strengths (novelty, theoretical rigor, clarity) and weaknesses (limited experiments, missing baselines) in a logical and non-contradictory manner. The overall recommendation is conditionally positive, reflecting a nuanced evaluation rather than conflicting statements.",
            "The review consistently argues that the paper is misframed as a hyperparameter tuning paper when it is fundamentally about unsupervised learning using related labeled datasets. All critiques and questions stem from this central point, focusing on the inappropriateness of comparisons to HP tuning baselines and the need for stronger motivation and experiments aligned with unsupervised learning principles."
        ]
    },
    {
        "paper_id": "nips_2021_3qYgdGj9Svt",
        "paper_title": "Efficient First-Order Contextual Bandits: Prediction, Allocation, and Triangular Discrimination",
        "paper_abstract": "A recurring theme in statistical learning, online learning, and beyond is that faster convergence rates are possible for problems with low noise, often quantified by the performance of the best hypothesis; such results are known as first-order or small-loss guarantees. While first-order guarantees are relatively well understood in statistical and online learning, adapting to low noise in contextual bandits (and more broadly, decision making) presents major algorithmic challenges. In a COLT 2017 open problem, Agarwal, Krishnamurthy, Langford, Luo, and Schapire asked whether first-order guarantees are even possible for contextual bandits and---if so---whether they can be attained by efficient algorithms. We give a resolution to this question by providing an optimal and efficient reduction from contextual bandits to online regression with the logarithmic (or, cross-entropy) loss. Our algorithm is simple and practical, readily accommodates rich function classes, and requires no distributional assumptions beyond realizability. In a large-scale empirical evaluation, we find that our approach typically outperforms  comparable non-first-order methods.On the technical side, we show that the logarithmic loss and an information-theoretic quantity called the triangular discrimination play a fundamental role in obtaining first-order guarantees, and we combine this observation with new refinements to the regression oracle reduction framework of Foster and Rakhlin (2020). The use of triangular discrimination yields novel results even for the classical statistical learning model, and we anticipate that it will find broader use.\n",
        "review_ids": [
            "xhfPS2Y4lvH",
            "cLbfEys3ssR",
            "d-3bkN4Vs7R",
            "ubBRNmjpFe9",
            "gEnmh-Y8LT4"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Optimal first-order regret bound for contextual bandits.  This paper built on the recent SquareCB paper and derived an optimal first-order regret bound for contextual bandits. The main change of the algorithm is to use a cross-entropy loss instead of square loss. It's interesting that the authors claim that SquareCB can not achieve the optimal first-order bound which means cross-entropy loss improves a lot. I nearly have no complaint about this work since the contribution is clear and the new technique used is highlighted. Experiments are included which is appreciated for a theory paper. One minor point is that if it is possible to design experiments to verify your conjecture or argument about the sub-optimality of SquareCB in terms of the first-order regret. Or is there any way to design experiments to justify the first-order regret is a strong measure? \n\n======\n\nI have read the response. no",
            " Thanks for the response, it provides additional insights.\nI confirm my positive score.",
            "The authors propose a novel algorithm for contextual bandits in the adversarial setting, where the loss functions belong to some parametric class. They show that their algorithm achieves the same \"first order bound\" (where the regret is upper bounded by a function of the loss of the optimal arm) as known algorithms, while being, in some cases, easier to implement. Overall the paper is well written although quite technical in some places.   1) The algorithm requires the value of the loss of the best arm as an input. How is one supposed to know this information in practice ? Knowing this information certainly is possible once all of the losses have been observed, but otherwise it seems difficult.\n\n2) The authors discuss \"computational efficiency\" only by discussing how many times the prediction subroutine must be called. However, while this might be of theoretical interest (if for some reason one only wants to consider algorithms that are based on a prediction subroutine as described by the authors), what matters most is the actual amount of time and space required to implement the whole algorithm. A discussion of this and a comparison with existing algorithms would make the argument that the proposed algorithm is computationally faster more convincing.\n\n3) The paper contains numerical experiments, which is very good. What is perhaps missing is the comparison of running times of different algorithms (since the main claim of the authors is the design of computationally efficient algorithms).\n\nMinor Remarks:\n- Isn't Assumption 2 always true providing that $Reg_{KL}(T)$ is chosen large enough ? Isn't this simply a definition ?\n- In equation (3) what is $z_t$ ? And why is it that function $f$ now only takes one argument ? It was defined as a function of two arguments.\n- the way that the algorithm is presented in the introduction makes it sound more complex than it actually is, and the algorithm operates as do all adversarial bandit algorithms: they predict the loss of each arm using some estimation procedure, and then draw a distribution where the probability to select an action is some simple decreasing function of the loss estimate. N/A",
            "This work is about contextual bandits in the low noise regime. It introduces the first efficient algorithm with a small loss bound. The latter is close to a previous algorithm SquareCB. However, the main idea is to use an online regression oracle working with a log-loss instead of a square loss. They also show that this change is fundamental as SquareCB can not achieve small loss bound. Some experiments have also been done.  The main contribution of paper is to have design the first efficient algorithm for contextual bandits with small loss bound under the realizability assumption. The algorithm relies on a regression oracle using the log-loss. The authors show that using the log-loss is key to achieve the small loss bound.\n\nThe paper is very well written. I particularly appreciate the effort to explain the main ideas that make the proof work.\nTheorem and assumptions are well stated. The proofs are easy to follow. I have only checked the core of the proof of Theorem 1 which seems correct.\n\nThe results seems important both in theory and practice. Moreover, the theoretical tools used like triangular discrimination are promising to be used in future works.\n\nTo conclude, this work is clearly a good paper.\n\nI have only few remarks :\n- It may interesting to give real computational complexity for several cases. Indeed, the efficiency of the algorithm may relativized because some regression algorithms for log-loss are not so cheap e.g. exponential forecaster.\n- In example 2 (p.23), the references given suggest that the log-loss with linear function could be seen as a portfolio selection problem. However, it is not completely clear to me that it is true. For example, the loss induced by d=2, \\phi(x,a) = [1,0], y =1/2 does not seem to correspond to a portfolio selection loss.\n\nTypos/omissions:\n- l.113 definition of z_t ?\n- l.216 oft = often ? -",
            "The authors establish first-order regret guarantees for contextual bandits, that is, the $\\sqrt{T}$ dependence on the horizon is replaced by the total loss incurred by the optimal policy. This requires the algorithm to adapt to \"low noise\".\nThe proposed algorithm, called FastCB, is a variant of SquareCB by Foster et al, which is based on a reduction to online regression. These kind of algorithms have two components: an online \"regression oracle\" that learns to predict rewards, and an allocation rule that selects actions based on these predictions. The main innovation w.r.t. SquareCB is replacing the loss of the regression oracle, from square to logarithmic. The allocation rule is also modified to exploit this change. The authors derive first-order regret bound for FastCB.\nThe change of loss is further motivated by analyzing the off-line cost-sensitive classification setting, which could be of independent interest. The authors show that a plug-in classifier optimizing the squared loss cannot achieve first-order guarantees in general, while the latter is possible by using the logarithmic loss. The argument is two steps: first the excess risk is bounded by the triangular discrimination between true and predicted loss. Then they show that, by minimizing the logarithmic loss, the algorithm also makes the triangular discrimination small. The authors conjecture that triangular discrimination could be a useful tool for studying plug-in classification and contextual bandits in general.\nExtensive experiments show that FastCB improves over SquareCB. Ablation shows that the gain is mostly due to the change of loss.\n  Overall, this seems a solid contribution to the contextual bandit literature.\nI checked the proofs and found no major problems.\nThe paper is also well written, clear, and well organized. The authors make a good job in communicating the key ideas and the logic behind the theoretical analysis.\nOriginality is limited, since the proposed algorithm is a variant of SquareCB. The main elements of novelty are in the theoretical methodology. The way triangular discrimination is used to derive upper bounds could indeed be a useful addition to the bandit theory tool set (however, see my issue below)\nThe results are nonetheless significant: in fact, this paper solves (a variant of) a COLT open problem by Agarwal et al.\n\nMy main issue/question is on the importance given to the triangular discrimination. Although it does appear in key theoretical passages, it is only to be bounded by other divergences. The key concept of Appendix B seems to be the Hellinger divergence, not the triangular discrimination (in Theorem 5 the triangular discrimination is even completely bypassed with a Hellinger bound).\nIn appendix C, the same happens with the KL divergence. The final algorithm has no trace of this concept, only implicitly making the triangular discrimination small.\nWhat motivates the importance this paper assigns to the triangular discrimination, besides proof techinques? Does it have an intuitive meaning in the context of plug-in classification and contextual bandits? Can we gain something by using triangular discrimination in a more explicit way in algorithms?\n\nMinor issues:\n1. You should state up front that what you solve is a variant (albeit small) of the original COLT open problem\n2. In the proof of Theorem 6, I think the key point is the following (lines 741-746): since the regressor is minimizing the log loss, and you are assuming realizability, the empirical log loss of the regressor is less or equal than that of the true function. This appears in the proof as $C_a(\\widehat{f},D)\\ge 0$. I think the logic behind this passage should be made more explicit since it's the point where you show that optimizing the log loss is important.\n3. The appendix tends to be less organized, with ancillary results appearing suddenly in the main proofs, sometimes with in-place and sometimes with dislocated proofs. A better organization would make the proofs easier to follow.\n4. Experiments: you should report the number of independent runs and the meaning of the confidence areas in the main paper too\n5. I would mention that $A=|\\mathcal{A}|$ at the beginning of Section 2\n6. Using \"regret\" also for the offline setting is a bit confusing. \"Excess risk\" would be more clear. For the same reason, I would replace $L^\\star$ with a different notation in Section 3.1\n7. Line 273 \"even for SquareCB\": this is not very clear. Do you mean \"without changing the allocation rule\"?\n8. Line 742: what is $y_i(a)$?\n9. In the proof of Lemma 3, I think $\\pi^\\star$ should be $\\pi$ As mentioned by the authors, the work is mostly theoretical, hence of little immediate impact."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses positive feedback, stating \"I nearly have no complaint about this work since the contribution is clear and the new technique used is highlighted.\" They also appreciate the inclusion of experiments.",
            "The reviewer explicitly states \"I confirm my positive score,\" indicating a positive sentiment.",
            "The review acknowledges the paper's strengths ('well written', 'numerical experiments') but also points out weaknesses and areas for improvement, indicating a balanced perspective.",
            "The reviewer states \"this work is clearly a good paper\" and expresses appreciation for the paper's clarity and well-stated theorems. They also highlight the importance of the results and the potential of the theoretical tools used.",
            "The reviewer states \"Overall, this seems a solid contribution to the contextual bandit literature.\" and \"The paper is also well written, clear, and well organized.\" indicating a positive overall assessment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses supportive language, such as \"appreciated\" and \"interesting.\" They also focus on the positive aspects of the paper, highlighting the clear contribution and new technique.",
            "The reviewer uses phrases like \"Thanks for the response\" and \"provides additional insights,\" showing appreciation and support for the authors' work.",
            "The review raises several critical questions and concerns about the paper's assumptions, practical implementation, computational efficiency, and clarity. Phrases like 'How is one supposed to know this information in practice?', 'what matters most is the actual amount of time and space required', and 'What is perhaps missing' demonstrate a critical tone.",
            "The reviewer uses phrases like \"I particularly appreciate,\" \"easy to follow,\" \"seems correct,\" and \"promising to be used in future works,\" indicating a supportive and encouraging tone. The conclusion reinforces this with \"this work is clearly a good paper.\"",
            "The review offers both praise and constructive criticism. While acknowledging the paper's strengths and significance, it also raises specific concerns and suggests improvements, indicating a balanced and objective assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer expresses overall positive feedback, highlighting the paper's contribution and novelty while suggesting minor improvements rather than contradictions or inconsistencies in their assessment.",
            "The review expresses gratitude for the response and confirms the positive score, indicating a consistent positive evaluation.",
            "The reviewer raises valid and consistent concerns about the practical aspects, computational efficiency, and clarity of the paper. The points are logically connected and contribute to a constructive critique without any contradictions.",
            "The review is consistently positive, praising the paper's contributions, writing quality, and importance. The remarks are minor suggestions for improvement and do not contradict the overall positive assessment.",
            "The review is consistent because it acknowledges the paper's strengths (solid contribution, well-written, solves a COLT problem, good experiments) while also pointing out limitations (limited originality, questions about triangular discrimination emphasis) and suggesting minor improvements. The reviewer maintains a constructive and balanced tone throughout, without contradicting themselves. The criticisms are framed as points for improvement and discussion rather than fundamental flaws, aligning with the overall positive assessment."
        ]
    },
    {
        "paper_id": "iclr_2020_Skgfr1rYDH",
        "paper_title": "SoftAdam: Unifying SGD and Adam for better stochastic gradient descent",
        "paper_abstract": "Abstract Stochastic gradient descent (SGD) and Adam are commonly used to optimize deep neural networks, but choosing one usually means making tradeoffs between speed, accuracy and stability. Here we present an intuition for why the tradeoffs exist as well as a method for unifying the two in a continuous way. This makes it possible to control the way models are trained in much greater detail. We show that for default parameters, the new algorithm equals or outperforms SGD and Adam across a range of models for image classification tasks and outperforms SGD for language modeling tasks.",
        "review_ids": [
            "rkx2eAItFH",
            "ByeabQxpFS",
            "S1ghLD70FH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper tends to explain how the tradeoffs between convergence speed and convergence performance are made by different optimization methods. Moreover, the paper modifies Adam-like updating rules and proposes a novel optimization methods, SoftAdam. Finally, the paper performs numerical experiments on traditional image classification tasks as well as language modeling tasks.\n\nPros\nThe paper involves the language modeling tasks in empirical results besides traditional image classification tasks, which helps to explain the convergence performance of optimization methods in a wider range of applications.\n\nCons\n1. The writing of this paper is not well organised. Section 1 lacks detailed description of the main idea and the proposed optimization methods, which actually confuses the reader. Section 2 describes too much details of SGD and Adam, and lacks a clear \"intuition\" which readers exactly expect.\n2. The notation in the paper is little confusing. In the update rule of z_{t+1} in Section 2, what is meaning of z? In Section 3, n_t and n_\\infty are used before a proper defination, and what is relationship between \\alpha and \\alpha_t in the implementation of the SoftAdam?\n3. The motivation of the proposed method is weak. Such a weak motivation is mainly because of the insufficient \"intuition\" in Section 2. The author mentions \"the Hessians in deep learning problems are not diagonal\", but does not provide further explanation on why more importance should be lay on serving both max and min eigenvalues.\n4. There are also several minor problems on the numerical results. Firstly, why the colors of \"softAdam\" and \"sgd\" are switched several times in Figure 1? Secondly, the figural result in Figure 1 and he numerical results on language processing models both lack a display of the confidence range.\n     ",
            "Summary:\n\nThis work proposed a new algorithm called softAdam to unify the advantages of both Adam and SGD. The authors also showed experiments to backup their theoretical results. \n\nPros:\n\nThe authors provided analysis of different algorithms including Adam and SGD on simple quadratic function, then proposed a new algorithm called softAdam which outperforms both Adam and SGD. Experiment results backup their theory. \n\nCons:\n\n- The novelty of this work is limited. The main contribution of this work is to provide a new adaptive gradient method called softAdam, which changes the update rules for some parameters including \\beta. However, neither intuition or theoretical guarantees are provided in this paper. I recommend the authors to add some explanation about why softAdam outperforms other existing algorithms. Besides, the difference between softAdam and original Adam method is little. \n- The theoretical analysis about existing adaptive methods provides nothing new. The authors showed some analysis on quadratic model, which is a very simple model and hence can not reflect the true model we face in the practice. I suggest the authors provide some analysis on more general model, including convex functions and non-convex functions. \n- The settings of experiments are limited. The authors should at least compare softAdam with other baseline algorithms on some modern deep learning tasks including Imagenet.  \n\nMinor comments:\n\n- Page 4, section 3, 'this understanding of has'... lacks object.\n- This paper lacks some references in this area. \n\nJ. Chen and Q. Gu. Closing the generalization gap of adaptive gradient methods in training\ndeep neural networks. arXiv preprint arXiv:1806.06763, 2018.\nWard, R., Wu, X. and Bottou, L. (2018). Adagrad stepsizes: Sharp convergence over nonconvex\nlandscapes, from any initialization. arXiv preprint arXiv:1806.01811 .\nLi, X. and Orabona, F. (2018). On the convergence of stochastic gradient descent with adaptive\nstepsizes. arXiv preprint arXiv:1805.08114 .\n\n",
            "This paper proposes a new algorithm which brings closer SGD and Adam while also incorporating new corrections to improve behavior of the optimizer in contexts where there is very small or very large eigen values.\n\nDecision\n\nI vote for weak rejection because the core modification proposed to Adam is minor and is mostly supported by intuition and preliminary experiments.\n\nJustification\n\nThere is many modifications proposed, but most are secondary corrections for stability, such as the warm-up schedule with the redefinition of beta_2. These modifications could as well be incorporated in Adam without the core modification that is the smoothing presented in section 3. These additional modifications also make it difficult to measure the importance of the core contribution. Without getting rid of them, an ablation study on toy problems (even synthetic data) would be necessary for a better understanding.\n\nIn section 3.1, the temporal definition of beta_2t is integrating a warm-up. While the reason for doing so is supported in introduction of section 3, the effect of this modification should be weighted against no warm-up, and also compared with its effect on Adam.\n\nThere is an error in algorithm 1. The last element of the last line (Perform the update) should be \\alpha (1 - \\eta) m_t/d_t. The code in Appendix corroborates this correction. Minor related note, the use of d_t to define the denominator of what d_i represents in section 3 is very confusing. I would suggest to use the ratio notation of d_i from the equations in the algorithm for coherence.\n\nIf we get pass the warm-up scheduling, by massaging the equation we get that the algorithm is different from Adam on 2 points, 1) the bias are not corrected and 2) the denominator sqrt(v) + epsilon is replaced by sqrt(v) + sqrt(mean(v) + epsilon^2). I have difficulty convincing myself that smoothing by the average is solving the issues raised in the paper and there is no experiments to study its effect directly.\n\nThe experiments are on 3 datasets, but only the computer vision ones are run on multiple architectures. Caption of figure 1 explains that each model is trained 3 times, but the source of variation between each run is not described. Are the models initialized differently? In any case, there is an overlap for 3 of the 5 models between SGD and SoftAdam which makes the comparison rather unconvincing. There is no standard deviation for Adam, and none on Penn Treebank dataset and IWSLT. For a better comparison, all hyper-parameters of the algorithms should be optimized for each run. I understand that SoftAdam is meant to be close to both SGD and Adam, but using the same hyper-parameters may induces misleading results by favoring some (model, optimizer) combination nevertheless.\n\nMinor comments\n\nIn section 2, second paragraph, the term 'mini-batch' should be used instead of 'batch'.\nIn section 2, last sentence, the betas should have no t.\nIn section 2.1, fourth equation (unnumbered), the eigen vector xi_i is presented as a vector and then used as a scalar. Notation should be uniformed.\nIn Section 2.1 around equation (2), the use of i and j is incoherent.\nIn Section 3:\n- Overall, this understanding *of* has\n- we consider the *an* update"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review identifies several significant weaknesses in the paper, including organizational issues, confusing notation, a weak motivation for the proposed method, and problems with the numerical results. These criticisms outweigh the single positive point mentioned.",
            "The review expresses several significant concerns, including limited novelty, lack of theoretical justification for the proposed algorithm, insufficient theoretical analysis, and limited experimental settings. The reviewer uses phrases like 'novelty of this work is limited,' 'provides nothing new,' and 'settings of experiments are limited,' indicating a negative assessment.",
            "The reviewer votes for weak rejection, citing concerns about the novelty and justification of the proposed algorithm. They criticize the modifications as being minor, supported mostly by intuition, and lacking sufficient experimental validation. The reviewer also points out errors in the algorithm and suggests improvements to the experimental setup."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct and critical language, pointing out specific flaws in the paper's writing, notation, motivation, and numerical results. Phrases like 'not well organised,' 'lacks detailed description,' 'too much details,' 'confusing,' 'weak motivation,' and 'insufficient' indicate a critical tone. The use of numbered cons further emphasizes the structured critique.",
            "The review adopts a critical tone by directly pointing out weaknesses in the paper, such as the limited novelty and insufficient theoretical analysis. Phrases like 'The novelty of this work is limited,' 'provides nothing new,' and 'The settings of experiments are limited' demonstrate a critical evaluation. The reviewer also uses 'I recommend' and 'I suggest' followed by specific improvements, suggesting a critical but constructive approach.",
            "The review expresses several criticisms, using phrases like \"minor modification,\" \"difficulty convincing myself,\" \"unconvincing,\" and pointing out errors in the algorithm. The reviewer also challenges the experimental setup and suggests improvements, indicating a critical assessment of the work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently negative, pointing out weaknesses in writing, notation, motivation, and experimental details without any contradictory statements.",
            "The review is consistent because while it acknowledges the authors' efforts in providing analysis and experiments in the 'Pros' section, the 'Cons' section consistently points out the limitations in novelty, theoretical depth, and experimental validation. The reviewer's concerns in 'Cons' directly address the weaknesses of the paper in relation to the claims made in 'Pros' and the summary.",
            "The review is consistent because the justification provided aligns with the decision of weak rejection. The reviewer identifies several weaknesses in the paper, such as the minor nature of the core modification, lack of ablation studies, questionable effectiveness of the smoothing technique, and limitations in the experimental validation. These points collectively support the reviewer's decision to vote for weak rejection."
        ]
    },
    {
        "paper_id": "iclr_2021_M88oFvqp_9",
        "paper_title": "Generating Furry Cars: Disentangling Object Shape and Appearance across Multiple Domains",
        "paper_abstract": "We consider the novel task of learning disentangled representations of object shape and appearance across multiple domains (e.g., dogs and cars).  The goal is to learn a generative model that learns an intermediate distribution, which borrows a subset of properties from each domain, enabling the generation of images that did not exist in any domain exclusively.  This challenging problem requires an accurate disentanglement of object shape, appearance, and background from each domain, so that the appearance and shape factors from the two domains can be interchanged. We augment an existing approach that can disentangle factors within a single domain but struggles to do so across domains.  Our key technical contribution is to represent object appearance with a differentiable histogram of visual features, and to optimize the generator so that two images with the same latent appearance factor but different latent shape factors produce similar histograms. On multiple multi-domain datasets, we demonstrate our method leads to accurate and consistent appearance and shape transfer across domains.",
        "review_ids": [
            "g-muCRg51kG",
            "Fdm0c6C7kjY",
            "8UBel5tLps2",
            "Cr3lUEvLAdD"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "### Summary\nThis paper proposes a generative model as an extension of FineGAN that aims to learn a disentangled representation for image shape and appearance across different domains rather than \"intra-domain\" disentanglement. To this end, the authors adopt the prior that features that correspond to an object's appearance should preserve frequency histograms. In order to incorporate this prior into the differential learning procedure, they learn a library of convolutional filters using a contrastive learning framework. They provide many convincing baselines and comparisons to related work and are able to attain reasonable results for style/content transfer between unrelated domains.\n\n### Explanation of rating\nI think this paper is a good steps towards truly being able to learn generic disentangled representations. Although the kind of data used for experiments is relatively simple, the results that are achieved go beyond existing state-of-the art generative models.\n\n### Pros\n- This provides some new insight into the kinds of disentanglements that previous generative models are (and are not) able to discover.\n- The frequency histogram assumption is a nice prior that is general enough to apply to different domains\n- The evaluation and comparisons are quite extensive and convincing.\n\n### Cons/questions\n- It might help to clarify and emphasize the novelty of the proposed method vs. the parts of FineGAN that it builds upon. For instance, the authors claim that that their method supports intra-domain disentanglement. While this is true, it seems like this is a feature of the base model and not really a contribution.\n- All of the results shown involve images with a single subject that takes up most of the canvas? How does this behave on less obvious images, e.g. with less prominent or multiple subjects?\n- How are $N_x$, $N_y$. $N_b$ chosen?\n- How is sim in eq. 1 defined?\n- What is temperature $\\tau$ in eq. 1? How is it chosen / is the method stable to choice of temperature?\n- Page 1: \"it's appearance\" -> \"its appearance\"\n- Page 2: \"acros\" -> \"across\"\n\n---\n\nThanks to the authors for the clarifications. I have read the other reviews and responses and still believe that the paper is a good contribution. Therefore, I am keeping my score.",
            "The submission describes a method to disentangle shape and appearance of images across two domains such that new images can be generated that have appearance and shape from either of these domains while still being visually convincing. Starting from an established method (FineGAN) to disentangle shape, appearance, background identity as well as a set of \"nuisance\" factors such as pose in one domain, the paper proposes to add a loss term that aims at retaining appearance when moving from one domain to another. This additional loss term essentials tries to keep the low-level image statistics between two images when both of them are generated with the same appearance, but possibly different shapes. It is trained such that it is invariant to the nuisance parameters (same object under differing views has same statistics), but discriminative towards the object appearance (different objects from the same view have differing statistics). The low level features are expressed as histograms of responses of convolution filters over the masked foreground pattern. The paper provides empirical evidences in the form of example images where appearance and shape are combined from two different domains (out of cars, birds, dogs, animals) as well as proxy measurements for the quality of the transfer: (a) how much do the low-level statistics differ in terms of $\\chi^2$ distance, (b) how well is shape disentangled under changing appearance by measuring the foreground overlap between samples, (c) a user preference study (which method transfers shape and appearance better?). The results are compared to some relevant baselines (FineGAN, CycleGan, AdaIn, MUNIT), and show moderate improvements over those.\n\n### Strengths\n**[S1]** The paper is written well and it seems that one could reproduce the method reasonably.\n\n**[S2]** The authors lay out their claims clearly.\n\n**[S3]** Based on the given motivation, retaining the low level statistics of the image, the authors derive how to model a solution, optimize and evaluate it with respect to this motivation in a structured manner.\n\n**[S4]** The empirical evaluation seems to demonstrate the effectiveness of the proposed solution towards the posed objective.\n\n**[S5]** The authors allude to the method being able to translate appearance under shape change when there is either part-part correspondence between the domains (dogs$\\leftrightarrow$cats) or none (dogs$\\leftrightarrow$cars). However, see [W2,W3].\n\n### Weaknesses\n**[W1]** While the overall motivation is fairly clear: How can I retain more appearance between domains where I do not have access to actual labeled samples for training?, the particular heuristical choice of approach, retaining the frequency statistics of low-level filter bank responses, is not well motivated. Low level filter bank response statistics are known to encode texture-like properties in the sense of repeatable patterns such as a cheetah's fur texture (Figure 5, rightmost panel). Are they sufficient to capture other properties of appearance that are less readily encoded in low-level frequency statistics? What other choices of embodying appearance, including texture, are there, and why is the choice of the presented low-level statistic favorable? I feel that the paper is lacking in setting the heuristic in context so that the reader can be confident in the choice of heuristic.\n\n**[W2]** The definition of intra- vs. inter-domain seems somewhat vague. It would be helpful to characterize this distinction either more theoretically or empirically: When are two domains close enough so that I do not need the additional term, when are they too far apart for even this method to work? Cats and dogs compared to cats and cars seem qualitatively different concept relations. The authors do imply hierarchies of closeness of domains (e.g. Section 2: having part level or no part level correspondences), but this is not used in the paper to clearly design or evaluate the method with respect to differing inter/intra-domain distributions or levels of domain proximity.\n\n**[W3]** Weakly supported claim: Section 2 claims \"Moreover, when part-level correspondences do exist (e.g., dogs \u2194 tiger), it combines appearance and shape in a way which preserves them.\" This indeed could be a strong point, see W2. As I read the paper however, this claim seems only anecdotally supported, e.g. by individual samples in figures 5/6. I feel that more thorough investigation and evidencing of this claim would strengthen the paper.\n\n**[W4]** Reference missing: I feel that [B1] below does indeed discuss and investigate related concepts, albeit from the perspective of style rather than object appearance. Nevertheless, it discusses disentanglement under disjoint domains, and it would be interesting to include this in the discussion of related work.\n\n**[W6]** I read the submission as more of an empirical paper than theoretical paper. From this perspective, the presented empirical evidence seems limited. In order to gauge the effectiveness and benefit of the method better, I wished for a wider range of data, where I can see the relation of shape/appearance transfer and the \"distance\" of the domains better. The authors do mention, but do not show for instance furniture. Can I transfer from cups to people or vice versa?\n\n### Further comments\n**[C1]** There is a flipped $\\pm$ in table 2 (\"Ours vs. FineGAN / dogs $\\leftrightarrow$ birds\")\n\n### Summary\nI feel that there is benefit to the direction that the submission is taking. Although at this point the weaknesses outweigh the strengths, a revision could make a strong contribution if for instance the choice of heuristic is motivated and evidenced more strongly in the context of potential alternatives, and the intra-/inter-domain distinction is worked out more clearly theoretically and/or empirically. As it is I feel that the paper would need significant revision for acceptance at ICLR.\n\n**[B1]** @InProceedings{Lee_2018_ECCV,\nauthor = {Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh and Yang, Ming-Hsuan},\ntitle = {Diverse Image-to-Image Translation via Disentangled Representations},\nbooktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},\nmonth = {September},\nyear = {2018}\n}",
            "The authors build upon prior work (FineGAN) in intra-domain disentanglement to extend to inter-domain transfer of separate attributes. Since no ground truth data exists for inter-domain transfer, they use contrastive losses to enforce similar statistics of low-level filter activations (averaged over the image) as a proxy for appearance similarity. \n\nAppearance transfer experiments on performed on a good selection of datasets and the results of the proposed method represent a qualitative step forward in unsupervised conditional generation across domains. Quantitative metrics support this point.\n\nThat said, the paper could be significantly improved by spending less space motivating and defining the problem, and more space describing the actual method used. The authors mention FineGAN in passing as their base model, but it is essential to the proposed method and could use further elaboration. As is, the relevant details of the losses and architecture choices are not contained within the paper itself. For instance, the loss terms in L_{base} are not defined and no diagrams are given to help understand the workings of the base model.  Similarly, the training procedure is a bit unclear from the text. If the content of Figure 3 were expanded, or a training algorithm table provided, even in the supplemental it would significantly improve the paper by not relying on a reference to provide the description of the core technique.\n\n",
            "--Summary:\nThe paper proposed a method to learn disentangled representation of shape and appearance for cross-domain (different object categories) data. Build upon FineGAN, the method uses contrastive learning combined with normalized temperature-scaled cross-entropy loss to further disentangle the shape and appearance information.\n\n--Strongness:\n1. The model is slightly novel. They combine contrastive learning with normalized temperature-scaled cross-entropy loss to learn the filter bank to construct the appearance feature histogram.\n2. They perform many experiments including comparisons with baselines and ablation studies on the proposed loss terms. They demonstrate the effectiveness of their approach to generating hybrid images.\n3. The paper is well-organized.\n\n--Weakness:\n1. The motivation is still unclear. I still don't get the point for the usefulness of appearance transfer across two different types of objects (e.g. car and animal) which they claim as their contribution. For example, I don't see the application for applying car appearance to animals.\n2. The comparison baselines are too old. For the appearance transfer comparisons as shown in Figure 4 are the papers before 2018. For example, why don't you compare your model with StarGANv2[1] which also demonstrates appearance transfer to different shapes (e.g. Figure 10)?\n\n--Questions:\n1. I'm curious about the results if you replace the histogram method by just using CNN to extract features on the masked output from FineGAN?\n\n--Recommendation:\nAlthough the authors demonstrate the effectiveness of the proposed method, there are some concerns to be addressed: \n1) Motivation is not intuitive. \n2) There are many more recent papers for transferring appearance to another shape, e.g. StarGANv2[1], which is not included in the experiment.\n\nI currently vote negatively but the authors are strongly encouraged to address these concerns.\n\n[1] StarGAN v2: Diverse Image Synthesis for Multiple Domains, CVPR'20"
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states \"I think this paper is a good steps towards truly being able to learn generic disentangled representations\" and \"the paper is a good contribution\". They also mention that the results \"go beyond existing state-of-the art generative models\".",
            "The review expresses both positive and negative aspects of the paper, indicating a balanced assessment. The reviewer acknowledges the paper's strengths in writing, clarity, and empirical evaluation, but also points out weaknesses in motivation, definition, claim support, references, and empirical evidence. The overall recommendation is for significant revision, suggesting neither strong acceptance nor rejection.",
            "The review acknowledges that the research builds upon previous work and achieves a qualitative step forward in unsupervised conditional generation across domains, supported by quantitative metrics. This indicates a positive assessment of the research's contribution.",
            "The reviewer expresses concerns about the motivation and comparison baselines, ultimately voting negatively, indicating a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses positive language such as \"good steps\", \"convincing baselines\", \"reasonable results\", and \"nice prior\". The reviewer also thanks the authors for the clarifications and maintains their positive score after reading the other reviews and responses.",
            "The review uses a mix of supportive language when discussing strengths (e.g., \"The paper is written well,\" \"The authors lay out their claims clearly\") and critical language when discussing weaknesses (e.g., \"not well motivated,\" \"somewhat vague,\" \"Weakly supported claim\"). This balanced approach creates a neutral tone.",
            "The review begins with positive feedback, praising the results and methodology. However, it also points out areas for improvement, such as the need for more detailed explanations of the method and base model, using phrases like 'could be significantly improved' and 'essential to the proposed method'. This balanced approach indicates a fair assessment of the paper's strengths and weaknesses.",
            "The review points out weaknesses in the paper, such as the unclear motivation and outdated baselines, using phrases like \"motivation is still unclear,\" \"baselines are too old,\" and \"I currently vote negatively.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive. The reviewer summarizes the paper positively, explicitly states it's a good step and beyond state-of-the-art, lists pros that highlight novelty and good evaluation, and while raising cons/questions, maintains a positive stance and keeps the score after author clarifications. There are no contradictions in the review, as the cons are framed as questions and areas for improvement, not fundamental flaws that would contradict the positive overall assessment.",
            "The review is consistent in its assessment. It highlights both strengths and weaknesses of the paper, providing specific points for each. The summary aligns with the detailed points, indicating that while the direction is promising (strengths), significant revisions are needed to address the weaknesses. There are no contradictory statements within the review; the criticisms and praises are logically separated and contribute to a coherent overall evaluation.",
            "The review is consistent because it acknowledges the paper's strengths (qualitative and quantitative improvements) while also pointing out specific areas for improvement (lack of detail in method description, reliance on external references for core techniques). The reviewer's points are all focused on improving the clarity and completeness of the paper, without contradicting themselves.",
            "The review is consistent because the reviewer identifies both strengths and weaknesses of the paper. The negative recommendation is logically derived from the identified weaknesses, such as unclear motivation and outdated baselines, despite acknowledging the paper's strengths in novelty, experiments, and organization. There are no contradictory statements within the review."
        ]
    },
    {
        "paper_id": "iclr_2019_HJMCdsC5tX",
        "paper_title": "A fully automated periodicity detection in time series",
        "paper_abstract": "This paper presents a method to autonomously find periodicities in a signal. It is based on the same idea of using Fourier Transform and autocorrelation function presented in Vlachos et al. 2005. While showing interesting results this method does not perform well on noisy signals or signals with multiple periodicities. Thus, our method adds several new extra steps (hints clustering, filtering and detrending) to fix these issues. Experimental results show that the proposed method outperforms the state of the art algorithms. ",
        "review_ids": [
            "Hye3POxspm",
            "SylwnL-I6m",
            "B1lxdWw927"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors present a heuristic method to detect periodicity in time series. It extends a previous approach in dealing with noise and the setting of multiple periodicities.\n\nThe topic does not match the scope of ICLR and would be better suited for a different venue.\n\nThe method is demonstrated in a purely experimental fashion. However, without detailed inspection of the datasets it remains unclear in what cases the heuristics apply and where they fail. A more thorough analysis of the robustness of the algorithm is necessary, in particular a detailed presentation of failure cases.",
            "This paper focuses on the extraction of the (multi) periodicities from a signal. The paper describes the conventional method based on the Fourier transformation and/or autocorrelation methods, and proposed method, which first detects a distribution of spectral leakages, and prune the periodicity hints by using a clustering algorithm. The proposed method is also extended to deal with multi-periodicities. The effectiveness of the proposed method is shown with the controlled simulation data and several real data. This paper is well written (note it is over 8 pages though), but it is not learning-based approach, and would not best fit to major ICLR interests.\n\nComments:\n- The abstract needs to be more self-consistent without referring the citation for a brief explanation. Also it should have more detailed experimental discussions.\n- Algorithm 1 needs some refinement (too code-like, although it is understandable). For example, several methods (nextBinValue and append) would be better to be replaced with other (human readable) expressions.",
            "This paper introduces a method to do period detection. It builds off of the autoperiod method by adding density clustering, a lowpass filter, and a linear detrending after auto correlation.\n\nThe results section was very vague. The process of  generating the synthetic signals was not specific. There were no visualizations, which would help the reader understand how this method performs better. Visualizations would have been especially useful for the real datasets.\n\nHaving said this, I don't think this paper is fit for this conference."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses negative sentiment due to the statement that the topic is outside the scope of the conference and the criticism regarding the lack of thorough analysis and unclear applicability of the method.",
            "The review acknowledges the paper is well-written and effective, but also points out a major drawback: it's not learning-based and thus may not align with ICLR's focus. This mix of positive and negative aspects results in a neutral sentiment.",
            "The review expresses dissatisfaction with the paper's vagueness, lack of specific details in the methodology, and absence of visualizations. The concluding statement explicitly states the paper is not suitable for the conference."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical as the reviewer states that the topic doesn't match the scope and that the analysis is insufficient ('without detailed inspection it remains unclear', 'A more thorough analysis is necessary').",
            "The review offers both positive feedback (well-written, effective method) and constructive criticism (abstract improvements, algorithm refinement, relevance to ICLR). This balanced approach indicates a neutral tone.",
            "The tone is critical due to phrases like \"very vague,\" \"not specific,\" and the overall negative assessment culminating in the statement \"I don't think this paper is fit for this conference.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the contribution of the work (extending a previous approach) but raises concerns about the suitability of the topic for ICLR and the lack of a thorough robustness analysis, particularly regarding failure cases. These points are logically connected and do not contradict each other; the reviewer is consistently arguing for rejection from ICLR based on scope and insufficient evaluation.",
            "The review is consistent because it acknowledges the paper's strengths (well-written) while also pointing out areas for improvement (abstract, algorithm description) and commenting on the paper's suitability for the ICLR conference. There are no contradictory statements within the review.",
            "The review is consistent because the reviewer identifies weaknesses in the results section, such as vagueness and lack of visualizations, and logically concludes that the paper is not suitable for the conference based on these weaknesses. The negative conclusion aligns with the criticisms raised."
        ]
    },
    {
        "paper_id": "iclr_2022_6y2KBh-0Fd9",
        "paper_title": "Revisiting flow generative models for Out-of-distribution detection",
        "paper_abstract": "Deep generative models have been widely used in practical applications such as the detection of out-of-distribution (OOD) data. In this work,  we aim to re-examine the potential of generative flow models in OOD detection. We first propose a simple combination of univariate one-sample statistical test (e.g., Kolmogorov-Smirnov) and random projections in the latent space of flow models to perform OOD detection.  Then, we propose a two-sample version of our test to account for imperfect flow models. Quite distinctly, our method does not pose parametric assumptions on OOD data and is capable of exploiting any flow model. Experimentally, firstly we confirm the efficacy of our method against state-of-the-art baselines through extensive experiments on several image datasets; secondly we investigate the relationship between model accuracy (e.g., the generation quality) and the OOD detection performance, and found surprisingly that they are not always positively correlated; and thirdly we show that detection in the latent space of flow models generally outperforms detection in the sample space across various OOD datasets, hence highlighting the benefits of training a flow model.",
        "review_ids": [
            "RNFKWpsIeWY",
            "bba6HqjCSm",
            "dF6ytmstlAm",
            "lvELoStMHKI",
            "EKx2bxVwHP",
            "wwGhz24elvD",
            "sje3NF3KoO"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thanks for your revisions, in my view they have improved the manuscript, therefore I increased the score to accept.",
            "The paper proposes evaluates using statistical tests on random 1d-projections in the latent space of a flow model for groupwise out-of-distribution (OOD) detection. Concretely, Komolgorov-Smirnov tests are used to compare latent encodings of a given batch of samples to the expected distribution of the in-distribution latent encodings. For the expected distribution, either the predefined latent prior is used, resulting in a 1-sample Komolgorov-Smirnov test, or the empirical distribution of the latent encodings of the training data  is used, resulting in a 2-sample Komolgorov-Smirnov test. The paper reports similar groupwise OOD detection performance as existing methods, with the 2-sample KS-test method having less failure cases. I found it easy to understand the ideas and methods of the paper, results are also well-understandable.\n\nOverall, the paper does not bring too many surprises from my view - I would have expected any use of 1d projections for OOD detection to work better in latent space than in input space. Also it seems not too surprising that this very generic method cannot result in very high OOD detection performance (compared to methods exploiting more assumptions about OOD data). But in any case good to see precisely how well OOD detection works in this way. The main surprising part might be that generation quality can be negatively correlated with OOD detection performance.\n\nThe manuscript would benefit from discussing other OOD detection approaches in latent space, https://arxiv.org/abs/2006.10848 also present one method using parts of the latent space of the Glow model, https://arxiv.org/abs/2102.08248 uses VAEs, not flows, but also look at parts of the latent space there. In the future, could be interesting to compare to such methods (using parts of latent space/log-likelihood contributions), which exploit more assumptions about where semantic differences are encoded in the latent space, on both semantic and non-semantic OOD detection. Note also that https://arxiv.org/abs/2006.10848 report a related result to the result that generation quality and OOD detection performance can be negatively correlated, finding that fully connected flow models have worse log likelihoods but better OOD detection performance (see Sec 2).\n\nI would like it if caption figures were more self-contained and figures could be understood separately from the text. E.g., figure 3 and figure 5 captions could also talk a bit more about what one could learn from the figures. The paper in my view does not present too many surprising results, but I also find it valuable to see non-surprising results that are clearly presented and easy to understand. And the result on generation quality and OOD detection performance being negatively correlated is also valuable to know.\n\n**Update Post Rebuttal**\nThe revisions have further improved the manuscript, I increase my score to 8.",
            " Thank you for your responses. I am entirely satisfied with your answers. Therefore, I am going to raise my score to (8: accept, good paper). ",
            "The paper provides an interesting approach for OOD detection problem for flow models. The approach is based on random projections on the real line where KD statistical test is applied in order to compare two distributions. Two variants of the approach are considered 1 sample test where the comparison is made with respect to flow prior and 2 sample test where the comparison is made with respect to transformed samples from two datasets. The quality of the approach is compared agains standard reference methods. \n Strengths\n\nThe proposed approach is simple and does not require training additional components. It can be applied both in data space and latent of the flow, but according to experimental studies application of latent space gives better results. The results are comparable to the results obtained to the reference approaches - the significant gain is observed on CIFAR10 vs SVHN setting. The paper is easy to follow and the idea of the paper is clear. The proposed approach is easy to applied so the results are easy to reproduce.\n\nWeaknesses \n\nThe contribution of this work is smart but novelty is a bit limited due to the fact that the authors combine two known approaches - KS statistical test and random projections. It seems that this framework is general and can be even applied to the bottleneck models like VAE or even for any low dimensional features extracted from the data. The question is can we apply this framework to any low dimensional representation of data or we need the invertible mapping that does not decrease the dimension and direct access to the likelihood. \n\nThe second question is about Carleman\u2019s condition. Considering the case where we use flow\u2019s latent is it sufficient that it would be satisfied by Gaussian (flow latent distribution) or it should be satisfied by data distribution (the normalised Gaussian by determinant of the Jacobian)?\n\nThe third issue is about the results. The model seems to be comparable to KLOD besides the CIFAR10 vs SVHN where it performs better. On the other hand, it has problems with CIFAR vs LeSUN setting. Do you have some intuition what is the reason and how to deal with that issue? Most of the experiments that examine the OOD problem are performed between different datasets. It would be beneficial to see some analysis for inside dataset split, where the split is indicated by two groups of classes. This case is somehow similar to Cifar10 vs. Cifar100 comparison.  I admire the simplicity of the approach and I tent to recommend to accept this work at this stage. ",
            " Thank you on interesting discussion!\n\nThe proposed motivation is somewhat misaligned with the presented experiments on natural images. Still, new experiments on EEG/ECG signals confirm applicability of the proposed method for real-world problems.\n\nIt would be a good idea to correct the title to avoid disappointment of the readers interested in point-wise OOD with normalizing flows. A more informative title would be simply: REVISITING FLOW GENERATIVE MODELS FOR GROUP-WISE OUT-OF-DISTRIBUTION DETECTION\n\nI am leaning towards keeping my original rating (weak accept).",
            "This paper applies a goodness of fit test (KS test) to the latent space of normalizing flows for purposes of out-of-distribution detection.  To combat high-dimensionality and model misspecification, extensions such as random projections and two-sample tests are also proposed.  The results report AUROC on RNVP (batch size 5 and 10), comparing against a typicality test and a kl-based test.  The paper also tests using autoencoders vs rand projections, alternative divergences, effects of model misspecification, and detection in latent vs original feature space.  The findings are that the KS-test is indeed a practical choice for OOD detection via GOF testing.  Moreover, it is observed that better models don't necessarily mean better OOD detection, as described earlier by Zhang et al. [ICML 2021]. ## Pros\n\n*Exploration of OOD in practice*:  While GOF-based OOD detection is theoretically well-founded, applying the technique successfully in practice for high-dimensional data is a challenge in its own right.  Thus, this paper provides value to practitioners, guiding them on how to make these crucial implementation choices.\n\n*Ablations*: The paper also does a nice job of testing the efficacy of specific parts of the experimental pipeline---for example, autoencoders vs random projections, GOF vs two-sample tests.\n\n## Cons\n\n*Novel only in implementation*:  The method's novelty is restricted to the implementation details.  Ultimately, the core methodology is to apply the KS test to a reparameterized distribution, which is not an original contribution.  Applying a KS test to flows for OOD purposes is novel, to the best of my knowledge.  Moreover, the paper's other insights such as model quality vs OOD ability mostly back already supported points from earlier work (e.g. [Zhang et al, ICML 2021]. The paper is fairly well executed; however, I can't say that I learned much from the work as its contribution is mostly in the implementation details.  And it is unclear to what extent these details generalize beyond standard image benchmarks.",
            "The manuscript addresses groupwise outlier detection with normalizing flows. The main idea is to project latent representations of the input batch onto several random directions and to evaluate the resulting 1D distributions according to the Kolmogorov-Smirnov test either with respect to the prior (GOD1KS) or with respect to the corresponding distribution of the training data (GOD2KS). Finally, OOD ranking is performed according to the average KS value across all random directions. GOD1KS is used when the underlying normalizing flow is well learned, while GOD2KS is an option when there is some uncertainty regarding the learning outcome. Experimental performance decreases  when the method is used in combination with undertrained normalizing flow or a normalizing flow with a larger capacity. Strengths:\n\nS1. The proposed approach is interesting, simple, mathematically sound and effective. In comparison with Zhang et al. (2020), the proposed approach does not require estimation of a multivariate distribution.\n\nS2. Experiments show robust overall performance. Notably, there are less failure cases than in related methods.\n\nWeaknesses:\n\nW1: Authors does not propose a compelling motivation for groupwise OOD detection.\n\nW2: The evaluation should also show the OOD AP metric as in Zhang et al. (2020). \n\nSuggestions\n- Can you offer an intuitive explanation for the failures of TyTest and KLOD in Table 1?\n- It may be interesting to evaluate the average density across the input batch as a straight-forward baseline. I do not expect this to be competitive, but averaging may improve over the pointwise result.\n- Can you provide any insight why the method can not exploit normalizing flows with large capacity?\n- Include algorithmic presentation of GOD2KS in the supplement (for completeness).\n- Page 2: absolute value of the Jacobian -> absolute value of the determinant of the Jacobian.\n- Figure 1, caption: explain S_0 and S_1.\n- Page 3, \"in order to evade the curse of dimensionality\": this becomes especially relevant in contemporary normalizing flows (eg. VFlow, DenseFlow) where latent dimensionality is greater than input dimensionality.\n- Page 3, \"is distinct from the distributions\": make clear that this refers only to Fig1(b).\n- Figure 2, caption: does not follow -> follow.\n This paper provides interesting insights about normalizing flows and OOD detection. "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Neutral",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states that the revisions have improved the manuscript and that they have increased the score to accept, indicating a positive assessment.",
            "The reviewer expresses overall positive sentiment. They state the paper is easy to understand, the results are well-understandable, and that the revisions further improved the manuscript. The reviewer also explicitly increases their score.",
            "The reviewer explicitly states they are \"entirely satisfied\" with the responses and will \"raise my score to (8: accept, good paper).\" These phrases indicate a positive sentiment.",
            "The reviewer states \"I admire the simplicity of the approach and I tent to recommend to accept this work at this stage.\"",
            "The review expresses both positive feedback (\"Thank you on interesting discussion!\") and negative feedback (concerns about the misalignment of motivation and experiments). The reviewer suggests improvements, indicating a balanced perspective.",
            "The review acknowledges the practical value of the paper while also pointing out limitations in novelty and generalizability, resulting in an overall neutral assessment.",
            "The review expresses an overall positive sentiment. It highlights the strengths of the paper, such as the interesting approach, simplicity, mathematical soundness, effectiveness, and robust performance. The concluding sentence also indicates a positive view."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses gratitude ('Thanks for your revisions') and explicitly states that the manuscript has improved, indicating a supportive and encouraging tone.",
            "The review provides both positive and negative feedback. While the reviewer appreciates the clarity and understandability of the paper, they also point out limitations such as the lack of surprising results and the need for more self-contained figures. The reviewer also provides constructive suggestions for improvement, indicating a balanced and critical tone.",
            "The reviewer expresses satisfaction and acceptance, using phrases like \"entirely satisfied\" and \"good paper,\" which conveys a supportive tone.",
            "The reviewer highlights the strengths of the paper, such as its simplicity, ease of application, and comparable results. The reviewer also expresses a willingness to recommend the work for acceptance, indicating a supportive tone.",
            "The tone is balanced, acknowledging the interesting discussion while also pointing out a misalignment and suggesting a title change. The reviewer also mentions leaning towards keeping the original rating, indicating a measured approach.",
            "The review presents both positive aspects ('Pros') and negative aspects ('Cons') of the paper. It acknowledges the practical value and ablations while also criticizing the lack of novelty and unclear generalizability. This balanced approach indicates a neutral and objective tone.",
            "The review presents both strengths and weaknesses of the manuscript. It offers constructive suggestions for improvement, indicating a balanced perspective. The language used is generally formal and objective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer explicitly states that the revisions improved the manuscript, leading to an increased score and acceptance recommendation. There are no contradictory statements or conflicting opinions within the review.",
            "The review maintains a consistent positive tone, appreciating the clarity and understandability of the paper, while suggesting constructive improvements. The reviewer's final score increase reinforces the initial positive assessment.",
            "The reviewer expresses satisfaction with the responses and consequently decides to raise the score to accept the paper. This is a consistent line of reasoning.",
            "The review is consistent because it acknowledges both the strengths and weaknesses of the paper. While pointing out limitations such as limited novelty and mixed experimental results, the reviewer also praises the simplicity and clarity of the approach and ultimately recommends acceptance. The questions raised are constructive and do not contradict the overall positive inclination towards the work.",
            "The review points out a misalignment between motivation and experiments but also highlights the positive results on EEG/ECG signals, suggesting applicability to real-world problems. The suggestion to change the title is constructive. The final leaning towards 'weak accept' aligns with this mixed but overall positive assessment, indicating consistency in the reviewer's evaluation.",
            "The review is consistent because the reviewer provides both positive and negative points that logically contribute to an overall assessment of the paper. While pointing out the lack of novelty in the core methodology, the reviewer acknowledges the novelty in applying the KS test to flows for OOD detection and appreciates the practical value and ablations. The reviewer's critique focuses on the implementation-centric novelty and questions the generalizability, which aligns with the initial statement about limited novelty.",
            "The review is consistent because it highlights both the strengths and weaknesses of the manuscript in a balanced way. The reviewer acknowledges the interesting and effective approach while also pointing out areas for improvement, such as the motivation and evaluation metrics. The suggestions are constructive and aim to improve the paper, not to dismiss its value. There are no contradictory statements or conflicting opinions within the review."
        ]
    },
    {
        "paper_id": "nips_2022_E28hy5isRzC",
        "paper_title": "Entropy-Driven Mixed-Precision Quantization for Deep Network Design",
        "paper_abstract": "Deploying deep convolutional neural networks on Internet-of-Things (IoT) devices is challenging due to the limited computational resources, such as limited SRAM memory and Flash storage. Previous works re-design a small network for IoT devices, and then compress the network size by mixed-precision quantization. This two-stage procedure cannot optimize the architecture and the corresponding quantization jointly, leading to sub-optimal tiny deep models. In this work, we propose a one-stage solution that optimizes both jointly and automatically. The key idea of our approach is to cast the joint architecture design and quantization as an Entropy Maximization process. Particularly, our algorithm automatically designs a tiny deep model such that: 1) Its representation capacity measured by entropy is maximized under the given computational budget; 2) Each layer is assigned with a proper quantization precision; 3) The overall design loop can be done on CPU, and no GPU is required. More impressively, our method can directly search high-expressiveness architecture for IoT devices within less than half a CPU hour. Extensive experiments on three widely adopted benchmarks, ImageNet, VWW and WIDER FACE, demonstrate that our method can achieve the state-of-the-art performance in the tiny deep model regime. Code and pre-trained models are available at https://github.com/alibaba/lightweight-neural-architecture-search.",
        "review_ids": [
            "t3lLBZpqSu9",
            "LsEb7vRccOI",
            "DNsmRaM7D5v",
            "J0PfAuR_Z6g",
            "Q1Q-Oeg-VKM"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I sincerely thank the authors for providing detailed response. Most of my concerns have been addressed and thus I decide to increase my score. ",
            " This paper proposes an entropy-driven mixed-precision quantization strategy. The basic idea is to formulate the neural network as an information system, and then define a quantization entropy score to capture the expressiveness of the system. Finally, the Quantization Bits Refinement algorithm quantizes the activation and weights to meet the peak memory and model size constraint. Strength:\n1. The concept of the quantization entropy score looks interesting.\n2. The proposed solution outperforms SOTA works.\n\nWeakness:\n1. The proposed solution does not work for GPU.\n\n\n 1. The QBR strategy looks heuristic, is it possible to fall into an oscillation process?\n2. Why only 8 bit and 2 bit in Figure 7? The proposed QE-scores can only use CPU resources instead of GPU.",
            " This paper presents an entropy-driven mixed-precision quantization strategy to design efficient CNNs for resource constrained IoT devices. The paper proposes a new metric, Quantization Entropy Score (QE-Score), to evaluate the CNN model capacity without the need of training the model. QE-Score is applicable when different layers are quantized to different bit widths, and thus can significantly speedup the design space exploration for efficient models. Extensive experiments have been performed to show that the proposed quantization strategy discovers tiny models with higher accuracy under strict resource constraints.  ### Originality\n\nWhile the connections between deep learning models and information systems have been studied and the entropy has been related with accuracy in the literature, it appears to be an original idea to evaluate CNN model capacity via a single QE-Score metric under the quantization scenario. \n\nThe discussion on the relation among quantization budget, neural architecture, initialization and entropy (accuracy) is original and useful. \n\n\n### Quality\n\n**Strengths**\n\nThe proposed QE-Score seems to be a simple yet effective metric to estimate the model capacity. This shows potential for practical IoT deployment. \n\nThe evaluation of the proposed strategy is extensive and covers various aspects. \n\nNecessity of mixed-precision quantization is clearly illustrated by the MobileNet layer breakdown in Fig 2. \n\n\n**Weaknesses**\n\nVarious technical descriptions in Section 3 are unclear, and I am challenged to interpret some of the plots. For example:\n* Equation 6: what variable is the summation applied on? Q based on its different value ranges?\n* Fig 6: it appears to me most subplots can show positive correlation between QE-Score and accuracy. What is so special about the plot of $\\sigma_A=5$ and $\\sigma_W=4$ plot? Why do you say that \"QE-Scores are gradually positive(ly) correlated with accuracy until $\\sigma_A=\\sigma_W=4$ Then, we adjust $\\sigma_A=5$ and $\\sigma_W=4$ to rank the diversity of activations and weights on accuracy\"? What do you mean by ranking the diversity of activations and weights?\n* The Quantization Bits Reinforcement (QBR, Fig 7) strategy is very unclear. Where do you explain this figure? Why do you need this refinement when you already have QE-Score to guide the allocation of bit-widths? You should at least briefly introduce this algorithm in the main text. \n\nThe fundamental assumption on the system is unclear: To execute the CNN on IoT devices, do you need the full model to fit in its memory? Or do you only need a single layer (the currently executed layer) to fit in the memory? From Fig 2, it seems that you are imposing a layer-wise constraint. From other parts of the paper, it seems that the constraints are on the full CNN. \n\nFrom the description, it seems that the main purpose of QE-Score is to simplify neural architecture search (e.g., channel widths, bit widths). Then why does initialization of X and W matter? Specifically, since you will train the model after fixing the architecture, the effect of different initialization scales will be automatically addressed by batch-norm or layer-norm. \n\nWhile the empirical results seem to justify QE-Score, its derivation seems problematic. Activation has a major impact on the distribution. For example, with ReLU, half of values will be cropped to 0 if the input follows 0-mean Gaussian. With a deep CNN, the final layer distribution will be far from Gaussian. Thus the derivation of QE-Score does not follow a realistic assumption. \n\n\n### Clarity\n\nThe initial portion of the paper is clear to motivate mixed-precision quantization. However, the technical description is overall unclear (see details above). \n\nTables and figures in experiments clearly show the advantages of the proposed strategy over the baseline w.r.t. various different metrics. \n\n\n### Significance\n\nJudging from the empirical gains, the proposed method makes a significant contribution to enable efficient IoT inference with compressed models. \n\nHowever, evaluations are mostly performed on a single backbone model (MobileNet). It is unclear if the proposed method can generalize to other CNN or non-CNN models. \n\nThe technical significance seems to be limited. First of all, the QE-Score and the proposed algorithm are both mostly heuristic based. Secondly, due to the questions on the technical sections (listed above), it is unclear if the proposed method brings in-depth insights to the community. \n Please clarify the questions regarding the technical descriptions (see comments on \u201cQuality\u201d). \n\nPlease provide more details on the main algorithm steps (e.g., Quantization bits refinement, Evolution algorithm). \n The paper does not discuss potential negative societal impact. ",
            " Previous works use an inefficient two-stage method that quantizes after designing a small network. However, the proposed method uses a one-stage method that can jointly optimize network design and mixed-precision quantization. Therefore, it is not only efficient in terms of time-cost, but also achieved high performance in various tasks. Strengths:\n1) The ranking strategy from the perspective of entropy and Quantization Bits Refinement in the evolutionary algorithm considering the HW budget are interesting and novel.\n2) This paper clearly defined the problems and demonstrated solutions through solid mathematical analysis. \n3) This paper is very clear and easy to read. I think that it would help both researchers related to this field and general readers to give a little bit more background on TinyML.\n4) The experiment showed the superiority of the proposed method on various datasets (ImageNet, VWW, WIDER FACE).\n\nWeaknesses:\n1) For all experiments, MobileNet-V2 was used as the base model. In particular, the analysis of peak memory and model size presented in Section 2 is also limited to MobileNet-V2. However, when a different model is used, the peak memory for each layer will also be different. Can we assume that the results of MobileNet-V2 are representative? The authors should show that the proposed method is compatible with other models.\n2) Since the key contribution asserted in this paper is the 1-stage approach, it is necessary to thoroughly compare the proposed method with the 2-stage methods. In other words, comparative experiments with more combinations of SOTA network design studies + SOTA mixed-precision studies should be presented.\n3) Experimental results in terms of power (energy) consumption should be presented. As the authors know, the most important factor in an IoT device is energy consumption.\n4) It is necessary to explain whether the proposed method is compatible with more complex networks (e.g., SSD, instance segmentation, Transformer). (If possible, the experimental results need to be presented together.) Also, it is difficult to understand which network was used for the current Tiny Object Detection experiment.\n5) [1] was used as a quantization-aware training (QAT) method. However, model performance may be improved by [1] in this paper. So, the current results cannot clearly show the effectiveness of the proposed method. Further experiments excluding the impact of QAT should be presented.\n6) In this paper, only SRAM and Flash are used as constraints, but it seems that DRAM should be considered.\n7) Why didn't the authors use the commonly used bit-precision configuration {2, 4, 8, 16} in the mixed-precision setting? Additional explanations related to this should be provided.\n8) In Table 3, the appropriate reference should be added on which tool was used to measure CO2 emission.\n\n[1] Learned step size quantization. ICLR\u201920. 1) Can you provide applicability and experimental results for networks other than MobileNet-V2?\n2) Can you show additional experimental results related to Weaknesses 2), 3) and 5)?\n3) Can you provide additional experimental results and explanations regarding the compatibility of the proposed method?\n4) Can you provide additional explanations related to Weaknesses 6) and 7)? The authors have adequately addressed the limitations and potential negative societal impact of their work.",
            " The paper proposes a new mixed-precision quantization in a new perspective of information entropy. Since the proposed Quantization Entropy Score can approximately measure the expressiveness of neural networks and is easy to compute, the authors can do the search in a short time. Strengths\n1. The paper is well developed and organized. The tables and figures are great.\n2. The authors extend the previous work of viewing neural networks from the perspective of information entropy. Since quantization can be treated as an operator, it is natural to apply this perspective to the quantization problem.\n3. The experiments are convincing. The authors cover three benchmarks.\n\nWeaknesses\n1. I am wondering if there is any work on using the entropy-based scores to conduct neural architecture search (NAS). If yes, the related work should be discussed and compared. If no, it should be a contribution of this paper. The authors should also show that the score can be used to search for a full precision model efficiently.\n2. How can the proposed method extend to other machine learning models, such as Transformers? 1. Since the search cost is very low, why do the authors expand the search space? We may expect to find better results in a larger solution space. For instance, could you relax the constraints mentioned in Line 186 (three layers in each block share the same precision value)?\n2. In Table 5, why does the QBR achieve better results? From my understanding, the QBR uses prior knowledge to shrink the search space. Without QBR, we should explore a larger space and achieve better results than that with QBR.\n3. Could the authors discuss other hardware-related metrics, such as latency, throughput, and energy?\n Limitations and social impacts are well discussed in Appendix E and F."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states that most of their concerns have been addressed and that they will increase their score, indicating a positive change in their evaluation of the work.",
            "The review highlights the interesting concept of the quantization entropy score and the outperformance of the proposed solution compared to SOTA works as strengths. While weaknesses are pointed out, the overall assessment leans towards a positive evaluation.",
            "The review expresses both positive and negative feedback. It highlights the originality and potential of the QE-Score but also points out issues with clarity, technical descriptions, and the underlying assumptions. The reviewer acknowledges the empirical gains but questions the technical significance and generalizability.",
            "The review identifies numerous weaknesses in the paper, including limitations in experimental setup, lack of comparisons, missing energy consumption analysis, and concerns about the compatibility and generalizability of the proposed method. The reviewer also raises questions about the experimental settings and constraints used.",
            "The review highlights several strengths of the paper, including its well-developed organization, great tables and figures, extension of previous work, and convincing experiments. While weaknesses are pointed out, the overall tone suggests a positive evaluation."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"sincerely thank the authors\" and \"concerns have been addressed,\" which convey a supportive and appreciative tone.",
            "The review presents both strengths and weaknesses of the paper, using objective language to describe the proposed solution and its performance. It also raises specific questions, indicating a critical yet fair assessment.",
            "The review uses a mix of supportive and critical language. It acknowledges the strengths of the paper, such as the potential for practical IoT deployment and the extensive evaluation. However, it also raises specific weaknesses, such as unclear technical descriptions and problematic derivations. The reviewer provides constructive feedback and requests clarification on several points, indicating a balanced assessment.",
            "The review uses direct and pointed questions, such as 'Can we assume that the results of MobileNet-V2 are representative?' and 'Why didn't the authors use the commonly used bit-precision configuration {2, 4, 8, 16}?' The reviewer also uses strong words like 'necessary' and 'thoroughly' when discussing the need for more comparative experiments.",
            "The review presents both strengths and weaknesses of the paper in a neutral and objective manner, using phrases like 'Strengths' and 'Weaknesses' to clearly delineate positive and negative aspects. The reviewer also uses questions to probe areas of improvement, rather than making outright criticisms."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer explicitly states that their concerns have been addressed by the authors' response, which logically leads to the decision to increase the score. There are no contradictory statements or illogical transitions in the review.",
            "The review is consistent because the strengths and weaknesses are distinct points that do not contradict each other. The strengths highlight positive aspects of the paper, while the weaknesses point out limitations and areas for improvement or questions. There is no self-contradiction within the review.",
            "The review consistently identifies both strengths and weaknesses of the paper. Strengths are mainly in originality and empirical results, while weaknesses are in technical clarity, theoretical justification, and generalization. The reviewer's concerns and praises are balanced and do not contradict each other.",
            "The weaknesses logically point out limitations of the paper, and the questions directly ask for improvements related to these weaknesses. The strengths and weaknesses are distinct and non-contradictory, presenting a balanced and constructive critique.",
            "The review is consistent because the strengths mentioned are genuinely positive aspects of the paper, such as organization, methodology, and experiments. The weaknesses are framed as questions and suggestions for improvement, rather than fundamental flaws or contradictions to the strengths. The reviewer is suggesting further explorations and clarifications, which is a constructive approach and does not contradict the initial positive assessment."
        ]
    },
    {
        "paper_id": "iclr_2020_Hkekl0NFPr",
        "paper_title": "Conditional Learning of Fair Representations",
        "paper_abstract": "We propose a novel algorithm for learning fair representations that can simultaneously mitigate two notions of disparity among different demographic subgroups in the classification setting. Two key components underpinning the design of our algorithm are balanced error rate and conditional alignment of representations. We show how these two components contribute to ensuring accuracy parity and equalized false-positive and false-negative rates across groups without impacting demographic parity. Furthermore, we also demonstrate both in theory and on two real-world experiments that the proposed algorithm leads to a better utility-fairness trade-off on balanced datasets compared with existing algorithms on learning fair representations for classification. \n      ",
        "review_ids": [
            "rJe6njy0KH",
            "S1e9hdUBcS",
            "SylzfAdqsH",
            "r1gh9fvAYB"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper focuses on learning representations which can simultaneously achieve equalized odds and accuracy parity without impacting demographic parity. The authors show both theoretically and empirically that the proposed algorithm show better utility-fairness tradeoff on balanced datasets. This is indeed a useful result. Overall, I liked the presentation of the paper including motivation and background of other methods. Theory is sound, and experiments are sufficient.   Therefore, I do not have any major concern. Some of minor concerns are mentioned below:\n\n1. I see some ambiguity in the definition of the classifiers. h is defined to be deterministic in section 2; whereas, later \\hat{Y}, which I believe is treated at h(g(x)) for some h, is taken to be randomized. Also, the theorems are mentioned with respect to h making them specific to deterministic classifiers according to the current definition.\n\n2. I am not sure why the last statement of the second last paragraph on page 2 is true. Can you please explain? There should be an additional condition on distribution D, which is not clear at that moment in the paper.\n\n3. I am wondering if the utility can be maintained for imbalanced datasets by taking two parameters \\lambda_1 and \\lambda_2 for BER_{D^0} and BER_{D^1} in equation 2. Did the authors check when we have different regularization parameters for both terms? If yes, then what was the conclusion?\n\n4. Please write theorems as fully independent statements. \"Assume the conditions in Proposition 3.2\" is probably not the right way to start a theorem. Other way is to mention the conditions separately and then use it throughout the paper.\n\n5. Can you please write or elaborate the final optimization problem after section 3.4?\n\n6. How did the authors construct the optimal classifiers in the experiments for real data? Can you provide some details?\n\nTypo: \"sensitive attribute A, then the second term\" --> remove then\n\n----  After Rebuttal ---\n\nI thank the authors for providing response to my questions. At this point, I am going to keep the same score.",
            "Update:\n\nThanks for providing additional results on the Adult dataset. I have increased my score. However I'd be nice to also see the balanced accuracy (i.e. sum of TPRs for each class divided by 2) results and compare to baseline trained with oversampling or re-weighted loss.\n\nI would suggest authors to add more extensive comparisons to other methods using Adult dataset. There are quite a lot of papers in the fairness literature that experiment with the Adult dataset. They focus on different metrics and there is probably no method that is uniformly the best. Your paper demonstrated that your approach can succeed in achieving accuracy parity, but it would be good to also show tradeoffs with other metrics (in addition to DP). I was able to do some back-of-the-envelop calculations and your results seem fine, but a clear comparison would be good.\n\nBelow are some examples of the papers studying Adult dataset from the fairness angle:\n[1] Mitigating unwanted biases with adversarial learning. Zhang et al., 2018. (already cited, but no comparison)\n[2] What\u2019s in a Name? Reducing Bias in Bios without Access to Protected Attributes. Romanov et al., 2019.\n[3] Learning fair predictors with Sensitive Subspace Robustness. Yurochkin et al., 2019.\n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------\n\nThis paper proposes an adversarial representation learning approach. The key difference with prior work is that the objective function is built around balanced error rates, one for classes, that is eventually used for classification, and two adversarial for predicting each of the protected attributes. Authors argue that proposed approach can simultaneously achieve accuracy parity and equalized odds.\n\nThe notion of accuracy parity does not seem to be very meaningful. For example, predicting uniformly at random seems like an intuitively fair classifier with EO gap 0 and DP gap 0. However it will not necessarily have error gap of 0 (i.e. satisfy accuracy parity), making me wonder if the notion of accuracy parity makes much sense.\n\nI am not really sure what is the Err0 + Err1 metric used in Figures 1 and 2. Is it not normalized and can vary between 0 and 2? In which case it seems counterintuitive for performance quantification. If it is normalized, then results on COMPAS do not make sense. Err0 + Err1 of all methods is above 60%, which is worse than predicting uniformly at random.\n\nPlease report your TPRs for classes grouped by protected attribute when reporting the results in the context of group fairness. Further for Adult dataset, reporting balanced TPR as a measure of accuracy seems to make more sense given the class imbalance.\n\n",
            "Thank you for the rebuttal!\n\nOf course you are right about the 0 error gap for a classifier predicting uniformly at random and I am sorry for a bad example. However, if we consider another \"fair\" classifier always predicting 1, then error gap is 0 only if probability of 1 is same across protected groups, which is often not the case. I also briefly went through some references you suggested, in particular [6] cites [3] for \"Overall accuracy equality\" and [3] in turn says \"Overall accuracy equality is not commonly used because it does not distinguish between accuracy for successes and accuracy for failures\". [1] seems to focus on disparate impact, i.e. eq. (1) in their paper (which would categorize an \"always 1\" classifier as fair) - could you please clarify where they study accuracy parity?\n\nWhile I still remain unconvinced that accuracy parity is preferable to other fairness notions, I acknowledge that this paper makes a contribution in the direction of improving fairness in ML. I'd be willing to increase my score if you could provide the TPRs by gender (not gaps) and overall balanced accuracy for the Adult dataset. The reason I am asking for these results is that TPR for females on Adult dataset is typically quite low when using a classifier without fairness considerations, however some results reported in the literature appear to \"fix\" the problem by essentially making TPR for males significantly lower. Accuracy at the same time does not seem to decrease by a lot because the overall proportion of the \">50k\" class is small. I'd like to know if this is also the case for your method.",
            "Summary: Authors extend on work that attempts to learn fair data representation (features) and propose an algorithm (which is a modification of a loss essentially) and show that it allows to achieve accuracy and equalized odds parity, and show that while achieving equalized odds they don't hurt demographic parity. The experiments demonstrate utility of the algorithm for balanced datasets (without sacrificing performance to fairness)\n\nDisclaimer: I am completely out of this area\nBut it is an easy read and an interesting angle. Authors show that simple modification of the loss makes it more fair (in a sense).  The experiments are somewhat thin.\n\nMin max problem in Section 3 - I think it requires more intro for people outside. I assume you use an architecture that extracts the representation and has two head (outputs) - h and h'. For a modified loss, do you have 3 outputs - h, h' and h''? A pic with an architecture would be really helpful\n\nProbably more datasets are required to have a more convincing empirical story\n\nSection 3.4: Even though you can't directly optimize for BER, there are ways that can work, instead of just replacing it with CE, for example this https://arxiv.org/pdf/1608.04802.pdf\n\nOne critique is based on 3.4 I don't understand  how can this be extended to multiple axis of intersecting groups- e.g. not just mutually exclusive race values, but also gender for example. "
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Neutral",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer states \"This is indeed a useful result.\" and \"Overall, I liked the presentation of the paper...Theory is sound, and experiments are sufficient. Therefore, I do not have any major concern.\"",
            "The review expresses both positive and negative feedback. The reviewer acknowledges the improvements made by the authors ('I have increased my score') but also raises concerns about the meaningfulness of the accuracy parity notion and the Err0 + Err1 metric. It also provides constructive suggestions.",
            "The review acknowledges the paper's contribution while expressing reservations about the chosen fairness metric. It contains both positive elements ('contribution in the direction of improving fairness') and critical ones ('unconvinced that accuracy parity is preferable').",
            "The review acknowledges the interesting angle and easy readability of the paper, but also points out weaknesses in the experiments and clarity of the method. The overall sentiment is therefore considered neutral."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "While the review expresses overall positive sentiment and appreciation for the paper's presentation and theoretical soundness, it also raises several specific concerns and questions, indicating a balanced and critical assessment. The reviewer points out ambiguities, questions assumptions, and suggests improvements to the presentation and clarity of the paper.",
            "The review offers both praise and criticism, presenting a balanced perspective. It acknowledges the paper's contribution while also pointing out areas that need improvement. The reviewer's suggestions are framed constructively.",
            "The review balances acknowledgement of the authors' points with continued skepticism. Phrases like 'I acknowledge that this paper makes a contribution' and 'While I still remain unconvinced' demonstrate this balanced approach. The reviewer also asks for specific data to further evaluate the method.",
            "The review raises specific concerns about the clarity of the explanation in Section 3, the limited scope of the experiments, and the lack of discussion on extending the approach to multiple intersecting groups. Phrases like \"experiments are somewhat thin\" and \"I think it requires more intro\" indicate a critical tone."
        ],
        "consistency": [
            "No",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer states \"no major concern\" but then lists six points and a typo. These points are not trivial and question the clarity and rigor of definitions, theorems, optimization problem, and experimental details. Listing these points after stating \"no major concern\" suggests a slight inconsistency in the review. While the reviewer might consider these as minor in the overall assessment, they are still significant enough to question the initial statement of \"no major concern\".",
            "The review is consistent because it acknowledges the improvements made by the authors and increases the score, while also raising valid concerns and suggestions for further improvement, focusing on the meaningfulness of the chosen metric (accuracy parity) and the clarity of the evaluation (Err0 + Err1, TPRs, balanced accuracy). The reviewer provides constructive criticism throughout the review without contradicting themselves.",
            "The reviewer acknowledges a mistake in their previous review and engages with the rebuttal by checking references and raising valid points. They express a nuanced opinion on accuracy parity while appreciating the paper's contribution and asking for specific results to further evaluate the method's fairness and performance. The reviewer's arguments and questions are logically connected and consistent throughout the review.",
            "The review is consistent because it starts with a positive summary and acknowledges the paper's interesting angle and easy readability despite the reviewer's disclaimer of being outside the area.  The reviewer then proceeds to offer constructive criticisms regarding the thinness of experiments, lack of clarity in Section 3, and the need for more datasets.  The suggestions for improvement, such as adding an architecture diagram and considering intersectional fairness, are all aimed at strengthening the paper. There are no contradictory statements or shifts in opinion throughout the review. The reviewer maintains a helpful and constructive tone."
        ]
    },
    {
        "paper_id": "iclr_2021_MJIve1zgR_",
        "paper_title": "Unbiased Teacher for Semi-Supervised Object Detection",
        "paper_abstract": "Semi-supervised learning, i.e., training networks with both labeled and unlabeled data, has made significant progress recently. However, existing works have primarily focused on image classification tasks and neglected object detection which requires more annotation effort. In this work, we revisit the Semi-Supervised Object Detection (SS-OD) and identify the pseudo-labeling bias issue in SS-OD. To address this, we introduce Unbiased Teacher, a simple yet effective approach that jointly trains a student and a gradually progressing teacher in a mutually-beneficial manner. Together with a class-balance loss to downweight overly confident pseudo-labels, Unbiased Teacher consistently improved state-of-the-art methods by significant margins on COCO-standard, COCO-additional, and VOC datasets. Specifically, Unbiased Teacher achieves 6.8 absolute mAP improvements against state-of-the-art method when using 1% of labeled data on MS-COCO, achieves around 10 mAP improvements against the supervised baseline when using only 0.5, 1, 2% of labeled data on MS-COCO.",
        "review_ids": [
            "Z4h3F5P6bS",
            "s_VugpC9XfT",
            "l-b9NP1HUh6",
            "QU0_W0jZcmn"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "+This paper presents a good work on semi-supervised object detection (SSOD), which is a very challenging task. Although there are great progresses on semi-supervised classification, the SSOD is lying behind. This paper shows very good results over the supervised baselines, even when all annotations are used in COCO. \n\n+The proposed method is very simple. It seems the paper is easy to be reproduced.\n\n+It is a good idea to use EMA of mean teacher for SSOD. In addition, the focal loss (FL) is shown to be very useful in SSOD. \n\nA few questions:\n\n-The default FL is mainly for +/- class imbalance. Do you have modified for imbalance among all positive classes?\n\n-It seems FL is more useful than EMA. However, FL is not well ablated. For example, it is the balance between +/- classes more important or among all positive classes? How about the other SSOD, e.g. STAC, using FL?\n\n-It is understandable that EMA model is more reliable. But I don't see it can be beneficial for class imbalance issue explicitly. \n\n-The comparisons with the baseline supervised models seem to be not very fair. For example, in the last column of Table 1, the supervised model uses 1x schedule (90k iterations), but the proposed method uses 4x schedule. From the Detectron2 github, the 3x model should have AP of 40.2. When compared with 4x supervised baseline, the gain of this paper should be much smaller. The same issue could also be in the other COCO-standard experiments. More fair comparisons should be presented.\n\n-Don't quite understand Fig. 4. Too few description. What are burn-in limit and GT. Why are they constant?\n\n-For the paragraph of \"Class-imbalance on pseudo-labels\", if you don't show results, why you write it in Sec. 4.1, instead of Sec. 4.2?\n\n-It is very weird to cite Law & Deng, 2018, when you refer to AP50 saturation. There are tons of paper out there showing that. \n\n-For the AP curves, do you evaluate on the full val2017 set? It seems the data points are very dense. Isn't it expensive?\n\n-I don't think the smoother curve of EMA teacher is the weights of teacher model is detached from the student model. \n\n=====updates========\n\nMost of my concerns have been addressed by the rebuttal and all the other reviews are positive. l will remain my original recommendation.",
            "Paper summary:\nThis paper focuses on the pseudo-labeling bias issue in semi-supervised object detection (SS-OD), and proposes an Unbiased Teacher framework to address this issue. More specifically, the unbiased teacher framework combines the Mean Teacher model for semi-supervised image classification (Tarvainen and Valpola 2017) and Focal Loss (Lin et al. 2017) for fully supervised object detection to address the bias issue. Experiments on COCO and PASCAL VOC show that the proposed method obtains the state-of-the-art semi-supervised object detection results.\n\n\nPros:\n\n+ The paper is well written and easy to understand.\n\n+ The motivations of this paper are clear and interesting.\n\n+ A very simple but effective solution is proposed.\n\n+ Very solid experiments are conducted.\n\n\nCons:\n\n- The main weakness of this paper is that the proposed method is a simple combination of the previous methods including Mean Teacher and Focal Loss.\nIn addition, for the foreground/background imbalance issue, it is straightforward to generate pseudo gts to address this issue, which also has been studied by previous semi-supervised object detection work (Sohn et al. 2020). For the class imbalance issue in object detection, it is also straightforward to use Focal Loss to address this issue.\n\n- One minor thing: It would be better to show results of different detectors / CNN backbones.\n\n\nReview summary:\nIn summary, I think this is a good semi-supervised object detection paper because of its simple but effective solution and solid experiments. So I would like to give a weak accept to this paper.\n\n\nPost-rebuttal comments: \nThe authors have addressed my concerns in their rebuttals. All reviewers give positive comments to this paper. So I would like to give an accept to this paper.",
            "This paper addressed an essential task for large-scale application of object detection -- semi-supervised learning. It introduced a simple but effective Unbiased Teacher to solve the traditionally problematic data imbalance issue. \n\nPros\n\n(1) Provides strong evidences and analysis on the class-imbalance problem inherited in pseudo-labeling methods;\n(2) Proposed \n      (a) an interesting learning paradigm where the teacher is the temporal ensemble of student networks;\n      (b) focal loss in place of cross entropy;\n(3) The experiment and ablation suggested that the resulting teacher model is not prone to class-imbalance-induced overfitting and the improvement from SOTA is significant.\n\nCons\n\nCurious to know why there is a drop in AP_50 comparing to STAC. ",
            "This work tackles the task of semi-supervised object detection via a teacher-student method. The authors introduced a training regime where a teacher and student network, who share the same initial weights pre-trained on labeled data, jointly learns on unsupervised data. They find that label imbalance in the object detection task can lead to inefficient pseudo-label training under the usual SSL training pipeline, and therefore proposes to train the teacher network via exponential moving average to avoid bias in pseudo-labels.\n\n\nPros:\n1. Revisiting of pseudo-labeling bias issue in semi-supervised object detection is good.\nThis paper analyze both classification and regression loss with different ratios of labelled data, which shows that classification branch can easily suffer from overfitting that limits current state-of-the-art semi-supervised object detectors. It shows that the misalignment between classification and regression branch not only exists in fully-supervised learning object detection, but also in self-supervised object detection. \n\n2. Experiments are solid and convincing.\nTable 1 & Table 2 show that unbiased teacher consistently improves state-of-the-art methods CSD and STAC by a large margin on both COCO and VOC dataset.\n\nCons:\n1. Process of VOC12 dataset.\nIn Table 2, since VOC12 has a large overlap with VOC07, is the VOC07 part removed from VOC12 as unlabeled data?\n\n2. Novelty\nNevertheless, some contributions seem a bit straight-forward and without significant novelty. The use of EMA is a direct adaptation from successful methods from classification, while Focal loss is already known to tackle class imbalance. The authors might need to provide more insights on the use of these methods (e.g. explaining in more detail how EMA alleviates such bias).\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer initially expresses positive sentiments by stating \"This paper presents a good work\" and \"This paper shows very good results.\" The final update indicates satisfaction with the rebuttal and other positive reviews, reinforcing the positive sentiment.",
            "The reviewer states \"I think this is a good semi-supervised object detection paper\" and initially recommends a \"weak accept,\" which is later upgraded to \"accept\" after the rebuttal.",
            "The review expresses overall positive feedback about the paper, highlighting its strengths, such as addressing an essential task, introducing an effective solution, providing strong evidence and analysis, and achieving significant improvement from SOTA. The cons section only expresses curiosity, not a strong critique.",
            "The review expresses overall positive feedback, highlighting the paper's strengths in addressing pseudo-labeling bias and presenting convincing experimental results. While it points out areas for improvement, the 'Pros' outweigh the 'Cons'."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review is balanced, offering both positive feedback (\"good work\", \"very good results\", \"easy to be reproduced\") and constructive criticism (questions about FL ablation, fairness of comparisons, unclear figures, citation issues). The reviewer maintains a professional tone throughout, indicating a balanced approach to evaluating the paper.",
            "The review presents both positive aspects ('Pros') and negative aspects ('Cons') of the paper, demonstrating a balanced assessment. The reviewer uses neutral language and provides specific reasons for both the strengths and weaknesses identified.",
            "The tone is supportive, using words like \"strong evidences,\" \"interesting learning paradigm,\" \"significant improvement,\" and generally praising the work's contributions. The reviewer is also asking a question in the cons section, which suggests a desire to understand the work better rather than criticize it harshly.",
            "The review adopts a balanced tone by providing both positive aspects ('Pros') and areas for improvement ('Cons'). It acknowledges the paper's strengths while also offering constructive criticism, resulting in a nuanced evaluation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review initially presents positive aspects of the paper, then raises several concerns and questions. However, the reviewer explicitly states that most of their concerns were addressed by the rebuttal and other reviews are positive, leading them to maintain their original recommendation. This indicates a consistent progression from initial positive impression, through critical questioning, to a reaffirmed positive stance after considering the rebuttal.",
            "The review is consistent because the reviewer clearly outlines both the strengths (pros) and weaknesses (cons) of the paper. The initial recommendation of 'weak accept' is a balanced judgment based on these points. The shift to 'accept' after rebuttal is also explained by the authors addressing concerns and positive feedback from other reviewers, indicating a logical progression of assessment rather than a contradiction.",
            "The review is consistent because the pros section highlights the strengths of the paper, such as addressing an important problem, providing strong evidence, proposing an interesting learning paradigm, and demonstrating significant improvement over SOTA. The cons section only raises a question for clarification and does not contradict any of the positive points mentioned in the pros section. The reviewer is curious about a specific result but does not negate the overall positive assessment of the paper.",
            "The review is consistent because the pros and cons are distinct and do not contradict each other. The reviewer praises the paper for addressing an important issue and providing solid experiments, while also raising valid concerns about dataset details and novelty. These points are independent and do not create any inconsistency in the reviewer's assessment."
        ]
    },
    {
        "paper_id": "nips_2022_9_O9mTLYJQp",
        "paper_title": "Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfitting",
        "paper_abstract": "We show that label noise exists in adversarial training. Such label noise is due to the mismatch between the true label distribution of adversarial examples and the label inherited from clean examples \u2013 the true label distribution is distorted by the adversarial perturbation, but is neglected by the common practice that inherits labels from clean examples. Recognizing label noise sheds insights on the prevalence of robust overfitting in adversarial training, and explains its intriguing dependence on perturbation radius and data quality. Also, our label noise perspective aligns well with our observations of the epoch-wise double descent in adversarial training. Guided by our analyses, we proposed a method to automatically calibrate the label to address the label noise and robust overfitting. Our method achieves consistent performance improvements across various models and datasets without introducing new hyper-parameters or additional tuning.",
        "review_ids": [
            "Hq7JnSMQ2UL",
            "C5y8Us2X73z_",
            "-hFdKxmVKan",
            "DidpPscqoru",
            "qFDS1slV_UZ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your detailed reply.\n\nThe idea of demonstrating the label noise in AT by comparing it with the performance on manually generated datasets with random flip noise is smart. \n\nI am still confused by the arguments in section 4.2:\n\nAccording to my understanding, this section is devoted to quantifying the lower bound of label noise in the case when the adversarial perturbation is generated by a probabilistic classifier.\\\nYou first show that \"after sufficient adversarial training, the predictive label distribution of $f_\\theta$ can approximate the true label distribution with high probability.\" (lemma 4.4)\\\nAnd the main conclusion of Theorem 4.5 is based on this result (lemma 4.4).\n\nOn the other hand, in your reply, you mentioned \"Our theory shows that an adversarially trained model **has to satisfy specific conditions** to be able to approximate true label distribution. The model trained to the end **may not match such conditions well**\"\\\nIt seems to me that this statement would have weakened the arguments in section 4.2 since if $f_\\theta$ does not approximate the true label distribution well in practice, the bound in Theorem 4.5 would also not reflect the amount of label noise in real situation.\n\nBTW, according to the reply the statement  \"after sufficient adversarial training, the predictive label distribution of $f_\\theta$ can approximate the true label distribution with high probability.\" (line 222) seems to be kind of overstated. ",
            " I deeply appreciate the authors' effort in providing the feedback. It's good to see that the effectiveness of the alternative labeling is maintained for different perturbations radii and data qualities. Thus, I would like to increase my rating to 6.",
            " The paper tackles the problem of adversarial overfitting, that is a critical breakdown of robust accuracy that commonly happens in adversarial training and makes early stopping a necessity for AT. They give an interpretation of robust overfitting from the standpoint of label noise, ie they argue that the adversarial attacks change the true target for the image but AT does not adapt to the change in label. \n\nThe paper first gives a theoretical analysis, in which they first show that for locally Lipschitz and optimal (P(Y|x) = f(x)) classifiers, the adversarially perturbed data distribution has non-negative label noise (if the labels remain unchanged) that depends, among others, on the size of the perturbation radius. A similar theorem is then given for the case of non-optimal classifiers. \n\nIn the practical section, they show various phenomena, for example, that robust overfitting follows a double descent curve and that robust overfitting correlates with radius and data quality. \n\nFinally, they introduce a new adversarial training formulation, that uses temperature rescaling, interpolation, and the output of an external adversarially trained classifier to relabel the perturbed samples during adversarial training. In the evaluation, they show that this scheme is less likely to overfit and can yield better robust accuracies.  Strengths:\n\n- The problem of robust overfitting is relevant and while the intuitive idea that AT can create a sample-label mismatch is quite obvious, the theoretical and practical connection to robust overfitting is interesting. \n\n- The paper is well written and clear in its explanations.\n\n- The robustness evaluation uses a strong and reliable attack. \n\n- The proposed training method improves over AT, Trades and KD-AT in terms of robust accuracy on all 3 datasets. \n\nWeakenesses: \n\n- Like every theoretical paper, the authors make certain assumptions. This is fine and I think that the local Lipschitz assumption is quite common and established in the AT literature. However for the others, such as external covering in 4.4, the authors do not comment on whether or not this is a realistic assumption or if it is highly likely to be violated in practice. \n\n- The proposed training scheme is not very novel and resembles for example the cited [Robust overfitting may be mitigated by properly\nlearned smoothening] with a heuristic for hyperparameter optimization.  - Section 3 assumes that Y and Y-hat are integers, ie this means that a true label exists. Now for certain images, an adversarial attack might introduce a new class without completely deleting the original one, eg we could have an image with both a dog and a cat. Is it possible to extend your framework such that Y is a distribution to handle those circumstances? \n\n- Figure 7 shows that the proposed method does not overfit in 150 epochs. Is this also true if we continue training for 1k epochs? \n\n- In the past, papers like \"Unlabeled Data Improves Adversarial Robustness\" have shown that quantity matters. In light of your theorem, it seems like data quality is more important. Do you think this behavior is simply not explained by your theory or is there another explanation? \n\n- In a way, this scheme resembles self-training. Is it possible to iterate the method, ie use the first classifier trained with your new loss to train a second classifier with the first one as \"teacher\" (to label the adv samples during training)?  The limitations in the appendix address the largest issues. The training of the additional classifier can be expensive and the entire scheme can only work if one can actually find a model that can properly classify the adv samples for the new classifier during training. ",
            " This paper demonstrates that adversarial training will introduce label noise through both theoretical and empirical investigations. This noise helps to interpret the robust overfitting phenomenon. The paper further proposes to use a rectified model probability in adversarial training to mitigate the robust overfitting. Experiments on CIFAR10/100 and Tiny-ImageNet are conducted to illustrate the effectiveness of the proposed method. Strengths:\n1. The observation that adversarial training introduces label noise is insightful and novel, and sheds light on the understanding of robust overfitting.\n2. The paper provides a clear and in-depth theoretical investigation of the label noise caused by adversarial perturbations. The example and relevant experiments are helpful for readers to grasp the key idea of this novel label noise.\n3. Based on the label noise understanding, the paper further proposes to use a rectified model probability to mitigate the robust overfitting. Comparisons with regular AT and KD-AT illustrate the effectiveness of the proposed method.\n\nWeaknesses:\n1. Though the experiments successfully demonstrate the connection between the label noise and robust overfitting, it would be overstated to say \"robust overfitting can be adequately explained by such label noise\" (line 76). A rigorous theoretical analysis is needed to establish that strong conclusion.\n2. while the example shows that the label noise introduced by adversarial perturbation can be significant in synthetic datasets, it is not clear how severe would the label noise be in realistic cases. \n\n1. Can the authors estimate the significance of such AT-caused label noise in realistic cases, e.g., considering the classification task on CIFAR100?\n2. The arguments in section 4.2 seem artificial.\nif an AT-trained model can approximate the true label distribution, then the robust overfitting won't be a problem. That is it doesn't matter whether the AT will distort the true label distribution. \n3. A possible typo in table 1, in the last column for CIFAR100, -0.25 is in bold instead of -0.09. The paper addresses the limitations in the supplementary material. As far as I am concerned, there is an additional limitation: \n- the paper does not discuss how severe would the label noise be in realistic applications.",
            " The authors show that label noise exists implicitly in adversarial training due to the mismatch between the true label distribution and the assigned label distribution of adversarial examples.  Based on a label noise perspective, the authors formalize robust overfitting and show that it is the early part of an epoch-wise double descent in adversarial training.  Also, the authors propose an alternative labeling of adversarial examples by rectifying model probability, which uses temperature scaling and interpolation together. The authors nicely presents that adversarial perturbation implicitly generates label noise.  This phenomenon is explained intuitively and proven formally.  The observation is correct in that there is a mismatch between the true label distribution and the assigned label distribution of adversarial examples.  To solve this issue, the authors propose an alternate labeling scheme based on temperature scaling and interpolation.  As a result, the proposed training method achieves higher accuracy than conventional adversarial training methods.  Overall, the observation is interesting, and the theoretical analysis is well presented.\n\nAt the same time, I have the following concerns.\n\nThe authors prove that temperature scaling and interpolation can be effective for reducing the distribution mismatch.  However, it is not clear why these two methods are specifically selected.  Are there any other techniques applicable for the same purpose?  To me, introducing these two techniques is somewhat abrupt.\n\nThere is a gap between the theory and the actual method.  In Theorem 5.1 and Theorem 5.2, there exists satisfying $T$ or $\\lambda$ for each example.  However, a single value is used for the hyper-parameters in the actual method (Equation (9)).  Thus, the connection between the theory and the method is not very rigorous.  In Section E. the optimal hyper-parameter values for each example are not highly concentrated, as opposed to the authors' claim.\n\nDifferent perturbations and dataset qualities need to be considered for validating the effectiveness of the alternate labeling.  Currently, one a single perturbation radius is used.\n\nIt would be better to show how many adversarial examples are assigned labels different from those of the original examples.  The authors can discuss whether the alternating labeling schemes chooses a reasonable label when a different label is generated.  Such case study will be very interesting to the reader.\n\nIt is not clear how the validation set is extracted for determining the optimal parameter values in Table 1.  What if a sufficiently large validation set does not exist?\n\np. 2 is not shown using Acrobat on Windows.  (Okay on iPad.)\n\n=== Post-rebuttal update ===\n\nI have increased my rating from 5 to 6, mainly due to the authors' additional experiments. Why do you assume that $f$ is a L-Lipschitz function?  Is this assumption reasonable?  What if this assumption does not hold? It would be better to discuss the possibility that the alternative labeling even increases the risk of robust overfitting."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses confusion and disagreement with the arguments presented in the paper, questioning the validity of the theoretical results and suggesting that a key statement might be overstated. The phrases \"I am still confused\", \"weakened the arguments\", and \"overstated\" indicate a negative sentiment.",
            "The reviewer explicitly states \"I deeply appreciate the authors' effort\" and \"It's good to see that...\", indicating a positive sentiment. The reviewer also states they would like to increase their rating.",
            "The review expresses a generally positive assessment of the paper. It highlights several strengths, including the relevance of the problem, clarity of writing, strong evaluation, and improvement over existing methods. While weaknesses are pointed out, they are framed as areas for improvement or further exploration rather than fundamental flaws.",
            "The review highlights several strengths of the paper, including insightful observations, clear theoretical investigation, and effective proposed method. Although weaknesses are pointed out, the overall tone suggests a positive evaluation.",
            "The review expresses both positive aspects of the paper (interesting observation, well-presented theoretical analysis, improved accuracy) and concerns regarding the clarity of method selection, the gap between theory and practice, and the need for further validation. The reviewer's initial positive assessment is tempered by specific criticisms."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review critically examines the paper's theoretical arguments, pointing out potential inconsistencies and weaknesses. Phrases like \"I am still confused\", \"It seems to me that this statement would have weakened the arguments\", and \"seems to be kind of overstated\" demonstrate a critical tone.",
            "The reviewer uses appreciative language like \"I deeply appreciate\" and \"It's good to see,\" which conveys a supportive and encouraging tone. Also, the reviewer decides to increase the rating.",
            "The review presents both strengths and weaknesses of the paper, indicating a balanced perspective. The reviewer uses formal language and provides constructive criticism, avoiding overly positive or negative statements. Phrases like 'interesting,' 'well written,' and 'improves over' are balanced with concerns about assumptions and novelty.",
            "The review presents both strengths and weaknesses of the paper, using objective language and providing specific examples to support the claims. It maintains a formal tone throughout.",
            "The review adopts a balanced tone by acknowledging the strengths of the paper while also raising specific concerns and suggesting improvements. Phrases like \"nicely presents,\" \"observation is interesting,\" and \"theoretical analysis is well presented\" indicate positive feedback, whereas phrases like \"I have the following concerns,\" \"it is not clear,\" and \"There is a gap\" express criticism."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in pointing out a potential contradiction between the paper's claim (Lemma 4.4) and the authors' reply, which questions the practical relevance of the theoretical bound in section 4.2. The reviewer consistently argues that if the model does not approximate the true label distribution well in practice, as suggested by the authors' reply, then the bound in Theorem 4.5 might not be meaningful in real-world scenarios.",
            "The reviewer expresses appreciation for the authors' work and states that they would like to increase their rating based on the positive results regarding the effectiveness of alternative labeling. This indicates a consistent positive evaluation.",
            "The review is consistent because it presents both strengths and weaknesses of the paper in a balanced and constructive manner. The reviewer acknowledges the relevance and clarity of the work while also pointing out specific limitations and areas for improvement, without contradicting themselves or presenting conflicting viewpoints.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the novelty and insightful observation about label noise, the clear theoretical investigation, and the empirical support for the proposed method. At the same time, it raises valid weaknesses and questions that are constructive criticisms, such as the need for stronger theoretical backing for certain claims, the unclear significance of label noise in realistic scenarios, and potential issues in a specific section. These criticisms do not contradict the acknowledged strengths but rather point out areas for improvement and further investigation, leading to a balanced and consistent review.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the interesting observation and well-presented theory, while also raising valid concerns and questions about the methodology, theory-method gap, experimental validation, and assumptions. The reviewer's critique is constructive and aims to improve the paper, rather than being contradictory. The post-rebuttal update shows a positive shift in the reviewer's opinion due to additional experiments, but remaining questions are still valid and consistent with the initial concerns."
        ]
    },
    {
        "paper_id": "nips_2021_4jPVcKEYpSZ",
        "paper_title": "Diverse Message Passing for Attribute with Heterophily",
        "paper_abstract": "Most of the existing GNNs can be modeled via the Uniform Message Passing framework. This framework considers all the attributes of each node in its entirety, shares the uniform propagation weights along each edge,  and focuses on the uniform weight learning. The design of this framework possesses two prerequisites, the simplification of homophily and heterophily to the node-level property and the ignorance of attribute differences. Unfortunately, different attributes possess diverse characteristics. In this paper, the network homophily rate defined with  respect to the node labels is extended to attribute homophily rate by taking the attributes as weak labels. Based on this attribute homophily rate, we propose a Diverse Message Passing (DMP) framework, which specifies every attribute propagation weight on each edge. Besides, we propose two specific strategies to significantly reduce the computational complexity of DMP to prevent the overfitting issue.  By investigating the spectral characteristics, existing spectral GNNs are actually equivalent to a degenerated version of DMP.  From the perspective of numerical optimization, we provide a theoretical analysis to demonstrate DMP's powerful representation ability and the ability of alleviating the over-smoothing issue.  Evaluations on various  real networks demonstrate the superiority of our DMP on  handling the networks with heterophily  and alleviating the over-smoothing issue, compared to the existing state-of-the-arts.\n",
        "review_ids": [
            "mrnNSQuOWTf",
            "2hSLnqQZdG",
            "OQyOGmex1Y9",
            "heixjKp8ZAO"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper focused on improving graph neural networks from homophily and over-smoothing issues by introducing a diverse message passing (DMP) framework. Experiments on 9 public datasets with a variety of homophily scores were provided to validate the effectiveness of the proposed method.   Pros:\n-\tFollowing previous works [8], the motivation of this work is clearly presented.  How the proposed method connects to existing GNN methods has been elaborated in both spatial and spectral domains. \n-\tWhile the proposed implementation is simple and straightforward, some theoretical analyses (Theorem 4 and 5, though with some key typos) have been provided to demonstrate the benefits of DMP over the uniform message passing. \n-\tExtensive experimental results have been provided in terms of different homophily graphs and model components. Plus, experiments on heterogenous graphs and deep GNNs have also been conducted. \n\nCons:\n-\tThe technical contribution of this work is limited since the proposed method mainly follows the framework provided in [8] and adopts a similar implementation way to compute \u201cattentions\u201d over attributes as in [18,22,23]. \n-\tSome claims are over-emphasized as the proposed method is clearly built on top of several theoretical frameworks [6-8, 28]. To name a few, \u201cexisting spectral GNNs are just equivalent to a degeneration of diverse message passing\u201d, \u201cdiverse message passing may break the ceiling of spectral GNNs\u201d, etc.\n-\tThe proposed method might be ad hoc to the datasets with low homophily rates.  Suffice.",
            "This paper proposes a diverse message passing framework (DMP) to generalize a classic uniform message passing framework and also suggests strategies preventing over-smoothing issues.  ================================================================= \n\n[Main Strengths] \n\nThis paper's main strength is that the motivations of the work are well described (as shown in SEction 3.1) and evaluations across various homophily rate datasets (see Table1) are well designed. \n\n================================================================= \n\n[Main Weaknesses] \n\nThe paper's main weakness is the somewhat limited merits of the theoretical analysis shown in Section 3.4. In particular, the potential benefits of the contribution of Theorem 5 toward preventing an over-smoothing issue are not clear to me.\n\n=================================================================  The authors did not provide the \u201climitations and social impact\u201d of this work in the paper.\n",
            "The authors extend network homophily rate to attribute homophily rate by taking attribute as weak label and design diverse message passing (DMP) framework to specifiy each attribute propagation weight along each edge. The authors show that DMP can prevent  over-smoothing and handle network with heterophily.\n  Strength:\n\n1. Although DMP looks like a simple extension of GAT, I think to form uniform and diverse message passing into an optimization problem is interesting.\n2. The visualization looks good.\n\nWeakness&Advice:\n\n1. Line 74, \u201cddimensions\u201d \u2014> \u201cdimensions\u201d\n\n2. In equation (3), is it possible to get a zero denominator? Do you assume all elements $x_{vf}$ are positive?\n\n3. In equation (4)(6), DMP looks like a channel-wise GAT with attention values in (-1,1). The contribution and novelty in section 3.2 are limited.\n\n4. In theorem 5, should add $f=1.\\dots, F$\n\n5. Line 213-216, \u201cdifferent form Uniform Message Passing, which directly generates a F-components partition, Diverse Message Passing generates F groups 2-components partitions as candidates and then the  classifier in semi-supervised task determines how to combine them to form the final F-components partition.\u201d For shallow GNNs, why generating  F groups of 2-components partitions is better than generating one F-components partition on semi-supervised learning. Need more explanation.\n\n6. The dimension of $X$ is $N\\times F$ and F is number of partitions you use in section 3.4. What is the relation between the two $F$ or do I miss something? $F$ is an assumption or a restriction for theorem 4?\n\n7. Making correction of theorem 5 in supplementary material seems not to be allowed.\n\n8. Should add a comparison with GPRGNN, which is a new model to solve heterophily problem.\n\n9. Standard deviation should be provided in table 2&3.\n\n10. There are two strategies proposed to reduce the number of learnable parameters. So what is the running speed of the current models and how they compare with the existing methods?\n\n11. How well does DMP on solving over-smoothing compared to the existing models.\n Yes, the authors adequately addressed the limitations and potential negative societal impact of their work.",
            "This paper extends the node-level network homophily based on the node label to attribute-level one by considering each attribute as a weak label. Then, it observes that different attributes have diverse attribute homophily rates. To alleviate the issue of existing uniform message passing on capturing this diverse attribute homophily rates, this paper proposes diverse message passing, where different attributes are assigned different propagation weights, and two implementations. Finally, it theoretically investigates the ability of diverse message passing on overcoming over-smoothing by analyzing the roles of the uniform and diverse propagation weights.   The extension from network homophily rate to attribute homophily rate makes sense since node attributes can be seen as the weak label to represent the node compared to the node label. It is interesting to separately consider the homophily rate of each attribute and propose diverse propagation weights for different attributes. By comparing the proposed two implementation strategies and the interpretation from the spectral perspective, the two strategies can be regarded as a compromise between the expressive power and the overfitting issue. Thus, I think the proposed diverse message passing is effective in capturing the diverse homophily rate of different attributes. Therefore, this paper is technically sound.\n\nBesides, the theoretical contribution of this paper is significant. Different from previous works, which often consider the over smoothing issue is caused by the over propagation, this paper shows that the over smoothing issue is actually induced by the uniform propagation. The theorems show an interesting finding that the propagation weight learning is equivalent to graph partition, and connect the graph partition and over smoothing issue. Thus, the theoretical analysis provides a new perspective to understand the over smoothing issue. \n\nThe experimental evaluation is sufficient. It demonstrates the superior performance on networks with heterophily and heterogeneous information networks and the ability on preventing the over-smoothing issues.  \n\nAlthough I think this paper is interesting and novel, there are also some concerns. \n1.\tWhy the performance improvements on networks with heterophily are more significant than those on networks with homophily?\n2.\tThe explanation of why multiple graph partition just below theorem 5 is hard to understand. It is better to reorganize this paragraph.\n3.\tThe description of the data in Figure 1 is not very clear in the appendix.\nThe performance of the proposed DMP can\u2019t outperform some SOTA methods designed for heterogeneous such as MAGNN.\n Yes"
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review presents both positive aspects (Pros) and negative aspects (Cons) of the paper, resulting in a balanced overall sentiment.",
            "The review presents both strengths and weaknesses of the paper, without expressing a clear positive or negative overall opinion. It points out well-described motivations and well-designed evaluations but also mentions limited merits in the theoretical analysis.",
            "The review presents both strengths and weaknesses of the paper. While acknowledging the interesting optimization problem and good visualization (strengths), it also raises several concerns regarding novelty, potential issues with equations, clarity of explanations, and the need for more comparisons and justifications (weaknesses). This balanced approach indicates a neutral sentiment.",
            "The reviewer states that the paper is 'technically sound,' 'interesting and novel,' and that the experimental evaluation is 'sufficient.' They also highlight the significance of the theoretical contribution and the effectiveness of the proposed diverse message passing."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses a formal tone and presents both positive (\"clearly presented\", \"theoretical analyses\", \"extensive experimental results\") and negative (\"technical contribution is limited\", \"claims are over-emphasized\", \"might be ad hoc\") feedback, indicating a balanced perspective.",
            "The review adopts a balanced tone by highlighting both the strengths ('main strength', 'well described', 'well designed') and weaknesses ('main weakness', 'somewhat limited merits', 'not clear') of the paper. It offers constructive criticism without being overly negative or positive.",
            "The review points out several weaknesses in the paper with specific questions and suggestions for improvement. Phrases like \"limited contribution and novelty,\" \"Need more explanation,\" \"Making correction... seems not to be allowed,\" and requests for comparisons and clarifications indicate a critical tone focused on improving the paper's quality.",
            "The reviewer uses positive language such as 'makes sense,' 'interesting,' 'significant,' 'superior performance,' and 'new perspective.' While they raise concerns, they frame them as areas for improvement rather than fundamental flaws."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer acknowledges both the strengths (clear motivation, experiments, theory) and weaknesses (limited novelty, over-claims, potential dataset dependency) of the paper without contradicting themselves. The criticisms are focused on the degree of contribution and potential limitations, which are valid points in the context of the acknowledged strengths.",
            "The review is consistent as it provides both strengths and weaknesses of the paper without any contradictions. The reviewer appreciates the motivations and evaluations while pointing out the limitations of the theoretical analysis and the absence of a section on limitations and social impact. The different points raised are distinct and do not contradict each other.",
            "The review is consistent because it identifies both strengths and weaknesses of the paper. The weaknesses are presented as constructive criticism and suggestions for improvement, without contradicting the identified strength. The reviewer provides specific points for improvement, indicating a consistent and focused evaluation of the manuscript.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper in a balanced way. The reviewer acknowledges the novelty and technical soundness while providing constructive criticisms for improvement. The overall tone is positive and supportive, leading to a clear recommendation for acceptance despite minor concerns."
        ]
    },
    {
        "paper_id": "nips_2022_W4ZlZZwsQmt",
        "paper_title": "Symplectic Spectrum Gaussian Processes: Learning Hamiltonians from Noisy and Sparse Data",
        "paper_abstract": "Hamiltonian mechanics is a well-established theory for modeling the time evolution of systems with conserved quantities (called Hamiltonian), such as the total energy of the system. Recent works have parameterized the Hamiltonian by machine learning models (e.g., neural networks), allowing Hamiltonian dynamics to be obtained from state trajectories without explicit mathematical modeling. However, the performance of existing models is limited as we can observe only noisy and sparse trajectories in practice. This paper proposes a probabilistic model that can learn the dynamics of conservative or dissipative systems from noisy and sparse data. We introduce a Gaussian process that incorporates the symplectic geometric structure of Hamiltonian systems, which is used as a prior distribution for estimating Hamiltonian systems with additive dissipation. We then present its spectral representation, Symplectic Spectrum Gaussian Processes (SSGPs), for which we newly derive random Fourier features with symplectic structures. This allows us to construct an efficient variational inference algorithm for training the models while simulating the dynamics via ordinary differential equation solvers. Experiments on several physical systems show that SSGP offers excellent performance in predicting dynamics that follow the energy conservation or dissipation law from noisy and sparse data.",
        "review_ids": [
            "8oRhqkZEoA2",
            "lFDT0GHnqB",
            "S6zy_NmMrp",
            "6aXYvNflSH8",
            "YWoqnYSMGYCZ",
            "EZEIt_lCCG5",
            "Arzzd5TqHWK",
            "GPu7yR7LP7W",
            "3hyDxvzW2bv"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you! I have no more questions.",
            " thanks for the clarification.\nA simple way to say could be that q factorizes as two factors separating the weights for the sines and cosines.",
            " Thanks for your comments.\nI understand past work and motivation and they are quite clear in your response.\n**Final suggestion**: Your manuscript would gain in clarity if you followed this presentation for the past work symgpr (maybe add the name when you cite, like (symgpr [10] )) Before you introduce VI, you could explain what would be the ideal thing to do: marginal likelihood, and why it is not tractable.\nThen introduce VI, RFF explaining what are the gains (scalability) and what it allows: reparameterization trick + ODE solver conditioned on samples for f.\n\nI have no further questions",
            " Thank you for your very detailed reply. But I have one more concern. I don't think SympNet needs a large dataset. They already maximize the symmetry of symplectic geometry. I hope you show a complete comparison with SympNet in the next round of submissions, at least methodologically.",
            " The additional discussions are also reasonable. The new information resolved all my concerns.\n\nI reread my review again and realized that I had submitted an older version of some parts of it. At the beginning of the Strengths section, I mentioned the method as the first study of GP for Hamiltonian system. This is not true; the aim is an extension to sampled data and dissipativity. I mentioned that later in the review. Sorry for the confusion. I clarify that my score has nothing to do with this (initial) misunderstanding.\n\nI am willing to raise the score during the discussion with AC.",
            " The manuscript proposes a way to learn Hamiltonian systems with additive dissipation from (scattered) observations.\nThe learning is framed as a (Bayesian) inference of a Hamiltonian endowed with Gaussian Process prior with the rest of the generative model relating trajectories to the hamiltonian via dissipative dynamical equations, and noisy observations on top of these trajectories.\n\nThe prior covariance matrix on trajectories is approximated using RFF for an ARD base kernel and propagating the approximation via the linear operator specifying the dynamics.\n\nVariational inference under this (approximate) prior model is used, which conveniently allows to use the reparametrization trick to evaluate the ELBO.\n\nThe resulting method is evaluated on a number of experiments and compared to pre-existing methods.\n -- Strengths\n\nThe method is a useful extension of the SymGPR method to dissipative systems.\n\nThe combination of the prior on hamiltonian, dissipative dynamics and RFF appears original to me.\n\nThe combination of many different approximations (RFF, VI, stochastic evaluation) leads to a practical algorithm for the problem at hand.\n\nThe possibility to make predictions without or with different dissipation is expected but neat.\n\nThe results are impressive especially in the low data regime, where the method clearly outperforms alternatives in its prediction accuracy.\n\n-- Weaknesses\n\nI report here a few points that made the manuscript a bit difficult to read.\nIt is mostly about the motivation rather than the technical content.\n\n1) Previous work, especially the symGPR is not introduced which makes it difficult to understand the novelty here.\nWhat did they do exactly? did they also use RFF to approximate the covariance or do the calculation closed form?\n\n2) the many approximations introduced are not necessarily well motivated. Why do you do the RFF? Is it necessary?\n convenient for VI? for scalability? I have my guess but this needs to be more explicitely stated in the paper.\n \n3) The consequence of the approximations introduced are not discussed.\nDoes the RFF preserves the symplectic structure? What is lost by approximating the prior does it bias the inference? if so how?\nThe same applies to using VI.\n\n\nBecause of these perceived weaknesses,\nI set my score to weak accept. I m willing to change my evaluation if my concerns are adequately addressed\n I don't understand the following sentence:\n\"We used a block diagonal approximation of C so that each pair of basis functions shared the same covariance\".\nIs this mean field $q(C)=\\prod_i^M q(C_i)$, or $q(C) = \\tilde{q}(C)^K$. In both case, this has strong consequence on the inference.\nThese choices should be discussed.\n None",
            " Paper propose a spectral representation of Hamiltonian. Demonstrated the procedure to solve the equations of motion given a set of noisy measured trajectories (line 202).\n\nThe learning process is on the parameters used in the formalism of the special Hamiltonian (line 224). Strengths:\nThe formalism and learning process is very different from a traditional Gaussian process approach. Hence there is some novelty in this paper.\n\nBack information is well given and pitch at the correct level of sophistication. e.g. Hamiltonian systems, Gaussian process and literature review.\n\n\nWeakness:\nThe main mathematical framework is given in page 5 and 6. I find that these two pages of the paper is extremely difficult to follow. This is the main weakness of this paper. I explain the reasons why these two pages are difficult to follow:\n1. Eq 5. w_m is drawn from a normal distribution. In line 224, it is said that w_m is a learned parameter. Perhaps something is not explained clearly.\n2. Line 195: why p(f|w) is a Dirac's delta function? How do we get Eq 9?\n3. Line 208: justification needs to be given for distribution of x being a delta function Eq. 11\n4. There are undefined math symbols, e.g. Eq 17, b and C\n5. Does Eq 17 contradicts Eq 5?\n6. The physical meanings of equations should be explained better.\n\n\nIt will be much better if the author rewrite the manuscript to explain more clearly and in a higher level of what their method is trying to do. After a good intuition is given to the readers, the authors may dive into details of the mathematics.\n\n\n\n\n Please read the questions given in the section \"Strengths and Weakness\". Discussion on limitations is given in a short paragraph towards the end of the paper. The authors propose to extend this formalism into high dimensions.",
            " This paper proposed a symplectic spectrum Gaussian process method. The method can predict systems whose dynamics follow energy conservation and dissipation laws from noisy and sparse data. The proposed method is a general tool and has the potential to use the design of kernel machines with prior knowledge in physics. Although the proposed method is solid in theory, the scenarios applied are relatively simple. In addition, some classic examples such as vortex-particle and gravitational systems should be compared with the recently proposed methods. Strengths\n\nThis paper proposed a symplectic spectrum Gaussian process method for modeling Hamiltonian systems with additive dissipation. They derived a new spectral representation by incorporating the symplectic structure to handle energy dissipation and energy conservation systems. They also proposed a variational inference procedure that offers numerical integration of the ODE solver as a subroutine. Finally, they demonstrated some experiments on several physical systems to verify the accuracy of the proposed symplectic spectrum Gaussian process method. \n\nWeaknesses\n\nThe demonstrated examples seem to be cherry-picked, and recently proposed methods, such as SympNet, can also accurately predict simple Hamiltonian systems.\n- Adding some citations of recent papers related to symplectic neural networks would be helpful. \n- The symplectic neural networks are divided into separable and non-separable. The current paper seems to discuss only separable Hamiltonian systems, which needs to be clarified.\n- Limitations of the method need to be discussed.\n Overall, the exposition of the paper is pretty straightforward. However, I assume it to be difficult for readers who are not familiar with Gaussian process models. More specific comments:\n- The equations of the predicted systems should be given in the main text.\n- Is the order of the subgraphs in Figure 4 reversed.\n- The introduction of the theoretical model could be more concise.\n- The idea in Figure 2 is not clear. Can you show a clear schematic?\n- Why do the other methods in Figure 3 perform well in their papers?\n- The selection of some parameters should be discussed in detail. For example, time step, parameter size, data set size, etc.\n I would love to hear some limitations of the proposed paper about failure modes that the authors might have encountered.",
            " Many neural network-based methods for continuous-time physical systems have been proposed recently. These methods are based on ordinary differential equations and consistent with geometric structures. In practice, the available data is noisy and sparse. To deal with such situation, this study proposes to use Gaussian process (GP) consistent with geometric structures. Because tha data is composed only of observations (without derivatives), the GP uses the random Fourier features.\n *Strengths*\n\nTo my best knowledge, this is the first study to tackle the modeling of the Hamiltonian system (with dissipation) by using GP. Like the great success of Hamiltonian neural networks (Greydanus et al., NeurIPS, 2019), this study potentially opens up a new research field.\n\nThis study tackled the observation noise, which is a crucial issue in this field, but has been ignored by most existing studies.\n\nThe impact of the sampling rate on the performance, shown in Figure 7, is interesting. The proposed method is superior especially in the case of the sparse sampling.\n\n*Weakness*\n\nThe explanation about geometric structures is inexact. See (1) below.\n\nThe generality of the spectral representation is unclear. See (2) below.\n (1) At line 67, the authors stated that \"whose covariance function incorporates the geometric structure (also called symplectic structure) for the energy conservation or dissipation laws\". The geometric structures appearing in physics are not limited to the symplectic structure, but include contact structure, Poisson structure, Dirac structure, and so on. Moreover, the symplectic structure is related to the energy conservation law, but not to the dissipation law. Please introduce and discuss the geometric structure exactly.\n\n(2) To deal with the case without the true derivatives, this study proposes the symplectic random Fourier features. The experiments demonstrated that the features worked well for pendulum and Duffing oscillator, which exhibit periodic behaviors and are easily captured in the frequency domain. The generality for non-periodic cases such as double pendulums is unclear.\n\nI am curious about the impacts of noise level on the accuracy. The comparison methods, namely HNN and SymODEN, ignore noise. In the noiseless case, is SymODEN enough?\n\n*Minor comments:*\n\nThe experimental setting is a bit unclear.\n(a) The difference between HNN and SymODEN is unclear. HNN learns the Hamiltonian using a single neural network, and SymODEN learns the weight matrix and the potential energy using two networks, obtaining the Hamiltonian. Is this OK?\n(b) At line 298, the authors stated \"Since HNN, D-HNN and SympGPR require derivative observations for training, we used the finite difference instead.\" Wasthe finite difference used as an approximation to the derivative?\n\nThe images are shown in the reverse order of the captions in the leftmost and second leftmost columns in Figure 4.\n\nDue to the inexact discussion about the geometric structures, I cannot accept the present paper as it is. In this regard, I believe that a revision of the text would be sufficient. Then, I will be happy to raise the score. As discussed in Section I (and potentially shown in Figure 10), the extrapolation performance might be limited. The authors sincerely expressed this limitation. I believe that this limitation will not diminish the value of this study.\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('Thank you!') and indicates satisfaction ('I have no more questions.'), suggesting a positive assessment.",
            "The reviewer expresses gratitude ('thanks for the clarification'), indicating a positive reception of the previous communication. The suggestion for simplification also implies a constructive and helpful attitude.",
            "The reviewer expresses understanding and appreciation for the author's work ('Thanks for your comments', 'I understand past work and motivation and they are quite clear in your response', 'I have no further questions'). The suggestions are framed as ways to improve clarity, not criticisms of fundamental flaws.",
            "The review expresses a concern and requests further comparison, indicating a neutral stance with a suggestion for improvement.",
            "The reviewer states that new information resolved all their concerns and expresses willingness to raise the score, indicating a positive shift in their assessment.",
            "The review expresses overall positive sentiment, highlighting strengths such as the method's usefulness, originality, practicality, impressive results, and clear outperformance in low data regimes. Although weaknesses are identified, the reviewer indicates willingness to change their evaluation if concerns are addressed, suggesting a generally favorable view.",
            "The review identifies significant weaknesses in the paper's clarity and mathematical explanations, particularly concerning the core mathematical framework on pages 5 and 6. The reviewer finds these sections \"extremely difficult to follow\" and lists several specific issues, including inconsistencies, undefined symbols, and a lack of physical meaning, leading to an overall negative assessment.",
            "The review provides both positive (\"solid in theory\", \"general tool\") and negative feedback (\"scenarios applied are relatively simple\", \"demonstrated examples seem to be cherry-picked\"). It offers constructive criticism and suggestions for improvement.",
            "The review identifies several weaknesses, including an 'inexact' explanation of geometric structures and unclear generality of the spectral representation. The reviewer explicitly states they 'cannot accept the present paper as it is' and requires 'a revision of the text'. While acknowledging strengths, the overall assessment is critical and necessitates significant changes."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Supportive",
            "Formal",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer's expression of gratitude and the statement that they have no more questions indicates a supportive and agreeable stance.",
            "The reviewer expresses thanks and offers a constructive suggestion for improvement ('A simple way to say could be that...'), indicating a supportive tone.",
            "The reviewer uses phrases like 'Your manuscript would gain in clarity' and offers constructive suggestions for improvement, indicating a supportive and helpful tone. The phrase 'I have no further questions' also suggests satisfaction.",
            "The reviewer uses polite language ('Thank you', 'I hope') and maintains a professional tone throughout the review. The language is direct and focused on the technical aspects of the paper.",
            "The reviewer uses phrases like 'are also reasonable' and 'resolved all my concerns', indicating a supportive attitude towards the work.",
            "The review adopts a balanced tone by explicitly dividing the feedback into 'Strengths' and 'Weaknesses' sections. While acknowledging the method's merits, it also raises critical questions about motivation, clarity, and the consequences of approximations, indicating a fair and objective assessment.",
            "The tone is critical due to the direct and specific identification of weaknesses in the paper. Phrases like \"extremely difficult to follow\" and questions highlighting inconsistencies (\"Does Eq 17 contradicts Eq 5?\") demonstrate a critical assessment of the manuscript's clarity and mathematical rigor. The suggestion to \"rewrite the manuscript to explain more clearly\" further emphasizes the critical nature of the review.",
            "The review balances positive feedback (Strengths) with constructive criticism (Weaknesses and More specific comments). The language is generally objective and aims to improve the paper, rather than simply dismissing it. The reviewer also expresses a desire to understand limitations and failure modes, indicating a genuine interest in the work.",
            "The review uses direct and critical language. Phrases like 'inexact explanation,' 'generality is unclear,' and 'I cannot accept the present paper as it is' demonstrate a critical tone. The reviewer also poses direct questions and points out specific errors in the manuscript."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is very short and expresses a single, consistent sentiment of satisfaction and lack of further questions. There are no contradictory statements within the review.",
            "The review is consistent as it expresses gratitude for clarification and provides a constructive suggestion without any contradictory statements.",
            "The review is consistent as it provides constructive feedback and suggestions for improvement without any contradictions. The reviewer appreciates the author's response and offers a clear suggestion to enhance the manuscript's clarity.",
            "The review is consistent because the reviewer expresses a concern about the necessity of a large dataset in light of existing methods like SympNet, and logically requests a comparison with SympNet to address this concern. There are no contradictory statements.",
            "The reviewer corrects a factual error in their initial review but clarifies that this error does not affect their overall assessment and expresses willingness to raise the score, indicating consistency in their assessment.",
            "The review is consistent because the reviewer clearly outlines both strengths and weaknesses of the manuscript, and the final recommendation of 'weak accept' aligns with the balance of these points. The weaknesses are focused on clarity, motivation, and explanation of choices, rather than fundamental flaws in the methodology, which justifies the conditional acceptance.",
            "The review is consistent because it identifies a strength (novelty) and a major weakness (lack of clarity and potential inconsistencies in the mathematical framework) of the paper. The reviewer provides specific and detailed reasons for the identified weakness, without presenting contradictory statements or undermining their overall assessment. The recommendation to rewrite the manuscript to improve clarity directly stems from the identified weaknesses.",
            "The review is consistent in its assessment. It acknowledges the theoretical soundness of the proposed method while pointing out limitations in the application scenarios and comparisons with existing methods. The weaknesses and suggestions for improvement are logically connected and aim to enhance the paper's clarity, completeness, and empirical validation. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent in its assessment. It highlights both strengths (novelty, addressing noise, performance in sparse sampling) and weaknesses (explanation of geometric structures, generality of spectral representation, unclear experimental settings). The reviewer clearly states that the paper is not acceptable in its current form due to the weakness in explaining geometric structures, but also suggests that a revision addressing this point would be sufficient for acceptance and a score increase. This indicates a consistent stance: the paper has potential but needs specific improvements."
        ]
    },
    {
        "paper_id": "iclr_2021_n5go16HF_B",
        "paper_title": "Adversarial Data Generation of Multi-category Marked Temporal Point Processes with Sparse, Incomplete, and Small Training Samples",
        "paper_abstract": "Asynchronous stochastic discrete event based processes are commonplace in application domains such as social science, homeland security, and health informatics.  Modeling complex interactions of such event data via marked temporal point processes (MTPPs) provides the ability of detection and prediction of specific interests or profiles. We present a novel multi-category MTPP generation technique for applications where training datasets are inherently sparse, incomplete, and small. The proposed adversarial architecture augments adversarial autoencoder (AAE) with feature mapping techniques, which includes a transformation between the categories and timestamps of marked points and the percentile distribution of the particular category.  The transformation of training data to the distribution facilitates the accurate capture of underlying process characteristics despite the sparseness and incompleteness of data. The proposed method is validated using several benchmark datasets. The similarity between actual and generated MTPPs is evaluated and compared with a Markov process based baseline. Results demonstrate the effectiveness and robustness of the proposed technique.",
        "review_ids": [
            "HeGtEvLLnnA",
            "cIfQreIhy_T",
            "FWXH1UqsSln",
            "5Ugsi6Eaam-",
            "A7ouY0w7wMb"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary:\nThe authors propose a method for multi-category marked temporal point processes (MTPPs) generation with sparse, incomplete, and small training dataset. They apply Adversarial Autoencoder (AAE) and feature mapping techniques, which include a transformation between the categories and timestamps of marked points and the percentile distribution of the category.  The paper shows effectiveness and robustness of the proposed method by comparing with Markov chain approach on three datasets: Radicalization Dataset, Mimic III Dataset and Stack Overflow Dataset.\n\nStrength:\n1.\tThe paper is clear in general. Firstly, they define MTPP and multi-category MTPP. Then, the multi-category MTPP generation with sparse, incomplete, small training data problem is addressed.  The authors argue that it is a popular real-world problem where we only can access small and incomplete data.\n2.\tThe description of the overall method is reasonable. But more details can improve understanding.\n\nWeakness:\n1.\tThe authors should provide justification of choosing to use AAE in the work. In particular, why is AAE an attractive approach for MTPP? \n2.\t The authors mention: \"The typical MTTP baselines like Reinforcement Learning, RNNs, Wasserstein GANs require a significant amount of data to train a network. Therefore, such techniques are not applicable to our proposed approach. As the baseline, we compare the proposed data generation technique with a Markov chain approach which was applied to the same dataset in (Klausen et al., 2018b).\" The authors could try to apply Reinforcement Learning, RNNs, Wasserstein GANs with data filled by simple methods, to empirically validate that these methods are not suitable for incomplete, small data.\n3.\tIn feature mapping method, the authors could provide justification for converting marked point times t_ij to days a_i. Besides, the method to convert t_ij to a_i and examples are not provided. If this is a common preprocessing method in MTPP, the authors should cite the relevant work. Overall, Step 1 of Algorithm 1 is not clear. \n4.\tAt the data approximation technique (step 3 of the Algorithm 1), the author randomly chooses a probability for the appearance of an unobservable data point but there is a lack of explanations. Can the authors explain the reason of selecting from [0, Pcj(0)]?\n5.\tThe authors should provide more details of incomplete and small dataset, and compare with their method when training with full data. This is to understand that if generated data is still good when training with small dataset\n\n\n=============== \nafter rebuttal: I thank authors for the responses. After reviewing the authors' response and other reviewers' comments, I keep my original rating. \n",
            "Thanks for the authors' response. The authors' response partly confirmed my rating. I recommend the author to put all extra experimental details into the appendix, rather than stating \"the 3rd dataset also performs the same in all experiments\" which is not convincing. I still feel that the contribution of the paper is very limited for ICLR and would like to keep my original rating. ",
            "Thank you for the response, which addresses my concerns in part. Since $p_d (H)$ is still the empirical distributions of the interarrival time, the proposed method is essentially equivalent to a generic renewal process and the renewal intervals are iid, i.e., the $t_i$ you generated are not dependent on location or time. I would like to raise my rating a bit as the response cleared some of my concerns. However, I feel that the contribution of the paper is still not significant enough for acceptance. ",
            "The paper is concentrated at dealing with the data missing problem of MTPP and applies an AAE for the \u201cincomplete multi-categorical MTPPs\u201d.  First, the problem description appears questionable. Point processes are a class of stochastic processes for modelling discrete event sequences in a continuous time domain. They are statistical models and have a well-defined mathematical meaning. The expression of \u201cincomplete point process\u201d is quite confusing. One possible reason is that the authors fail to distinguish between the model and the data. One can say \u201cincomplete data\u201d or \u201cincomplete observations of a model\u201d, but \u201cincomplete model\u201d is not acceptable unless properly defined. Therefore, the proposed method seems not specifically for point processes but for the sequential data. \n\nThe authors seem to misunderstand the difference between empirical distribution and probabilistic distribution. The probabilities, e.g., $p_d(\\mathcal{H})$, are actually empirical distributions of the time. Here the authors see the arrival time of the events as a random variable, and $t_{ij}$\u2019s are independent samples of the random variable, which is unrelated to point processes. The percentiles used in the algorithm are essentially the cumulative empirical distributions of the arrival time. If point processes need to be considered here, the authors should define their probabilistic structures as they are probabilistic models, where the intensity function is often inevitable unless otherwise defined. I believe the confusing mathematical formulation should be considered as a fatal flaw.\n",
            "This work studies the generation technique for multi-category MTPP using adversarial autoencoders for sparse and incomplete datasets. To address the sparsity and incompleteness, some pre-processing and post-processing methods are utilized to adapt the data to AAE.\n\nStrengths: The model setup is reasonable, and the idea is straightforward to understand. \n\nI recommend rejection of the paper for the reasons below.\n\nWeakness: The major concern is the contribution of the work is very limited. The generation framework is a combination of adversarial auto-encoder and feature mapping encoder (decoder), where the AAE part is completely same as the original work in [Makhzani et al., 2015]. In my eyes, the only contribution is the addition of the feature encoder and decoder. Although the author named it like that, the so-called encoder and decoder are in fact some heuristic pre-processing and post-processing (shift and normalization) of the raw data. \n\nSome specific concerns: the notation in Eq.(2) is confused. If c_j is defined as the category, then c_j should \\in [1,m] not j. I understand what the author means by the current notation. It is better to define e_i=(t_i,c_i) where i \\in [1,n] and c_i \\in [1,m]. \nIn algorithm 1, the steps 1-3 are some heuristic pre-processing of the raw data. Why do that? Why change the timestamps to days and scale it to a probability value? Any intuition behind this operation? My understanding is the raw data is shifted, scaled and normalized to [0,1]. Add some theoretical analysis why performing that pre-processing will help AAE handle the sparse and incomplete data. \nIn experiments, there is only one baseline model to compare with. Add more DNN-based generative baseline models will make the experiment more convincing. \nIn experiments, although the author claimed 3 real datasets, I did not see any experimental results of the 3rd (stackoverflow) dataset. \n\nTypo: above Eq.(5), significance-->significant"
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses both strengths and weaknesses of the paper. While acknowledging the clarity and reasonable description of the method, it also points out several areas needing further justification, clarification, and empirical validation.",
            "The reviewer explicitly states that the contribution of the paper is 'very limited for ICLR' and expresses a desire to 'keep my original rating,' indicating dissatisfaction with the paper's impact and quality despite the authors' response.",
            "While the reviewer acknowledges that their concerns were partially addressed, they ultimately state that the paper's contribution is \"not significant enough for acceptance,\" indicating a negative overall sentiment.",
            "The review expresses strong criticism of the paper's problem description and methodology, using phrases like \"appears questionable,\" \"quite confusing,\" \"fail to distinguish,\" \"misunderstand,\" and \"fatal flaw.\"",
            "The reviewer recommends rejection and points out several weaknesses, including limited contribution, concerns about notation, lack of theoretical analysis for pre-processing steps, insufficient baseline models in experiments, and missing experimental results."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"should provide justification,\" \"lack of explanations,\" \"not clear,\" and \"could try to apply...to empirically validate\" which indicate a critical assessment of the paper's arguments and methodology. The reviewer also maintains their original rating after the rebuttal, suggesting their concerns were not fully addressed.",
            "The review uses phrases like 'not convincing' and 'very limited contribution,' which are direct criticisms of the authors' work. The reviewer also maintains their original rating, implying that the authors' revisions were insufficient to address their concerns.",
            "The tone is critical because the reviewer points out a potential flaw in the method (\"essentially equivalent to a generic renewal process\") and ultimately concludes that the paper's contribution is insufficient, despite acknowledging improvements.",
            "The tone is critical, using direct and negative language to point out perceived flaws in the paper. Examples include \"the problem description appears questionable,\" \"quite confusing,\" \"not acceptable,\" \"misunderstand,\" and \"fatal flaw.\"",
            "The reviewer uses phrases like \"major concern is the contribution of the work is very limited,\" \"the only contribution is the addition of the feature encoder and decoder,\" \"notation in Eq.(2) is confused,\" and \"I did not see any experimental results.\" These phrases indicate a critical tone and highlight the reviewer's concerns about the paper's quality and validity."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents both strengths and weaknesses and maintains the original rating after rebuttal, indicating a consistent evaluation based on the identified points. There are no contradictory statements within the review.",
            "The reviewer maintains their original rating despite the authors' response, indicating a consistent stance on the paper's limited contribution. The suggestion for improvement is a minor point and does not contradict the overall negative assessment of the paper's contribution.",
            "The review is consistent because it acknowledges that the authors addressed some concerns and expresses a willingness to raise the rating. However, it consistently maintains that the contribution of the paper is not significant enough for acceptance. There is no contradiction in acknowledging improvement while still having reservations about the overall significance of the work.",
            "The review consistently argues that the authors misunderstand fundamental concepts of point processes and their mathematical formulation, leading to questionable problem description and methodology.",
            "The reviewer acknowledges a minor strength (reasonable model setup and understandable idea) but immediately recommends rejection and provides substantial weaknesses and specific concerns that justify this recommendation. The weaknesses, such as limited contribution, heuristic pre-processing without theoretical justification, weak experiments with only one baseline, and missing results, strongly support the rejection recommendation, making the review consistent in its negative assessment."
        ]
    },
    {
        "paper_id": "iclr_2021_43VKWxg_Sqr",
        "paper_title": "Unsupervised Audiovisual Synthesis via Exemplar Autoencoders",
        "paper_abstract": "We present an unsupervised approach that converts the input speech of any individual into audiovisual streams of potentially-infinitely many output speakers. Our approach builds on simple autoencoders that project out-of-sample data onto the distribution of the training set. We use exemplar autoencoders to learn the voice, stylistic prosody, and visual appearance of a specific target exemplar speech. In contrast to existing methods, the proposed approach can be easily extended to an arbitrarily large number of speakers and styles using only 3 minutes of target audio-video data, without requiring any training data for the input speaker. To do so, we learn audiovisual bottleneck representations that capture the structured linguistic content of speech. We outperform prior approaches on both audio and video synthesis.\n      ",
        "review_ids": [
            "x7_Sw1_PWnP",
            "ZiG-N9jdFhb",
            "64xXqDOhwP7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Interesting paper covering a lot of ground; exposition could be tightened/clarified?\n\nSuper interesting topic; strong references to prior work (I add a reference at the very end that I think you'd be interested in, Hani Yehia et al. (2002).)\n\nI found the paper slightly verbose -- I'm not sure i fully \"got\" the central concept of Exemplar Autoencoders. Looking at Section 3, with that in the title, I was waiting to find the canonical definition of E.A.'s, but don't quite see it -- it seems somewhat unclear given the many variants of auto-encoders discussed. The discussion in Section 2, and e.g. Figure 2c, help clarify what the author's mean by EA's. What I'm not fully sure about is, are EA's basically what I would call Auto-encoders? Fig 3c is labeled \"Exemplar Autoencoder\", but then the caption for 3c mentions \"Non-linear autoencoder\", which is confusing. The caption starts with \"Figure 3: Our insights for Exemplar Autoencoders, \" leading me to think that actually all three of these sub-figures are variants of Exemplar Autoencoders. Maybe they could clarify their terminology or re-read the draft from the perspective of someone who is not as \"close\" to the work as they are.\n\nThe contrast with zero-shot conversion is very interesting -- but here too I feel I am somewhat missing the explanation of the essence of these zero-shot methods -- though it is possible most readers do not need any additional explanation. In particular, when I see Fig 2a, I'm wondering how the content and speaker embedding vectors are trained (and on what data), and the text doesn't quite clarify that for me.\n\nMore specific comments follow:\n\n\"(a) Zero-shot conversion that learns a generic low-dimensional embedding from a training set that _are_ designed ...\": \"are\" --> \"is\".\n\n\" In supp material, ...\" --> \"In supplementary material, ...\"\n\nDefine \"retargeting\"?\n\nTable 2: Use up & down errors, as you did in a later table, to indicate whether higher/lower is better.\n\nNo conclusion? It seems you have one, but it's \"Discussion\", as a subsection of the last experiments section.\n\nThe references seem very comprehensive, but I urge you to look at, and cite, Yehia et al. 2002, \n\nYehia, Hani Camille & Kuratate, Takaaki & Vatikiotis-Bateson, Eric. (2002). Linking facial animation, head motion and speech acoustics. Journal of Phonetics. 30. 555-568. 10.1006/jpho.2002.0165. \n\nas a seminal study in this area.\n",
            "In this paper, the authors propose a generic system for performing one-shot audiovisual synthesis from only one small sample. The results are impressive for in-the-wild speech synthesis and their approach could have a broader impact in the community. \n\nStrengths:\n + One shot audiovisual synthesis for a target speaker.\n + The publication of a new dataset for AV synthesis evaluation.\n + Comprehensive analysis\n\nWeaknesses: \n - No theoretical novelty. It seems much of the benefits of the approach comes from the extra data and training procedure. \n\nOther comments:\n\nIn Sec 3.2. Autoencoders as projection operators, the authors here make it sound that they are the first ones that noted that autoencoders can capture the data-generating distribution. ",
            "This paper covers a very interesting topic and method to convert any input speech to many audiovisual syntheses via exemplar autoencoders. The manuscript is well written and presented. It is easy to follow the concept. However, there are a few major concerns. \n\nPros: This approach is novel. The presented approach is unsupervised, which makes it more practical. This approach required only a small amount of data to train the exemplar autoencoders when learning specialized models tailored to particular target data.\n\ncons: \n1- After listening to the provided sample demo files. I think this approach is still in an immature status. The generated output speech is highly distorted, without knowing the input speech, it is hard to understand the generated speech. Hence, more work is required to improve on the audio decoder part. \n2- It is not mentioned if the data used for training is all speech of native English speakers or not.\n3- Any reasons authors did not use the x-vector or i-vectors, which are proper for the speaker characterization, and instead used mel features? \n\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment ('Interesting paper', 'Super interesting topic', 'strong references'). While it points out areas for improvement, the tone remains constructive and encouraging.",
            "The review expresses overall positive feedback, highlighting impressive results and potential impact on the community. The strengths listed further reinforce this positive sentiment.",
            "While acknowledging the paper's strengths (interesting topic, novel approach, unsupervised nature), the reviewer raises significant concerns about the quality of the generated speech, its intelligibility, and the lack of clarity regarding training data and feature selection. The explicit 'cons' section and the statement that the approach is 'still in an immature status' contribute to the negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review balances positive feedback with constructive criticism. It uses phrases like 'interesting paper' and 'super interesting topic' to show appreciation, but also points out areas where the exposition could be 'tightened/clarified' and expresses uncertainty ('I'm not sure I fully \"got\" the central concept'). The reviewer offers specific suggestions for improvement, indicating a balanced approach.",
            "The review presents both strengths and weaknesses of the paper, indicating a balanced perspective. While praising the results and impact, it also points out the lack of theoretical novelty and raises a concern about a specific claim in the paper, suggesting a neutral and critical assessment of different aspects.",
            "The review adopts a critical tone by pointing out specific flaws in the generated speech ('highly distorted', 'hard to understand') and questioning the authors' choices regarding training data and feature extraction ('Any reasons authors did not use x-vector or i-vectors...?'). The use of 'cons' to list the weaknesses further emphasizes the critical perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critique, highlighting the paper's interesting topic and strong references but consistently pointing out the need for improved clarity and exposition. The reviewer expresses confusion about key concepts and terminology, suggesting the paper needs to be more accessible to readers not already deeply familiar with the work.  The review maintains a constructive tone throughout, offering specific suggestions for improvement without contradicting itself.",
            "The review presents both strengths and weaknesses of the paper without contradicting itself. The strengths are focused on the practical results and resources (impressive results, new dataset, comprehensive analysis), while the weaknesses are focused on the theoretical novelty and a specific claim made in the paper. These points are distinct and do not create any inconsistency in the reviewer's assessment.",
            "The review is consistent because it starts with general positive remarks about the topic and presentation, but then clearly outlines major concerns and specific cons related to the quality of the generated speech and experimental details. The pros and cons sections are logically separated and address different aspects of the paper without contradicting each other. The reviewer acknowledges the potential of the approach while pointing out its current limitations."
        ]
    },
    {
        "paper_id": "nips_2022_sADLRl2STMe",
        "paper_title": "Combining Implicit and Explicit Regularization for Efficient Learning in Deep Networks",
        "paper_abstract": "Works studying implicit regularization have focused on gradient trajectories during the optimization process in explaining why deep networks favor certain kinds of solutions over others. For deep linear networks, it has been shown that gradient descent implicitly regularizes toward low-rank solutions in matrix completion/factorization tasks. Akin to an accelerative pre-conditioning, these effects become more pronounced with increased depth. Inspired by this, we propose an explicit penalty that mirrors the rank minimization and generalization performance independently of depth but interestingly only takes effect with certain adaptive gradient optimizers (e.g. Adam and some of its close variants)---performing competitively with or outperforming several approaches in matrix completion over a range of parameter and data regimes. Together with the choice of the optimization algorithm, our findings suggest that explicit regularization can play an important role in designing different, desirable forms of regularization and that a more nuanced understanding of this interplay may be necessary.",
        "review_ids": [
            "RuBHHMrJhU-",
            "-05C1AxCZuk",
            "aaiYGse5OXH",
            "472VXBrDArj",
            "XudPiCFayiz",
            "E0jAZKL3qI2",
            "ClzcxH2gAdk",
            "3LmF3znz-tj"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your explanations about this observation and for looking deeper into the possible reasons. I think the two hypothesis are reasonable and I agree that this would need a more in-depth analysis in future work.",
            " Thank you for answering my questions and considering the suggestions. In particular, I very positively value the inclusion of multiple runs per experiment and the update of the figures with error bands. I believe this should make a more convincing and solid paper.",
            " Maybe I'm very slow, but could the authors explain this question: Bottom line of page 19 (For some reason, the line numbers disappear), I cannot think of why the last equality holds.",
            " Thanks authors for your response, I enjoyed reading the paper. This work is very solid and I look forward to reading your future work on the convergence rates.",
            " This paper proposes a new regularization for deep linear networks. The analyses sections showed theoretical analysis empirical results of this regularizer in the context of 1-layer/deep networks and SGD/Adam optimizer in matrix completion tasks where the ground truth matrix is low-rank. The main experiment was conducted on a recommender system task, showing the proposed regularizer on very simple models could perform complicated methods. \n  ## Strengths:\n1. I quite like the style of studying simple tasks and the way that the authors interleave illustrative examples with theoretical analyses. \n2. The synthetic example is used to compare a wide range of alternative methods/regularizations, clearly showing the advantage of the proposal. \n2. The authors performed extensive parameter sweeps as described in the appendix. \n3. The authors are clearly technical in their analyses. \n\n## Weaknesses:\n1. Writing is unclear to the extent that I cannot clearly see if the authors delivered the promise made in the introduction/contribution paragraph. I'm also not an expert in this field so could just be a background mismatch, but I find that the main message is overcomplicated and partially obscured by very scattered writing. \n2. This method does not seem to be directly applicable to nonlinear networks, but the main experiment might have demonstrated this but the description was not clear. \n3. The main experiment was only conducted in a single setting and dataset, which is somewhat lacking.\n3. The main theoretical advantage seems to be some form of preconditioning, but the authors did not go deep enough to suggest why such preconditioning may be beneficial, apart from being pos-semidef.\n3. Mathematical presentation needs to be thoroughly tidied up. \n4. The synthetic experiments are trained under full-batch, which does not really produce stochastic gradients.\n\n## Details:\n1. Line 54: depth is not necessary compared to deep linear networks, not any networks. I also find this statement and many other statements very strong, and then realize that the authors probably mean in the context of deep linear networks. \n2. Lines 30-31 say the task is to \"recover the unseen entries\", then in 58-62 the author describes that their regularizaton has an effect on the \"rank and generalization\", and the figure clearly does not give errors on how well the model recovers unseen entries. What exactly is the goal of the task? Is it to recover the unseen entries? To get the correct rank? Or test prediction which I assume is generalization? Please be clear here and everywhere.\n3. Lines 120-121: This part describes how the student network weights are set up. The first time I read it, I thought the authors meant that they focused on rank-5 matrices in each layer, but then understood that the rank-5 applies to the teacher matrix $W^*$. \n4. 135: what is singular value separation? This is undefined in the paper but gets mentioned a lot later. I'm a non-expert but I think doesn't take an expert to understand, it must be some trends shown in the figures. \n5. Line 158: \"the trajectory of ... its singular values $\\sigma_i$\" does not appear in the Lemma. Is this in equation (14) of the Appendix?\n6.  I don't think the rates in equations (3) and (6) are ever proved in the appendix. \n7. Line 190, comparing figures 1 & 2, top rows, I don't think this statement is supported by the figures. The curves stabilize at mostly some iteration numbers. Further, the green curve in Test Error actually converges slower.\n8. Section 4: I have no idea what the authors have done for the recommender system. Please use equations or give a reference.\n9. Line 519: the authors should define the rank in the main text explicitly rather than giving a reference in the appendix. If an approximate rank is used, perhaps the authors should also try other possible rank definitions. \n10. Line 211: \"evolves\" -> \"evolve\" 1. Line 45: what does \"acceleration\" mean? Please specify or give examples here. Also, I don't think this paper later even partially answers this question, except by deferring it to some sort of preconditioning. \n1. In the main experiments, are the features produced by linear mapping or nonlinear networks? If it's nonlinear networks, how is the regularization applied? \n1. Line 291: what's meant by \"without any factorizations\"? Isn't the model itself a factorized representation of the data matrix?\n1. I see that most derivations aim to express dynamics as preconditioned gradient descent, but could the authors justify why this particular preconditioning is good? Why not any preconditioning?\n2. Buttom line of page 19 (For some reason, the line numbers disappear), I cannot think of why the last equality holds. What's the \"factorization of Kronecker product\"? I did a simple search but could not find any properties related to this line.  The discussion mentions some limitations and future work, but should also discuss limitations of the experiment design, including full-batch training, a single matrix factorization task and a single dataset. ",
            " Deep Linear Neural Networks gradient descent implicitly regularizes towards low-rank solutions in the matrix completion/factorization tasks. This work proposes an explicit penalty term that mimics the rank minimization behavior of deep linear networks. The proposed method is invariant to depth. A depth 1 linear neural network equipped with the proposed penalty produces the same rank reduction with comparable performance as deep matrix factorization. The authors note that their proposed penalty works really well with Adam optimizer but not with Gradient Descent. They speculate that the observed behavior could be due to the interaction of inductive biases inherent to the optimizers with explicit regularizers. They conduct a series of experiments to demonstrate this difference between Adam and Gradient Descent. Finally, they run experiments on real world data - MovieLens100k and show that they perform competitively against the existing methods which often use specialized architecture, parametrization, additional side information, etc.  Strengths:\n* This work takes a unique approach towards rank minimization. They propose an explicit penalty that mirrors the implicit rank minimization of deep matrix factorization.\n* The proposed method is depth invariant which is an interesting property. \n* The proposed method achieves better generalization and rank-reduction performance beyond many techniques even in low data regimes.\n*  The authors note that their proposed penalty works really well with Adam optimizer but not with other optimizers such as Gradient Descent. The authors have done sufficient empirical evaluations to demonstrate the differences between the behavior of the penalty term under different optimizers.\n\nWeaknesses:\n* The proposed method is depth invariant. If the authors could list potential areas where this could be useful, it would immensely help the paper.\n* It would be great if the authors could throw some theoretical insight into the reason behind performance discrepancy between various optimizers. If the authors could address the following weaknesses, I'd be more than happy to increase the scores -\n* The proposed method is depth invariant. If the authors could list potential areas where this could be useful, it would immensely help the paper.\n* It would be great if the authors could throw some theoretical insight into the reason behind performance discrepancy between various optimizers. The authors have adequately addressed the limitations and potential negative societal impact of their work.",
            " This paper proposes a penalty for gradient descent that consists of the ratio between the nuclear (trace) norm of the weight matrix and its Frobenius norm. The paper shows analytically and empirically, in the task of matrix completion, that linear networks trained with the proposed penalty exhibit a large degree of independence on depth, as long as the models are optimised with Adam (or Adamax), but not with vanilla gradient descent. In particular, the paper presents results that show that in the degenerate case, that is a 1-layer linear network, the model trained with Adam and the proposed penalty achieves generalisation similar to deep networks as well as low rank and spectral sparsity. This is, to the best of my judgement, an overall very good paper. First of all, the paper makes a very concrete contribution: the proposal of a penalty for gradient descent. The authors present a set of experiments that analyse 1) baseline conditions without the penalty, 2) the impact of the penalty on both vanilla gradient descent and models optimised with Adam, with particular attention to 3) the degenerate case (depth 1). Furthermore, the paper extends the empirical analysis by studying other optimisers and penalties, as well as some experiments on a real world data set (movies recommendations). With this experimental setup, the authors find an interesting result, namely that the proposed penalty induces certain degree of depth independence when the models are optimised with Adam but not with natural gradient descent. The paper provides a theoretical analysis of the dynamics under certain common assumptions and shows empirical results that analyse the generalisation performance, the rank of the weight matrix and the dynamics of the 1st and 5th singular values. \n\nIn terms of significance, I believe these are interesting results that can encourage further research into understanding the optimisation dynamics of neural networks and the interplay between implicit and explicit regularisation, which are topics that have attracted significant interest in the last decade. In terms of originality, the authors claim that the proposed penalty is novel and I am myself not aware of previous work proposing it, though I am not sufficiently up to date with the most recent literature on this specific domain.\n\nThe paper is very well written, much better than the average submission I have reviewed at machine learning conferences. The introduction is very clear and it smoothly motivates the problem by summarising the relevant related works and gaps in the literature. Section 2, on related work is easy to follow and concise. Then, the core section of the paper, Section 3, is very reasonably structured, following a logical order (setup, baseline models without penalty, models with the penalty, the degenerate case and other optimisers and penalties. The paper ends with a brief section where the role of the penalty is analysed on a real world data set and a discussion. I would also like to note that I have not identified any typo, which is extremely rare in the papers I have reviewed.\n\nRegarding quality, I think the paper is technically sound, the analyses seem correct to my judgement (though I have not thoroughly reviewed all the supplementary material) and the results support the claims. Furthermore, I would like to highlight that the authors humbly discuss the significance of their paper, in light of what the paper does show and does not show, as well as the limitations and aspects that are left for future work. This is, as well, and unfortunately, rare in machine learning submissions, but I highly appreciate it.\n\nI can identify some weaknesses, but these are minor and most are actually mentioned by the authors in Section 5. These mostly have to do with the limited scope of the experiments and analyses, which leave some open questions, such as: the role of the penalty on non-linear networks, on other tasks and data sets, whether other penalties can achieve the same depth-independence, a more thorough analysis of why the penalty only works with Adam and whether the same dynamics could be induced with a modification of the penalty on vanilla gradient descent. Since it is impossible to address all these questions properly in a single paper and the authors actually mention them, I consider these weaknesses minor, or not at all. \n\nFinally, one aspect that could be improved in the paper is the fact that the results (plots) from the empirical analyses seem to reflect single experiments. I think the conclusions and insights would be stronger if repetitions were carried out (with various initialisation seeds, for instance, and, ideally, choices of $\\Omega$) in order to obtain a measure of the confidence intervals for the dynamics. * Question: While the main claim is that the proposed penalty induces depth invariance with Adam, in Figure 2 we see some differences in the test error dynamics (bottom left) across different depths. In particular, with 2 layers, the dynamics are qualitatively very different and the model achieves lower error than with more layers. What do the authors think this is due to?\n* Suggestion: include repetitions in the experimental setup in order to obtain confidence intervals for the experimental analysis.\n* Suggestion: the actual proposed penalty is only expressed in the middle of a paragraph in Section 1. I think it would be clearer to include the explicit equation of the optimisation (Eq. 1) with the penalty. This would make it easier for future readers.\n* Suggestion: the title of Section 3.5, \"Comparative performance\" seems quite general. I would consider making it more specific, such as \"Comparison with other penalties and optimisers\". \n* Suggestion: same goes for the title of Section 4. How about \"Results on real-world data\"? As mention above, the authors do briefly but explicitly discuss the limitations of their paper.",
            " The authors develop a novel regularization method that improves the performance of shallow linear neural networks on tasks that can be represented as low-rank matrix completion/factorization. They offer theoretical results for analyzing such models under vanilla gradient descent vs. ADAM, and show that an adaptive scheme allows shallower networks to converge to better solutions. Strengths\n\nThe proposed method is simple and effective, supported by thorough theoretical and empirical justification.\nThe experiments are logical and comprehensive (including appendix), with good interesting results that are easy to interpret.\nThe text is clear and well-written. The figures are also clear.\n\n\nWeaknesses\n\nIn general, it would be helpful to be even more explicit about the connection between the forms of the various dynamics equations and the empirical results. For example, it would be nice to plot the predicted trajectories of the singular values against the actual trajectories. I still don't have good intuition for the behavior of the dynamics equations, so highlighting different regimes of behaviors would be helpful. For example, when the authors say that a term may accelerate convergence (line 168-9, 202), it would be helpful to describe (or give intuition for) the conditions under which such acceleration occurs, before jumping to the empirical demonstrations that acceleration is indeed observed. How does the convergence rate or error scale with effective rank of the underlying matrix (Appendix experiment does not really answer this)?\n\nAs the authors acknowledge in limitations, the scope of their findings is fairly narrow. Even though this is a theory paper, perhaps the motivation could be strengthened by noting that shallow networks are extremely fast and lightweight and thus could find use in applications where these attributes are important.\n\nThe title would suggest a very different line of work than what is being examined. Indeed, it is a little misleading in the sense that the paper's main results pertain to depth-1 and depth-2 linear networks, whereas the title writes \"deep networks\". IMO the title should highlight that the authors develop a novel regularization method that is effective for shallow networks. The title also suggests that the authors design a novel implicit regularization scheme, whereas it seems that implicit regularization refers to an existing property of gradient-based methods (the tendency to favor low-norm solutions). It seems that this implicit regularization is innate to both SGD and Adam, but it would be clearer if the authors instead highlight the property of adaptive optimizers that makes them more effective in the regime studied. Either make each lemma a theorem / corollary, or explicitly state the overarching theorem.\n\nLine 168-9: It would be helpful to characterize the condition under which normalization will dampen or accelerate the trajectory. Why does it accelerate in the case you specified?\n\nLine 230-1: \"This similarly holds for un-regularized Adam\" is confusing after reading the previous sentence, and suggests the opposite of what you want to say. Reword / swap the order.\n\nEqn (6): GD -> GF\n\nI think the appendix should include explicit proofs of Lemmas 2 and 3. Even if they are easy to compute, it will still help some readers follow along more easily.\n\nAdd bolding to Table 1 to highlight that your penalty is superior for all the adaptive optimizers. The authors acknowledge that the connection between linear and nonlinear networks in this context is not explored, and that the gradient flow regime may not hold in some practical settings. The authors point to good future directions."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('Thank you'), finds the hypotheses 'reasonable,' and agrees with the need for future work, indicating a positive evaluation.",
            "The reviewer explicitly states they \"very positively value\" the changes made to the paper. The reviewer also expresses that the updates \"should make a more convincing and solid paper.\"",
            "The reviewer expresses confusion and inability to understand a specific part of the paper, indicated by phrases like \"Maybe I'm very slow\" and \"I cannot think of why the last equality holds.\"",
            "The reviewer expresses enjoyment in reading the paper and describes the work as \"very solid.\" They also express anticipation for future work.",
            "The review expresses several concerns about the paper's clarity, experimental setup, and theoretical justifications. Phrases like 'Writing is unclear,' 'partially obscured by very scattered writing,' 'description was not clear,' 'somewhat lacking,' and numerous specific questions and criticisms indicate a negative sentiment overall.",
            "The review highlights several strengths of the paper, including its unique approach, depth invariance, and strong performance. While it also points out weaknesses, it expresses willingness to increase the score if these are addressed, indicating an overall positive sentiment.",
            "The reviewer states that the paper is \"overall very good\" and praises its concrete contribution, well-structured sections, technical soundness, and the authors' humble discussion of the paper's limitations. Phrases like \"interesting results,\" \"very well written,\" and \"highly appreciate it\" further reinforce the positive sentiment.",
            "The review contains both positive feedback (strengths of the paper: simple and effective method, clear writing, logical experiments) and negative feedback (weaknesses of the paper: unclear connection between dynamics equations and empirical results, narrow scope, misleading title, confusing statements). The reviewer suggests improvements but does not strongly reject or accept the paper."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Supportive",
            "Critical",
            "Balanced",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like 'Thank you' and 'I agree' which indicate a supportive and encouraging tone. The acknowledgement of the author's work ('for your explanations', 'for looking deeper') further contributes to this tone.",
            "The reviewer uses encouraging language such as \"Thank you\" and \"I believe this should make a more convincing and solid paper.\" This indicates a supportive and positive tone towards the authors and their work.",
            "The reviewer directly questions the authors about a specific point of confusion, using phrases like \"could the authors explain this question\" and stating they \"cannot think of why the last equality holds,\" indicating a critical evaluation of the paper's clarity.",
            "The reviewer uses positive language such as \"enjoyed reading\" and \"very solid,\" and expresses enthusiasm for future work, indicating a supportive tone.",
            "The tone is critical, as evidenced by the direct identification of weaknesses, detailed questions about specific lines and sections, and challenges to the validity of certain claims ('I don't think this statement is supported by the figures'). The reviewer also points out omissions and areas where the authors could have gone deeper.",
            "The review presents both strengths and weaknesses of the paper in a measured way. Phrases like \"Strengths:\" and \"Weaknesses:\" are used to clearly delineate positive and negative aspects. The reviewer also offers constructive suggestions for improvement and expresses willingness to increase the score if the weaknesses are addressed, indicating a balanced and helpful approach.",
            "The reviewer uses encouraging and appreciative language, such as \"I highly appreciate it,\" and offers constructive suggestions for improvement rather than harsh criticism. The reviewer also acknowledges the authors' own awareness of the limitations, indicating a supportive stance.",
            "The review presents both 'Strengths' and 'Weaknesses' sections, indicating a balanced assessment. The reviewer offers constructive criticism and suggestions for improvement, using phrases like 'it would be helpful' and 'could be strengthened,' which demonstrates a balanced and helpful approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and expresses agreement with the authors' approach and conclusions. There are no contradictory statements or conflicting opinions presented in the review.",
            "The review expresses positive feedback and appreciation for the changes made by the authors. All statements align with a positive assessment of the revised manuscript, indicating consistency.",
            "The review is consistent as it presents a single, clear question without any contradictions.",
            "The review is consistently positive and expresses appreciation for the paper. All statements are in agreement and there are no contradictions.",
            "The review praises the style of studying simple tasks and the technical depth, but strongly criticizes the writing clarity to the point of not understanding the main message. This creates an inconsistency because if the writing is so unclear that the reviewer cannot grasp the core contributions, the positive aspects of the style and technicality are undermined. The review expresses appreciation for the approach but simultaneously states that the execution (writing) is so poor that it hinders understanding, making the overall assessment somewhat inconsistent.",
            "The review is consistent. While the reviewer mentions 'depth invariance' and 'optimizer discrepancy' in both strengths and weaknesses, these are not contradictions. The reviewer acknowledges them as positive aspects while also suggesting areas for improvement or further investigation, which is typical in peer reviews and does not indicate inconsistency.",
            "The review is consistently positive, praising the paper's contributions, experimental design, results, writing quality, and technical soundness. While mentioning minor weaknesses, the reviewer explicitly states they are minor and even acknowledged by the authors, reinforcing the overall positive assessment and consistency of the review.",
            "The review is consistent because the weaknesses pointed out are constructive suggestions for improvement and do not contradict the strengths acknowledged. The reviewer appreciates the novelty and effectiveness of the method, supported by theory and experiments, while suggesting improvements in clarity, scope, and presentation. The weaknesses are focused on enhancing the paper rather than undermining its core contributions."
        ]
    },
    {
        "paper_id": "iclr_2019_r1f78iAcFm",
        "paper_title": "GRAPH TRANSFORMATION POLICY NETWORK FOR CHEMICAL REACTION PREDICTION",
        "paper_abstract": "We address a fundamental problem in chemistry known as chemical reaction product prediction. Our main insight is that the input reactant and reagent molecules can be jointly represented as a graph, and the process of generating product molecules from reactant molecules can be formulated as a sequence of graph transformations. To this end, we propose Graph Transformation Policy Network (GTPN) - a novel generic method that combines the strengths of graph neural networks and reinforcement learning to learn the reactions directly from data with minimal chemical knowledge. Compared to previous methods, GTPN has some appealing properties such as: end-to-end learning, and making no assumption about the length or the order of graph transformations. In order to guide model search through the complex discrete space of sets of bond changes effectively, we extend the standard policy gradient loss by adding useful constraints. Evaluation results show that GTPN improves the top-1 accuracy over the current state-of-the-art method by about 3% on the large USPTO dataset. Our model's performances and prediction errors are also analyzed carefully in the paper.",
        "review_ids": [
            "BylmJnLcnX",
            "Hkx3QH_uyE",
            "Hke2ZC4B14",
            "SJegDj_53Q",
            "rJldqyBDnX"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Update:\n\nScore increased.\n\n___________________________________\n\nOriginal review:\n\nThe paper presents an approach to predict the products of chemical reactions, given the reactants and reagents. It works by stepwise predicting the atom pairs that change their bonds in course of reaction, and then adjusting the bonds between them. This can be interpreted as a stepwise graph transformation.\n\nI think this is an interesting applied ML paper with fair results. The presentation is clear and understandable. The experimental setup is reasonable. However, the paper is not ready yet to be accepted in my opinion.\n\nI think a higher score is justified if the authors address the following points:\n\n- Relation to previous work, originality\n\nIn contrast to what the authors claim, what is predicted here is not exactly the reaction mechanism, but an implementation of the principle of minimal chemical distance, which was already described by Ugi and coworkers in 1980 [see Jochum, Gasteiger, Ugi, The Principle of Minimum Chemical Distance Angew. Chem. Int. Ed. Engl. 1980, 19, p 495-505]. \nThe \u201cinsight\u201d the authors have about treating reagents and reactants jointly is Organic Chemistry 101, and that reactions are stepwise graph transformations was also reported by Ugi et al, however, already in 1979! [Ugi, et al. \"New applications of computers in chemistry.\" Angewandte Chemie International Edition in English 18.2 (1979): 111-123. ] I assume the authors were not aware of these papers, but now they are, so this needs to be modified accordingly, and these papers need to be referred to in the introduction. \n\n\n\n- Questions:\n\nThe authors suggest that graph neural networks are more generic that so-called heuristic features (fingerprints) \u2013 which, as Duvenaud et al have elaborated, can be interpreted just as graph neural networks themselves \u2013 with fixed weights. Also, there are results by the Hochreiter group which show that graph neural networks perform worse that classical chemical features under rigorous testing { DOI: 10.1039/C8SC00148K } Do the authors think their models could also improve if they used the classical fingerprints?\n\n\nIs the GRU really needed to encode the past bond changes? What happens if you remove it?\n\nThe statement that the method has the advantage of not relying on handcrafted reaction templates is somewhat overselling, because instead it uses a handcrafted complex neural network architecture. How complicated is it to train the network? If you remove some of the \u201ctricks\u201d of shaping the loss function, does it still train well?\n\n\nTo what degree is the ranking of the different models just a matter of hyperparameter tuning or different architectures? If you used a different graph neural net instead of an MPNN on top of your GTPN method, what would you expect? Are the differences between the models significant?\n\n\nDuring prediction, you apply a flag to the starting molecules if they are a reagent or reactant. How do you know upfront what a reagent or reactant is during inference? \n\nOn page 5 and 7, you speak of the correct sequence of reaction triples (which implies an ordering), even though earlier you claim the algorithm is order-invariant? Where do you get the ground truth labels from? I assume these are already annotated in the data.\n\n\nIn the appendix, please replace the pie chart with a bar chart.\n\n- Language:\nI would suggest the authors to adapt the language of their paper towards a more academic tone. Science is not a sports competition of getting slightly higher numbers in benchmarks, but rather about providing insights and explanations. Words like \u201cbeating\u201d or \u201crecord\u201d are locker room talk, and to be avoided.",
            "I want to thank the authors for the update, and have increased the score.",
            "Although the authors have addressed some concerns, I am now even more concerned that this paper is way too long to be fairly compared to other submissions that stick to the guideline of using ten pages.\n\nThis also shows in the very long review responses by the authors.  ",
            "Summary:\nThis paper develops a novel method for predicting organic chemical reactions, in particular, final product prediction from given reactants. Organic molecules consist of covalent bonds, and hence organic reactions can be regarded as an alternating multistep series of bond breaking and bond forming (i.e. qualitative understanding as \"reaction mechanisms\" against quantum chemical calculations). The developed method aims to predict this series of bond changes through a form of reinforcement learning guided with neural networks. In this sense, the setup and formulation seem largely inherited from cited previous papers by Bradshaw et al, 2018 or Kayala & Baldi, 2011 (though they are not used any RL formulation). Organic compounds at each elementary step are represented as molecular graphs, and reactions are thus a series of graph transformations. Each bond change can be considered as \"action\" at that state to form the next states to head for the final product. The method itself seems quite natural: States transitions are shared by an RNN and the hidden states and observations (molecular graphs at each step, and bond changes as actions) are used to learn \"policy\" and \"state transition\" for RL via graph neural networks. Atom pairs to lead the bond change are detected from such graph embeddings and state observations through self-attentive architectures. In addition, masking by additional indicator variables is introduced to avoid redundant training as well as determine the termination of the reaction. Experimental evaluations on a standard large benchmark dataset of USPTO show improved prediction performance compared to previous methods of Jin et al, 2017 and Schwaller et al, 2018.\n\nComment:\n- Given that a chemical reaction can be regarded as a multi-step chain of bond breaking and forming, thus the method part seems a quite natural extension of the past effort but also sounds rather incremental even though the performance gain exists.\n\n- The method part is written clearly but the problem setup seems rather unclear. The most unclear point is how the environment for RL can give any reward to each bond change. How we can know each prediction of a bond change is correct or not? Can it be supervised? Does this mean that the training set of reactions has the correct perfect information of these multi-step bond changes?? How did you construct such curated dataset for USPTO? If this is the case, the motivation to go for RL would be more understandable but this part is not explained at all. (Because it is inherited from previous work of Bradshaw et al 2018 or something?? )\n\n- Why this proposed method has no limitation whereas the previous method by Bradshaw et al, 2018 are limited to \"linear chain topology\" (?). If this method is the direct competitor, what points are important to remove this limitation of the previous method should be clarified more clearly. This method is referred multiple times in the paper, but no direct explanations exist. \n\nPros:\n\t- Nice and solid design of proposed \"graph transformation policy network\"\n\t- Better prediction performance against previous methods\n\nCons:\n\t- Why this extension could break the previous limit (of ELECTRO?) remains unclear\n\t- The descriptions on the problem setup and the data are unclear. Perfectly curated as chemically correct multi-steps of bond changes are given as training set? How we can know each prediction of a bond change is correct or not?\n\t- the proposed architecture is nice but somewhat seems incremental. (heuristic combinations of existing techniques of RL and graph neural nets)\n",
            "The paper provides a new system that combines a number of neural networks to predict chemical reactions. The paper brings together a number of interesting methods to create a system that outperforms the state of the art.\n\nGood about this paper: \n - reported performance: the authors report a small but very consistent performance improvement.\n - the authors propose an approach that puts together many pieces to become an effective approach to chemical reaction prediction. \n -\n\nProblematic with this paper\n - this paper is impossible to understand if you only refer to the ten pages of content. There are at least 5 pointers in the paper where the authors refer to the appendix for details. Details in many of these cases are necessary to even understand what is really being done:  p3: rewards p4: message passing functions, p5: updating states, p9: training details. Further, The paper has some details that are unnecessary - e.g. the discussion of global vs. local network on p4 - this could go into the appendix (or be dropped entirely)\n\n - the model uses a step-wise reward in the training procedure (p3) -> positive reward for each correct subaction. It is not clear from the paper whether the model requires this at test time too (which should not be available). It's not clear what the authors do in testing. I feel that a clean RL protocol would only use rewards during training that are also available in testing (and a final reward)\n\n - eq 7: given there is an expontential in the probability - how often will the sampling not pick the top candidate? feels like it will mostly pick the top candidate. \n\n - eq 9: it's unclear what this would do if the same pair of atoms is chosen twice (or more often)\n\n - the results presented in table 3: it appears that GTPN alone (and with beam search) is worse than the previous state of the art. only the various post processing steps make it better than the previous methods. It's not clear whether the state of the art methods in the table use similar postprocessing steps or whether they would also improve their results if the same postprocessing steps were applied. \n \n\nminor stuff: \np2: Therefore, one can view GTPN as RL -> I don't think there is a causality. Just drop \"Therefore\"\np2: standard RL loss -> what is that? \neq. 2: interestin gchoice to add the vectors - wouldn't it be easier to just concatenate?\np4: what does \"NULL\" mean? how is this encoded?\np4 bottom: this is quite uncommon notation for me. Not a blocker but took me a while to parse and decrypt.\np5: how are the coefficients tuned?"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review raises significant concerns about the paper's originality, relation to previous work, and overclaims. It also points out several questions regarding the methodology and experimental setup, indicating dissatisfaction with the current state of the paper. The reviewer also criticizes the language used in the paper.",
            "The reviewer expresses gratitude ('thank the authors') and indicates an improvement in their assessment ('increased the score').",
            "The reviewer expresses increased concern about the paper's length, stating it's \"way too long\" and unfair compared to other submissions. This indicates a negative sentiment towards the manuscript.",
            "The review raises several critical questions about the methodology, problem setup, and clarity of the paper. Phrases like \"rather incremental,\" \"unclear,\" and the repeated questioning of the data curation process indicate a skeptical and critical evaluation.",
            "The review identifies several significant issues, including lack of clarity, reliance on the appendix for essential details, unclear testing procedures, questions about the model's design choices (reward system, equation implementations), and concerns about the reported performance compared to the state of the art. The reviewer also points out multiple instances of missing information and questions about specific equations and notations."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct and challenging questions. Phrases like \"In contrast to what the authors claim...\", \"The statement that the method has the advantage... is somewhat overselling\", and \"How do you know upfront...\" demonstrate a critical tone. The reviewer also uses phrases like \"Organic Chemistry 101\" and \"locker room talk\" which indicate a critical and somewhat dismissive attitude towards certain aspects of the paper.",
            "The reviewer uses appreciative language ('thank the authors') and explicitly states that they have increased the score, indicating a positive shift in their evaluation.",
            "The reviewer uses phrases like \"even more concerned\" and \"way too long\" which convey a critical tone. The reference to the \"very long review responses\" also suggests a critical perspective on the authors' handling of feedback.",
            "The reviewer employs phrases like \"rather incremental,\" \"unclear,\" and poses several direct questions that challenge the paper's methodology and clarity, indicating a critical and probing tone.",
            "The tone is critical, evidenced by phrases like \"impossible to understand\", \"not clear\", \"unclear what this would do\", and direct questions raising doubts about the methodology and results. The reviewer also uses words like \"problematic\" to describe the paper's issues."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critical assessment of the paper. Despite the \"Score increased\" update, the reviewer raises significant concerns about the paper's originality, relation to prior work, and methodological choices. The questions posed are critical and point to areas needing substantial improvement. The overall tone, even with the score update, remains that the paper is not yet ready for acceptance without major revisions addressing the identified issues.",
            "The review expresses a positive sentiment in both parts, thanking the authors for the update and stating an increased score. These statements are consistent and logically connected.",
            "The review is consistent because it focuses on the single concern of the paper being too long. The reviewer argues that the paper's length is unfair compared to guideline-adhering submissions and uses the length of the author's response as further evidence of the paper's excessive length.",
            "The review is consistent in its criticism, repeatedly pointing out the unclear problem setup, data description, and the incremental nature of the method. The reviewer consistently questions the novelty and the explanation of how the proposed method overcomes limitations of previous work. The concerns raised in the 'Comment' section are reiterated and reinforced in the 'Cons' section, indicating a consistent line of critique throughout the review. While acknowledging positive aspects, the core message remains consistently critical of the clarity and novelty of the paper.",
            "The review is consistent in its overall assessment. While it acknowledges positive aspects like performance improvement and the combination of methods, the core of the review focuses on significant problems related to clarity, completeness, methodology, and result interpretation. The 'Problematic' section heavily outweighs the 'Good' section, indicating a consistent critical stance. There are no contradictory statements within the review."
        ]
    },
    {
        "paper_id": "iclr_2019_Bygre3R9Fm",
        "paper_title": "DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation",
        "paper_abstract": "Generating novel molecules with optimal properties is a crucial step in many industries such as drug discovery. \n      Recently, deep generative models have shown a promising way of performing de-novo molecular design. \n      Although graph generative models are currently available they either have a graph size dependency in their number of parameters, limiting their use to only very small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters, therefore preventing them to be used in scenarios such as conditional graph generation. In this work we propose a model for conditional graph generation that is computationally efficient and enables direct optimisation of the graph. We demonstrate favourable performance of our model on prototype-based molecular graph conditional generation tasks.",
        "review_ids": [
            "rylQT5-c27",
            "B1glS5H76X",
            "SklCA18chQ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.\n\nStrength:\n\n1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. \n\n2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.\n\nWeakness:\n\n1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.\n\n2, Authors motive their work by saying in the abstract that \u201cother graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters\u201d. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.\n\n3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.\n\n4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?\n\n5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.\n\nTypos:\n\n1, There are a few references missing (question mark) in the first and second paragraphs of section 2.\n\n2, Methods in the experiment section are given without explicit reference, like GCPN.\n\n3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. \n\nOverall, I do not think this paper is ready for publishing and it could be improved significantly.\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nUpdate:\n\nThanks for the detailed explanation. The new figure 1 is indeed helpful for demonstrating the overall idea. \n\nHowever, I still found some claims made by authors problematic. \nFor example, it reads in the abstract that \"...or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters...\". \nClearly, Li et al. 2018b has a differentiable formulation which falls under your description.\n\nBesides, I suggest authors adjust the experiment such that it focuses more on comparing conditional generation. \nAlso, please set up some reasonable baselines based on previous work rather than saying it is not directly comparable.\nDirectly taking numbers from other papers for a comparison is not a good idea given the fact that these experiments usually involve quite a few details which could potentially vary significantly.\n\nTherefore, I would like to keep my original rating. \n",
            "The paper proposes a conditional graph generation that directly optimizes the properties of the graph. The paper is very weak.\n1. I think almost all probabilistic graph generative models are differentiable. If the  objective is differentiable function of real   \n    variables, it is usually differentiable.\n\n2.  The authors claim that existing works Simonovsky and Komodakis (2018) and Cao & Kipf (2018) are restricted to use small graphs with predefined maximum size. This work does not overcome the limitation of small graphs issue too.\n\n3. The authors do not show any measure on validity, novelty or uniqueness which are now standard in literature.\n   Also I do not find any comparison with molGAN paper which tackles a similar objective.\n\n4. Could the authors show if the decoding process is permutation invariant? I am not really sure of that. I was trying to prove that thing formally, but I failed.\n\n\n ",
            "This paper proposed a variant of the graph variational autoencoder [1] to do generative modeling of graphs. The author introduced an additional conditional variable (e.g., property value) into the decoder. By backpropagating through the discriminator, the model is able to find the graph with desired property value. \n\nOverall the paper reads well and is easy to follow. The conditional generation of graphs seems also helpful regarding the empirical performance. However, there are several concerns regarding the paper:\n\n1) The edge factorization-based modeling is not new. In fact [1] already uses the node embeddings to factorize the adjacency matrix. This paper models extra information including node tags and edge types, but these are not fundamental differences compared to [1].\n\n2) The paper claims the method is \u2018cheaper\u2019 and \u2018scalable\u2019. Since essentially the computation cost is similar to [1] which requires at least O(n^2) to generate a graph with n nodes, I\u2019m not super confident about the author\u2019s claim. Though this can be parallelized, but the memory cost is still in this order of magnitude, which might be too much for a sparse graph. Also there\u2019s no large graph generative modeling experiments available.\n\n3) Continue with 2), the adjacency matrix of a large graph (e.g., graph with more than 1k nodes) doesn\u2019t have to be low rank. So modeling with factorization (with typically ~256 embedding size) may not be suitable in this case. \n\nSome minor comments:\n4) Regarding Eq (2), why the lstm is used, instead of some simple order invariant aggregation?\n\n5) the paper needs more refinement. E.g., in the middle of page 2 there is a missing citation. \n\n[1]  Kipf & Welling, Variational Graph Auto-Encoders, https://arxiv.org/pdf/1611.07308.pdf\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's readiness for publication due to missing comparisons, unclear explanations, lack of ablation studies, and problematic claims. The reviewer maintains their original negative rating after the update.",
            "The review contains multiple criticisms, including the statement that the paper is \"very weak,\" questioning the novelty and validity of the approach, and pointing out limitations. The reviewer expresses doubts about the decoding process and highlights missing comparisons with relevant work.",
            "The review raises several concerns about the paper's novelty, scalability claims, and modeling approach. The reviewer questions the originality of the edge factorization, the validity of the 'cheaper' and 'scalable' claims, and the suitability of the factorization method for large graphs. These concerns, along with the need for more refinement, indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"missing comparisons,\" \"not explained,\" \"not written clearly,\" \"problematic claims,\" and \"not a good idea,\" indicating a critical assessment of the paper's methodology and presentation.",
            "The tone is critical due to the direct negative assessment (\"The paper is very weak\"), the use of questioning statements (\"Could the authors show if the decoding process is permutation invariant?\"), and the identification of shortcomings in the paper's contributions and comparisons to other works (\"The authors do not show any measure on validity, novelty or uniqueness\", \"I do not find any comparison with molGAN paper\").",
            "The review uses direct questions and challenges the author's claims with phrases like 'not super confident about the author\u2019s claim,' 'may not be suitable,' and 'the paper needs more refinement.' The numbering of concerns and minor comments also contribute to the critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer maintains a consistent negative stance throughout both the initial review and the update.  In the initial review, the reviewer lists several weaknesses related to experimental validation, model clarity, and justification of design choices, concluding the paper is not ready for publishing. In the update, while acknowledging the improvement with Figure 1, the reviewer reiterates concerns about problematic claims and the need for better experimental setup, ultimately maintaining their original negative rating. There is no contradiction in the reviewer's feedback; they consistently point out areas needing improvement and maintain a critical perspective on the paper's readiness for publication.",
            "The reviewer consistently points out weaknesses and limitations of the paper throughout the review, without contradicting themselves.",
            "The review is consistent in its critique. While acknowledging the paper's readability and potential, it raises several valid concerns regarding novelty, scalability, and the suitability of the proposed method for large graphs. The reviewer's arguments are logically connected and build a coherent critical assessment of the paper."
        ]
    },
    {
        "paper_id": "iclr_2018_r1saNM-RW",
        "paper_title": "Small Coresets to Represent Large Training Data for Support Vector Machines",
        "paper_abstract": "Support Vector Machines (SVMs) are one of the most popular algorithms for classification and regression analysis. Despite their popularity, even efficient implementations have proven to be computationally expensive to train at a large-scale, especially in streaming settings. In this paper, we propose a novel coreset construction algorithm for efficiently generating compact representations of massive data sets to speed up SVM training. A coreset is a weighted subset of the original data points such that SVMs trained on the coreset are provably competitive with those trained on the original (massive) data set. We provide both lower and upper bounds on the number of samples required to obtain accurate approximations to the SVM problem as a function of the complexity of the input data. Our analysis also establishes sufficient conditions on the existence of sufficiently compact and representative coresets for the SVM problem. We empirically evaluate the practical effectiveness of our algorithm against synthetic and real-world data sets.",
        "review_ids": [
            "H11vg1wVf",
            "Byu50uOxf",
            "BJu6VdYlf",
            "rkydZLAef"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I checked the new experimental results. \n\nThe new sampling method provides moderate improvement over the naive uniform sampling (in many cases). \nThe running time part is not so convincing, as in many cases, it is significantly slower than other methods.\nAlso, some text explaining those figures should be helpful.\n\nwhy the last figure in Figure 6 only has 2 curves?\n\nThe new results are certainly helpful. \nBut in my opinion, the paper may not be a clear accept of ICLR.",
            "This paper studies the approach of coreset for SVM. In particular, it aims at sampling a small set of weighted points such that the loss function over these points provably approximates that over the whole dataset. This is done by applying an existing theoretical framework to the SVM training objective.\n\nThe coreset idea has been applied to SVM in existing work, but this paper uses a new theoretical framework. It also provides lower bound on the sample complexity of the framework for general instances and provides upper bound that is data dependent, shedding light on what kind of data this method is suitable for. \n\nThe main concern I have is about the novelty of the coreset idea applied to SVM. Also, there are some minor issues:\n-- Section 4.2: What's the point of building the coreset if you've got the optimal solution? Indeed one can do divide-and-conquer. But can one begin with an approximation solution? In general, the analysis of the coreset should still hold if one begins with an approximation solution. Also, even when doing divide-and-conquer, the solution obtained in the first line of the algorithm should still be approximate. The authors pointed out that Lemma 7 can be extended to this case, and I hope the proof can be written out explicitly.\n-- section 2, paragraph 4: why SGD-based approaches cannot be trivially extended to streaming settings? \n-- Definition 3: what randomness is the probability with respect to? \n-- For experiments: the comparison with CVM should be added.\n",
            "The paper suggests an importance sampling based Coreset construction for Support Vector Machines (SVM). To understand the results, we need to understand Coreset and importance sampling: \n\nCoreset: In the context of SVMs, a Coreset is a (weighted) subset of given dataset such that for any linear separator, the cost of the separator with respect to the given dataset X is approximately (there is an error parameter \\eps) the same as the cost with respect to the weighted subset. The main idea is that if one can find a small coreset, then finding the optimal separator (maximum margin etc.) over the coreset might be sufficient. Since the computation is done over a small subset of points, one hopes to gain in terms of the running time.\n\nImportance sampling: This is based on the theory developed in Feldman and Langberg, 2011 (and some of the previous works such as Langberg and Schulman 2010, the reference of which is missing). The idea is to define a quantity called sensitivity of a data-point that captures how important this datapoint is with respect to contributing to the cost function. Then a subset of datapoint are sampled based on the sensitivity and the sampled data point is given weight proportional to inverse of the sampling probability. As per the theory developed in these past works, sampling a subset of size proportional to the sum of sensitivities gives a coreset for the given problem.\n\nSo, the main contribution of the paper is to do all the sensitivity calculations with respect to SVM problem and then use the importance sampling theory to obtain bounds on the coreset size. One interesting point of this construction is that Coreset construction involves solving the SVM problem on the given dataset which may seem like beating the purpose. However, the authors note that one only needs to compute the Coreset of small batches of the given dataset and then use standard procedures (available in streaming literature) to combine the Coresets into a single Coreset. This should give significant running time benefits. The paper also compares the results against the simple procedure where a small uniform sample from the dataset is used for computation. \n\n\nEvaluation: \nSignificance: Coresets give significant running time benefits when working with very big datasets. Coreset construction in the context of SVMs is a relevant problem and should be considered significant.\n\nClarity: The paper is reasonably well-written. The problem has been well motivated and all the relevant issues point out for the reader. The theoretical results are clearly stated as lemmas a theorems that one can follow without looking at proofs. \n\nOriginality: The paper uses previously developed theory of importance sampling. However, the sensitivity calculations in the SVM context is new as per my knowledge. It is nice to know the bounds given in the paper and to understand the theoretical conditions under which we can obtain running time benefits using corsets. \n\nQuality: The paper gives nice theoretical bounds in the context of SVMs. One aspect in which the paper is lacking is the empirical analysis. The paper compares the Coreset construction with simple uniform sampling. Since Coreset construction is being sold as a fast alternative to previous methods for training SVMs, it would have been nice to see the running time and cost comparison with other training methods that have been discussed in section 2.\n",
            "The paper studies the problem of constructing small coreset for SVM.\nA coreset is a small subset of (weighted) points such that the optimal solution for the coreset is also a good approximation for the original point set. The notion of coreset was originally formulated in computational geometry by Agarwal et al.\n(see e.g., [A])\nRecently it has been extended to several clustering problems, linear algebra, and machine learning problems. This paper follows the important sampling approach first proposed in [B], and generalized by Feldman and Langberg. The key in this approach is to compute the sensitivity of points and bound the total sensitivity for the considered problem (this is also true for the present paper). For SVM, the paper presents a bad instance where the total sensitivity can be as bad as 2^d. Nevertheless,\nthe paper presents interesting upper bounds that depending on the optimal value and variance of the point set. The paper argues that in many data sets, the total sensitivity may be small, yielding small coreset. This makes sense and may have significant practical implications.\n\nHowever, I have the following reservation for the paper.\n(1) I don't quite understand the CHICKEN and EGG section. Indeed, it is unclear to me \nhow to estimate the optimal value. The whole paragraph is hand-waving. What is exactly merge-and-reduce? From the proof of theorem 9, it appears that the interior point algorithm is run on the entire dataset, with running time O(n^3d). Then there is no point to compute a coreset as the optimal solution is already known.\n\n(2) The running time of the algorithm is not attractive (in both theory and practice).\nIn fact, the experimental result on the running time is a bit weak. It seems that the algorithm is pretty slow (last in Figure 1). \n\n(3) The theoretical novelty is limited. The paper follows from now-standard technique for constructing coreset.\n\nOveral, I don't recommend acceptance.\n\nminor points:\nIt makes sense to cite the following papers where original ideas on constructing coresets were proposed initially.\n\n[A]Geometric Approximation via Coresets\nPankaj K. Agarwal Sariel Har-Peled Kasturi R. Varadarajan\n\n[B]Universal epsilon-approximators for integrals, by Langberg and Schulman\n\n---------------------------------------------------------\n\nAfter reading the response and the revised text, I understand the chicken-and-egg issue.\nI think the experimental section is still a bit weak (given that there are several very competitive SVM algorithms that the paper didn't compare with).\nI raised my score to 5. \n\n"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses reservations about the paper's suitability for ICLR, stating \"the paper may not be a clear accept.\" It also points out weaknesses in the running time analysis and lack of explanation for figures, indicating a negative assessment.",
            "The review acknowledges the paper's contribution while also raising concerns about novelty and suggesting improvements. It doesn't express strong positive or negative feelings overall.",
            "The review provides a balanced assessment, highlighting both the strengths (clarity, theoretical bounds) and weaknesses (lack of comparison with other training methods) of the paper without expressing strong positive or negative feelings.",
            "The reviewer initially recommends against acceptance due to concerns about the \"CHICKEN and EGG\" section, running time, and limited theoretical novelty. Although the score is raised to 5 after revisions, it still indicates a lukewarm evaluation."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, pointing out specific weaknesses like \"not so convincing\" regarding running time, suggesting improvements with \"some text explaining those figures should be helpful,\" and directly questioning a figure's content: \"why the last figure in Figure 6 only has 2 curves?\"",
            "The review raises concerns about the novelty of the approach and points out specific issues in the paper, using phrases like \"The main concern I have is about the novelty\" and posing direct questions that require clarification or justification from the authors.",
            "The review offers both praise and criticism. It uses phrases like 'reasonably well-written,' 'nice theoretical bounds,' but also points out a 'lacking' empirical analysis. This balanced approach contributes to a neutral and objective tone.",
            "The review uses critical language such as \"I don't quite understand,\" \"unclear to me,\" \"hand-waving,\" \"not attractive,\" \"a bit weak,\" and \"limited.\" The reviewer also states \"I don't recommend acceptance.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the positive aspects of the new method (moderate improvement, helpful results) while also pointing out its weaknesses (slow running time, lack of figure explanation, unclear accept for ICLR). The reviewer provides a balanced assessment without contradicting themselves.",
            "The review is consistent because the reviewer acknowledges the paper's contribution and novelty in applying a new theoretical framework to coreset for SVM, while also raising valid concerns about the overall novelty of the coreset idea in SVM and suggesting specific improvements and clarifications. The reviewer's points are logically connected and do not contradict each other.",
            "The review provides a balanced assessment of the paper, highlighting both its strengths (theoretical contribution, clarity, originality in application) and weaknesses (lack of empirical validation compared to existing SVM methods). The different evaluation points (Significance, Clarity, Originality, Quality) are consistent with each other and contribute to a coherent overall assessment of the paper. There are no statements that contradict each other.",
            "The reviewer initially had strong reservations and recommended rejection. After reading the response and revised text, the reviewer understood one major concern and raised the score, but still has reservations about the experimental section. The review shows a consistent progression of opinion from negative to less negative based on new information, without contradicting previous statements."
        ]
    },
    {
        "paper_id": "iclr_2022_u2GZOiUTbt",
        "paper_title": "Task Affinity with Maximum Bipartite Matching in Few-Shot Learning",
        "paper_abstract": "We propose an asymmetric affinity score for representing the complexity of utilizing the knowledge of one task for learning another one. Our method is based on the maximum bipartite matching algorithm and utilizes the Fisher Information matrix. We provide theoretical analyses demonstrating that the proposed score is mathematically well-defined, and subsequently use the affinity score to propose a novel algorithm for the few-shot learning problem. In particular, using this score, we find relevant training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning a few-shot model. Results on various few-shot benchmark datasets demonstrate the efficacy of the proposed approach by improving the classification accuracy over the state-of-the-art methods even when using smaller models.",
        "review_ids": [
            "i3yqLKCjm2q",
            "KRePbukEMz",
            "y4IUqc-6_65"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors propose a task affinity score based on maximum bipartite matching algorithm and Fisher information matrix. And then utilize this score to find the closest training data labels to the test data and leverage the discovered relevant data for episodically fine-tuning the few-shot model. Experimental results on few-shot learning setting achieve the state-of-the-art performance on four widely-used benchmarks. pros:\nThe paper is well-written and neatly presented. \nThe idea that leveraging task affinity score to find most relevant tasks for better few-shot learning is impressive, and seems to be technically sound.\n\ncons:\n\tThe contributions are not well highlighted.\n\tThere is no ablation study in this paper, the experimental results are insufficient to validate the idea and many detailed experiments are not provided.\n\tThe main idea and the motivation are easy to follow, however some details of the proposed model are still not well specified.\n\nDetailed comments and questions:\n\t1. In Definition 1, what is the meaning of epsilon-approximate network? How do you determine the value of epsilon, does it have a significant impact on the performance of the model?\n\t2. In Definition 3, why use task Ta's query data to compute the Fisher information matrix instead of using support data?\n\t3. It is mentioned in this paper that the classifier model is fine-tuned only with the labels of the related-training set, the complexity of training of the few-shot model is reduced. However, from Algorithm 1, the relevant task selection seems quite time consuming. Has the author considered the computational complexity of the model?\n\t4. In 1-shot setting, the class centroids of the target task are the sample itself. Will the closest source task calculated in this case be biased due to the randomness of the sample selection?\n\t5. In Section 4.2.2, the author says that top-R scores and their corresponding classes are finally selected. Then Algorithm 1 selects the task with the minimum TAS. Which method does the author use?\n\t6. How to use the source task fine-tuning model, which the authors did not explain? For example, in the miniImageNet, 2000 source tasks are randomly selected, then the model is fine-tuned on each source task to calculate TAS to the target task. Is this process time-consuming? \n\t7. In the tieredImageNet, why only 120 labels are selected for each source task when the test set of this dataset has 160 labels?\n\t8. In Table 1 and Table 2, why use different backbone to compare IE-Distill methods respectively? miniImageNet uses standard resnet-12 while tieredImageNet uses wide-layer resnet-12.\n\t9. What is the baseline of the paper? Is the performance gain from TAS-Simple to TAS-Distill derived from knowledge distillation or the method proposed by the authors?\n The overall idea of using task affinity score for few-shot learning is impressive. However, I think this submission is incomplete and lacks a lot of technical details, which makes the paper not easy to be understood. Meanwhile, the paper also has not conducted any ablation study to show the real empirical improvement of the proposed strategies and modules. Moreover, although the method proposed by the author does not introduce additional model parameters, I am quite concerned that the computation cost of the fine-tuning source tasks during the task affinity score calculation.",
            "This paper presents a new affinity score based on the Fisher Information matrix from a source to a target task. The authors also develop a few-shot learning procedure based on a pre-trained Whole-Classification network approach. In this procedure, training labels are matched via a maximum matching algorithm, and the target task for training is selected using the proposed affinity score. The effectiveness of the proposed score is demonstrated using benchmark datasets in the problem of few-shot learning. Strong points.\n- The authors introduce a new score based on the Fisher Information matrix; its mathematical analysis is also presented.\n- The performance is compared with many meta-learning methods in the empirical study.\n- The paper is well-organized.\n\nWeak points.\n- The proposed few-shot learning scheme seems incremental.\n- The advantage of the proposed score (TAS) is not clear.\n- A more thorough evaluation is needed to show the effectiveness of the proposed method.\n\nComments.\n1. In my understanding, the main contribution of this study is to incorporate a process of constructing a related-training set via Task Affinity Score (TAS) into a few-shot learning procedure. Another approach to consider the task similarity is to embed tasks into hidden space, as in conditional neural processes (NPs) [R1]. The authors should clarify the advantage of the proposed approach compared with such prior works. Also, it would be helpful to add the empirical evaluation with conditional NPs, etc. \n\n2. In Section 3, the authors present TAS as a general one; however, the few-shot learning scheme is constructed using a specific approach, i.e., Whole-Classification. Is it possible to use TAS for few-shot learning based on another approach?\n\n3. In Tables 1 and 2, the authors demonstrate that the proposed method outperforms many previous meta-learning methods in terms of prediction accuracy. However, this is not enough to convince the effectiveness of the proposed method. I would like to see more evidence of why the proposed method works well. For example, it would be helpful to analyze the related source tasks selected by TAS.\n\n4. As another aspect, the authors would add a discussion on the computation complexity of the proposed method.\n\n[R1] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shana- han, Y. W. Teh, D. Rezende, and S. A. Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1690\u20131699, 2018.  This paper proposes a new few-shot learning procedure and shows its effectiveness in terms of prediction accuracy. However, the advantage of the proposal should be discussed compared with previous works; the authors should conduct a more thorough evaluation. Accordingly, I vote for rejecting this paper.",
            "This paper proposes a few shot learning method, which tries to measure the affinity degree between different tasks. Based on the affinity score, the relevant tasks are exploited for training to boost the performance of target tasks. The Task Affinity Score (TSA) is proposed, which is novel  to measure the dependency between different tasks. The reasonability of TSA is also validated with mathematical proof. Strength:\n1. The idea to exploit Fisher Information Matrix for relevance measurement between different tasks is novel.\n2. The experimental results are promising.\n\nWeakness:\n1. The paper exploits the empirical Fisher Information matrix. The reason for this approximation should be discussed in detail.\n2. The authors did not conduct some necessary experiments to validate the effectiveness of TSA, e.g., the authors should prove that models trained with source tasks selected via TSA are consistently better than those trained with randomly sampled source tasks.\n3. The proof of TSA holds when the loss function is strictly convex, which is a strong assumption. This paper exploits the fisher matrix to measure the relevance between different tasks which is an interesting attempt. In addition, the experimental validate the effectiveness of the proposed method."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's contributions, lack of ablation studies, missing technical details, and potential computational cost. Phrases like 'contributions are not well highlighted,' 'experimental results are insufficient to validate the idea,' 'some details of the proposed model are still not well specified,' 'submission is incomplete,' and 'lacks a lot of technical details' indicate a negative sentiment.",
            "The reviewer ultimately recommends rejection due to concerns about the novelty and thoroughness of the evaluation.",
            "The review expresses overall positive feedback, highlighting the novelty of the Task Affinity Score (TSA) and the promising experimental results. The reviewer also acknowledges the interesting attempt of using the Fisher matrix."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review provides specific criticisms and questions about the paper's methodology, experimental setup, and clarity. Questions such as 'why use task Ta's query data to compute the Fisher information matrix instead of using support data?' and statements like 'the paper also has not conducted any ablation study to show the real empirical improvement' demonstrate a critical tone.",
            "The reviewer provides both positive and negative points but focuses on weaknesses and areas needing improvement, ultimately recommending rejection.",
            "The review presents both strengths and weaknesses of the paper, using objective language and providing specific examples. It avoids overly enthusiastic or critical language, maintaining a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critical assessment of the paper, pointing out both the potential of the idea and the significant shortcomings in its presentation, validation, and clarity. While acknowledging the impressiveness of the core concept, the reviewer consistently raises concerns about the lack of details, insufficient experimental validation (no ablation study), unclear methodology, and potential computational cost. The pros and cons, as well as detailed comments, all contribute to a consistent message that the idea is interesting but the paper needs substantial improvement in terms of clarity, completeness, and empirical support.",
            "The review acknowledges the paper's strengths, such as the novelty of the proposed score and good organization. However, it consistently raises concerns about the unclear advantage of the proposed method over existing approaches, the need for more thorough evaluation, and the lack of sufficient evidence to demonstrate its effectiveness. These concerns logically lead to the reviewer's rejection recommendation, indicating a consistent negative assessment of the paper's overall contribution despite acknowledging some positive aspects.",
            "The review presents both strengths and weaknesses of the paper without contradicting itself. The strengths highlight the novelty and promising results, while the weaknesses point out areas for improvement regarding justification, experimental validation, and theoretical assumptions. The summary aligns with both the strengths and weaknesses, indicating a consistent evaluation of the paper's contributions and limitations."
        ]
    },
    {
        "paper_id": "iclr_2018_SkZ-BnyCW",
        "paper_title": "Learning Deep Generative Models With Discrete Latent Variables",
        "paper_abstract": "There have been numerous recent advancements on learning deep generative models with latent variables thanks to the reparameterization trick that allows to train deep directed models effectively.  However, since reparameterization trick only works on continuous variables, deep generative models with discrete latent variables still remain hard to train and perform considerably worse than their continuous counterparts. In this paper, we attempt to shrink this gap by introducing a new architecture and its learning procedure.  We develop a hybrid generative model with binary latent variables that consists of an undirected graphical model and a deep neural network. We propose an efficient two-stage pretraining and training procedure that is crucial for learning these models. Experiments on binarized digits and images of natural scenes demonstrate that our model achieves close to the state-of-the-art performance in terms of density estimation and is capable of generating coherent images of natural scenes.",
        "review_ids": [
            "r1yDSeCJM",
            "SkHg5PQxf",
            "rkp8JGcef",
            "SkaVZ-9xf"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary of the paper:\nThe paper proposes to augment a variational auto encoder (VAE) with an binary restricted Boltzmann machine (RBM) in the role of the prior of the generative model. To yield a good initialisation of the parameters of the RBM and the inference network a special pertaining procedure is introduced. The model produces competitive Likelihood results on MNIST and was further tested on CIFAR 10. \n\nClarity and quality: \n\n1. From the description of the pertaining procedure and the appendix B I got the impression that the inference network maps into [0,1] and not into {0,1}.  Does it mean, you are not really considering binary latent variables (making the RBM model the values in [0,1] by its probability p(z|h))? \n\n2. on page 2:\nRWS....\"derive a tighter lower bound\": Where does the \"tighter\" refer to? \n\n3. \"multivariate Bernoulli modeled by an RBM\": Note,  while in a multivariate Bernoulli the binary variables would be independent from each others, this is usually not the case for the visible variables of RBMs (only in the conditional distribution given the state of the hidden variables).\n\n4. The notation could be improved, e.g.:\n-x_data and x_sample are not explained\n- M is not defined in equation 5. \n\n5. \"this training method has been previously used to produce the best results on MNIST\" Note, that parallel tempering often leads to better results when training RBMs (see http://proceedings.mlr.press/v9/desjardins10a/desjardins10a.pdf) . Furthermore, centred RBMs are also get better results than vanilla RBMs (see: http://jmlr.org/papers/v17/14-237.html).\n\nOriginality and significance:\nAs already mentioned in a comment on open-review the current version of the paper misses to mention one very related work: \"discrete variational auto encoders\". Also \"bidirectional Helmholtz machines\" could be mentioned as generative model with discrete latent variables.  The results for both should also be reported in Table 1 (discrete VAEs: 81,01, BiHMs: 84,3). \n\nFrom the motivation the advantages of the model did not become very clear to me. Main advantage seems to be the good likelihood result on MNIST (but likelihood does not improve compared to IWAE on CIFAR 10 for example). However, using an RBM as prior has the disadvantage that sampling from the generative model requires running a Markov chain now while having a solely directed generative model allows for fast sampling. \n\nExperiments show good likelihood results on MNIST. Best results are obtained when using a ResNet decoder. I wondered how much a standard VAE is improved by using such a powerful decoder. Reporting this, would allow to understand, how much is gained from using a RBM for learning the prior. \n\nMinor comments:\npage 1:\n\"debut of variational auto encoder (VAE) and reparametrization trick\" -> debut of variational auto encoders (VAE) and the reparametrization trick\",\npage 2:\n\"with respect to the parameter of  p(x,z)\" -> \"with respect to the parameters of  p(x,z)\"\n\"parameters in p\" -> \"parameters of p\" \n\"is multivariate Bernoulli\" ->  \"is a multivariate Bernoulli\"\n\"we compute them\" -> \"we compute it\" \npage 3:\n\"help find a good\" ->  \"help to find a good\"\npage 7:\n\"possible apply\" -> \"possible to apply\"",
            "While I acknowledge that training generative models with binary latent variables is hard, I'm not sure this paper really makes valuable progress in this direction. The only results that seem promising are those on binarized MNIST, for the non-convolutional architecture, and this setting isn't particularly exciting. All other experiments seem to suggest that the proposed model/algorithm is behind the state of the art. Moreover, the proposed approach is fairly incremental, compared to existing work on RWS, VIMCO, etc.\n\nSo while this work seem to have been seriously and thoughtfully executed, I think it falls short of the ICLR acceptance bar.",
            "Interesting work, but I\u2019m not convinced by the arguments nor by the experiments. Similar models have been trained before; it\u2019s not clear that the proposed pretraining procedure is a practical step forwards. And quite some decisions seem ad-hoc and not principled. \n\nNevertheless, interesting work for everyone interested in RBMs as priors for \u201cbinary VAEs\u201d. \n\n",
            "Interesting work!\n\nJust to clarify: You pretrain the encoder/decoder pair and pretrain a RBM on their latent representation? And then during joint training (section 3.2) you block direct gradient flow (disable the soft-binarization) and use VIMCO? \n\nI\u2019m not convinced I follow the second part of the argument in 3.0: Training SBNs or DBNs with e.g. Gumbel-softmax based methods would allow gradients to flow in a very similar fashion. Doesn\u2019t the \u201cgradient flow\u201d depend on the training method / gradient estimator (in your case soft-binarization) rather than this models structure? \n\nFrom this perspective your model is directly comparable to Discrete VAEs, isn\u2019t it? Where Discrete VAEs introduced a different reparam. based training method. \n\nHave you tried alternative pretraining methods? E.g. using Gumbel-softmax based instead eqn. (3), or VIMCO trained factorial top later SBN? Do we have any idea why joint training might be so hard?\n\nThe title seems very broad - large parts of the paper propose and evaluate a pretraining procedure for a specific two layer DBM architecture. \n\nI\u2019m curious: What are typical log Z estimate for your models in table 1? "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review points out several issues with the paper, including unclear descriptions, missing definitions, lack of comparison with related work, and questionable advantages of the proposed model. The reviewer also suggests improvements to the notation and grammar.",
            "The reviewer expresses doubt about the paper's value, stating it doesn't make \"valuable progress.\" They highlight shortcomings in the results and describe the approach as \"fairly incremental,\" ultimately recommending rejection.",
            "The review expresses doubt regarding the arguments and experiments, stating that the pretraining procedure may not be a practical step forward and that decisions seem ad-hoc.",
            "The review expresses curiosity and poses several questions, indicating a desire for clarification and further information. While the reviewer acknowledges the work as \"interesting,\" they also raise concerns about the argument in section 3.0 and suggest alternative approaches, resulting in an overall neutral sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct questions and suggestions for improvement, indicating a critical analysis of the paper. Phrases like 'The notation could be improved' and questions about the model's advantages demonstrate a critical perspective.",
            "The review uses phrases like \"not sure this paper really makes valuable progress,\" \"isn't particularly exciting,\" \"behind the state of the art,\" and \"fairly incremental.\" These phrases indicate a critical evaluation of the paper's contributions and novelty.",
            "The reviewer uses phrases like \"I\u2019m not convinced\", \"it\u2019s not clear\", and \"seem ad-hoc and not principled\", indicating a critical assessment of the work's methodology and contributions.",
            "The review asks clarifying questions that suggest the reviewer has some reservations about the method. Phrases such as \"I\u2019m not convinced I follow\" and questions about gradient flow and alternative pretraining methods indicate a critical examination of the work. The comment about the title being too broad is also critical."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it raises valid questions and provides constructive criticism regarding clarity, technical details, related work, significance, and presentation. All points are focused on improving the paper and there are no contradictory statements or arguments. The reviewer's concerns and suggestions are logically connected and contribute to a coherent assessment of the paper.",
            "The review is consistent because it acknowledges the difficulty of the task but argues that the paper does not make valuable progress, citing limited promising results and incremental approach compared to existing work, ultimately concluding that it falls short of the acceptance bar.",
            "The review expresses a nuanced opinion, acknowledging the interest of the topic while criticizing the arguments, experiments, and novelty of the work. The reviewer is not contradicting themselves, but rather presenting a balanced assessment with both positive and negative aspects. The final positive statement is conditional and does not negate the preceding criticisms, maintaining a consistent overall message of 'interesting but flawed'.",
            "The review is consistent because it raises a series of related questions and critiques that are logically connected and aim to understand and improve the work. There are no statements that contradict each other. The reviewer starts with a positive note and then proceeds to ask clarifying questions, raise concerns about the methodology's novelty compared to existing methods like Discrete VAEs, and suggest alternative approaches. All points are focused on understanding the method and suggesting improvements, maintaining a consistent line of inquiry."
        ]
    },
    {
        "paper_id": "iclr_2019_HJf7ts0cFm",
        "paper_title": "State-Regularized Recurrent Networks",
        "paper_abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.",
        "review_ids": [
            "SJeM_VnbaX",
            "Hyel1qtb67",
            "HJxlpD793m"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a novel architecture and regularization technique for RNN, where the hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata from an RNN is much simpler, and (2) forces RNN to operate like an automata and less like finite state machines. The authors make (1) immediately clear, and show (2) with empirical results.\n\nMajor comments:\n\n(1) No experiments on widely used benchmarks for RNNs (e.g. language modeling, arithmetic tasks (for instance see Zaremba and Sutskever, 2015) ). Have you tried this by any chance?\n\n(2) Theorems 3.1 and 3.2 are presented without proof. Will be good to at least include it in the appendix.\n\n(3) IMDB experiments: you claim that SR-LSTM and SR-LSTM-p have \"superior\" extrapolation capabilities than vanilla LSTMs. However, as SR-LSTM and SR-LSTM-p give far lower train error rate, it's not strictly fair to claim that they extrapolate better to longer sequences than encountered during training time. \n\nIs the number of parameters held constant across 3 models? I'm struggling to understand why the training performance of the proposed models is significantly better than pure LSTM. For SR-LSTM-P I can see this being the case (the peephole connections effectively increase the hidden state size), but why does SR-LSTM (whose hidden states should be more constrained than pure LSTMS) perform better than LSTM during training? This makes me wonder whether SR-LSTM and SR-LSTM-P have higher capacity than LSTM somehow.\n\n(4) MNIST experiments : please include results for SR-LSTM\n\nMinor comments:\n\n(1) page 8 : MNIST imagse -> images",
            "\nSummary:\n\nThis paper is based on the observation that LSTMs use the hidden state to memorize information and the cell state (memory) is not fully utilized. To encourage the LSTM to utilize the cell state, authors constraint the hidden state to a set of centroid states and learn to transition between these centroids in a soft way. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks. The proposed model also has some interpretability of internal state transitions.\n\nMajor comments:\n\n1.\tThe main claim of the paper is that SR-LSTM can extrapolate to longer sequences, unlike LSTM. However, the sequence lengths considered are too small. It would be interesting to train both models with specific sequence length and then keep testing them with longer sequence length and compare the performance. If SR-LSTM behaves like a DPDA, then with larger cell state, the performance should not drop as you increase the sequence length till the capacity of the cell state.\n\n2.\tTheorem 3.1 and 3.2 have no proofs. Please make them as notes rather than theorems.\n\n3.\tWhat do different colors in Figure 6 stands for?\n\n4.\tIn the MNIST task authors claim that they have significant improvement when compared to LSTM. I am not sure if that is accurate. Also, why do you compare SR-LSTM-p only with LSTM? What is the performance of LSTM-p? Please report that as well.\n\n5.\tEven in table 3, can you please report the performance of LSTM-p?\n\nEven though the paper does not show strong empirical performance in real-world tasks, I would still recommend for accepting this paper for its contributions in understanding RNNs better, provided authors answer to question 1, 4, and 5.\n\n\nMinor comments:\n\n1.\tFig 6 is not referred anywhere.\n\n",
            "The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids.\n\nIn general, the paper is well written and the proposed model is theoretically grounded. Unfortunately, the proposed approach shines only on specifically designed benchmarks. It is not a surprise that a CF can be learned by an architecture very similar to DPDA (with addition of learnable parameters). There is a number of specifically designed tasks to test long-term memorization, such as copy/addition, etc. Furthermore, RNNs are mostly used for natural language processing tasks. This paper only conducts experiments on IMDB sentiment analysis ignoring better benchmarked tasks, such as language modelling.\n\nIt is not absolutely clear why authors claim that cell is playing the role of memory. It is always possible to rewrite LSTM formulas with h' which is concatenation of hidden state h and cell c. Results on \"peephole connection\"-inspired SR-LSTM-p should be benchmarked against an LSTM with peephole connections.\n\nThe claim repeated several times that RNNs operate like DFAs, not DPDAs. This is an important point in the paper and should be verbalized more. Does it mean that it is easier to learn regular languages with RNNs?\n\nWhile intuitive, theorems 3.1-3.2 are very vague to be theorems. Otherwise, they should be proven or provided a sketch of proof. For example, how do you formalize \"state dynamics\"?\n\nThe quality of writing of the related work section is worse that the rest of the paper. Authors should explore more other hidden state regularization methods. And, perhaps, give less attention to stochastic RNNs since the final version of the proposed model is not stochastic.\n\nTo summarize, this paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown.\n\nMinor points:\n- Citation of Theano is missing\n- Give a sentence explaining what is hidden state \"drifting\"\n- a-priori -> a priori"
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review provides both positive and negative feedback, highlighting the paper's strengths and weaknesses without expressing a clear overall positive or negative opinion.",
            "The review expresses both positive aspects (contribution to understanding RNNs) and criticisms (lack of strong empirical performance, missing proofs, unclear details). The final recommendation is for acceptance, but conditional on addressing specific concerns.",
            "The review expresses several criticisms, including the limited applicability of the proposed approach, the lack of benchmarking against established tasks, concerns about the theoretical grounding of the theorems, and issues with the related work section. The concluding summary states that the paper 'lacks in terms of experimentation and global coherence'."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer raises several concerns about the paper's experimental setup, lack of benchmarks, and missing proofs, using phrases like \"not strictly fair,\" \"struggling to understand,\" and questioning the validity of the claims. The reviewer also directly asks the authors to include results for SR-LSTM, indicating a critical assessment of the current content.",
            "The review uses direct questions and criticisms like \"However, the sequence lengths considered are too small,\" \"Theorem 3.1 and 3.2 have no proofs,\" \"I am not sure if that is accurate,\" and \"can you please report the performance of LSTM-p?\" These indicate a critical evaluation of the work.",
            "The review uses critical language such as 'Unfortunately', 'It is not a surprise', 'It is not absolutely clear', 'very vague', 'worse', and 'lacks'. The reviewer also directly questions the authors' claims and suggests improvements in a demanding manner."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critique, raising valid points about experimental validation, theoretical rigor, and interpretation of results. The reviewer's comments are logically connected and aim to improve the paper's quality without contradicting themselves.",
            "The review is consistent because the reviewer points out weaknesses and areas for improvement (major comments), but also acknowledges the potential contribution of the paper and recommends acceptance if major comments are addressed. The reviewer is critical but constructive and provides a clear rationale for the recommendation.",
            "The review is consistent in its critique. It starts with acknowledging the paper's strengths (well-written, theoretically grounded) but quickly transitions to point out significant weaknesses, primarily in experimental validation, clarity of claims, and theoretical rigor. The reviewer consistently emphasizes the need for better benchmarks, clearer justifications, and more robust theoretical backing. The summary reinforces this consistent negative assessment, highlighting the paper's shortcomings in experimentation and coherence despite acknowledging an 'interesting direction'."
        ]
    },
    {
        "paper_id": "iclr_2020_rJe5_CNtPB",
        "paper_title": "Attention Forcing for Sequence-to-sequence Model Training",
        "paper_abstract": "Auto-regressive sequence-to-sequence models with attention mechanism have achieved state-of-the-art performance in many tasks such as machine translation and speech synthesis. These models can be difficult to train. The standard approach, teacher forcing, guides a model with reference output history during training. The problem is that the model is unlikely to recover from its mistakes during inference, where the reference output is replaced by generated output. Several approaches deal with this problem, largely by guiding the model with generated output history. To make training stable, these approaches often require a heuristic schedule or an auxiliary classifier. This paper introduces attention forcing, which guides the model with generated output history and reference attention. This approach can train the model to recover from its mistakes, in a stable fashion, without the need for a schedule or a classifier. In addition, it allows the model to generate output sequences aligned with the references, which can be important for cascaded systems like many speech synthesis systems. Experiments on speech synthesis show that attention forcing yields significant performance gain. Experiments on machine translation show that for tasks where various re-orderings of the output are valid, guiding the model with generated output history is challenging, while guiding the model with reference attention is beneficial.",
        "review_ids": [
            "BJejk40KFr",
            "r1gXxa_TtB",
            "SJgSxr_k5B"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes an alternative mechanism of training the attention values of a sequence to sequence learning model as applied to tasks like speech synthesis and translation.  During training they compute two forms of attention: (1) the standard soft-attention from a decoder fed with teacher forced output, and (2) the inference-time attention from a decoder fed with predicted outputs.  Their training objective consists of two terms: The first is the token-wise cross entropy loss but by conditioning on the predicted output  but with teacher-forced attention.  The second is a KL distance between the above two types of attention distributions.   Experiments with mechanical  turks indicate that their attention forcing mechanism is strongly preferred over the existing teacher forced output and attention model.  On translation their method provides little or no improvement.\n\nI am inclined towards rejecting the paper because the experiment and related work section still requires a lot of work before 1. The claimed utility of the idea is established, and 2. The novelty over the many existing attention architectures is established.   I elaborate on each of these next.\n\nRelated work: Recently, many papers have directly or indirectly handled the problem of exposure bias that this paper attempts to address.  The paper does not discuss most of these.  Here are some that are missed from the paper:\n\n1.   Sequence level training with recurrent neural networks\nMA Ranzato, S Chopra, M Auli, W Zaremba, 2015.\nThis paper shows that the scheduled sampling method (discussed in the paper) is much worse than a reinforce-based training mechanism of handling exposure bias.  \n\n2. An actor-critic algorithm for sequence prediction\nD Bahdanau, P Brakel, K Xu, A Goyal, R Lowe\n\n3.  Posterior Attention Models for Sequence to Sequence Learning\nS Shankar, S Sarawagi - 2019\n\n4. Latent Alignment and Variational Attention\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander M. Rush 2018\n\n5. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings\nShaohui Kuang, Junhui Li, Ant\u00f3nio Branco, Weihua Luo, Deyi Xiong\n\nExperiments:  Their experiments are rather sketchy and limited.\nThe TTS experiments are only on one dataset.  Their method is compared only with the standard seq2seq learning approach.  Even the scheduled sampling or professor forcing methods are not compared with.  In addition, state of the art TTS methods have gained significantly from hierarchical attention.  As such as far as the TTS task is concerned the significance of the improved quality over a baseline seq2seq method is limited.\n\nFor translation they consider only the English-Vietnamese task whereas there are tens of other translation tasks that are used in recent literature.\n\nOverall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete.\n\n*********\nI read the author response but I do not think the paper is ready for publication yet without the thorough comparison with related work.\n",
            "This paper proposes a method for fixing exposure bias (ie. training vs generated distribution mismatch) in seq2seq modeling with attention, particularly for the application of speech synthesis where reference alignments are available.\n\n\nRelated Work is missing:\n\n- Another paper that studies fixing exposure bias in seq2seq learning:\n\nWiseman & Rush. Sequence-to-Sequence Learning as Beam-Search Optimization\nhttps://arxiv.org/pdf/1606.02960.pdf\n\n- Other papers that try to enforce attention to attend to specific locations:\n\nBao et al. Deriving Machine Attention from Human Rationales. https://arxiv.org/abs/1808.09367\n\nLiu et al. Neural Machine Translation with Supervised Attention. https://www.aclweb.org/anthology/C16-1291/\n\nYu et al. Supervising Neural Attention Models for Video Captioning by Human Gaze Data. https://arxiv.org/abs/1707.06029\n\nWithout the comparison against other related papers that also aim to supervise attention mechanisms (there are other beyond the ones I cited above)s, it is unclear how much is novel about this paper.\n\n- Furthermore, it is conceptually clear to me that attention-forcing fully matches the training vs generated distributions. The authors should describe in greater detail why this happens this, or whether these distributions are not required to fully match in attention-forcing (and in this case, why this would be desirable).\n\n- The experiments are not very convincing (only 30 human evaluators for Speech synthesis with no other quantitative evaluation, NMT results\u00a0that are not particularly promising).\n\n- Use of non-anonymous github link is questionable for blinded submissions.",
            "This paper proposes a novel training scheme for seq2seq models where attention or reference alignment is used in combination with free-running mode for improving training.\n\nThe positives of this paper are that it is well written and very clear. It also is very relevant as seq2seq models can be hard to train and techniques like scheduled sampling and x-forcing algorithms are good heuristics but heuristics none-the-less.\n\nThe downside of this paper is in the experimental results and also complexity. It would\u2019ve been good to see a broader set of experiments to really benchmark attention-forcing from other self-attention models.\n\nAttention forcing also requires a reference or ground-truth alignment, which is often not available. Hence the authors propose to simultaneously train another teacher-forcing model to estimate the reference alignment. However, this would incur twice the computation complexity. \n\nAttention forcing could also be used in conjunction with scheduled sampling. How does that compare with the reported results for attention forcing?"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer is \"inclined towards rejecting the paper\" due to concerns about the experiment, related work, claimed utility, and novelty. They explicitly state that the experiments are \"sketchy and limited\" and that the related work discussion is \"incomplete\".",
            "The review expresses concerns about missing related work, lack of novelty, unclear explanation of the method's effectiveness, weak experimental results, and questionable anonymity practices, indicating an overall negative assessment.",
            "The review expresses both positive aspects (clarity, relevance) and negative aspects (experimental results, complexity) of the paper, resulting in a neutral overall sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses critical language, pointing out deficiencies in the paper's methodology and presentation. Phrases like \"requires a lot of work\", \"missed from the paper\", \"experiments are rather sketchy and limited\", and \"idea proposed seems quite incremental\" demonstrate a critical tone. The reviewer also directly disagrees with the author response, stating \"I do not think the paper is ready for publication yet\".",
            "The reviewer uses phrases like \"missing\", \"unclear how much is novel\", \"not very convincing\", and \"questionable\" to express their concerns and criticisms. They also directly point out shortcomings in the paper's content and methodology.",
            "The review presents both positive and negative feedback, using phrases like \"The positives of this paper are...\" and \"The downside of this paper is...\" which indicates a balanced perspective. It also raises questions for the authors to consider, suggesting a constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its negative assessment of the paper. It starts by stating an inclination towards rejection due to insufficient experimental validation and lack of novelty compared to existing attention architectures.  The review then elaborates on these points by detailing the shortcomings in the related work section (missing relevant papers) and the experiments section (limited datasets, lack of comparison with relevant baselines). The final statement after reading the author response reinforces the initial negative assessment, maintaining the concern about the lack of thorough comparison with related work.  There are no contradictions in the reviewer's arguments, and all points support the overall negative recommendation.",
            "The review is consistent because all the points raised (missing related work, unclear novelty, conceptual questions, weak experiments, anonymity issue) logically contribute to a coherent critique of the paper's weaknesses and areas for improvement. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because it presents a balanced view, acknowledging both the strengths (clarity, relevance) and weaknesses (experimental results, complexity, practical limitations) of the paper in a logical and coherent manner. The critique is focused and well-reasoned without any self-contradictions."
        ]
    },
    {
        "paper_id": "iclr_2022_q2ZaVU6bEsT",
        "paper_title": "CONTEXT AUGMENTATION AND FEATURE REFINEMENT NETWORK FOR TINY OBJECT DETECTION",
        "paper_abstract": "Tiny objects are hard to detect due to their low resolution and small size. The poor detection performance of tiny objects is mainly caused by the limitation of network and the imbalance of training dataset. A new feature pyramid network is proposed to combine context augmentation and feature refinement. The features from multi-scale dilated convolution are fused and injected into feature pyramid network from top to bottom to supplement context information. The channel and spatial feature refinement mechanism is introduced to suppress the conflicting formation in multi-scale feature fusion and prevent tiny objects from being submerged in the conflict information. In addition, a data enhancement method called copy-reduce-paste is proposed, which can increase the contribution of tiny objects to loss during training, ensuring a more balanced training. Experimental results show that the mean average precision of target targets on the VOC dataset of the proposed network reaches 16.9% (IOU=0.5:0.95), which is 3.9% higher than YOLOV4, 7.7% higher than CenterNet, and 5.3% higher than RefineDet.",
        "review_ids": [
            "SsgfGIXixZR",
            "3grkXvGoyV",
            "9bz9Qam5VHK"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper aims at the tiny object detection and point out the issues are small context feature, semantic feature conflicts, and less tiny objects in training data.  To solve the aforementioned problems, authors introduce context augmentation module (CAM), design a feature refinement module, and adopt data-augmentation manner in training process. Experiments are conducted on PASCAL VOC dataset to validate the proposed methods and modules. ## Strength\n### 1. The tiny object detection is a challenging and meaningful research topic in both theory and application.\n## Weakness\n### 1. The motivation of the work is not clear and the explanation is not convincing. In paper, the authors mention that \"different densities directly will cause semantic conflicts\", this should be justified experimentally or theoretically. \n### 2. The authors claim that the proposed CAM and FPM can solve the feature conflicts, this should extensively demonstrated and explained in-depth, not just some number and trivial feature visualization map.\n### 3. The novelty of proposed method CAM, FPM, and data augmentation methods are slim. CAM is initially proposed Yu & Koltun, 2015. Data augmentation is a common operation to improve performance. If the authors want to justify the superiority of the proposed data augmentation method, they should systematically compare theirs with other data augmentation methods.\n### 4. In order to validate the performance of the proposed method regarding tiny object, COCO dataset is more suitable than VOC.\n### 5. Too many typos. Here some of them are listed. In abstract, \"the precision of target targets\" might be \"the precision of tiny targets\"\n\"great progress(Tonget al., 2020\" should be \"great progress (Tong et al., 2020\", a space should be placed between text and bracket. In figure 1, the CEM should CAM.  Based on apparent weakness, the paper is nor ready to publish.",
            "This paper proposed a method of combined context augmentation and feature refinement to eliminate the limitation of network and imbalance of dataset issues for tiny object detection. A data enhancement method is proposed to increase the contribution of tiny objects to loss during the training.  There are several technical issues of this work.\n\n1) In context augmentation module, the method applies 1*1 convolutions to do the feature fusion. Although 1*1 convolution can improve the long-distance dependence to some extent, it requires much computations and overhead. So I\ndoubt if it is too heavy to apply so many 1*1 convolution in this module?\n\n2) In the feature refinement module, the expression and the symbol of variables are too complex that may affect reader's understanding. The method is somehow similar to the dual attention network in [R1], which proposed both spatial and channel attention to do the segmentation. However that paper is not cited.\n\n[R1] J. Fu, J. Liu, H. Tian, Y. Lin, Y. Bao, Z. Fang and H. Lu, \u201cDual Attention Network for Scene Segmentation,\u201d In ICCV, 2019.\n\n3) In the copy-reduce-paste data part, there is no description regarding how to avoid the overlap issue of pasting images, which I believe to be important. If the pasted object and previous one have some overlapping, the training performance will be definitely affected.\n\n4) The paper lacks details on how data enhancement is performed, such as the selection of large objects to be copied and the scale to reduce the large objects. How to control such selections so that the dataset can be balanced?\n\n5) To my understanding, the approach of [R2] is similar or highly related to the proposed work in solving the data balancing issue in tiny object detection. I suggest the authors to compare results with [R2]\n\n[R2] M. Kisantal, Z. Wojna, J. Murawski, J. Naruniec and K. Cho, \u201dAugmentation for small object detection,\u201d In CVPR 2019.\n\n6) Detailed implementation details of the proposed method is lacking.\n\n7) For the experiment, the paper utilized a different backbone that may make the comparison not fair.\nComparison results in Table 4 and Table 5 are not convincing. Table 4 compares the overall MAP on the VOC dataset (which is not designed specifically for tiny object detection). I think the authors should target datasets such as https://neurohive.io/en/news/a-summary-of-the-1st-tiny-object-detection-challenge/\n\nTable 5 compares the detection performance of tiny objects, however all comparison methods are not designed specifically for tiny object detection.\n\n9) As the title suggests, contexts are used for tiny object detection. However, there is no reference to relevant works that are specifically design to leverage contexts, such as \n\n[R3] J. Lim, M. Astrid, H. Yoon and S. Lee, \u201dSmall Object Detection using\nContext and Attention,\u201d In ICAIIC 2021.\n[R4] X. Tang, D. K. Du, Z. He and J. Liu, \u201dPyramidBox: A Context-assisted\nSingle Shot Face Detector,\u201d In ECCV 2018.\n[R5] H. Hu, J. Gu, Z. Zhang, J. Dai and Y. Wei, \u201dRelation Networks for Object\nDetection,\u201d In CVPR 2018.\n\n10) There are some obvious written errors in the paper such as CEM and\nFRM in Figure 1, which are supposed to be CAM and FRM. Both the technical method and evaluation results have many issues indicated above, so I recommend rejection.",
            "This paper focuses on tiny object detection, and a new feature pyramid network is proposed to combine context augmentation and feature refinement. Experimental results show that the proposed method is better than YOLOV4, CenterNet, and RefineDet. Strengths:\n1.\tThis paper mainly solve the problem of tiny object detection and obtain good results of tiny object detection on VOC dataset.\n2.\tA data augmentation method called copy-reduce-paste is proposed, which can increase the contribution of tiny objects to loss during training, ensuring a more balanced training.\nWeaknesses:\n1.\tThe innovation in the paper is limited, more like an assembly of existing work, such as DeepLabv3+, attention and spatial attention\u3002\n2.\tThe proposed method was not evaluated on other datasets, such as MS COCO dataset.\n3.\tThe main focus of this paper is tiny object detection, but the analysis of small object is limited in the experimental results.\n\nQuestion:\n1.\tWhat\u2019s the \u2018CEM\u2019 and \u2018FPM\u2019 mean in Figure 1?\n2.\tThe novelty of CAM is limited, A similar structure has been proposed in DeepLabv3+.\n3.\tThe proposed FRM is a simple combination of channel attention and spatial attention. The innovative should be given in detail.\n The innovation of this article is limited, and the methods proposed exist in prior works, with minor changes. The originality is not strong. And the experiments are not enough with only a relatively simple VOC dataset."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review identifies several weaknesses in the paper, including unclear motivation, lack of convincing explanations, limited novelty, and the use of an unsuitable dataset. The concluding statement explicitly states that the paper is 'not ready to publish'.",
            "The review identifies multiple technical issues, lack of details, unfair comparisons in experiments, and written errors. The concluding recommendation is 'rejection', signaling a negative overall assessment.",
            "The review expresses concerns about the paper's limited innovation, lack of evaluation on diverse datasets, and insufficient analysis of small objects. Phrases like 'innovation in the paper is limited,' 'not evaluated on other datasets,' 'analysis of small object is limited,' 'innovation of this article is limited,' and 'originality is not strong' indicate a negative assessment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses critical language by pointing out specific flaws and shortcomings. Phrases like 'not clear and the explanation is not convincing', 'should be justified experimentally or theoretically', 'novelty...are slim', and 'too many typos' demonstrate a critical tone. The reviewer also directly states the paper is 'not ready to publish'.",
            "The review uses phrases like 'several technical issues', 'doubt if it is too heavy', 'expression and the symbol of variables are too complex', 'no description regarding', 'lacks details', 'not convincing', 'many issues indicated above', and 'I recommend rejection'. These indicate a critical evaluation of the paper's methodology and results.",
            "The tone is critical due to phrases like \"The innovation in the paper is limited\", \"The proposed method was not evaluated on other datasets\", \"The analysis of small object is limited\", and \"The innovation of this article is limited, and the methods proposed exist in prior works, with minor changes. The originality is not strong.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the identified weaknesses logically support the conclusion that the paper is not ready for publication. The reviewer points out a strength but then provides several substantial criticisms regarding motivation, novelty, validation, and presentation, which collectively justify the negative assessment.",
            "The review is consistent because the reviewer consistently raises criticisms and concerns about various aspects of the paper, including methodology, clarity, completeness, evaluation, and comparison with existing works, ultimately leading to a rejection recommendation.",
            "The reviewer consistently points out the limited innovation and novelty of the proposed method throughout the review.  They mention it as a weakness ('innovation in the paper is limited, more like an assembly of existing work'), in questions ('The novelty of CAM is limited', 'The innovative should be given in detail'), and in the final summary ('The innovation of this article is limited, and the methods proposed exist in prior works, with minor changes. The originality is not strong')."
        ]
    },
    {
        "paper_id": "iclr_2021_RB0iNPXIj60",
        "paper_title": "BBRefinement: an universal scheme to improve precision of box object detectors",
        "paper_abstract": "We present a conceptually simple yet powerful and flexible scheme for refining predictions of bounding boxes. Our approach is trained standalone on GT boxes and can then be combined with an object detector to improve its predictions. The method, called BBRefinement, uses mixture data of image information and the object's class and center. Due to the transformation of the problem into a domain where BBRefinement does not care about multiscale detection, recognition of the object's class, computing confidence, or multiple detections, the training is much more effective. It results in the ability to refine even COCO's ground truth labels into a more precise form. BBRefinement improves the performance of SOTA architectures up to 2mAP points on the COCO dataset in the benchmark. The refinement process is fast; it adds 50-80ms overhead to a standard detector using RTX2080, so it can run in real-time on standard hardware. The code is available at https://gitlab.com/irafm-ai/bb-refinement.",
        "review_ids": [
            "BES1WrhrSLp",
            "6ztLO2NuAx",
            "cNLrqvODeX8",
            "RgmaLuaZ1w3"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "-The idea of this paper is just to crop the detections and then forwarded to a second stage for more accurate predictions. This idea can be traced back to the original R-CNN paper, which is not even referred and discussed. There are also many papers having a second stage to refine the detection predictions, e.g. Cascade R-CNN, RefineDet, Revisiting RCNN, but none of them are discussed in this paper.\n\n-The writing is terrible.\n\n-Tons of literature is missing.\n\n-To be honest, this paper should be desk rejected.\n\n=====updates======== \n\nAfter reviewing the other reviews and rebuttal, I will remain my original recommendation.",
            "##########################################################################\n\nSummary: \n\nThis paper proposes BBRefinement, which is a post-processing for object detection to refine the predicted bounding boxes. BBRefinement takes cropped images from predicted bounding boxes as input and refine the bounding box with a separate network that is only targeted in predicting box offsets.\n\n##########################################################################\n\nPros:\n\nThe proposed method is simple and experiment shows the effectiveness of proposed method.\n\n##########################################################################\n\nCons:\n\n1. Missing literature review. This paper is not the first one to study how to refine bounding boxes. There are a lot of works on refining bounding boxes [A, B, and many more], but this paper fails to discuss related works and explain the connection as well as the differences with them.\n\n2. Missing ablation studies. Section 2.2 discusses principals of BBRefinement, including the importance of using mixture data. However, there is no experiment supporting this claim. Also the expanding ratio of bounding boxes is an important parameter, but there is also no experiment on this parameter.\n\n3. This paper only applies the proposed method to very simple baselines like Faster R-CNN and RetinaNet. These methods (Faster R-CNN, RetinaNet) are known to predict not tight bounding boxes. I wonder if BBRefinement is still necessary when a method already predicts tight bounding boxes like Cascade R-CNN [C].\n\n4. It is not clear to me how the model is trained, especially how boxes are sampled during training. Is it an image-centric sampling or an instance-centric sampling? Does sampling strategy matter? There could also be false positives in the prediction, does these boxes harm the training (e.g. existence of background box)?\n\n5. Table 1 needs more explanation, do you need to train separate BBRefinement for each model? If not, what boxes do you use to train the BBRefinement.\n\n6. The timing of BBRefinement, is \"23ms for B1 and 44ms for B3\" a single box? What is the average end-to-end runtime on the whole dataset? The timing is also much faster than EfficientNet speed in [D], [D] reports 52 ms for B1 and 114 ms for B3. Can you explain why your timing is 2x faster?\n\n7. The dataset split on COCO also has problem, COCO train 2014 and part of val 2014 is exactly the same as train 2017. And the 5000 minival 2014 is exactly the same as val 2017.\n\n##########################################################################\n\nReasons for score:\n\nAlthough this paper presents a simple and effective solution, the overall quality of the paper is poor. First, this paper does not have discussion on related works **AT ALL**. Second, it misses important implementation details and important ablation studies. Third, this paper only applies the method to weak detectors and fails to apply it to methods that give tight bounding boxes like Cascade R-CNN. Finally, the authors put a [link to the code](https://gitlab.com/irafm-ai/bb-refinement) which leaks the authorship (one author's name and institute); this is a violation of the double-blind review policy. Considering all this facts, this paper is a clear reject to me.\n\n##########################################################################\n\nReferences:\n\n[A] Object detection via a multi-region & semantic segmentation-aware CNN model, ICCV 2015  \n[B] A MultiPath Network for Object Detection, 2016  \n[C] Cascade R-CNN: Delving into High Quality Object Detection, CVPR 2018  \n[D] Designing Network Design Spaces, CVPR 2020  \n",
            "This paper proposes an approach, BBRefinement, to refine the bounding box predictions of an object detector. After the object detector predicts boxes, a patch is cropped around each box and passed into a separate network for refinement. The authors show that this improves the performance of several object detectors on the COCO dataset.\n\nI think the novelty of this paper is limited as a similar idea has already been published in Cascade R-CNN [a]. The authors do not compare their work to Cascade R-CNN. To me, the major difference between this and Cascade R-CNN seems to be that Cascade R-CNN uses the RoI-pooled features for refinement, but this uses the image crops, which does not seem to be significant. Other differences such as BBRefinement use the centers and the classes of the boxes as additional information also seem to be minor. Furthermore, Cascade R-CNN seems to be able to provide better improvement on the APs. Since the authors do not provide any comparison with Cascade R-CNN, it is unclear whether BBRefinement is better at refining boxes than Cascade R-CNN or significantly different.\n\nThe authors claim an advantage of BBRefinement is that it can be trained with more images as it takes crops around each box as input while conventional detector uses the full images as input. I find this argument a little bit weird. Considering the number of actual pixels, BBRefinement actually has much fewer training data in terms of the number of pixels as large number of pixels are thrown away during cropping.\n\nThe authors also claim that BBRefinement is more robust to missing labels. The conventional detector may get penalized incorrectly for producing boxes for missing labels, while BBRefinement does not suffer from such issue as it only takes cropped images around each label as input. I don\u2019t think this claim is fair. The authors should not compare BBRefinement to a detector because they are different. BBRefinement only refines the predictions from a detector but a detector classifies and localizes objects from an image. And actually, the regressors in convetional detectors also only process cropped images/feature maps. I am not seeing why this is an advantage to BBRefinement.",
            "Summary:\nThis paper presents a simple yet powerful and flexible framework to refine the predictions of a two-stage detector. The approach can produce more precise predictions by using mixture data of image information and the objects' class and center. They showed a simple scheme can increase the mAP of the SOTA models and it is able to produce predictions that are more precise than ground truth.\n\nWeakness:\n+ The idea of this paper is to use a refinement module to boost the performance of the two-stage detectors. I find this work to contain very limited novelty that other researchers can use/build on. The proposed method simply uses a naive refine module to extract the Region feature from the crop. In my opinion, this simple module is similar to Cascade R-CNN. The only difference is that it extracts features from the crop of the images. It does not advance the understanding of this field although is reasonable to me.\n\n+ The ablation experiments are weak and inadequate. Only one experiment is provided to compare the performance of the refine module. The author should do more ablation studies to support his contribution. e.g. (1) the comparison with the Cascade R-CNN which extracts the feature from the region feature maps rather than the images. (2) the architecture or the refinement module number. \n\n+ The claim of run in real-time on standard hardware, without any time cost or FPS results in this paper.  However, the speed of the refinement module may be slow owing to the extracting feature from the crops. For the two-stage detector, e.g. FPN, the proposals of the detector are 512 under the common setting. \n\n+ The results would have been more complete if results were shown in a setting where the region feature is used without the use of the original crops. In other words, an ablation study on the effect of the feature extraction strategies. \n\n+ How important is the crop size to the proposed method? Considering the paper states that this is required to get a good crop, some ablation studies on showing the crop strategies would be useful for understanding.\n\n+ In Abstract, the author of this paper provides his code which is non-anonymous. It shows that the repository of BBREFINEMENT is the \"IRAFM AI\" and the author's name is easy to be found.  This behavior violates the rules of the anonymous code mentioned in the Author Guide. \n\nFinally, I suggest rejecting the paper. BTW, The author should pay attention to the rules the next time.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses strong disapproval with phrases like \"writing is terrible\", \"tons of literature is missing\", and \"this paper should be desk rejected\". The reviewer also criticizes the paper's core idea as unoriginal and lacking proper citation.",
            "The review expresses multiple concerns about the paper, including missing literature review, missing ablation studies, unclear implementation details, dataset split issues, and a violation of the double-blind review policy, leading to a recommendation for rejection.",
            "The reviewer expresses several concerns regarding the paper's novelty, comparison to existing work (Cascade R-CNN), and the validity of the authors' claims about advantages. Phrases like 'novelty of this paper is limited,' 'do not compare their work,' 'does not seem to be significant,' 'unclear whether BBRefinement is better,' and 'I don\u2019t think this claim is fair' indicate a negative sentiment.",
            "The reviewer recommends rejecting the paper due to limited novelty, weak ablation experiments, unsubstantiated claims, and a violation of anonymity guidelines. The reviewer uses phrases like 'very limited novelty,' 'weak and inadequate,' and 'violates the rules,' indicating a negative assessment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical due to the use of harsh language and direct negative feedback. Phrases such as \"writing is terrible\", \"tons of literature is missing\", and \"this paper should be desk rejected\" clearly indicate a highly critical stance.",
            "The review uses phrases like \"missing literature review\", \"missing ablation studies\", \"poor quality\", \"fails to apply\", \"violation of the double-blind review policy\", and \"clear reject\", indicating a strongly critical stance.",
            "The review uses critical language to question the authors' methodology and claims. Phrases like 'I think the novelty of this paper is limited,' 'this argument a little bit weird,' 'I don\u2019t think this claim is fair,' and direct challenges to the authors' reasoning demonstrate a critical tone.",
            "The review uses direct and critical language, pointing out specific weaknesses in the paper's methodology, experimental design, and adherence to guidelines. Phrases like 'I find this work to contain very limited novelty,' 'The ablation experiments are weak and inadequate,' and 'This behavior violates the rules' demonstrate a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently negative, criticizing the novelty of the idea, writing quality, and lack of literature review, ultimately recommending desk rejection. The reviewer maintains this negative stance even after considering other reviews and the rebuttal.",
            "The review is consistent because despite mentioning a positive aspect in the 'Pros' section (simplicity and effectiveness), the reviewer raises numerous significant concerns in the 'Cons' section and 'Reasons for score'. These concerns, including missing literature review, lack of ablation studies, unclear implementation details, and a double-blind violation, all lead to the consistent conclusion that the paper is a 'clear reject'. The reviewer's final recommendation aligns with the detailed criticisms provided throughout the review.",
            "The review is consistently critical of the paper. It raises concerns about the novelty by comparing it to Cascade R-CNN, questions the claimed advantage of training with more images due to cropping, and refutes the robustness to missing labels argument by highlighting the different roles of BBRefinement and object detectors. The reviewer maintains a skeptical stance throughout the review.",
            "The review is consistent because despite starting with a positive summary of the paper's premise, the reviewer clearly identifies several significant weaknesses related to novelty, experimental validation, performance analysis, and ethical concerns (anonymity violation). These weaknesses logically lead to the reviewer's final recommendation of rejection. There are no contradictory statements within the review; the initial summary serves as a brief overview before the critical evaluation begins."
        ]
    },
    {
        "paper_id": "iclr_2019_BygfghAcYX",
        "paper_title": "The role of over-parametrization in generalization of neural networks",
        "paper_abstract": "Despite existing work on ensuring generalization of neural networks in terms of scale sensitive complexity measures, such as norms, margin and sharpness, these complexity measures do not offer an explanation of why neural networks generalize better with over-parametrization. In this work we suggest a novel complexity measure based on unit-wise capacities resulting in a tighter generalization bound for two layer ReLU networks. Our capacity bound correlates with the behavior of test error with increasing network sizes (within the range reported in the experiments), and could partly explain the improvement in generalization with over-parametrization. We further present a matching lower bound for the Rademacher complexity that improves over previous capacity lower bounds for neural networks. ",
        "review_ids": [
            "Sygwq4iZC7",
            "SkgLfnBiTm",
            "SkezHFcITm",
            "rkeQQzTK3m"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors aim to shed light on the role of over-parametrization in generalization error. They do so for the special case of 2 layer fully connected ReLU networks, a \"simple\" setting where one still sees empirically that the test error decreasing as over-parametrization increases.\n\nBased on empirical observations of norms (and norms relative to initialization) in trained overparametrized networks, the authors are led to the definition of a new norm-bounded class of neural networks. Write u_i for the vector of weights incoming to hidden node i. Write v_i for the weights outgoing from hidden node i. They study classes where the Euclidean norm of v_i is bounded by a constant alpha_i and where the Euclidean norm of u_i - u^0_i is bounded by beta_i, where u^0_i is the value of u_i after random initialization. Call this class F_{alpha,beta} where alpha,beta are specific vectors of bounds.\n\nThe main result is a bound on the empirical Rademacher complexity of F_{alpha,beta}. \nThe authors also given lower bounds on the empirical Rademacher complexity for carefully chosen data points, showing that the bounds are tight. These Rademacher bounds yield standard bounds on the ramp loss for fixed alpha,beta, and margin, and then a union bound argument extends the bound to data-dependent alpha,beta and margin.\n\nThe authors compare the bounds to existing norm-based bounds in the literature. The basic argument is that the terms in other bounds tend to grow as networks get much larger, while their terms shrink. Note that at no point are the bounds in this paper \"nonvacuous\", ie they are always larger than one.\n\nIn summary, I think this is a strong paper. The explanatory power of the results are still oversold in my opinion, even if they use hedged language like \"could explain the role...\". But the work is definitely pointing the way towards an explanation and deserves publication. The technical results in the appendix will be of interest to the learning theory community.\n\nissues:\n\n\"could explain role of over-parametrization\". Perhaps this work might point the way to an explanation, but it does not yet provide an explanation.  It is a big improvement it seems.\n\n\"bound improves over the existing bounds\". From this statement and the discussion comparing the bounds, it is not clear whether this bound formally dominates existing bounds or merely does so empirically (or under empirical conditions). \n\ntypos: \n\nbigger than the Lipschitz CONSTANT of the network class\n\nH undefined\n\nRademacher defined for H but must be defined on loss class (or a generic function class, not H)\n\n\"we need to cover\" --> \"it suffices to\"\n\n\"the following two inequaliTIES hold by Lemma 8\"\n\nbibliography is a mess: half of the arxiv papers are published. typos everywhere, very sloppy.\n\n(This review was requested late in the process due to another reviewer dropping out of the process.)\n\n[UPDATE]. The authors addressed my concerns stated in my review above. I think the bibliography has improved and I recommend acceptance. ",
            "Thank you for the quick reply, at this point I believe both of the major issues are properly addressed, and the proofs are rigorous. As promised, I would recommend accepting this paper. \n\nOne more minor typo in Lemma 10 - in the last equation block where we plug in the value of \\| alpha \\|_p, I believe you initially plugged in the value of p-th power of it. Instead I believe it should be \n  beta D^{1/2 - 1/p} (1 + D/K)^{1/p}\nOnce again, this is a very minor issue, and I can see the rest of the results follow from this correction. ",
            "Let me start by apologizing for the delayed review - in fact I was asked today to replace an earlier assigned reviewer. Hopefully the clarifications I request won't be too time consuming to meet the deadline coming up. \n\n###\n\nFirst of all, the problem which the authors are attempting to answer is quite important: the effect of over-parametrization is not well understood on a theoretical level. As the paper illustrate, 2-layer networks are already capable of generalizing while being over-parameterized, therefore justifying their setting. \n\nNext this paper motivates the study of complexity quantities that tend to decrease with the number of parameters, in particular figure 3 motivates the conjecture that the complexity measure in Theorem 2 can control generalization error. The paper also does a great job comparing related work, motivating their results. \n\n###\n\nAt this point, I would like to request a couple of clarifications in the proofs. Perhaps it's due to the fact that I only spent a day reading, but at least I think we could improve on its readability. Regardless, I currently do not yet trust a couple of the proofs, and I believe the acceptance of this paper should be conditioned on confirming the correctness of these proofs.\n\n(1) Let's start with Lemma 10. In the middle equation block, we obtain a bound \n  \\| alpha^prime \\|_p^p <= beta^p ( 1 + D/K )\nand the proof concludes alpha^prime is in Q. However this cannot be the case for all alpha^prime. \n\nConsider x=0 which is in S_{p, beta}^D, then we have alpha^prime = 0 as well. In the definition of Q, we require all the j's to sum up to K+D, which is not met here. \n\nAt the same time, the next claim \n  \\| alpha \\|_2 <= D^{1/2 - 1/p} \\| alpha^prime \\|_p\ndoes not seem to follow from the above calculations. In particular, alpha^prime seems to be defined with respect to an x in S_{p, beta}, however in this case we did not specify such an x. Perhaps did you mean there exist such an alpha^prime?\n\n(2) In the proof of Theorem 3, there is an important inequality needed to complete the proof \n  max{ <s, f_i> , <s, -f_i> } >= 1/2 * ( <s, [f_i]_+> + <s, [-f_i]_+> )\n\nPerhaps I am missing something obvious, but I believe this inequality fails when we choose s as a constant vector, and f_i to have the same number of positive and negative signs (which is possible in a Hadamard matrix). In this case, the left hand side should be equal to zero, where as the right hand side will be positive. \n\n###\n\nTo summarize, if these proofs can be confirmed, I believe this paper would have made significant contribution to the problem of over-parametrization in deep learning, and of course should be accepted. \n\n###\n\nI corrected several typos and found minor issues as I read, perhaps this will be useful to improve readability as well.\n\nPage 13, proof of Lemma 8\n  - after the V_0 term is separated, there is a sup over \\|V_0\\|_F <= r in the expectation, which should be \\|V-V_0\\|_F <= r instead.\n\nPage 14, Lemma 9\n  - the lemma did not define rho_{ij} in the statement\n\nPage 15, proof of Lemma 9\n  - in equation (12), there is an x_y vector that should x_t\n\nPage 15, proof of Theorem 1\n  - while I eventually figured it out, it's unclear how Lemma 8 is applied here. Perhaps one more step identifying the exact matrices in the statement of Lemma 8 will be helpful to future readers, and maybe explain where the sqrt(2) factor come from as well. \n\nPage 16, proof of Lemma 10\n  - in the beginning of the proof, to stay consistent with the notation, we should replace S_{p, beta} with S_{p, beta}^D\n  - I believe the cardinality of Q should be (K + D - 1) choose (D - 1), as we need to choose positive j's to sum up to (K+D) in the definition of Q. This reduces down to the problem of choosing natural numbers j's summing K, which is (K+D-1) choose (D-1). Consider the stack exchange post here:\nhttps://math.stackexchange.com/questions/919676/the-number-of-integer-solutions-of-equations\n\nPage 16, proof and statement of Lemma 11\n  - I believe in the first term, the factor should be m instead of sqrt(m). I think the mistake happened when applying the union bound, as it should only affect the term containing delta\n\nPage 17, Lemma 12\n  - same as Lemma 11, we should have m instead of sqrt(m)\n\nPage 18, proof of Theorem 3\n  - at the bottom the statement \"F is orthogonal\" does not imply the norm is less than 1, but rather we should say \"F is orthonormal\"\n\nPage 19, proof of Theorem 3\n  - at the top, \"we will omit the index epsilon\" should be \"xi\" instead\n  - in the final equation block, we have the Rademacher complexity of F_{W_2}, instead it should be F_{W^prime}\n\n",
            "It is shown empirically that common algorithms used in supervised learning (SGD) yield networks for which such upper bound decreases as the number of hidden units increases. This might explain why in some cases overparametrized models have better generalization properties.\n\nThis paper tackles the important question of why in the context of supervised learning, overparametrized neural networks in practice generalize better. First, the concepts of \\textit{capacity} and \\textit{impact} of a hidden unit are introduced. Then, {\\bf Theorem 1} provides an upper bound for the empirical Rademacher complexity of the class of 1-layer networks with hidden units of bounded \\textit{capacity} and \\textit{impact}. Next, {\\bf Theorem 2} which is the main result, presents a new upper bound for the generalization error of 1-layer networks. An empirical comparison with existing generalization bounds is made and the presented bound is the only one that in practice decreases when the number of hidden units grows. Finally {\\bf Theorem 3} is presented, which provides a lower bound for the Rademacher complexity of a class of neural networks, and such bound is compared with existing lower bounds.\n\n## Strengths\n- The paper is theoretically sound, the statement of the theorems\n    are clear and the authors seem knowledgeable when bounding the\n    generalization error via Rademacher complexity estimation.\n\n- The paper is readable and the notation is consistent throughout.\n\n- The experimental section is well described, provides enough empirical\n    evidence for the claims made, and the plots are readable and well\n    presented, although they are best viewed on a screen.\n\n- The appendix provides proofs for the theoretical claims in the\n    paper. However, I cannot certify that they are correct.\n\n- The problem studied is not new, but to my knowledge the\n    presented bounds are novel and the concepts of capacity and\n    impact are new. Theorem 3 improves substantially over\n    previous results.\n\n- The ideas presented in the paper might be useful for other researchers\n    that could build upon them, and attempt to extend and generalize\n    the results to different network architectures.\n\n- The authors acknowledge that there might be other reasons\n    that could also explain the better generalization properties in the\n    over-parameterized regime, and tone down their claims accordingly.\n\n## Weaknesses\n\\begin{itemize}\n- The abstract reads \"Our capacity bound correlates with the behavior\n    of test error with increasing network sizes ...\", it should\n    be pointed out that the actual bound increases with increasing\n    network size (because of a sqrt(h/m) term), and that such claim\n    holds only in practice.\n\n- In page 8 (discussion following Theorem 3) the claim\n    \"... all the previous capacity lower bounds for spectral\n        norm bounded classes of neural networks (...) correspond to\n        the Lipschitz constant of the network. Our lower bound strictly\n    improves over this ...\", is not clear. Perhaps a more concise\n    presentation of the argument is needed. In particular it is not clear\n    how a lower bound for the Rademacher complexity of F_W translates into a\n    lower bound for the rademacher complexity of l_\\gamma F_W. This makes the claim of tightness of Theorem 1 not clear. Also this makes\n    the initial claim about the tightness of Theorem 2 not clear.\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states \"In summary, I think this is a strong paper\" and \"deserves publication\". The reviewer also states that the authors addressed their concerns and recommends acceptance.",
            "The reviewer explicitly recommends accepting the paper and states that major issues are properly addressed and proofs are rigorous. They also acknowledge a minor typo but emphasize its insignificance.",
            "The reviewer acknowledges the importance of the problem addressed, praises the paper's motivation and comparison to related work, and concludes that the paper would make a significant contribution if the proofs are confirmed. This indicates a generally positive sentiment despite the identified issues.",
            "The review expresses overall positive feedback, highlighting strengths such as theoretical soundness, readability, well-described experiments, novel bounds, and potential for future research. While weaknesses are pointed out, the tone remains constructive and appreciative of the paper's contributions."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review provides both positive feedback (\"strong paper\", \"deserves publication\", \"technical results in the appendix will be of interest\") and constructive criticism (\"explanatory power of the results are still oversold\", specific issues with clarity and typos). This indicates a balanced approach, acknowledging the paper's strengths while also highlighting areas for improvement.",
            "The reviewer uses phrases like \"Thank you\", \"I believe both of the major issues are properly addressed\", \"proofs are rigorous\", \"I would recommend accepting this paper\", and \"this is a very minor issue\", indicating a supportive and encouraging tone.",
            "The review balances positive feedback with critical analysis. The reviewer acknowledges the paper's strengths (importance of the problem, motivation, comparison to related work) while also pointing out specific issues with the proofs and suggesting improvements for readability. The tone is constructive and aims to help improve the paper rather than simply criticizing it. The reviewer also uses phrases like 'Perhaps I am missing something obvious' which shows humility and openness to being wrong.",
            "The reviewer uses phrases like 'Strengths', 'the paper is theoretically sound', 'the paper is readable', 'experimental section is well described', 'ideas presented in the paper might be useful', and 'authors acknowledge'. These indicate a supportive and encouraging tone towards the authors and their work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially expresses a positive view of the paper, highlighting its strengths and potential contribution. While pointing out areas for improvement and clarification (issues and typos), the reviewer maintains a positive overall assessment.  The reviewer's update confirms that the authors addressed the concerns, leading to a final recommendation for acceptance, thus demonstrating a consistent positive stance throughout the review process.",
            "The reviewer expresses overall satisfaction with the revisions, stating that major issues are addressed and proofs are rigorous. They recommend acceptance and point out only a minor typo, which does not contradict the positive assessment.",
            "The reviewer expresses a positive view on the paper's topic and motivation, highlighting its importance and potential contribution.  However, the reviewer also raises specific concerns about the correctness of proofs in Lemma 10 and Theorem 3, and requests clarifications. The reviewer concludes by recommending conditional acceptance, contingent on the verification of these proofs. This conditional acceptance is consistent with the identified strengths and weaknesses, indicating a balanced and coherent assessment.",
            "The review is consistent in its assessment, highlighting both the strengths and weaknesses of the paper without contradicting itself. The reviewer acknowledges the paper's theoretical soundness, novelty, and empirical validation as strengths, while pointing out areas for improvement in clarity and accuracy of certain claims as weaknesses. The criticisms are constructive and do not undermine the overall positive evaluation of the paper's contributions."
        ]
    },
    {
        "paper_id": "iclr_2018_Sy-dQG-Rb",
        "paper_title": "Neural Speed Reading via Skim-RNN",
        "paper_abstract": "Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives a significant computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models.  In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.",
        "review_ids": [
            "BkjQVC8Sz",
            "r1izCPYlG",
            "HJpgrTKxf",
            "rkZtyy5gf",
            "HyDWxCXNf",
            "SkC43pf4G"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "NT",
            "The paper proposes a way to speed up the inference time of RNN via Skim mechanism where only a small part of hidden variable is updated once the model has decided a corresponding word token seems irrelevant w.r.t. a given task. While the proposed idea might be too simple, the authors show the importance of it via thorough experiments. It also seems to be easily integrated into existing RNN systems without heavy tuning as shown in the experiments. \n\n* One advantage of proposed idea claimed against the skip-RNN is that the Skim-RNN can generate the same length of output sequence given input sequence. It is not clear to me whether the output prediction on those skimmed tokens is made of the full hidden state (updated + copied) or a first few dimensions of the hidden state. I assume that the full hidden states are used for prediction. It is somehow interesting because it may mean the prediction heavily depends on small (d') part of the hidden state. In the second and third figures of Figure 10, the model made wrong decisions when the adjacent tokens were both skimmed although the target token was not skimmed, and it might be related to the above assumption. In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token).\n\n* This paper would gain more attention from practitioners because of its practical purpose. In a similar vein, it would be also good to have some comments on training time as well. In a general situation where there is no need of re-training, training time would be meaningless, however, if one requires updating the model on the fly, it would be also meaningful to have some intuition on training time.\n\n* One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. In this sense, it makes this manuscript more comprehensive if there are some comparisons with RNNs with limited-sized hidden dimensions (say 10 or 20). So that readers can check benefits of the skim RNN against skip-RNN and small-sized RNN.\n",
            "Summary: The paper proposes a learnable skimming mechanism for RNN. The model decides whether to send the word to a larger heavy-weight RNN or a light-weight RNN. The heavy-weight and the light-weight RNN each controls a portion of the hidden state. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Although it doesn\u2019t contribute to much speedup on modern GPU hardware, there is a good speedup on CPU, and it is more power efficient.\n\nContribution:\n- The paper proposes to use a small RNN to read unimportant text. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN.\n\nPros:\n- Models that dynamically decide the amount of computation make intuitive sense and are of general interests.\n- The paper presents solid experimentation on various text classification and question answering datasets.\n- The proposed method has shown reasonable reduction in FLOPS and CPU speedup with no significant accuracy degradation (increase in accuracy in some tasks).\n- The paper is well written, and the presentation is good.\n\nCons:\n- Each model component is not novel. The authors propose to use Gumbel softmax, but does compare other gradient estimators. It would be good to use REINFORCE to do a fair comparison with (Yu et al., 2017 ) to see the benefit of using small RNN.\n- The authors report that training from scratch results in unstable skim rate, while Half pretrain seems to always work better than fully pretrained ones. This makes the success of training a bit adhoc, as one need to actively tune the number of pretraining steps.\n- Although there is difference from (Yu et al., 2017), the contribution of this paper is still incremental.\n\nQuestions:\n- Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.\n- One recommended task would probably be text summarization, in which the attended text can contribute to the output of the summary.\n\nConclusion:\n- Based on the comments above, I recommend Accept",
            "This paper proposes a skim-RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference.\n\nPros.\n-\tThe idea of switching small and standard RNNs for skimming and full reading respectively is quite simple and intuitive.\n-\tThe paper is clearly written with enough explanations about the proposal method and the novelty.\n-\tOne of the most difficult problems of this approach (non-differentiable) is elegantly solved by employing gumbel-softmax\n-\tThe effectiveness (mainly inference speed improvement with CPU) is validated by various experiments. The examples (Table 3 and Figure 6) show that the skimming process is appropriately performed (skimmed unimportant words while fully read relevant words etc.)\nCons.\n-\tThe idea is quite simple and the novelty is incremental by considering the difference from skip-RNN.\n-\tNo comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel-softmax may require more iterations).\n\nComments:\n-\tSection 1, Introduction, 2nd paragraph: \u2018peed\u2019 -> \u2018speed\u2019(?)\n-\tEquation (5): It would be better to explain why it uses the Gumbel distribution. To make (5) behave like argmax, only temperature parameter seems to be enough.\n-\tSection 4.1: What is \u201cglobal training step\u201d?\n-\tSection 4.2, \u201cWe also observe that the F1 score of Skim-LSTM is more stable across different configurations and computational cost.\u201d: This seems to be very interesting phenomena. Is there some discussion of why skim-LSTM is more stable?\n-\tSection 4.2, the last paragraph: \u201cTable 6 shows\u201d -> \u201cFigure 6 shows\u201d\n",
            "thanks for the detailed description, but they still do look quite similar. the \"partial update\" model is also not exactly what VCRNN does, in the sense that it's a very much crippled version of VCRNN without, e.g., saving any computation. it'll be important to carefully compare the full implementation of VCRNN against the skim-RNN on at least one task. after all, the VCRNN was proposed in the *same* venue just one year ago.\n\nplease feel free to make another revision however as early as you could.",
            "Note: this is not an official meta-review\n\nthe idea in this paper looks very similar to the idea from <VARIABLE COMPUTATION IN RECURRENT NEURAL NETWORKS> which was presented at ICLR'17: https://arxiv.org/abs/1611.06188. Especially, looking at Fig. 1's of both papers clearly indicate the similarities between these two approaches.\n\ni'd like the authors to clarify how they differ, and would like to ask the reviewers to read https://arxiv.org/abs/1611.06188 and see how this affects your judgement of the submission."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review consists of 'NT', which provides no indication of positive or negative feedback.",
            "The review highlights the potential practical benefits of the proposed method, stating it 'would gain more attention from practitioners because of its practical purpose.' It acknowledges the thoroughness of the experiments and ease of integration into existing systems. While raising concerns, the reviewer frames them as suggestions for improvement rather than fundamental flaws.",
            "The review concludes with a recommendation to 'Accept' the paper, indicating an overall positive assessment. The reviewer also highlights several 'Pros' such as solid experimentation, reasonable reduction in FLOPS and CPU speedup, and good writing.",
            "The review expresses overall positive feedback, highlighting the simplicity and intuitiveness of the idea, clarity of writing, elegant solution to a key problem, and validation of effectiveness through experiments. While it mentions some cons, the pros are emphasized more.",
            "The review expresses concerns about the similarity of the proposed method to existing work (VCRNN) and suggests that the described \"partial update\" model is a crippled version of VCRNN. The reviewer also emphasizes the importance of a careful comparison with the full implementation of VCRNN, indicating a lack of sufficient differentiation in the current state.",
            "The reviewer points out a significant similarity between the submitted paper and a previously published work, suggesting a potential lack of originality. This raises concerns about the novelty and impact of the current submission, leading to a negative sentiment."
        ],
        "tone": [
            "Neutral",
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review consists of 'NT', lacking any stylistic features or specific words that would indicate a particular tone.",
            "The review offers both positive feedback ('the authors show the importance of it via thorough experiments', 'easily integrated into existing RNN systems') and constructive criticism ('It is not clear to me', 'it would be more beneficial if', 'it makes this manuscript more comprehensive if'). This balanced approach indicates a desire to improve the paper while acknowledging its strengths.",
            "The review presents both positive aspects ('Pros') and negative aspects ('Cons') of the paper, along with specific questions. This balanced approach is characteristic of a formal and constructive peer review.",
            "The review adopts a balanced tone by presenting both positive aspects ('Pros') and negative aspects ('Cons') of the paper. It also includes specific comments and questions for improvement, indicating a constructive approach.",
            "The tone is critical, as evidenced by phrases like \"still do look quite similar,\" \"not exactly what VCRNN does,\" \"very much crippled version,\" and the emphasis on the need for a \"careful comparison.\" These phrases suggest that the reviewer has significant reservations about the novelty and contribution of the work.",
            "The reviewer uses direct and questioning language, such as \"i'd like the authors to clarify how they differ,\" and \"see how this affects your judgement.\" This indicates a critical stance, challenging the authors to justify their work in light of existing research."
        ],
        "consistency": [
            "No",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "No review text provided.",
            "The review is consistent because it acknowledges the potential benefits of the proposed method while also raising valid questions and suggesting improvements. The reviewer's points are logically connected and aim to enhance the paper's clarity and comprehensiveness without contradicting any previous statements. The reviewer appreciates the practical aspect of the work and suggests further experiments and analyses to strengthen the paper.",
            "The review presents a balanced perspective by highlighting both the strengths (Pros) and weaknesses (Cons) of the paper. Despite pointing out areas for improvement and incremental contribution, the reviewer concludes with a recommendation to 'Accept', indicating that the strengths outweigh the weaknesses. The questions raised are pertinent and do not contradict the overall positive assessment. The review is constructively critical and leads to a consistent conclusion.",
            "The review is consistent because the reviewer provides both positive and negative feedback, highlighting the strengths (simplicity, clarity, effectiveness) and weaknesses (incremental novelty, lack of discussion on training cost) of the paper. The comments are specific and actionable suggestions for improvement, and they do not contradict the overall assessment of the paper.",
            "The review is consistent in pointing out the similarity between the described model and VCRNN, and consistently argues for a more thorough comparison with VCRNN to clarify the differences and potential limitations.",
            "The review is consistent as it identifies a potential overlap with existing work and suggests further investigation and clarification from the authors. It does not present any contradictory statements or viewpoints."
        ]
    },
    {
        "paper_id": "iclr_2022_JedTK_aOaRa",
        "paper_title": "Private Multi-Winner Voting For Machine Learning",
        "paper_abstract": "Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. This task has been understudied in the machine learning literature despite its prevalence in many domains such as healthcare. We propose three new privacy-preserving multi-label mechanisms: Binary, $\\tau$, and Powerset voting. Binary voting operates independently per label through composition. $\\tau$ voting bounds votes optimally in their $\\ell_2$ norm. Powerset voting operates over the entire binary vector by viewing the possible outcomes as a power set. We theoretically analyze tradeoffs showing that Powerset voting requires strong correlations between labels to outperform Binary voting. We use these mechanisms to enable privacy-preserving multi-label learning by extending the canonical single-label technique: PATE. We empirically compare our techniques with DPSGD on large real-world healthcare data and standard multi-label benchmarks. We find that our techniques outperform all others in the centralized setting. We enable multi-label CaPC and show that our mechanisms can be used to collaboratively improve models in a multi-site (distributed) setting.",
        "review_ids": [
            "cJdYz_ltou",
            "srV2CAbgb9",
            "MgPNQdVmQTC",
            "3JxxjJ6LiV",
            "88glTCoYVc8",
            "vBuOXuHRb25",
            "GYp1p0zMlwf"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper is proposing differential privacy (DP) solutions for multi-label classification (MLC). In particular, it introduces three novel noisy perturbation methods for the MLC setup, analyzes them in terms of DP, and test them experimentally for accuracy, AUC, and other metrics. Strengths:\n- The paper is relatively clearly written (see also the \"Further comments\" part though)\n- The theoretical results related to privacy are interesting, and are discussed in sufficient depth and details.\n- The solutions are clean and reasonably simple.\n\nWeaknesses:\n- The MLC learning part is not treated carefully. First of all, the canonical MLC metrics are not calculated in the experiments. (In fact, they are not even discussed in the paper.) This is important though, because this is one of the crucial, distinctive aspect of MLC. Therefore, it is not clear, how the proposed solutions perform in terms of these metrics.\n- The proposed algorithms are not compared to the state of art MLC solutions, and thus we don't know how much we lose in terms of the MLC metrics when we want to DP guarantees.\n\nBecause of these two shortcomings, it is hard to assess the contribution of the paper. The main goal of the authors was to show how DP can be enforced in MLC but the results presented in the paper are not sufficient to draw any conclusion: although the DP requirements are analyzed, the MLC performance remains unclear.\n\nFurther comments:\n- The acronym CaPC is used already on p1, but is only explained on p3.\n- Why do you use different notations (d resp. X) for the same notion (datasets) in Def. 1 and Def. 2?\n- What does p_{M(X)}(\\theta) denote in Def. 2?\n- Shouldn't Lemma 2.2 be stated with an inequality?\n- Def. 4 mixes sets with their membership vectors. Which is fine, but this should be mentioned explicitly.\n- Def. 5 is confusing. The meaning of the math formulation is that there is no element appearing in each of the P_i sets, whereas the text means that the P_i sets are all disjoint.\n- Below Def. 6: \"For any mechanism f, \\Delta_p f is maximized when the mechanism\u2019s output for each coordinate i is flipped from predicted to not predicted or vice versa. The pair of teacher votes achieving this differ in each coordinate i.\" Aren't we supposed to work here with teacher votes that differ only by one bit? The DP results are solid, but the MLC aspects of the work has serious shortcomings.",
            "This paper considers differentially private multi-winner voting. This problem is a generalization of single-winner voting, which is widely used in PATE type private semi-supervised learning. The authors give three private mechanisms and perform empirical comparisons on multi-label semi-supervised learning settings.  Specifically, when there is no total votes constraint on each ballot, the binary mechanism performs report noisy argmax for each candidate. When there is a total votes constraint, the tau mechanism performs l2 clipping first and then performs report noisy argmax for each candidate as the binary mechanism. The powerset mechanism converts the multi-label problem into a single label problem and performs regular report noisy argmax.  Final update: After the discussion period, I still vote for rejection. There are two main reasons.\n1) The authors do not seem to understand the issue I raised and clarified over and over again. Now I started to doubt whether the algorithm implemented in the experiment is end-to-end differentially private with the designed privacy budget. (I) I do not agree that data-dependent privacy is a different privacy guarantee or a relaxed version of differential privacy. I believe it is not private and give my reasons through the Alice Bob example. I understand this is not the first paper to do it under data-dependent privacy and I have made it clear that I strongly disagree with all the other works especially Papernot et al. (II) The authors responded by citing the monograph by Dwork and Roth, which clearly shows that they do not understand the work they quoted and the issue I raised here. In the entire monograph, the differential privacy is based on the worst-case dataset (data-independent). The authors claim that \"data-dependent analysis is an improvement over propose-test-release\". The propose-test-release is an algorithm. The data-dependent privacy guarantee is a weak privacy notion. The data-dependent analysis referred to as data-dependent accuracy bound in this book is about utility analysis, not privacy. I do understand how privacy notion can be an improvement on an algorithm. Most importantly, this paragraph by Dwork and Roth is saying, you can improve your utility with local sensitivity because privacy has to be for the worst-case dataset, but the utility is only for the good dataset. Typically, to propose a private algorithm, you first prove the privacy for any dataset without any assumptions on the data and then prove the utility for some good datasets like Gaussian data. (This monograph explicitly used the word data-dependent **Accuracy** bounds not privacy bounds)  (II) The authors claim that they can fix it by controlling different numbers of queries. However, the authors did not formally prove it. More specifically, in the authors' data-dependent analysis, each query (each sample in the public unlabeled dataset) has a different privacy cost.  I do not see how Alice could determine the number of queries before running it. Now suppose the privacy budget is 10, and Alice has reached 9.9. when Alice found that her last query has privacy cost 0.2 and exceeds the overall privacy budget, she cannot hide it from the public anymore. Thus, the overall process would break the eps=10 privacy promise. I believe this is fixable since there is some new work on private hyper-parameter tuning. But like I said, the main point here is you have to prove it satisfies (data-independent)-DP. (III) My main concern is about the released data being non-private not the epsilon. (IV) The comparison with Bayesian DP is not helpful here. As long as you privacy depends on the dataset, the same problem arises here for both notions. That is why all the other privacy notions are all data-independent. \n\n2)The main contribution is to generalize PATE to multi-label. I think this is a valid problem setting. But the novelty is not enough for being a separate paper considering the fact that the three proposed methods are either minor modifications of prior works or simple generalizations of PATE. The tau mechanism is just a simple l2 generalization of Zhu et al 2019.\n\nUpdate: You are missing the point. Like I said previously, there are many ways to make a \"data-dependent mechanism\" be provably data-independently differentially private. As long as you can prove your end-to-end algorithm is eps-10 differentially private for any input dataset, then you can make the promise. But it is not here in the current version. For example, you could spend some budget checking the dataset first. If it is good, run your mechanism. If it is bad, output fail. And this is exactly propose-test-release. The methods I mentioned below are all end-to-end data-independently differentially private. You could run your algorithm on all the possible input dataset S and find the largest one, i.e. $\\max_S \\varepsilon(S)$. This would also satisfy data-independent DP. The point is you have to prove that your algorithm is data-independently DP. Alice as a privacy engineer cannot make any assumptions about the nature that generates the dataset. There is a nice blog post https://differentialprivacy.org/average-case-dp/. It has a discussion about the pitfalls of Bayesian DP, which is (in my opinion) similar to data-dependent DP (data-dependent DP: $\\varepsilon(S)$, Bayesian DP: $E_{S\\sim P}[\\varepsilon(S)])$, data-independent DP: $\\max_S\\varepsilon(S)$).  I believe the proposed algorithm in this paper can be proved to be data-independently private even without any change to the algorithm itself. You might want to prove the data-independent RDP of Confident GNMax. (Theorem B.2 is data-dependent). Another caveat I forgot to mention is about comparisons with other works. The gap between data-dependent DP and data-independent DP can be huge (also reported in this paper). It is really unfair to compare with data-independent DP methods like DP-SGD.\n\nUpdate regarding DP and data-dependent DP: This is already **irrelevant** to the contributions of this paper and the proposed algorithm since the proposed method can also be analyzed by traditional (data-independent) DP. First of all, I never claimed that the proposed algorithm is not differentially private(or completely vulnerable against membership inference attacks). And I never said data-dependent DP is not formal. My main concern is about the definition of privacy and the wording. My critique of data-dependent DP is based on the fact that this notion is a function of the dataset. Now suppose we are the nature that generates the dataset. Alice is the privacy engineer who runs some private algorithm and releases the output to the public. Bob is the adversary who tries to attack the sensitive information in the dataset with the released information. Differential privacy is a promise made by Alice to defend against attacks such as membership inference attack. Now when Alice claims that her algorithm is eps=10-DP. We would expect that the output to be eps=10-DP differentially private with respect to whatever the dataset we give to Alice. If not, the next time, when we generate a \"bad\" dataset such that it is not eps=10-DP, Bob can perform a membership inference attack with higher accuracy than promised by Alice. Stability-based methods/propose-test-release/smooth-sensitivity-based methods are all adaptive to the dataset by using local sensitivity but satisfy promised privacy budget for whatever input dataset. Different variations of differential privacy like RDP/CDP/zCDP are all with respect to any input datasets(data-independent). However, if you calculate and output an epsilon of your algorithm for a particular dataset (even if it is private with respect to the dataset), you get different epsilons for different datasets (even if you assume the same number of queries). Then how can you make that promise? This is why I believe data-dependent privacy notions are useless. You could and should get a tight and data-independent epsilon by exactly the same algorithm (Confident GNMax). Listing previous papers do not make things correct. Please answer my doubts directly. \n\nAfter the authors' response: 1) I understand this is not the main subject of this paper.  I still think data-dependent privacy is useless. Theoretically, you could come up with a mechanism and a dataset such that data-dependent epsilon is around 1 and data-independent epsilon is 1000, which is practically useless in defending say membership inference attack. I know this may raise some controversy since there are some prior works that compute some data-dependent epsilon, but I would like to emphasize that the reason why differential privacy is used as a gold standard for private data analysis is that it is a worst-case definition. 2) Overall, I think for this specific problem, this paper might be doing the correct thing with the correct analysis. But the technical contributions are not enough. \n\nStrengths: \n1. this problem is an extension of PATE to a multi-label setting, which is practically interesting. \n2. The authors compare three mechanisms empirically. The empirical analysis is solid.\n\nWeaknesses: \n1. Although the considered problem is practically useful, the proposition 3.1 and appendix A are not theoretically interesting. Basically it says there exists example such that the sensitivity is achieved. (Any private algorithm should have tight analysis of sensitivity) The authors claim that this shows binary mechanism is optimal. However, even for single winner setting, we do not know if Report noisy Argmax is optimal or not. (You could potentially improve it by stability-based methods/ propose-test-release, depending on the problems.)\n\n2. It would be better if this paper contains a preliminary section for \"data-dependent privacy bounds\". In general, differential privacy should not be data dependent. So I am a little bit confused when I first read this concept. If I understand this correctly, I think this is related to local sensitivity/ or stability based methods (see Section~3 of [1]). For example, when the top candidate and the second candidate has large gap (high consensus), the local sensitivity can be small. You could potentially improve it by stability-based methods. But the end-to-end algorithm must satisfy data-independent differential privacy.\n\n3. The emprical comparisons with DP-SGD is not fair. DP-SGD algorithms do not assume a public unlabeled dataset. I understand this comparison exists in prior works. It would be better if the authors could acknowledge it. \n\n4. The discussion about related work Zhu et al seems confusing. As far as I know, Zhu et al (2020) is also model agnostic. The only thing different from original PATE is that they replace teacher model with k nearest neighbor classifiers. Unlike DP-SGD, the privacy mechanism does not scale with the architecture of teacher models. Also the improvement of Tau mechanism is from l2 clipping, which is straightforward. As Zhu et al pointed out in their appendix, they do not use stability-based methods( or they called noisy screening/ Confident GNMax) because it is hard for a data sample to have high consensus for every label. I am just curious how this is resolved in this paper.\n\nSome minor comments:\n1. Is there a typo in equation (2)? I am a little bit confused about the brackets.\n\n[1] Vadhan, Salil. \"The complexity of differential privacy.\" Tutorials on the Foundations of Cryptography. Springer, Cham, 2017. 347-450. Although this is a good practical problem, I think this paper needs some revision.",
            " Dear Authors,\n\nAs AC I have briefly checked your paper and come to the question what is the relation between the task loss for which multi-label classification (MLC) model is optimized and the privacy guarantees. As in MLC we deal with label vectors, there is a multitude of task losses defined and used. They are usually of a different nature, having different properties (e.g., Bayes optimal decisions), leading to very different models. For example, the One-vs-All approach or optimizing the binary cross entropy is the right approach for Hamming loss, as marginal probabilities of labels are enough for making optimal decisions. This is however not the case of the subset 0/1 loss, for which we need to get the joint mode of the conditional distribution. This is somehow related to your binary voting and powerset mechanism, but I do not see such a discussion in your paper. \n\nI would appreciate your response.\n\nBest, AC for Paper 65",
            " Dear Authors and Reviewers,\n\nLet me first thank you for supporting ICLR 2022: Authors for submitting their contributions, and Reviewers for going through them and sending their comments and remarks!\n\nAs the discussion phase has just begun, let me ask Authors to answer all questions appearing in reviews and to defend your paper, and reviewers to check all other reviews to see whether you coincide and to be ready to respond to authors rebuttals.\n\nWe are also looking forward for public comments. I hope for a vivid discussion for this paper.\n\nBest regards, AC for Paper 65\n",
            "This paper considers the design of differentially private multi-label mechanisms. In particular, the authors employ multi-winner voting protocols to existing differentially private single-label learning algorithms e.g. PATE. They consider three multi-label voting protocols -- binary voting, $\\tau$-voting and powerset voting. \n\nBinary voting works by independently applying majority voting on each coordinate. It is obvious that such an aggregation mechanism has does not provide a better privacy guarantee than $k$ applications of binary voting. The privacy guarantee can be improved when the coordinates are dependent. This motivates the authors to consider $\\tau$-voting where the $\\ell_2$-norm of each ballot is bounded by $\\tau$. Finally, in the powerset voting, the voting is done over the universe of all subsets of the alternatives i.e. $2^k$ alternatives.\n\nThe authors make two important observations through experiments. First, when there is high consensus and $\\sigma_G \\rightarrow 0$, binary voting performs best. On the other hand, $\\tau$-voting outperforms with lower consensus and larger values of $\\sigma_G$. Second, even in centralized setting, multi-label methods outperform existing benchmarks. Strengths:\n- The authors consider an interesting problem as designing a privacy preserving mechanism with multi-label outcomes would be applicable in many contexts.\n- I also like the design choices as the proposed mechanisms can be easily implemented with existing DP aggregation mechanisms like PATE. Moreover, the theoretical bounds can be derived by building on top of the existing DP guarantees.\n\nWeaknesses:\n- As far as I understand, the parameter $\\tau$ in the $\\tau$-voting needs to be set based on the domain and there is no automatic way to choose a value.\n- Powerset voting becomes intractable to implement when $k$ is large. But probably there is a way to combine $\\tau$-voting and the powerset voting for large values of $k$.\n\nSome questions for the authors.\n1. From definition 3, it seems like the threshold T is independent of the candidate $i$. However, in definition 4, the constant $T$ depends on the index $i$ and it seems that the mechanism picks an index whose votes exceed $n/2$ (upto noise). So I think the presentation and the definition here is misleading.\n2. How do you define the threshold $T$ in definition 7?\n3. I think there is a range of design options between $\\tau$-voting and powerset voting. For example, you can ask each voter to vote on the best subset of size b. Did the authors consider other alternatives?\n\n I think the paper considers an important problem as the design of differentially private aggregation mechanism for multi-label outcomes is applicable in a lot of domains. The proposed mechanisms are also simple and build on top of the existing DP aggregation mechanisms. However, I felt that the experiments were not quite exhaustive. For example, it's not clear if it is beneficial to apply powerset voting on real datasets.",
            "The authors study differentially private multi-winner voting, which is designed for multi-label learning subject to a privacy constraint meant to limit information leakage about training data to an adversary. They propose three mechanisms: Binary, which essentially runs an existing differentially-private election for each label independently, \\tau voting, which works with votes that have bounded \\ell_2 norm, and powerset voting, which explicitly encodes each possible subset of winners as an alternative in an election and then votes over them. They show that Binary voting (the naive approach) generally outperforms powerset voting as long as there aren\u2019t strong correlations between votes. Lastly, they show that they can use these multi-winner DP techniques to a extend single-label technique, PATE, and empirically demonstrate the effectiveness of their approach. Differentially private multi-label classification is an interesting and well-motivated problem. \n\nThe Binary voting mechanism is quite naive in the sense that separating the problem into k separate elections, each to be evaluated in a differentially private manner (i.e., with Gaussian noise). It\u2019s somewhat surprising that this seems to be optimal among the mechanisms (in the case where voters aren\u2019t constrained in how many candidates they can approve) unless there is a lot of correlation between votes, at which point the (exponentially-sized?) powerset voting method becomes better. \n\nIs it correct to interpret the thresholds T in Definitions 4 and 6 as the cutoff point at which the sum term is at least n/2 (intuitively corresponding to a majority vote)? It could be useful to the reader to have a bit more explanation here.\n\nTo me, the most interesting contribution is that of \\tau voting, where all votes are assumed to have \\ell_2 norm at most \\tau. I have a few questions here: (1) how did you go about choosing \\tau in practice, and do you have proposals for how to choose it on new datasets; and (2) have you thought about an \u201caverage-case\u201d approach where the average vote has \\ell_2 norm at most \\tau instead of requiring all votes to have \\ell_2 norm at most \\tau? \n\nI found the writing quality to have some issues (some minor comments below).\n\nMinor comments: \nDefine CaPC\nSection 2.2, CaPC paragraph: model's --> models\nSection 3.2, first paragraph: it is a popular method --> is a popular method\nBefore Definition 6: missing period\nThere are also many missing articles (a/an/the) throughout the paper The problem is interesting and the results seem to be quite comprehensive. My main concern is that the voting mechanisms proposed are relatively naive (separate into independent elections or run one big single-winner election). However, the technical results seem robust.",
            "Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. They propose three new mechanisms. 1. Binary voting operates independently per label through composition. 2. \\tau voting bounds votes optimally in their l2 norm.  3. Powerset voting operates over the entire binary vector by viewing the possible outcomes as a power set. They prove that Powerset voting requires strong correlations between labels to outperform Binary voting. \nThey also use these mechanisms to enable privacy-preserving multi-label learning. They empirically compare techniques with DPSGD on large real-world healthcare data and standard multi-label benchmarks. Their techniques outperform all others in the centralized setting, and show that mechanisms can be used to collaboratively improve models in a multi-site (distributed) setting. \n This paper introduce mechanisms for private multi-winner voting and multi-label learning, which in important in machine learning. The paper structure and the writing are good. These three mechanisms are shown clear, but for the experiment, the choose of privacy term epsilon is somewhat larger than the common privacy requirements.  It's good but may be not enough"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral",
            "Positive",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review highlights significant weaknesses, stating that the MLC learning part is not treated carefully, and the proposed algorithms are not compared to state-of-the-art solutions. The reviewer concludes that it is hard to assess the contribution of the paper and that the MLC performance remains unclear.",
            "The reviewer expresses strong disagreement with the authors' approach to data-dependent privacy, stating \"I still vote for rejection\" and claiming the authors \"do not seem to understand\" the issue. The reviewer also criticizes the novelty of the proposed methods and the fairness of the empirical comparisons.",
            "The review raises a specific question about the relationship between the task loss and privacy guarantees in the context of multi-label classification. It expresses a lack of understanding regarding the connection between the chosen approach and existing methods, but it does so in a polite and inquisitive manner without explicitly criticizing the work.",
            "The message expresses gratitude and encourages active participation in the review process, indicating a positive and supportive attitude.",
            "The reviewer finds the problem interesting and the design choices appealing, highlighting the ease of implementation and derivation of theoretical bounds. The reviewer also acknowledges the importance of the problem and the simplicity of the proposed mechanisms.",
            "The review expresses both positive and negative aspects. It acknowledges the interesting and well-motivated problem and the robustness of the technical results but also points out the naivety of the proposed mechanisms and issues with writing quality. The overall sentiment is therefore considered neutral.",
            "The review expresses overall positive feedback, highlighting the paper's contributions to an important area, the clarity of the mechanisms, and the good paper structure and writing. However, it also points out a potential weakness regarding the choice of the privacy term epsilon, but still acknowledges the overall value."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Formal",
            "Supportive",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses critical language, pointing out shortcomings such as 'The MLC learning part is not treated carefully' and 'The proposed algorithms are not compared to the state of art MLC solutions.' The reviewer also poses direct critical questions like 'Why do you use different notations?' and identifies confusing definitions.",
            "The review is highly critical, directly challenging the authors' understanding and methodology. The reviewer uses strong language, such as \"You are missing the point\" and \"data-dependent privacy notions are useless,\" and provides detailed rebuttals to the authors' arguments.",
            "The review uses formal language (\"Dear Authors\", \"I would appreciate your response\", \"Best\"). The reviewer identifies themselves as an AC (Area Chair) and refers to the paper by its number, indicating a professional and formal review process.",
            "The tone is supportive, using phrases like \"thank you for supporting\" and \"I hope for a vivid discussion\" to encourage participation and create a positive environment.",
            "The review presents both strengths and weaknesses of the paper, along with questions for the authors. It offers constructive criticism and suggestions for improvement, indicating a balanced perspective.",
            "The review adopts a balanced tone by providing both positive feedback (e.g., \"interesting and well-motivated problem\", \"technical results seem robust\") and constructive criticism (e.g., \"voting mechanisms proposed are relatively naive\", \"writing quality to have some issues\"). The reviewer also asks clarifying questions, contributing to a balanced and objective assessment.",
            "The tone is balanced, offering both praise ('paper structure and the writing are good', 'mechanisms are shown clear') and criticism ('the choose of privacy term epsilon is somewhat larger than the common privacy requirements'). The reviewer acknowledges the paper's strengths while also pointing out a potential area for improvement."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the identified weaknesses regarding the insufficient evaluation of MLC aspects directly support the overall conclusion that the contribution is hard to assess due to unclear MLC performance. While strengths are mentioned, the core criticism revolves around the lack of proper MLC analysis, which is consistently argued throughout the review.",
            "The reviewer consistently expresses concern about the paper's approach to differential privacy, focusing on the issue of data-dependent privacy versus data-independent privacy. The reviewer argues that differential privacy should be data-independent and criticizes the paper for not adequately addressing this, maintaining this stance throughout the review process from initial comments to final assessment.",
            "The review is consistent because it raises a focused and relevant question about the relationship between the task loss used in multi-label classification and the privacy guarantees of the proposed method. The reviewer logically connects different types of loss functions in MLC to the paper's approach, highlighting a potential gap in the discussion regarding the choice of loss and its implications for privacy, particularly in relation to subset 0/1 loss and the mentioned mechanisms. The review maintains a clear line of reasoning without contradictions.",
            "The message is consistent as it is a general announcement from an Area Chair to encourage authors and reviewers to engage in the discussion phase. It clearly outlines the expected actions from both groups and maintains a consistent tone throughout.",
            "The review is consistent because it provides a balanced assessment of the paper, highlighting both strengths (importance of the problem, simplicity and implementability of the mechanisms) and weaknesses (parameter setting, scalability, and experimental evaluation). The questions raised are relevant and aim to improve the clarity and completeness of the paper. The reviewer's overall positive tone is aligned with the identified strengths, while the suggestions for improvement are consistent with the identified weaknesses. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review maintains a consistent perspective throughout. It acknowledges the interesting problem and robust technical results while also pointing out the naivety of the proposed mechanisms and areas for improvement in writing clarity and explanations. The reviewer's questions and comments are constructive and aimed at enhancing the paper without any self-contradictory statements.",
            "The review is consistent. It praises the paper's contributions, clarity, and empirical results, while raising a minor concern about the choice of epsilon in the experiments. There are no contradictory statements."
        ]
    },
    {
        "paper_id": "iclr_2018_ByJWeR1AW",
        "paper_title": "Data augmentation instead of explicit regularization",
        "paper_abstract": "Modern deep artificial neural networks have achieved impressive results through models with very large capacity---compared to the number of training examples---that control overfitting with the help of different forms of regularization. Regularization can be implicit, as is the case of stochastic gradient descent or parameter sharing in convolutional layers, or explicit. Most common explicit regularization techniques, such as dropout and weight decay, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity. Although these techniques have been proven successful in terms of results, they seem to waste capacity. In contrast, data augmentation techniques reduce the generalization error by increasing the number of training examples and without reducing the effective capacity. In this paper we systematically analyze the effect of data augmentation on some popular architectures and conclude that data augmentation alone---without any other explicit regularization techniques---can achieve the same performance or higher as regularized models, especially when training with fewer examples.",
        "review_ids": [
            "S1KIF7olf",
            "BJHwtGogM",
            "rJrLHq3yz"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper presents an empirical study of whether data augmentation can be a substitute for explicit regularization of weight decay and dropout.  It is a well written and well organized paper.  However, overall I do not find the authors\u2019 premises and conclusions to be well supported by the results and would suggest further investigations.  In particular:\n\na) Data augmentation is a very domain specific process and limits of augmentation are often not clear.  For example, in financial data or medical imaging data it is often not clear how data augmentation should be carried out and how much is too much.  On the other hand model regularization is domain agnostic (has to be tuned for each task, but the methodology is consistent and well known).  Thus advocating that data augmentation can universally replace explicit regularization does not seem correct.\n\nb) I find the results to be somewhat inconsistent.  For example, on CIFAR-10, for 100% data regularization+augmentation is better than augmentation alone for both models, whereas for 80% data augmentation alone seems to be better.  Similarly on CIFAR-100 the WRN model shows mixed trends, and this model is significantly better than the All-CNN model in performance.  These results also seem inconsistent with authors statement \u201c\u2026and conclude that data augmentation alone - without any other explicit regularization techniques - can achieve the same performance to higher as regularized models\u2026\u201d\n",
            "The paper proposes data augmentation as an alternative to commonly used regularisation techniques like weight decay and dropout, and shows for a few reference models / tasks that the same generalization performance can be achieved using only data augmentation.\n\nI think it's a great idea to investigate the effects of data augmentation more thoroughly. While it is a technique that is often used in literature, there hasn't really been any work that provides rigorous comparisons with alternative approaches and insights into its inner workings. Unfortunately I feel that this paper falls short of achieving this.\n\nExperiments are conducted on two fairly similar tasks (image classification on CIFAR-10 and CIFAR-100), with two different network architectures. This is a bit meager to be able to draw general conclusions about the properties of data augmentation. Given that this work tries to provide insight into an existing common practice, I think it is fair to expect a much stronger experimental section. In section 2.1.1 it is stated that this was a conscious choice because simplicity would lead to clearer conclusions, but I think the conclusions would be much more valuable if variety was the objective instead of simplicity, and if larger-scale tasks were also considered.\n\nAnother concern is that the narrative of the paper pits augmentation against all other regularisation techniques, whereas more typically these will be used in conjunction. It is however very interesting that some of the results show that augmentation alone can sometimes be enough.\n\nI think extending the analysis to larger datasets such as ImageNet, as is suggested at the end of section 3, and probably also to different problems than image classification, is going to be essential to ensure that the conclusions drawn hold weight.\n\n\n\nComments:\n\n- The distinction between \"explicit\" and \"implicit\" regularisation is never clearly enunciated. A bunch of examples are given for both, but I found it tricky to understand the difference from those. Initially I thought it reflected the intention behind the use of a given technique; i.e. weight decay is explicit because clearly regularisation is its primary purpose -- whereas batch normalisation is implicit because its regularisation properties are actually a side effect. However, the paper then goes on to treat data augmentation as distinct from other explicit regularisation techniques, so I guess this is not the intended meaning. Please clarify this, as the terms crop up quite often throughout the paper. I suspect that the distinction is somewhat arbitrary and not that meaningful.\n\n- In the abstract, it is already implied that data augmentation is superior to certain other regularisation techniques because it doesn't actually reduce the capacity of the model. But this ignores the fact that some of the model's excess capacity will be used to model out-of-distribution data (w.r.t. the original training distribution) instead. Data augmentation always modifies the distribution of the training data. I don't think it makes sense to imply that this is always preferable over reducing model capacity explicitly. This claim is referred to a few times throughout the work.\n\n- It could be more clearly stated that the reason for the regularising effect of batch normalisation is the noise in the batch estimates for mean and variance.\n\n- Some parts of the introduction could be removed because they are obvious, at least to an ICLR audience (like \"the model would not be regularised if alpha (the regularisation parameter) equals 0\").\n\n- The experiments with smaller dataset sizes would be more interesting if smaller percentages were used. 50% / 80% / 100% are all on the same order of magnitude and this setting is not very realistic. In practice, when a dataset is \"too small\" to be able to train a network that solves a problem reliably, it will generally be one or more orders of magnitude too small, not 2x too small.\n\n- The choices of hyperparameters for \"light\" and \"heavy\" motivation seem somewhat arbitrary and are not well motivated. Some parameters which are sampled uniformly at random should be probably be sampled log-uniformly instead, because they represent scale factors. It should also be noted that much more extreme augmentation strategies have been used for this particular task in literature, in combination with padding (for example by Graham). It would be interesting to include this setting in the experiments as well.\n\n- On page 7 it is stated that \"when combined with explicit regularization, the results are much worse than without it\", but these results are omitted from the table. This is unfortunate because it is a very interesting observation, that runs counter to the common practice of combining all these regularisation techniques together (e.g. L2 + dropout + data augmentation is a common combination). Delving deeper into this could make the paper a lot stronger.\n\n- It is not entirely true that augmentation parameters depend only on the training data and not the architecture (last paragraph of section 2.4). Clearly more elaborate architectures benefit more from data augmentation, and might need heavier augmentation to perform optimally because they are more prone to overfitting (this is in fact stated earlier on in the paper as well). It is of course true that these hyperparameters tend to be much more robust to architecture changes than those of other regularisation techniques such as dropout and weight decay. This increased robustness is definitely useful and I think this is also adequately demonstrated in the experiments.\n\n- Phrases like \"implicit regularization operates more effectively at capturing reality\" are too vague to be meaningful.\n\n- Note that weight decay has also been found to have side effects related to optimization (e.g. in \"Imagenet classification with deep convolutional neural networks\", Krizhevsky et al.)\n\nREVISION: I applaud the effort the authors have put in to address many of my and the other reviewers' comments. I think they have done so adequately for the most part, so I've decided to raise the rating from 3 to 5, for what it's worth.\n\nThe reason I have decided not to raise it beyond that, is that I still feel that for a paper like this, which studies an existing technique in detail, the experimental side needs to be significantly stronger. While ImageNet experiments may be a lot of work, some other (smaller) additional datasets would also have provided more interesting evidence. CIFAR-10 and CIFAR-100 are so similar that they may as well be considered variants of the same dataset, at least in the setting where they are used here.\n\nI do really appreciate the variety in the experiments in terms of network architectures, regularisation techniques, etc. but I think for real-world relevance, variety in problem settings (i.e. datasets) is simply much more important. I think it would be fine if additional experiments on other datasets were not varied along all these other axes, to cut down on the amount of work this would involve. But not including them at all unfortunately makes the results much less impactful.",
            "This paper provides a systematic study of data augmentation in image classification problems with deep neural networks and argues that data augmentation could replace some common explicit regularizers like the weight decay and dropout. The data augmentation techniques are also shown to be insensitive to hyper parameters, so easier to use than explicit regularizers when changing architectures.\n\nIt is good to have a systematic study of data augmentations, however, the materials in this paper in the current state might not be a strong ICLR publication. The paper could potentially be made more interesting or solid if some of the followings could be investigated:\n\n- considering a wider range of different problems apart from image classification, and investigate the effectiveness of domain specific data augmentation and general data augmentation\n- systematically study each of the data augmentation techniques separately to see which is more important (as oppose to only having 'light' and 'heavy' scheme); potentially also study other less-traditional augmentation schemes such as adversarial examples, etc.\n- propose novel data augmentation schemes\n- more analysis of the interplay with Batch Normalization, why the results for BN vs no-BN is not presented for WRN?\n- carefully designed synthetic (or real) data / task to verify the statements. For example, the explicit regularizers are thought to unnecessarily constraint the model too much. Can you measure the norm (or other complexity measures) of the models learned with explicit regularizers vs models learned with data augmentation?\n\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the authors' premises and conclusions are not well supported by the results. They also point out inconsistencies in the results and disagree with the authors' conclusion that data augmentation can replace explicit regularization.",
            "The review expresses concerns about the paper's experimental setup, scope, and clarity. Phrases like \"falls short of achieving this\", \"a bit meager to be able to draw general conclusions\", \"I think the conclusions would be much more valuable if variety was the objective instead of simplicity\", \"another concern\", and multiple specific criticisms indicate a negative sentiment.",
            "The review acknowledges the value of the study but ultimately suggests that the paper, in its current state, is not strong enough for ICLR publication. It proposes several significant improvements, indicating substantial concerns about the paper's current quality and depth."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"I do not find the authors\u2019 premises and conclusions to be well supported,\" \"does not seem correct,\" and \"I find the results to be somewhat inconsistent,\" indicating a critical tone. The reviewer also provides specific examples to support their criticism.",
            "The review uses direct and critical language to point out weaknesses in the paper. Examples include \"this paper falls short of achieving this\", \"a bit meager to be able to draw general conclusions\", \"choices of hyperparameters seem somewhat arbitrary and are not well motivated\", and \"phrases like 'implicit regularization operates more effectively at capturing reality' are too vague to be meaningful\". The reviewer also questions the validity of some claims and suggestions.",
            "The review uses phrases like \"might not be a strong ICLR publication\" and directly suggests areas where the paper is lacking (e.g., \"wider range of different problems,\" \"more analysis,\" \"carefully designed synthetic data\"). The suggestions are framed as necessary improvements, implying a critical assessment of the current work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critique. It acknowledges the paper's writing quality but argues that the conclusions are not well-supported by the results. The reviewer provides two main points: (a) data augmentation's domain-specificity limits its universal applicability compared to regularization, and (b) the results presented in the paper are inconsistent and do not fully support the authors' claims. Both points consistently argue against the paper's central thesis.",
            "The review is consistent because the reviewer's initial concerns about the limited experimental scope, particularly the lack of diverse datasets and tasks, are maintained throughout the review and the revision assessment. Even after acknowledging the authors' efforts to address some comments and raising the rating, the core critique regarding the need for stronger experimental validation with more varied datasets remains the central point. The reviewer's arguments and justifications are aligned and do not contradict each other.",
            "The review is consistent because it acknowledges the value of the systematic study while suggesting concrete improvements to make the paper stronger and more suitable for ICLR. The reviewer provides constructive criticism without contradicting themselves."
        ]
    },
    {
        "paper_id": "iclr_2019_rkgsvoA9K7",
        "paper_title": "Dirichlet Variational Autoencoder",
        "paper_abstract": "This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior for a continuous latent variable that exhibits the characteristic of the categorical probabilities. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the Gamma distribution, which is a component of the Dirichlet distribution, with the inverse Gamma CDF approximation. Additionally, we reshape the component collapsing issue by investigating two problem sources, which are decoder weight collapsing and latent value collapsing, and we show that DirVAE has no component collapsing; while Gaussian VAE exhibits the decoder weight collapsing and Stick-Breaking VAE shows the latent value collapsing. The experimental results show that 1) DirVAE models the latent representation result with the best log-likelihood compared to the baselines; and 2) DirVAE produces more interpretable latent values with no collapsing issues which the baseline models suffer from. Also, we show that the learned latent representation from the DirVAE achieves the best classification accuracy in the semi-supervised and the supervised classification tasks on MNIST, OMNIGLOT, and SVHN compared to the baseline VAEs. Finally, we demonstrated that the DirVAE augmented topic models show better performances in most cases.",
        "review_ids": [
            "r1x7APd30m",
            "H1xeSd-7R7",
            "BygtBi5MAQ",
            "r1ergtDq37",
            "Hke6jd7w27",
            "ByxO1XWLsm"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thanks for your comments and sorry for late reply, I needed to reread the whole paper again to see how cohesive are different sections of paper and what specific message is tried to be delivered.\n1-  I think something should have gone wrong with your experiments with normalizing flow, the negative marginal log likelihood reported to on  Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" arXiv preprint arXiv:1505.05770 (2015) is 86.5 on MNIST data or less and it is better than GAVE and makes sense to be better than GVAE, so I really think that you need to reconsider your reported results, then comparisons could be correct from here on \n2- I have no argument that you can get related word for a topic the discussion was over sparsity and interpretability. In your model latent  parameter sparsity was not imposed so it will have e.g. many topics for each word if applied directly to the topic modeling problem.\n3- I have big concern about the massage that this paper carries. If paper is about to solve the  multimodality of posteriori distribution, the message and experimints should be tailored around that message. If the focus is to present the paper more as an application paper which makes improvement over different area, the message should be put out differently.\nI really like the extended experiments and improvement which are reported in different settings, but as other reviewer and i are concerned with novelty and I can not see the cohesive message is carried by this paper, I do not feel that any further comment can improve the current paper. Unless with unique message in mind the paper is written and experiments are done around the unique message.\nagain, Although I like the paper based on the extend of experiments, i am more intended to mild reject the paper as there are issue with having a message to be carried, how paper is presented and novelty.\nThanks and sorry \n",
            "I appreciate your clarifications. My concern about novelty still remains, but I agree that the proper application of well known technique leading to notable performance gains deserve a contribution, so won't argue for the reject if other reviewers vote for the acceptance. As in my initial reveiw, I suggest you to tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". ",
            "Thank you for the response. The sentence looks appropriate. ",
            "In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF,  it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian.\n\nAuthors pointed out, one of the weak points in competing models such as  Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. \n\n In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. \n\nTo summarize the contribution of this paper, following three points can be named as main contribution of this paper:\n- proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques  to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables.  \n\n- used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters.\n\n-discussed how to overcome  Stick-breaking VAE \u201ccomponent collapse\u201d issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE.\n\n\nQuality and Novelty:\nclaims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points:\n\n- Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\"\u00a0arXiv preprint arXiv:1505.05770\u00a0(2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper.\n\n- second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA  etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. \"Autoencoding variational inference for topic models.\"\u00a0arXiv preprint arXiv:1703.01488\u00a0(2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point\n\n- Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk\n\n\nClarity: \nThe paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear.\n\n\n\nSignificance of experiments:  \nAs discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.",
            "This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented.\n\nThe motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement \"to our knowledge, combining the two statistical results is the first finding in the machine learning field\". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. \n\nMinor question\n- What is the difference between negative LL's and reconstruction losses in experiments?\n- The approximation for inverse CDF of gamma works well only when alpha << 1. How did you treat the regime alpha > 1?\n\n\nReferences\n[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.\n[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.",
            "Review:\n\nThis paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out.\n\nQuality: \n\n\tI think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section.\n\nClarity: \n\n\tThe paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice.\n\nOriginality: \n\t\n\tThe paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. \n\nSignificance:\n\n\tThe results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high.\n\npros:\n\n\t- Good results.\n\n\t- Simple method proposed.\n\n\t- Extensive experiments.\n\n\t- Well written paper.\n\ncons:\n\t\n\t- The idea is a combination of already known techniques put in practice for the VAE.\n\n\t- A better motivation that the Dirichlet VAE gives good results should be given at the introduction.\n"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Neutral",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses \"big concern\" and states they \"are more intended to mild reject the paper\" due to issues with the paper's message, presentation, and novelty.",
            "The reviewer acknowledges the clarifications and potential contribution but still has reservations about novelty. The reviewer is willing to defer to other reviewers' opinions, indicating a neutral stance.",
            "The reviewer expresses gratitude (\"Thank you\") and approval (\"looks appropriate\").",
            "The review presents both positive and negative aspects of the paper. It acknowledges the paper's contributions, clarity, and experimental results, but also raises concerns about novelty, motivation, and interpretability. The overall sentiment is therefore neutral.",
            "The reviewer expresses a lack of novelty in the paper, stating 'I fail to find any novelty in the paper.' and suggests the authors 'tone down' a specific statement. They also mention the observation is 'not really surprising'.",
            "The reviewer expresses a generally positive view of the paper, highlighting its high quality, well-written nature, strong experimental section, good results, and simplicity. Phrases like 'improves over previous work,' 'good results,' 'simple method proposed,' and 'well written paper' contribute to this positive sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Balanced",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The review directly questions the validity of experimental results (\"something should have gone wrong with your experiments\"), expresses concern about the paper's message, and ultimately leans towards rejection (\"I am more intended to mild reject the paper\"). This is indicative of a critical tone.",
            "The review expresses both appreciation and remaining concerns. Phrases like \"I appreciate your clarifications\" and \"My concern about novelty still remains\" demonstrate a balanced perspective. The suggestion to \"tone down the statement\" is constructive criticism delivered in a neutral manner.",
            "The reviewer uses positive language and confirms the appropriateness of a sentence, indicating a supportive stance.",
            "The review presents both strengths and weaknesses of the paper in a measured way. It provides specific examples and justifications for its claims, avoiding overly positive or negative language. Phrases like \"Authors need to be clear about the motivation,\" and \"the paper is well written\" contribute to a balanced perspective.",
            "The reviewer uses phrases like 'I fail to find any novelty,' 'should tone down,' and 'just trivial' which indicate a critical assessment of the paper's contributions and originality.",
            "The reviewer uses encouraging language, such as 'I think the quality of the paper is high,' and focuses on the strengths of the paper while offering constructive criticism. The tone is generally positive and aimed at helping the authors improve their work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently expresses concerns about the paper's lack of a clear message, novelty, and potentially flawed experimental results, despite acknowledging the extensive experiments. The concerns raised in different points all contribute to the overall negative assessment and the inclination towards rejection.",
            "The reviewer acknowledges initial concerns about novelty but is willing to accept the paper due to performance gains and proper application of a known technique. The reviewer's conditional acceptance is consistent with their points and suggestions for improvement, and there are no self-contradictory statements.",
            "The review is consistent as it expresses a positive sentiment about the sentence being appropriate without any contradictions.",
            "The review is consistent in its assessment. It acknowledges the strengths of the paper, such as technical soundness, experimental results, and clarity. Simultaneously, it raises valid concerns regarding novelty, particularly in comparison to existing methods like normalizing flows and topic modeling approaches, and questions the interpretability of the results in relation to performance metrics. The reviewer's points are logically connected and build upon each other, presenting a balanced and coherent critique without internal contradictions.",
            "The review is consistent in its assessment. It acknowledges the motivation behind using Dirichlet distribution but expresses concerns about the novelty of the paper, suggesting that the claimed contribution is not as significant as presented. The reviewer supports this by mentioning prior use of similar techniques and pointing to relevant literature. The questions raised are pertinent and contribute to the overall critical but constructive tone of the review.",
            "The review maintains a consistent perspective by praising the paper's quality, significance, and writing style while also pointing out the limited originality and the need for better motivation for using the Dirichlet distribution. The pros and cons sections align with the overall assessment and specific feedback in the quality, clarity, originality, and significance sections. There are no contradictory statements or conflicting viewpoints within the review."
        ]
    },
    {
        "paper_id": "nips_2021_edmYVRkYZv",
        "paper_title": "TacticZero: Learning to Prove Theorems from Scratch with Deep Reinforcement Learning",
        "paper_abstract": "We propose a novel approach to interactive theorem-proving (ITP) using deep reinforcement learning. The proposed framework is able to learn proof search strategies as well as tactic and arguments prediction in an end-to-end manner. We formulate the process of ITP as a Markov decision process (MDP) in which each state represents a set of potential derivation paths. This structure allows us to introduce a novel backtracking mechanism which enables the agent to efficiently discard (predicted) dead-end derivations and restart the derivation from promising alternatives. We implement the framework in the HOL theorem prover. Experimental results show that the framework using learned search strategies outperforms existing automated theorem provers (i.e., hammers) available in HOL when evaluated on unseen problems. We further elaborate the role of key components of the framework using ablation studies.\n",
        "review_ids": [
            "20K-9Z0wd9Q",
            "FRbp7PnhP3U",
            "_0XCkr8Q8zH",
            "_4EmvaNWsLO"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper discusses how to train a tactic-based theorem prover from\nscratch with a limited set of tactics and no supervised learning data.\nOne particularly interesting feature is that TacticZero can teach itself\nstrategies to guide the search.\n  The connection between the MDP and tree search is not explicit enough in the paper. Some motivation could be provided along the following lines.\nTactic-based theorem proving is naturally understood as a tree search problem where actions are tactics and states are sets of goals.\nIn the reformulation, states are the search tree (set of fringes) and actions update the search tree (choose a node and apply a tactic).\nThis way, the agent can backtrack in the search tree using during a single episode (linear sequence of actions).\n\nA drawback to this linear approach (compared to the MCTS approach in RLcop) is that this does not explicitly distinguish between actions that were necessary for the final proofs and actions that were not.\nFor example, an action that produces a new fringe that is irrelevant for the final proof will receive a reward of 0.1 at step 1 and 5.0 at step n if the top goal is proven. I would expect that when a proof is found, such irrelevant actions would be removed (or re-evaluated) before training but this does not seem to be the case.\n\nIn the experiments, outperforming ATPs/hammers is a strong point that validates the approach presented here and the ablation study shows that the search strategy developed is better than DFS and BFS. The comparison could have included a slightly more advanced strategy that alternates between BFS and DFS such as the one used in Tactician (should be cited). Also, higher-order ATPs have been doing very well recently - see the GRUNGE evaluation (done on HOL4) and the two latest CASC THF competitions.  And TacticToe seems a natural candidate to compare with.\n\nOverall, the approach presented here is interesting as it is conceptually simple and shows how one can learn to backtrack using the REINFORCE algorithm which is of general interest.\n\nThe experiments demonstrate learning proof strategies for tactic-based theorem proving from scratch on a thousand carefully selected theorems but the usability of TacticZero on larger developments remains to be demonstrated.\n\n\nMinor:\n> Large action space\n     \nHow large?\n\n> After selecting a fringe, by default we select the first goal in that fringe to work on, because all of the\n     goals within a fringe have to be proved in order to prove the main goal, and the order in which they\n     are proven is irrelevant.\n\nAlthough all goals need to be proven at some point, the choice of a goal would influence the search by producing different fringes and therefore would affect fringe selection.\n\n> holyHammer\n\nHOL(y)Hammer\n\n> Background on RL-based approaches:\n\nA number of RL (Dagger-style) based AI/TP systems are missing, starting with MaLARea and ATPBoost, plcop/graphcop, ENIGMA, etc.\n\n=========\n\nUpdate after the author response:\n\nThanks to the authors for their replies and additional experiments. I have decided to increase my score, even though I would much prefer to see the evaluation on all of HOL4.\n\nI am quite unconvinced by the answer on comparisons to other HOL4 systems, but my impression is that running their system on all of HOL4 may be too expensive for the authors.\n\n=========\n\n\n\n\n\n Yes.",
            "This paper presents a \u201chammer\u201d-style ATP for the HOL4 theorem prover. Theorem proving is characterized as a Markov decision process in which states are sequences of subgoals that arise during theorem proving and in which the action space is defined over tactics (which can take either theorem labels or terms as input). Within this setting, the paper introduces a reinforcement learning algorithm that uses a novel architecture specific to the ATP task. The proposed algorithm out-performs state of the art ATPs. A core claim of the paper is that the system is capable of backtracking during proof search.  Overall, the paper addresses and important problem, is well-written, and contributes several novel and significant ideas.\n\nThe onerous burden placed on users of interactive theorem provers has hindered their adoption by working mathematicians and software engineers as actually useful assistants in the construction of proofs/verification of software. The success of deep reinforcement learning on other very large search problems, such as two player games, suggests that RL could contribute significantly to the usability and utility of interactive theorem provers by automating a significant amount of the proof construction and proof search processes. However, as the Related Work section of this paper discusses, existing approaches either focus on specific supervised tasks or else lack capabilities that are intrinsic to any non-trivial search process (e.g., backtracking).\n\nThe paper takes several well-established ideas from DRL and adapts them to the ATP domain:\n\n1. Structuring the state space of the MDP in a way that is amenable to backtracking during proof search.\n2. Replay in order to deal with sparse rewards (\u201cfringes\u201d).\n3. Reward shaping to discourage local optima.\n\nAlthough these ideas are already well-studied in the DRL literature, effectively adapting each to the ATP domain in a way that actually out-performs existing ATP methods is a highly non-trivial contribution.\n\nIn addition to adapting existing ideas from DRL, the paper also contributes a novel architecture with several features that are relatively unique to ATP:\n\n1. a transform-based autoencoder for HOL terms,\n2. fringe (e.g., subgoal), tactic, and argument selection networks, and\n3. a clever definition of policy distribution in terms of both subgoal and tactic selection. \n\nThe empirical evaluation of the paper is sound \u2014 the system is compared against state of the art hammer-style provers, including some that contain machine learning components, and an ablation study validates the primary claim of the the paper (that TacticZero is effectively learning to backtrack during proof search). I scrolled through the theorems used for the empirical evaluation and believe these are a reasonable set of benchmark problems for evaluating hammer-like systems. None-the-less, I wonder if papers on this topic should start using a standardized set of problems (a la CASC).\n\nThe most substantial weakness of this submission is the lack of source code combined with insufficient material in the appendix to easily reproduce the system and relevant experiments.\n\nI recommend accepting this paper because it addresses an important problem, contributes and clearly describes several important conceptual insights, and presents promising empirical evidence that these insights, combined with relatively straightforward adaptations of established ideas from DRL, are capable of competing with the state of the art in ATP for higher-order theorem proving.\n I agree with the author's assessment that \"We don\u2019t think the automation of theorem proving at its current level has potential negative societal impacts.\"",
            "TacticZero proposes a reinforcement learning approach to theorem proving in interactive theorem provers (HOL4 in this paper, but the approach could be applied to others). The RL agent selects both a node in the search tree to expand and a proof tactic to apply in this state. This means the RL agent learns to generate good proof steps, but also learns how to search effectively. The paper formalizes this process as an MDP and demonstrates experimentally, that we can learn good search strategies in this way (compared to various BFS and DFS algorithms). The paper also shows that their approach compares favorably to existing theorem provers (not relying on deep learning). The RL agent is trained with policy gradient.  The main novelty is that the RL algorithm does not only learn what a good proof step is, but also learns a search strategy. The experimental evaluation is relatively strong, demonstrating not only an improvement over BFS and DFS baselines (which have been used until recently), but also good absolute performance.\n\nMy only critique is that the evaluation misses the comparison to a pretty obvious baseline: selecting the fringe states by the cumulative log probability, as used in GPT-f. If this baseline was present this would make me even more convinced of the approach.\n\nThe paper is very well written, I found it a pleasure to read. Some minor comments:\n- lines 5ff: \"This structure allows us ...\" This sentence makes it sound as if your algorithm has an explicit backtracking component. However, as I understand it, your algorithm simply selects a node in the search tree to expand in each step. I think this description can be improved.\n- line 9: The emphasis on the comparison to \"existing theorem provers available in HOL4\" is a bit besides the point. Other neural theorem provers have shown that already. In my view, the main contribution in this paper is that the search strategy is learned as well. So the emphasis of the evaluation maybe should be the comparison to other search strategies.\n- line 38: I'm confused what the authors refer to. Existing neural theorem provers are not limited by the IO. The time to run inference and the time to run the proof tactics dominates all other factors.\n- Figure 5: could you also list the probabilities, not only the total number of proved statements? See main review.",
            " The paper implements a reinforcement learning framework for HOL4 interactive theorem prover based on the Markov decision process. The performance is compared with existing automated theorem provers.  To my knowledge, the paper is the first reinforcement learning framework for general theorem proving tasks in HOL4, that focuses on argument inference. The experiments are well performed and clearly show the improvements compared to HOL4 hammer. However the comparison with the current version of TacticToe might be performed in more detail. The paper is a good attempt at reinforcement learning on mainstream proof assistants.\n\nI wonder why the paper chooses the specific nine tactics for prediction. Is it because they are the most used tactics? The framework may not perform well on the theorems whose proof in the library contains tactics other than the nine. Are there better choices of tactics such that the framework can handle more general cases?\n\nTacticToe does not handle the selection of HOL4 terms as arguments ==> TacticToe has a conference version and a journal version. The journal version can predict arguments while the conference version cannot. However, you cite the journal version.\n\nThe paper uses \"interactive theorem-proving\" several times, but I never see such usage.  \"Interactive theorem proving\" is more common. I find no negative societal impact."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses overall positive sentiment, highlighting the interesting features of the paper, such as learning search strategies, and the strong point of outperforming ATPs/hammers. The reviewer also mentions that the approach is 'conceptually simple' and shows how to learn to backtrack, which is of general interest. The final decision to increase the score also signals a positive evaluation.",
            "The review expresses a generally positive assessment of the paper, highlighting its contributions, novel ideas, and promising empirical evidence. Phrases like \"addresses an important problem,\" \"well-written,\" \"contributes several novel and significant ideas,\" and \"promising empirical evidence\" indicate a favorable sentiment.",
            "The reviewer expresses overall positive sentiment, highlighting the paper's novelty, strong experimental evaluation, and readability. They also state they found the paper 'a pleasure to read'.",
            "The review expresses overall positive feedback, highlighting the paper's novelty, well-performed experiments, and clear improvements. While it raises some concerns, the overall assessment is favorable, describing the paper as a \"good attempt\"."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review presents both positive aspects (interesting features, strong experimental results, conceptual simplicity) and negative aspects (lack of explicit connection between MDP and tree search, potential issues with irrelevant actions, missing comparisons). The reviewer also provides specific suggestions for improvement, indicating a constructive and balanced approach.",
            "The tone is supportive, emphasizing the paper's strengths and potential impact. The reviewer uses phrases like \"effectively adapting,\" \"highly non-trivial contribution,\" and \"clever definition\" to praise specific aspects of the work. The final recommendation to accept the paper further reinforces this supportive tone.",
            "The review provides both positive feedback (e.g., 'The paper is very well written, I found it a pleasure to read') and constructive criticism (e.g., 'My only critique is that the evaluation misses the comparison to a pretty obvious baseline'). The reviewer also offers specific suggestions for improvement, indicating a balanced approach.",
            "The tone is balanced as it acknowledges the paper's strengths (\"experiments are well performed\", \"clearly show the improvements\", \"good attempt\") while also pointing out areas for improvement and raising specific questions (\"However the comparison with the current version of TacticToe might be performed in more detail\", \"I wonder why the paper chooses the specific nine tactics\", \"The paper uses 'interactive theorem-proving' several times, but I never see such usage\")."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it provides constructive criticism, highlighting both the strengths and weaknesses of the paper. The reviewer appreciates the interesting approach and strong experimental results, but also points out areas for improvement such as clarifying the connection between MDP and tree search, addressing the limitations of the linear approach, and suggesting more comprehensive comparisons in the experiments. The update after author response shows a logical progression where the reviewer acknowledges the authors' efforts and increases the score, while still maintaining some reservations, which is a reasonable and consistent evolution of opinion in a review process.",
            "The review is consistently positive, highlighting the paper's strengths such as addressing an important problem, novel contributions, sound empirical evaluation, and significant conceptual insights. While it mentions a weakness (lack of source code), it does not contradict the overall positive assessment and recommendation for acceptance. The reviewer consistently argues for the paper's value and impact in the field.",
            "The review is consistently positive and constructive. It praises the paper's novelty and strong experimental evaluation, while offering specific, actionable suggestions for improvement, such as adding a baseline comparison and clarifying certain points. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent as it provides both positive feedback (novelty, good experiments) and constructive criticism (comparison with TacticToe, choice of tactics, terminology). There are no self-contradictory statements. The reviewer acknowledges the strengths while pointing out areas for improvement and clarification."
        ]
    },
    {
        "paper_id": "iclr_2021_0IO5VdnSAaH",
        "paper_title": "On the Universality of the Double Descent Peak in Ridgeless Regression",
        "paper_abstract": "We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functions like sigmoid, tanh, softplus or GELU. As further examples, we show that feature maps from random Fourier features and polynomial kernels also satisfy our assumptions. We complement our theory with further experimental and analytic results.",
        "review_ids": [
            "SDvSLJGlwB9",
            "XMMqP6uQBss",
            "BsZt3HrISIV",
            "kHYxLVmN-d-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper studies the phenomenon of double descent for ridgeless regression. They show that when the label noise in the regression problem is lower bounded, the test error for regression must peak at the interpolation threshold (n=p) before descending again in the over-parameterized regime and that this holds with very weak assumptions making it a universal phenomenon when we consider unregularized linear regression. \nThese results extend our understanding of double descent and point that under most general settings it is impossible to avoid for ridgeless linear regression. \n\nThe paper is well-written and the analysis and comparison to prior work provided appears thorough. I have not verified all the proofs in the appendix.\n\n-------\nThank you to the authors for their response and update. I have read the response and am keeping my current rating for the paper.",
            "# Contributions\n\nThis work studies linear regression with feature maps (or kernel regression) without regularization in order to theoretically explore double descent phenomena seen empirically when training over-parametrized networks.\n\nThis paper provides lower bounds on the out of sample error or generalization error caused by the label noise. In the over-parametrized regime or beyond the interpolation threshold this is the primary source of error. \n\nWhile this setting has been studied before this work strictly generalizes previously provided bounds especially around the interpolation threshold. They also consider analytic feature maps including random ones and thus imply results for random deep neural networks. \n\nThe main text of the paper is well organized. However, it could benefit from some more clarity in presentation of the technicalities especially in section  5 when comparing with prior work and in section 6 when providing examples. \n\n# Concerns\n1. It seems that a big part of the assumptions is that the data is generated from a full dimensional distribution. Given that in the high dimensional settings where d ~ n a key problem is to characterize the behavior of estimators when true data has low intrinsic dimension can we say anything about this regime given the results in the paper? \n2. It seems that the main novelty over prior work especially Muthukumar et al. (2020) is around the interpolation threshold or slight weakening of the assumption on the noise. Since the over-parametrized regime is currently most relevant to the community is there a relevant example in this regime where the results provided in this work are strictly better (in terms of actual rates or in understand of this regime) than those from prior work? \n3. Or just as importantly are there examples of proof techniques that are used here that are substantially novel over those in the prior work that may be beneficial to the community in general in understanding these over-parametrized regimes? \n4. There is some recent work on ridgless kernel regression by [Liang et al. (2020)][1]. Since these two settings are fairly intertwined it would be nice to understand how results in current work compare to the results in this paper.\n\n[1]:https://arxiv.org/abs/1808.00387",
            "Summary:\n\nThe paper focuses on the theoretical understanding of the so-called double descent phenomenon, which may offer insights into the practical success of deep learning methods and has been observed in both overparametrized neural networks and kernel machines.  In particular, the authors derive a nonasymptotic distribution-independent lower bound on the excess generalization error of the ridgeless linear regression under mild conditions on the input distributions and feature maps. More specifically, their analysis applies to the cases where the input distribution has a Lebesgue density and the features are induced by random deep neural networks with analytic activation functions,  random Fourier features, polynomial kernels, and so on. The sharpness of the lower bound has been demonstrated by some numerical experiments. The results should be of interest to the community of theoretical deep learning. Overall, I vote for accepting.\n\n\nConcerns:\n\n1. Is it possible to derive a nonasymptotic upper bound which matches the lower bound in Theorem 3?\n2. In Theorem 3 and Corollary 4, which lower bound should be adopted when $p=n$, $\\sigma^2n$ or $\\sigma^2$?\n3. In the underparameterized regime ($\\gamma<1$), the derived lower bound seems to be not asymptotically sharp by looking at Theorem 2 of Hastie et al. (2019), any special reason for this?\n4. The authors may consider using subsections since sections 2 and 3 are very short compared to other sections.",
            "This work studies the double descent phenomenon in ridgeless regression with deterministic or random features. The work provides a lower bound on the generalization error that requires weaker assumptions than bounds given in previous work and applies to many interesting learning methods.\n\nStrengths:\n-- The generalization bound presented this work requires fewer assumptions than previous such bounds while also being stronger.\n-- The work is theoretically rigorous and helps to shed light on why various learning methods perform well.\n-- The authors thoroughly investigate the applicability of their bound with specific discussion of each of their assumptions.\n\nWeaknesses:\n-- While the analysis applies to a class of feedforward neural networks with analytic activation functions, some common activationswith a perfectly linear component like ReLU, however are not covered by the results in this work.\n\nThe second sentence in Remark 7 says, \"If (d) in Propositions 8 does not hold...\" but I don't think you mean to reference Prop. 8 here."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses positive feedback: \"The paper is well-written and the analysis and comparison to prior work provided appears thorough.\" The reviewer also indicates satisfaction with the authors' response and update.",
            "The review acknowledges the contributions of the paper, such as generalizing previous bounds and considering analytic feature maps. However, it also raises concerns about the assumptions made, the novelty compared to prior work, and the comparison to recent related research. The mix of positive and negative feedback results in a neutral sentiment.",
            "The reviewer states \"Overall, I vote for accepting.\", indicating a positive sentiment towards the paper.",
            "The review expresses overall positive feedback, highlighting the work's strengths such as requiring fewer assumptions, being theoretically rigorous, and thoroughly investigating the applicability of their bound. The weaknesses are minor and do not outweigh the strengths."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer uses supportive language like \"well-written\" and expresses satisfaction with the authors' response, indicating a positive and encouraging attitude towards the work.",
            "The review presents both positive aspects (contributions, organization) and negative aspects (concerns about assumptions, novelty, clarity). It uses a formal tone and provides specific examples and questions to support its points, indicating a balanced and objective assessment.",
            "The reviewer expresses interest in the results and provides constructive suggestions for improvement, indicating a supportive tone. The language used is polite and encouraging, such as \"The results should be of interest...\" and \"The authors may consider...\".",
            "The reviewer uses positive language ('Strengths', 'stronger', 'thoroughly investigate') and constructive criticism ('Weaknesses' followed by specific points). The reviewer also points out a minor error respectfully."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive. It highlights the paper's contributions and significance, praises the writing and analysis, and indicates a positive overall assessment by maintaining the rating after author response. There are no contradictory statements or criticisms within the review.",
            "The review is consistent as it acknowledges the contributions of the paper while also raising valid concerns and questions regarding the assumptions, novelty, and clarity of the work. The concerns are aimed at improving the paper and do not contradict the initial positive assessment of the contributions.",
            "The review is consistent because the concerns are raised as questions for clarification and improvement, and they do not contradict the overall positive assessment and the recommendation for acceptance.",
            "The review is consistent. It praises the theoretical rigor and the improved generalization bound compared to previous works in the strengths section. The weaknesses section points out a limitation in the applicability to certain activation functions and a minor editorial issue (typo). These points are not contradictory and provide a balanced assessment of the paper's contributions and limitations."
        ]
    },
    {
        "paper_id": "iclr_2021_lU5Rs_wCweN",
        "paper_title": "Taking Notes on the Fly Helps Language Pre-Training",
        "paper_abstract": "How to make unsupervised language pre-training more efficient and less resource-intensive is an important research direction in NLP. In this paper, we focus on improving the efficiency of language pre-training methods through providing better data utilization. It is well-known that in language data corpus, words follow a heavy-tail distribution. A large proportion of words appear only very few times and the embeddings of rare words are usually poorly optimized. We argue that such embeddings carry inadequate semantic signals, which could make the data utilization inefficient and slow down the pre-training of the entire model. To mitigate this problem, we propose Taking Notes on the Fly (TNF), which takes notes for rare words on the fly during pre-training to help the model understand them when they occur next time. Specifically, TNF maintains a note dictionary and saves a rare word's contextual information in it as notes when the rare word occurs in a sentence. When the same rare word occurs again during training, the note information saved beforehand can be employed to enhance the semantics of the current sentence. By doing so, TNF provides a better data utilization since cross-sentence information is employed to cover the inadequate semantics caused by rare words in the sentences. We implement TNF on both BERT and ELECTRA to check its efficiency and effectiveness.  Experimental results show that TNF's training time is 60% less than its backbone pre-training models when reaching the same performance.  When trained with same number of iterations, TNF outperforms its backbone methods on most of downstream tasks and the average GLUE score. Code is attached in the supplementary material.",
        "review_ids": [
            "hwxn__QtOXB",
            "UjRfDZyugv3",
            "bwdCMiYR3IM",
            "7Zi5Fw4IEwZ",
            "X5Vd0tD6yDk",
            "qaon3QH0jIP",
            "6VSXyrFpymN",
            "Lud0JlJRD9h",
            "cdFOrFGz6wJ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes Taking Notes on the Fly, a technique to improve the training efficiency of language-modeling style pretraining. It works by identifying rare words in the pre-training and adding a \u201cnote-taking\u201d component to the masked language model which augments these words with an extra \u201cnote\u201d embedding at the input layer. The note embedding is constructed from an exponential moving average of mean-pooled contextualized representations of context windows in which that word was previously seen during training. The notes are dropped in fine-tuning. Experiments find that this pre-training method improves fine-tuning results on English NLP tasks in the GLUE benchmark when used in the original BERT pre-training setup. In particular, the model can achieve similar performance to the original BERT model with less than 40% of the training steps, and similarly for ELECTRA.\n\n### Strengths\n\nThis paper is clearly written, the proposed technique is simple, and the results seem strong. It is laudable that the authors give experiments in the appendix to give a sense of hyperparameter sensitivity. The paper has a strong backbone and it seems that the proposed technique or something similar may serve as the basis for solid future work.\n\n### Weaknesses\n\nWhile the backbone of the paper is strong, I think it could be improved in its head (motivation) and legs (experimental studies).\n\nFirst, motivation. While the framing around rare words with the COVID-19 example is interesting, I think it has gaps. The introduction argues that since \u201cCOVID-19\u201d is a rare word, in the course of training the model may lack the necessary signal to predict the masked word \u201clives.\u201d But isn\u2019t this fact exactly what should lead the model to improve its embedding of \u201cCOVID-19\u201d? Because gradients flow into the embeddings both through the softmax layer and the input layer.\n\nSo while adding to the context may help the model get a foothold with more effective training signal for the masked token, it seems to me that the note could also \u201cexplain away\u201d the rare word\u2019s embedding in the input layer, reducing the learning signal on it. If that\u2019s the case, then to the extent that TNF works, it would be by the tradeoff between improving the learning signal at the output layer for all words (and in contextualization) and degrading it at the input layer (for rare words).\n\nAs a broader example, see https://openreview.net/pdf?id=3Aoft6NWFej. That paper argues for a masking scheme which eliminates easy shortcuts from the prediction problem to increase learning efficiency, whereas this paper argues essentially the opposite\u2014that shortcuts must be added to hard cases in order to facilitate learning. It seems that there may be a line to walk here between a task being too hard to learn from and too easy to be useful. Because it\u2019s not clear where that line is, I think it\u2019s not enough to motivate TNF from only one direction. It would be better to also have an explanation of why the note-taking approach does not also make things \u201ctoo easy.\u201d It\u2019s not obvious to me how to best make this argument, though results from some of the ablations I will suggest below might help.\n\nThis brings me to my second point: Ablation experiments. If the motivation is to improve the representations of rare words in the input, then there are even simpler ways to do this. Experiments with simple baselines and ablations are important for figuring out why exactly TNF works.\n\nFirst, if the note is such a useful addition to the word embedding, why not just use it to update the embeddings directly? At that rate, the method for constructing the note embeddings looks quite similar to word embedding training objectives like word2vec and GloVe. This suggests a critical ablation:\n\n* Initialize the word embeddings with word2vec, GloVe, or similar run over the wordpieces in the pretraining corpus. (Weirdly, I can\u2019t find an example of this in the literature. It seems like an obvious thing to try. I may have just missed it.) Indeed, it seems to me that the framing in the paper could just as easily motivate this (much simpler) technique than TNF.\n\nIf TNF outperforms the critical ablation, that implies that its gains are coming from some of the other particulars of the technique, such as 1) the extra degree of freedom provided by decoupling the note embeddings from the wordpiece embeddings, or 2) the use of contextualized vectors for note embeddings (rather than the non-contextualized ones in the word embedding objectives). \n\nTo investigate these issues, I would suggest three more ancillary ablations on TNF:\n* Directly update the rare word\u2019s embedding with a version of Eq. 5 rather than keeping a separate note dictionary.\n* Update the note embeddings via backprop instead of Eq. 5. This would amount to \u201cpartially tying\u201d the input and output embeddings, giving more freedom to the input layer, which is partly what\u2019s happening in TNF.\n* Pool over non-contextualized instead of contextualized representations in Eq. 4.\n\nFinally, to address the \u201ctoo easy\u201d vs \u201ctoo hard\u201d distinction, two more ablations that might help would be:\n* Instead of using an exponential moving average for the note embedding update, just use the pooled context vectors from the last instance of the rare word (i.e., set $\\gamma$ to 1 in Eq. 5).\n* instead of using an explicit note dictionary, augment the input context with retrieved text containing the rare word. See TEK-enriched representations (https://arxiv.org/pdf/2004.12006.pdf) for an example of this. For consistency, the exact last-seen context of the rare word could be used.\n\nThe first will help identify to what extent aggregating over many multiple inputs to get a high quality representation is necessary for TNF. This could then serve as a reference point for the second ablation, which may help determine whether the fixed embedding size and pooling operation helps by creating a bottleneck for the retrieved information and preventing things from getting \u201ctoo easy.\u201d (although context window sizes might also be a confound here, that could also be controlled carefully.)\n\nAll together I think these ablations would shed a lot of light on why TNF works, and make this work much more useful to researchers who wish to build on it in the future. However, I know I\u2019ve suggested a lot of crazy experiments here. I would not expect all of this necessarily to be done and I leave it up to the discretion of the researchers which are most important. I am also sure the authors could come up with better ablations than these as well. But my sticking point is the first ablation \u2014 initializing with non-contextualized embeddings \u2014 which I think is critical. And I think it behooves the authors to address some of the lingering questions (including more written below), even if not all of them.\n\n### Recommendation\n\nUnfortunately, reject. The technique is simple and the results seem good, but the paper does not provide empirically-justified insight on why TNF works. I think ablations and investigation into the \u201cwhy\u201d aspect is the most important part of this kind of model engineering research.\n\n### More comments & questions\n\nI am left with some more questions about how TNF works:\n\n* How does the quality of the representations of rare words specifically compare in your approach? Does it improve the representations of common words and contextualization at the expense of rare words? While it may be tricky to try to directly assess embedding or contextualization quality, breaking down the MLM perplexities by word frequency (or presence of rare words in the context) after removing the note dictionary might be informative. I admit this might also be tricky because I imagine the model would have to be fine-tuned without the notes for a bit before doing such an experiment. But any insight into this issue would be appreciated.\n* If this method indeed works by more narrowly refocusing the training signal on the masked token than the context tokens, then would you be able to further increase the learning efficiency by oversampling rare words when determining the masks in training? I am not aware of anyone showing such a thing to work, though I might have missed it. Just a thought.\n\nWhile the pretraining corpus is huge, 100 occurrences still seems like a pretty high threshold for rare words given the justification provided in the paper. Questions:\n* What do the even rarer words look like? Are they just a source of noise? e.g., because they are components of names or don\u2019t have clear and consistent semantic content?\n* What proportion of contexts contain words appearing less than 100 times? It seems that the 20% figure in the paper is meant to apply to your definition of rare words, which appear between 100 and 500 times.\n* What is the word vocabulary size? i.e., how many words appear more than 500 times, and less than 100?\n* Did you do any preliminary experiments with other thresholds? Would you expect this to work with more common words as well? Why or why not? (This may also relate to the \u201ctoo easy\u201d vs \u201ctoo hard\u201d issue.)\n\nOn pre-training efficiency results: I think Figs 3a and 3b need to be explicitly qualified a little better. AFAICT, having lower loss here doesn\u2019t necessarily mean the model (modulo the note dictionary) is learning better, because it sees the notes in the input. So we\u2019re looking at the loss in a different setting than we intend to fine-tune in. It\u2019s still interesting to see, but I think it's best to include an explicit caveat.\n\nWhat about training the models for more steps? Will the trend hold and performance improve overall, or will the gains eventually level off as the representations of rare words get better? Especially for pretrained models, since they are used as the starting point for many models, it is often worthwhile to train them longer (as in the RoBERTa paper), so it\u2019s important to understand the usefulness of this method in that regime.\n\n### Typos etc.:\n\n* P.3: neglectable -> negligible\n* P.3: Representation -> Representations (in BERT acronym)\n* P.6 Sec. 4.1: after \u201cMNLI\u201d there is a space missing after the period.\n* P.6: \u201cFULL-SENTENCES\u201d would look better & be consistent with Liu et al if it were in small caps.\n* Please cite the individual dataset creators for the datasets in the GLUE benchmark.\n\n---\n\nUpdate: upped score from 4 to 5; see comment thread.\n\nUpdate again: score further updated from 5 to 6 with GloVe context ablations and perplexity results on sentences with rare words.\n",
            "Thanks for your response.\n\n1. On the quality of the context vectors \u2014 Yes! I agree that BERT's contextualized vectors intuitively should be quite a bit more informative than non-contextualized GloVe vectors for these purposes. The experiment seems to confirm this intuition. Thank you!\n\n2. On the quality of the rare word embeddings \u2014 Yes again! This is also what I suspected and validates the intuition that you are essentially giving up some of the learning signal on the rare word embeddings in order to improve the signal on the rest of the model. I felt that this is what the paper was basically getting at but fell short of saying outright. These experiments are starting to give a more explicit accounting of what is going on in TNF.\n\nI think these experiments put the paper over the publication threshold. I'm upping my score again.\n\nGiven the results that you've shared, I'm still pretty curious about one of the remaining ablations: optimizing the notes via backprop. I assume it should be very simple to implement, so I hope you are able to run it (assuming you have the resources). If you can, I think it would further strengthen the case for this paper. You've shown that the TNF update rule with contextualized vectors works better than GloVe, but we still haven't seen whether it works better than backprop (at least, without the potentially destructive influence of the output layer). There are two possible outcomes:\n\n1. It works as well as TNF. In this case it might be preferable since it's a bit simpler, and it would also point to some potential problems with BERT: 1) problems with the loss on the output layer may be holding the model back, or 2) the wordpiece tokenization method makes it hard to learn good representations of composite/rare words. Disentangling those two contributors would be future work (though I think there is some work which has hinted towards the second). There may also be other interpretations I haven't thought of.\n\n2. It *doesn't* work as well as TNF. Then the reason that comes to mind is that backprop is not a great update rule when gradients on a parameter are extremely sparse, either because there are too few updates to get it into a good part of the space at all, or because the updates are made so sparsely during training that the rare word's representation lags behind the latent space learned by the rest of the model. These questions could be further tested by 1) some kind of clever initialization with GloVe (though this might be a bit weird for the sub-word wordpieces), or 2) using low-rank or computed embeddings from a method like DeFINE (https://arxiv.org/abs/1911.12385) which doesn't have the problem of sparse updates. If none of those methods work either, then this would mean TNF provides strong evidence in favor of more generally combining backprop with discrete update rules like TNF's equation (5) when dealing with parameter sets that have extremely sparse gradients. This would suggest some very intriguing directions for future work.\n\nAnyway those are just some thoughts I wanted to share on interpreting the results \u2014 besides the backprop ablation the rest of these experiments seem out of scope to me.\n\nI also think Reviewer 1 makes a good point about model capacity. Recent work suggests that in the very-large-model regime, more parameters may help a model learn faster (https://arxiv.org/pdf/2002.11794.pdf). So TNF may be working partly by just increasing the effective capacity of the model. A fairer comparison *may* be to compare BERT+TNF to a version of BERT enlarged to have a similar number of parameters, and I think such an experiment might further strengthen the paper \u2014 particularly if the argument is about speeding up training under a memory budget. (TNF has the advantage of not needing the extra memory at fine-tuning time, but perhaps the disadvantage of worse representations of rare words, which could hurt performance depending on domain, following along with Reviewer 1's comments.)  However, I think the experiment you shared here with GloVe is mostly convincing that a lot of the value is indeed provided by the TNF update rule anyway, so this doesn't seem like a deal-breaking observation to me.",
            "I do not mean that Transformer will memorize the text in the training corpus word by word. What I mean is that you need a significant amount of parameters/memory in the language model in order to achieve good perplexity. For example, in the validation corpus, you might sentence like \"In 2001, the New York City mayor [Mask] [Mask] handles 911 attack well\". In the training corpus, some sentences might mention that the mayor is Rudy Giuliani in 2001. In order to achieve low perplexity in that validation sentence, the model needs to memorize the fact [2]. That is why the larger model will achieve lower perplexity and the model size of GPT3 is so large. [3] further discovers that the perplexity of LM depends on mostly the model size rather than the hyperparameters such as hidden state size. [1] also shows that extra memory capacity could achieve lower perplexity. All of them talk about validation less. That is why I and other reviewers said that it is not surprising that TNF could achieve lower perplexity by using extra parameters. Your new experiment results also support this hypothesis. \n\nMy hypothesis is that most of the improvement on GLUE does not come from reducing the noise or modeling the word interaction in general as you claim. Instead, the improvement simply comes from the fact that BERT itself (without note dictionary) in TNF spends less memory on rare words, so the saved memory power could be used to memorize the facts involving popular words and interactions between popular words. Such facts are interactions are much more important for the datasets in GLUE. Currently, there is no experimental evidence that contradicts this hypothesis. \n\nIf you want to claim that TNF is better at reducing the noise or modeling the word interaction in general, I suggest that you can plot a Figure like Figure 1 in [3]. Using the experiments that are already done in [3] and [1], you can compare TNF with other ways of increasing the model size (e.g., simply increasing the hidden state size or adding the memory capacity like [1]). Given the same amount of extra parameters, if TNF boosts the performance quicker (with a larger slope) than other approaches, I will agree that some of the improvement might come from reducing the noise. In the end, if you find that your slope is not significantly better than other ways of increasing the model size, I strongly recommend you to rewrite the paper to tell a different story (e.g., the hypothesis I mention above and the potential application for the case of handling mismatch between the training corpus and downstream tasks).\n\n[1] Lample, Guillaume, et al. \"Large memory layers with product keys.\" Advances in Neural Information Processing Systems. 2019.\n\n[2] Petroni, Fabio, et al. \"Language Models as Knowledge Bases?.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.\n\n[3] Kaplan, Jared, et al. \"Scaling laws for neural language models.\" arXiv preprint arXiv:2001.08361 (2020).",
            "5. Quality of learned representations when the task is easy / hard / semantically meaningful.\n\nI did not suggest that harder tasks are always better for pretraining and easier are always worse. I explicitly said that it seems that it's possible for a task to be too hard (i.e., not learnable and thus indistinguishable from noise to the model) *or* too easy (i.e., leading the model to learn simple shortcuts instead of generalizable semantics). The \"too easy\" issue is the same as the \"explaining away\" issue \u2014 if a simple shortcut suffices, the model is less incentivized to learn anything more sophisticated. So in this case the thing to explain is why reducing signal on the rare word embeddings doesn't harm them enough to hurt performance in fine-tuning.\n\nI don't think your argument about the task being \"semantically meaningful\" is relevant to TNF. You could just as easily view the case of rare words without TNF as a \"semantically meaningful\" testbed through which to learn embeddings for rare words; in this case, TNF reduces the \"semantically meaningful\" aspect by the explaining away effect. The point here is that learning the relationship between a rare word and its context was hard enough not to be efficiently learnable by the model, so you added a note dictionary and update rule to make it easier. Indeed, this may mean that *less* is learned about relating rare words to their context than in the non-TNF case, but making that subproblem easier facilitates learning in the *rest* of the model, as you argue, and this is ultimately more important for downstream performance.\n\n6. Ablations\n\nThanks for sharing your results on the first two. However, be careful about the claim that word2vec or GloVe cannot learn good representations for rare words. If that is the case, then it would naturally follow that TNF couldn't either, since the update rules are so similar. You would need to explain what the difference is that matters.\n\nOn point 6.3: No, the embeddings are not always used together. If I understand correctly, TNF only brings in the note embeddings in the input layer, and not the output. This ablation is very close to (and can be substituted with) simply untying the word embeddings of the rare words in the input and output layers. As I said above, your reference [1] seems to indicate this might be a good idea, at least for BERT.\n\n7. I wasn't saying that TNF has to improve all of the embeddings. Rather, it seems to me it would make embeddings of rare words worse. It would be interesting to know if this is the case \u2014 that's all. Your results you provided in point 3 seem to answer the question for common words, which is very informative on that end. Thanks.\n\n8. Ah, thanks for pointing that out. I was not aware of this experiment (just found it in the ELECTRA paper's appendix). I was curious about whether TNF would make such a thing work better but I guess it seems unlikely and it isn't important.\n\n9. Word vocabulary and frequency cutoffs \u2014 I see. Makes sense; the tail is just way too long because we're working with words and not wordpieces. Thanks.\n\n10. Sounds good. I think your results from point 3 would be very nice to include in the paper.\n\n---\n\nAnyway, the results in point 3 and for word2vec/GloVe initialization are helpful. I think these improve the paper's case. However, particularly because word2vec/GloVe didn't perform any better than BERT, the explanation in the paper for why/how TNF works is quite inadequate. More experiments are needed. For this reason I am still recommending rejection, though I will up my score to 5.",
            "Thank you for your active and engaged reply as well. I will address the points in order.\n\n1. On whether \"the gradient flow in BERT can already lead to good rare word embeddings.\"\n\nI did not say this, and I am not sure where you got this quote from. My point was that there is an apparent tradeoff between the learning signal on the rare word embeddings and the learning signal on the rest of the network. This does not mean that the rare word embeddings will be \"good,\" though intuitively we would expect them to be better when there is more learning signal on them.\n\nActually, your reference [1] on this point proposes something very interesting: that the degeneration of the embeddings of rare words results from their use in the *output* layer. That suggests that the real problem here might be weight tying between inputs and outputs, which indicates that the key component of the TNF might indeed be that it decouples the input embeddings from the learning signal at the output layer. This is exactly what I suggested investigating in the second ancillary ablation, to see how much of the benefit comes from untying the weights versus the specific method of updating the note dictionary. Also note that while ELECTRA does binary classification, its embeddings are tied to its generator model during pretraining, which does use the MLM objective, so still may have this problem.\n\n2, 3. On rare word embeddings as noise, and evidence that TNF mitigates this\n\nUpon random initialization, all embeddings start as \"noise.\" Then they are learned. Or, in the case of rare words, perhaps they are not learned, or not learned well. It certainly seems that TNF helps mitigate this problem. I believe this from the experiments in the paper and in your point 3 (thanks for that). But many questions remain about *how* and *why* TNF mitigates it, which is the principal complaint in my review.\n\nYour explanation with the COVID-19 example is insufficient to explain why TNF would work but simpler methods wouldn't: particularly, initializing with GloVe (or similar), or just updating the rare word embeddings directly instead of having a separate note dictionary. Additively incorporating information from nearby tokens in previously observed context windows is *exactly* how the update rules in methods like GloVe and word2vec work. So going just by your explanation, there is no reason to think the representations in TNF's note dictionary will be any higher quality than GloVe or word2vec representations. And yet, TNF works better, as you say in your point 6.1. That means there is something about TNF beyond the reasons you give that is responsible for why it works. The point of the ablations I suggest is to get at what the real difference is that matters. In fact, that brings another ablation idea to mind, which is to use fixed GloVe embeddings as the note embeddings. (I do expect this to perform worse than TNF, but it would be very informative to see just how much worse).\n\n4. \"explaining away\"\n\nMy point here was about the learning signal on the rare word embedding (not including the note embedding) which is retained in fine-tuning. If this has less learning signal (which it well, due to the \"explaining away\" effect of including the note embedding), we would expect it to be even worse quality at the end of training, especially if the low quality is related to its use in the output layer. Maybe the effect isn't large because the embedding was already bad, so the tradeoff works out \u2014 but there is still a tradeoff here. I don't see the point of denying that.",
            "The paper proposes an external memory architecture. When encountering the rare words (with a frequency between 100-500), the method will store the average contextualized word embedding of nearby words into a dictionary. Next time it encounters the same rare word, it will retrieve the average embedding and input it into BERT encoder. The experiment results show that given the same number of training steps, adding the external memory improves the MLM loss and significantly improves the results on RTE (Recognizing Textual Entailment) dataset, which leads to a slightly better GLUE score. The experiment also shows that keeping the external memory during the fine-tuning stage slightly degrades the performance.\n\nPros:\n1. The method is simple and easy to understand\n2. The experimental results on GLUE are quite surprising. It shows that we should take note when training BERT but throw away the note dictionary when fine-tuning the model.\n\nCons:\n1. Missing an important citation [1]\n2. The paper does not well explain the surprising results on GLUE. This is a crucial weakness. The comparison of the MLM loss is not very fair because the proposed method has a large external memory. The benefit of the proposed method relies on the improvement of the average GLUE score. However, Table 2 shows the most of the improvement of GLUE actually comes from the improvement of a single dataset, RTE. Without understanding why it improves RTE, the readers do not know when they want to adopt the proposed method for their downstream applications.\n\nClarity:\nThe text is fluent, but the main story is not well supported by the experiment results. The story is that using an external dictionary could accelerate the training, but the main experiment finding actually says that using an external dictionary can very significantly improve the results on RTE dataset while performing similarly on other datasets in GLUE.\n\nOriginality:\nThere has been some effort of using an external dictionary to help the training of BERT [1], but I am not aware of existing papers that apply the dictionary to only the rare words. I also do not know any other work that shows the external dictionary could improve the GLUE scores.\n\nSignificance of this work:\nIf the authors could well explain the experimental results on GLUE and justify the explanation using some analysis, this might lead to more important findings.\n\n\nFigure 3c seems to contradict with Table 1 and 2 because in Table 1 and 2, the GLUE score of BERT (ours) is 83.1 but all the points in the BERT curve in Figure 3c is below 83. \n\nUsually, when a study tries to sell its method as a way to accelerate the training, it means the method reaches some performance faster but the method will converge the same performance eventually. However, Figure 3 does not show that they will converge the same value, so selling the method as a way to accelerate the training is weird. Furthermore, I think the lower MLM loss is due to the extra parameters in the note dictionary rather than the note dictionary accelerates the training. \n\nIt is not surprising that taking notes for rare words could achieve lower loss/perplexity because the note dictionary gives the extra memory capacity [1]. It is also not surprising that it can achieve better performance on GLEU if using the note dictionary during the fine-tuning stage due to the extra parameters. The really interesting results are that the authors report that the model could very significantly improve the RTE task and mildly improve CoLA without using the note during the fine-tuning stage. \n\nIntuitively, the proposed model stores lots of knowledge about the rare words into the note dictionary. Does the fact that the note is not needed in the fine-tuning stage imply that the knowledge about rare words is actually not needed? Does it mean the RTE or CoLA do not contain many rare words or does it mean the rare words do not affect the decision of BERT and ELECTRA in RTE or CoLA? Is the reason of improvement that we could store more interactions between popular words in the parameters of BERT itself because the information of rare words has been stored in the note (maybe you can test this by reporting the MLM loss on the sentences without any rare words)? If that is the case, why do we only stably improve RTE and CoLA? If the authors can show the above hypothesis is true, I think this is a significant contribution because that means this paper provides a way to control what LM should learn when there is a mismatch between MLM training corpus and downstream applications (e.g., MLM training corpus contains many rare words but we should ignore the rare words in the downstream applications).\n\nThis paper lacks a good explanation of the above weird result (in my opinion, the most valuable finding in this paper) and lacks the analysis that supports the explanation. The main paper says that taking notes improves the tasks with the small datasets the most. The STS-b (7k) and MRPC (3.7k) have smaller training datasets than CoLA (8.5k). Why are the results of STS-b and MRPC cannot be stably improved? If the authors really want to explain the performance improvement using the training dataset size, the authors can just randomly sample several small subsets of training data from each dataset and show that the GLUE score improves a lot in that setting. In the appendix A.4, the authors hypothesize that the small proportion of rare words in each dataset of GLUE (from 0.47% to 2.31%) might be the reason that we can ignore the note dictionary during the fine-tuning stage. This also did not explain why most of the improvement of the GLUE score comes from RTE. Moreover, if the rare words are not important in the testing datasets, why do we want to take notes in the first place?\n\nI will vote for acceptance if the authors could answer these critical questions I raise above strongly.\n\n\nMinor:\n1. Although the chance is not high, I think it is possible that parts of MLM improvement could be achieved by simply sampling the sentences containing the rare words more (This is a minor concern. If you do not have time to finish the experiments for this baseline, you can choose not to do it or compare the results after training fewer steps).\n2. I guess the dictionary overhead is small but it should be measured and reported because you say the method accelerates the training. \n\n\n[1] Lample, Guillaume, et al. \"Large memory layers with product keys.\" Advances in Neural Information Processing Systems. 2019.",
            "First, although I still think it would be better to know why TNF improves RTE more than CoLA, I think your explanation is reasonable.\nSecond, I think the new results you provide are significant. Like I said in the original comment, I think the results show that TNF could become a way of controlling what LM should learn. For example, if you want to train BERT on Wikipedia but your downstream tasks do not contain any sentences in the biomedical domain, you can alleviate the mismatch by taking notes for biomedical terminology and remove the note during testing time. Just like you find that rare words are indeed rare in downstream tasks, so taking notes for rare words during training allows BERT to spend more memory on the interactions between other words. I will change my vote to 6 for this contribution. \n\nAs for the explanation about why TNF works better, I still think the story you try to sell lacks support. We should discuss the cases of containing rare words and the case of not containing rare words separately. \nFor the case not containing rare words, it is possible that the improvement comes from better modeling the interactions between words as you said. It is also possible that the improvement comes from the fact that BERT could spend more memory on the sentences without rare words with the help of the note dictionary like I said. \nFor the case containing rare words, it is possible that the loss improvement comes from better modeling the interactions between words or avoiding the noise as you said. It is also possible that the improvement comes from the extra memory in the dictionary like I said. \nWe need more analyses to tell us which explanation is more true (or both of them are equally true). Some possible analyses include you can test the MLM loss without using the note dictionary (as you did in the GLUE benchmark), and you can just sample the sentences containing the rare words more frequently in the baseline (i.e., not using TNF) to improve the quality of the rare word embeddings. These two suggestions are just some random ideas. Feel free to ignore them if you have a better idea. I understand that it is difficult to design the experiments to investigate which explanation is better. If the authors can use more analyses to better support the story of the paper, I will further boost my score to 7.",
            "*Summary*: This paper proposes a method for improving pretraining convergence speed by augmenting the representations of rare words with the mean-pooled representations from their previously-occuring contexts (\u201cnotes\u201d, stored in a \u201cnote dictionary\u201d). The method considerably speeds up the convergence of pretraining BERT and ELECTRA, and the authors furthermore show that these models perform better when fine-tuning on downstream GLUE tasks (likely because the models were undertrained to begin with, so converging faster alleviates this issue).\n\n*Strengths*: The method is surprisingly simple and empirically quite effective. It's especially interesting to see that BERT + TNF at 400K steps has better GLUE performance than BERT at 1M steps.\n\n*Weaknesses*: the paper does not do a convincing job of arguing that the reasons for the faster convergence comes from better modeling of rare words---I\u2019m still not entirely sure why this works so well. Do these rare words commonly show up in GLUE (and thus, the method is helping because your representations of rare words are better)? It seems like TNF is actually improving the representations of more-common words as well.\n\n*Recommendation*: 7 Despite the lack of clarity around why exactly this method works so well, the method seems empirically useful and straightforward to apply. I expect that this will be useful to practitioners interested in applying BERT and similar pretraining strategies to new corpora and domains.\n\n*Questions*:\n\nIt\u2019s a bit unclear to me that note-taking itself is required for this to work well...in the COVID example presented in the introduction, if you see the sentence \u201cThe COVID-19 pandemic is an ongoing global crisis\u201d, isn\u2019t it possible that MLM itself is sufficient to associate the embedding of \u201cCOVID-19\u201d with \u201cpandemic\u201d and \u201cglobal crisis\u201d? Do you have further evidence to show that note-taking is actually improving the representations of rare words, besides GLUE score (which might not be very indicative, since the rare words might not show up in GLUE).\n\nThe Construction of Note Dictionary: Does 3.47B refer to the number of types or the number of tokens? Why not define keys with frequencies less than 100 in the dictionary as well (since you only use types that show up between 100 to 500 times)?\n\n\u201cIt means that to reach the same performance, TNF can save 60% of pre-training time. If models are trained on 16 NVIDIA Tesla V100 GPUs, BERT-TNF can reach BERT\u2019s final performance within 2 days while it takes BERT 5.7 days.\u201d: Is the 2 days vs 5.7 days an actual wallclock measurement? Or, are you hypothesizing this based off of the loss curves?\n\n*Missing / Erroneous Citations:*\n\n\u201cIt is well-known that in a natural language data corpus, words follow a heavy-tail distribution (Larson, 2010)\u201d This is more-commonly known in the NLP community as Zipf\u2019s law. Better cites would be:\n  - Zipf G. The Psychobiology of Language. London: Routledge; 1936.\n  - Zipf G. Human Behavior and the Principle of Least Effort. New York: Addison-Wesley; 1949.\n\n*Miscellaneous comments:*\n\n\u201cMoreover, completely removing those sentences with rare words is not an applicable choice either since it will significantly reduce the size of the training data and hurt the final model performance.\u201d: I agree that it\u2019s a bad idea to remove sentences with rare words, but I disagree that the issue is reducing the size of the data---you can always go collect more data and filter it to not include rare words. It\u2019s more likely that the issue is that removing sentences with rare words would reduce the diversity of the pretraining data, which would be harmful\n\n\u201cOur method to solve this problem is inspired by how humans manage information.\u201d: I think the connection to human note-taking is tenuous at best, and would omit it; the motivation remains clear without this.\n",
            "This work aims at accelerating pre-training by leveraging the contextual embeddings for the rare words. It is argued that the inadequate training of rare words slows down the pre-training. The authors then proposed to keep a moving average of the contextual embeddings for the rare words and use it to augment the input embeddings of the rare words. This technique is applied to BERT and ELECTRA and is shown to improve over the baseline. \n\nStrength:\n\n1. This work proposes a simple approach to accelerate the pre-training, with only a small memory and compute cost during training. The empirical study on BERT and ELECTRA supports the claimed improvements. \n\n2. It provides an interesting view towards the rare words problem that the rare word not only has worse embeddings but also slows down training of the whole model. \n\nWeakness:\n\n1. It is argued that the proposed approach helps with rare words problem. But it will help to add more experiments to see how much more benefit we can get from it. For example, maybe the use of contextual embeddings are actually helpful for all the words or sub-words instead of just the rare words. \n\nSpecifically, regarding \" we define keys as those words with occurrences between 100 and 500 in the data corpus\", How are the range 100 to 500 chosen? Have you tried it on words appearing lower than 100 or higher than 500? As mentioned above, it would be interesting to see if this approach can be applied to more words or subwords to get even more gains. \n\n2. Some design choices needs more details or explanations. \n\nFor example, why does the NoteDictionary use \"words\" instead of \"sub-words\" as keys? It seems using \"sub-words\" could cover a broader range of sentences with a NoteDictionary of the same size. It will also be easier to use during pre-training, for example, you could use the contextual embeddings to improve the word embeddings of the sub-words directly to avoid having an extra NoteDictionary. \n\nAnother example is how the window size is chosen, since it seems an important new hyperparameter. \n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Negative",
            "Neutral",
            "Negative",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review recommends rejection, stating that the paper lacks empirically-justified insight on why the proposed technique works. The reviewer believes that ablations and investigation into the 'why' aspect are crucial for this type of model engineering research.",
            "The reviewer explicitly states that the experiments 'put the paper over the publication threshold' and is 'upping my score again'. The reviewer uses encouraging language such as 'Thank you!' and finds the results 'convincing'.",
            "The review expresses skepticism about the paper's claims, suggesting the improvement might stem from a different reason than what the authors propose ('improvement simply comes from the fact that BERT itself...'). The reviewer also states there's no evidence contradicting their hypothesis, implying a lack of support for the authors' claims. The reviewer suggests rewriting the paper to tell a different story.",
            "The reviewer recommends rejection and states that the explanation for how TNF works is \"quite inadequate\" and \"more experiments are needed\". Although they acknowledge some improvements, the overall assessment is critical.",
            "The review presents a balanced critique, acknowledging the authors' efforts while also pointing out areas for improvement and further investigation. The language is objective and focuses on the technical aspects of the paper.",
            "The review expresses concerns about missing citations, lack of explanation for surprising results, contradictions in the experiments, and the overall justification for the method's effectiveness. The reviewer uses phrases like \"crucial weakness\", \"does not well explain\", \"contradict\", \"weird\", and \"lacks a good explanation\".",
            "The reviewer acknowledges the authors' explanation as 'reasonable,' finds the new results 'significant,' and expresses willingness to increase the score to 6 and potentially 7. This indicates a generally positive assessment of the work.",
            "The review expresses overall positive sentiment, highlighting the method's simplicity, effectiveness, and potential usefulness to practitioners. The reviewer recommends acceptance (score of 7).",
            "The review presents both strengths and weaknesses of the paper, indicating a balanced perspective. The language used is objective and analytical, focusing on the methodology and results rather than expressing strong positive or negative feelings."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses critical language, pointing out weaknesses in the paper's motivation and experimental studies. It suggests several ablations and questions the underlying reasons for the observed results. The recommendation of 'reject' further emphasizes the critical tone.",
            "The reviewer expresses agreement with the authors ('Yes! I agree...') and appreciation for their work ('Thank you!'). The reviewer also offers constructive suggestions and explores potential interpretations of the results, indicating a desire to help improve the paper.",
            "The reviewer challenges the author's interpretation of the results and suggests alternative explanations. Phrases like 'Currently, there is no experimental evidence that contradicts this hypothesis' and 'I strongly recommend you to rewrite the paper to tell a different story' indicate a critical stance. The reviewer also proposes a specific experiment to test the author's claim and states they will only agree with the claim if the experiment shows a significant difference.",
            "The reviewer uses phrases like \"be careful about the claim\", \"explanation in the paper for why/how TNF works is quite inadequate\", and \"More experiments are needed.\" These phrases indicate a critical assessment of the paper's claims and methodology.",
            "The reviewer raises several points of concern and disagreement, using phrases like \"insufficient to explain,\" \"principal complaint,\" and \"I don't see the point of denying that.\" While the tone is professional, it is clearly critical of certain aspects of the authors' reasoning and experimental design.",
            "The review contains direct criticisms of the paper's arguments, experimental setup, and explanations. It questions the validity of the claims and points out inconsistencies. Phrases such as \"missing an important citation\", \"does not well explain\", \"Figure 3c seems to contradict\", \"selling the method as a way to accelerate the training is weird\", and \"This paper lacks a good explanation\" indicate a critical tone.",
            "The review provides both positive feedback ('explanation is reasonable,' 'new results you provide are significant') and constructive criticism ('the story you try to sell lacks support'). The reviewer also offers suggestions for improvement ('Some possible analyses include...'). This balanced approach suggests a fair and objective evaluation.",
            "The review presents both strengths and weaknesses of the paper, offering constructive criticism and suggestions while acknowledging the empirical value of the method. The tone is generally respectful and objective.",
            "The review uses a balanced tone by highlighting both positive aspects ('simple approach', 'supports the claimed improvements', 'interesting view') and areas for improvement ('add more experiments', 'design choices needs more details or explanations'). The reviewer provides constructive criticism and suggestions for further research."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its assessment. It acknowledges the strengths of the paper, such as clear writing and promising results, but consistently emphasizes the lack of mechanistic understanding and empirical justification for why the proposed technique works. The reviewer's recommendation to reject is directly based on this identified weakness, arguing for the necessity of ablation studies to provide deeper insights. The updates at the end show a positive change in score based on author response, but the initial review and its core argument remain consistent.",
            "The review is consistently positive and constructive. The reviewer expresses agreement with the authors' responses, explicitly states that the paper is now above the publication threshold, and frames further suggestions as ways to strengthen the paper rather than essential criticisms. There are no contradictory statements or shifts in tone.",
            "The review is consistent because the reviewer's arguments are logically connected and build upon each other. The reviewer starts by questioning the authors' claim about noise reduction and word interaction, proposing an alternative hypothesis based on model capacity and memorization. This hypothesis is supported by citations and leads to a concrete suggestion for an experiment to validate the authors' claims. The reviewer's final recommendation to rewrite the paper is also consistent with the initial critique and the proposed alternative explanation.",
            "The review maintains a consistent critical stance, focusing on the need for a better explanation of TNF's mechanism and more experimental validation. The reviewer's points logically follow each other and build upon the initial concerns. While acknowledging the author's responses and some positive aspects, the core criticism remains consistent throughout the review, leading to a consistent recommendation of rejection based on the perceived inadequacy of the explanation.",
            "The review is consistent because the reviewer's arguments are logically connected and build upon each other, focusing on the need for deeper understanding of the proposed method. There are no contradictory statements or shifts in opinion. The reviewer consistently emphasizes the need for more mechanistic understanding of TNF's effectiveness and suggests specific ablations to achieve this.",
            "The review is consistent in its critique, focusing on the lack of explanation for the surprising GLUE results, especially the improvement on the RTE dataset. The reviewer's arguments and questions logically build upon each other to emphasize this central concern, without any self-contradictions.",
            "The review is consistent. While initially expressing a desire for more explanation on TNF's performance difference between RTE and CoLA, the reviewer acknowledges the provided explanation as reasonable and the new results as significant. The reviewer consistently emphasizes the need for more analysis to support the explanation of why TNF works, suggesting specific experiments and linking further improvement in the paper's justification to a potential score increase. There are no self-contradictory statements; the reviewer's stance is consistently critical yet constructive, seeking stronger evidence for the claims made in the paper.",
            "The review is consistent in its assessment. It acknowledges the empirical strengths of the proposed method (faster convergence, improved performance) while also pointing out weaknesses related to the clarity of the underlying mechanism and justification. The recommendation to accept (score 7) aligns with this balanced view, emphasizing the practical utility despite the lack of complete theoretical understanding. The reviewer's questions and suggestions for improvement further support this consistent stance of appreciating the empirical value while seeking better explanation and refinement.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper without contradicting itself. The strengths praise the simplicity and effectiveness of the proposed method, while the weaknesses point out areas for further investigation and clarification, such as the scope of the method's applicability and the rationale behind certain design choices. The reviewer is constructively critical, suggesting improvements rather than invalidating the work."
        ]
    },
    {
        "paper_id": "nips_2022_4u-oGqB4Lf6",
        "paper_title": "Efficient Active Learning with Abstention",
        "paper_abstract": "The goal of active learning is to achieve the same accuracy achievable by passive learning, while using much fewer labels. Exponential savings in terms of label complexity have been proved in very special cases, but fundamental lower bounds show that such improvements are impossible in general. This suggests a need to explore alternative goals for active learning. Learning with abstention is one such alternative.  In this setting, the active learning algorithm may abstain from prediction and incur an error that is marginally smaller than random guessing. We develop the first computationally efficient active learning algorithm with abstention. Our algorithm provably achieves $\\mathsf{polylog}(\\frac{1}{\\varepsilon})$ label complexity, without any low noise conditions. Such performance guarantee reduces the label complexity by an exponential factor, relative to passive learning and active learning that is not allowed to abstain. Furthermore, our algorithm is guaranteed to only abstain on hard examples (where the true label distribution is close to a fair coin), a novel property we term \\emph{proper abstention} that also leads to a host of other desirable characteristics (e.g., recovering minimax guarantees in the standard setting, and avoiding the undesirable ``noise-seeking'' behavior often seen in active learning). We also provide novel extensions of our algorithm that achieve \\emph{constant} label complexity and deal with model misspecification.",
        "review_ids": [
            "4XZa2aCbsPt",
            "4yu5LDpBI3CR",
            "80QUogWmHmK",
            "9ZJcTEYLLyD",
            "FDVTnCl_hAM",
            "QiZFKf9wJs"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the detailed response. The rebuttal has addressed most parts of my concerns. But I am still a little bit confused by the boundedness of $\\theta$.\n\nIt seems that the main theorems of this paper rely on the crucial realizability assumption, where the $\\mathcal{F}$ should contain a regressor characterizing the true conditional probability. But it is unclear to me whether the listed examples (such as linear function and generalized linear function) meet the condition in general. Does there exists a function class containing all possible true conditional probability but has bounded $\\theta$. I believe a clearer discussion on this issue would make this paper more competitive.",
            " I would like to thank the authors for the detailed responses provided in the rebuttal. While the manuscript has not been updated yet on openreview, I hope that the authors will incorporate the suggestions from reviewers for the camera ready version of the paper. All things considered, I believe this paper would be a good addition to the conference, and hence, I keep my score and recommend acceptance.",
            " Paper proposes an active learning algorithm in which labels are not acquired when the model chooses to abstain from predicting. [note confidence score of 1 (@authors + @AC). this is largely outside my comfort zone with many proofs, of which I did not study in details. I would recommend discarding my opinion.]\n\nI think lines 35-38 already starts this paper off on the wrong foot. The objective of active learning is to the learn the decision boundary with minimal labels; what the model does with points close to the decision boundary or with high uncertainty once this is learnt could be considered another problem. If the model can the learn the decision boundary with high accuracy given more queries around the decision boundary then this is justified.\n\nStrengths\n- The introduction makes clear what the paper is trying to achieve.\n\nWeaknesses\n- The paper is not easy to read.\n- Scenario proposed does not reasonable.\n- No experimental results whatsoever.\n- Lots of propositions and theorems are stated in the paper, but all the proofs in the appendix. I have but skimmed these. \n\nOriginality:\nCertainly seems new, but a more directed related work would be appreciated.\n\nQuality:\nI find the theoretical exposition somewhat lacking in places-- e.g. for proposition 3 and the statement follows of the algorithms superiority over any uncertainty-based AL method. No experimental results.\n\nClarity:\nNeither well written nor well organised. Language is imprecise and unclear. Parts of the paper make strong statements without backing.\n\nSignificance:\nDifficult to assess the impact of the method. The impact of the paper however will be small, given the problems above. N/A N/A",
            " The paper proposes an active learning algorithm that can avoid sampling from regions of the input space with high label noise. The algorithm satisfies two important properties: 1) it achieves exponential improvements compared to passive learning with respect to an evaluation metric that penalizes abstentions (Chow\u2019s excess error); and 2) the algorithm is computationally tractable for finite pseudo dimension function classes.\n Strengths:\n\n- The paper employs in a creative way techniques from contextual bandit literature to extend the idea of Puchkin et al and propose a computationally tractable algorithm. Moreover, the result is particularly remarkable since it does not require a condition constraining the amount of label noise, but rather captures it in the bound.\n\n- The analysis for deriving the results is non-trivial and some of the connections to quantities from the contextual bandit literature (e.g. eluder dimension, disagreement coefficient) may be of independent interest for the active learning community.\n\n- The paper contains several results that help to position the proposed algorithm in the broader active learning literature. For instance, the analysis of Section 3 confirms that the algorithm is minimax optimal (albeit not substantially better than passive learning) with respect to the standard excess risk.\n\nWeaknesses:\n\n- While the paper is generally easy to follow, certain details regarding the algorithm or the analysis could be discussed in more detail in the main text (see the Questions section)\n\n- Minor remarks: There are a few typos in the paper (e.g. lines 259, 266, 274 etc). Also the pseudocode of Algorithm 1 can be made a bit more precise and easier to follow (perhaps add a notation for the labeled set; Q_t, x_t, y_t are not defined when they appears first in step 4; unspecified how $\\hat{f}_1$ is selected etc).\n - How does the analysis of Algorithm 1 change if we assume a finite unlabeled set? Can the bound of Theorem 4 be changed to factor in the size of the unlabeled set?\n\n- Steps 5-6 of Algorithm 1 approximate the lcb/ucb over the set of functions $F_m$. For what function classes $F$ is this step tractable? How can the approximation error of the lcb/ucb influence the result of Theorem 4? For what function classes is the approximation of the lcb/ucb \u201creasonable\u201d?\n\n- Algorithm 1 enjoys strong guarantees at arbitrary noise levels. This is captured in the bounds via $\\gamma$ which needs to be chosen as a function of the noise level (indeed this is explicit in the proof of Theorem 6) and $\\theta$ (which captures the disagreement compared to $f^\\star$ at a fixed level $\\gamma$). How do $\\theta$ and the bound on sample complexity change when $\\gamma$ is chosen inappropriately in Theorem 6, without knowledge of the noise level?\n\n- Can the noise-seeking noise condition be relaxed for Proposition 3? The negative result for uncertainty-based AL is similar in spirit to the one in \u201cOn the relationship between data efficiency and error for uncertainty sampling\u201d Mussman & Liang, 2018. While the analysis here is significantly different, the result in Mussman et al only requires non-vanishing Bayes error to reveal the failure of uncertainty sampling.\n\n- Minor remark: \u201cNoise-seeking Massart/Tsybakov noise\u201d can be a bit confusing. Perhaps something like \u201cRandom flip-allowing Massart noise\u201d could make it easier to grasp (although it\u2019s a bit of a mouthful)?\n The paper generally addresses some of the poignant limitations of the analysis (e.g. focus on finite pseudo dimensions function classes, realizable vs agnostic case etc). See the Questions section for other limitations that could also be discussed in the paper.\n",
            " This paper studies the pool-based active learning problem. The main contribution is to propose a computationally efficient algorithm to train a rejection model. Under the realizable case, the model enjoys $\\epsilon$ chow's excess risk with $\\widetilde{O}(\\mathrm{polylog}(1/\\epsilon))$ label complexity. The guarantee is achieved without any low noise assumption commonly used to achieve the exponential savings label complexity in literature. Although a similar rate (for learning with abstentions) has already appeared in the literature, the proposed method is more efficient (or practical) than the previous one. Besides the main result, the authors also show that (a slight modification of) the proposed method enjoys minimax optimal label complexity for the standard excess risk with the low noise assumption. Furthermore, this paper has shown a constant label complexity in a special case (with a finite hypothesis set) and presented the guarantees with model misspecification. ### Strength:\nOverall, I think this is a nice paper with fruitful results. Specifically, the strengths of this paper are listed as follows,\n+ novelty& significance: although the algorithm framework shares a similar spirit as the previous work [Krishnamurthy et al. 2017] in standard active learning with abstention, the new criterion for label querying is interesting to me. A similar rate for active learning with abstention has been achieved by [Puchkin and Zhivotovskiy, 2021], but a computationally efficient algorithm is always what we desire. \n\n+ clarity: this paper is well written and clearly structured for the most part. Although there are fruitful results regarding Chow's excess risk and the standard excess risk (under different conditions), the authors have clearly organized them to make the results easy to follow.\n\n### Weakness:\nIn general, I like the results of the paper, but I have still some reservations about the assumption and the computational cost as follows,\n- about the realizable assumption: although the realizable has frequently appeared in the active learning literature,  the most related work on active learning with abstention seems not to require such an assumption (Theorem 1.1 of [Puchkin and Zhivotovskiy, 2021]). It seems to me that the realizable assumption is the price for the efficient algorithm since Algorithm 1 requires to approximate $\\eta(y=+1|\\mathbf{x})$ with the function $f(\\mathbf{x})$ (by ucb and lcb). So, I think it would be necessary to make a more clear comparison with the previous work.\n\n- about the efficient algorithm:\n\t- issue on the parameter setting: the algorithm takes the disagreement coefficient $\\theta$ as the input. I am not sure whether such a coefficient can be calculated efficiently in general? (maybe an upper bound for $\\theta$ is enough in special cases, but the realizable assumption could be violated.)\n\t- issue on the estimation of lcb and ucb: although the authors have referred to [Krishnamurthy et al. 2017] for the calculation of the lcb and ucb, I think it would be nice to discuss their computational costs since the efficiency is one of the main contributions of this paper. (for example how hard it is to compute lcb or ucb for a $\\mathcal{F}$ containing the $f_\\star$?).\n Q1: can the proposed method achieves similar Chow's excess risk without the realizable assumption when comparing with the best model in the hypothesis space $\\mathcal{F}$.\n\nQ2: how to compute the parameter $\\theta$ efficiently (please refer to the second point of the weakness for more details)\n\nQ3: what is the computational cost of ucb and lcb for a hypothesis space $\\mathcal{F}$ containing $f_\\star$ This paper has discussed its limitation on the realizable assumption in Section 4.2. It has shown that the same exponential saving label complexity is achieved with misspecified model space as long as $\\epsilon$ is less than the approximation error $\\kappa$. The results partially address the limitation on the assumption, but I think the paper would become even strong if the author could show a similar convergence rate for Chow's excess risk compared with the best model in the hypothesis when $f_*$ is not in $\\mathcal{F}.",
            " The paper studies active learning of general concept classes. Lower bound is known in this regime to rule out savings in label complexity over passive learning. However, [PT21] showed that with the additional action of abstention, active learning does provide exponential savings in terms of the error rate. This work follows the research line, and the main contribution falls into a computationally efficient algorithm that archives label complexity comparable to [PT21]. The main algorithm relies on efficient implementation of regression oracles, which has been developed in prior works.\n Strengths:\n+ Active learning is a very useful tool to reduce labeling cost, and this paper studies an interesting and practical extension.\n+ The core contribution on efficient learning paradigm is important.\n+ The paper is well written and easy to follow, with right amount of reminders and pointers.\n\nWeakness:\n- The computational efficiency is phased in terms of number of calls to an oracle, yet leaving the runtime of that oracle unsettled. Please provide concrete computational cost analysis to justify the main contribution.\n- It is true that [PT21] runs with minimizing an empirical 0/1 loss which is NP-hard. Can you give more intuition on why the 0/1 loss is vital for their analysis, and why the regression oracle approach in the paper works as well?\n \nIt is true that [PT21] runs with minimizing an empirical 0/1 loss which is NP-hard. Can you give more intuition on why the 0/1 loss is vital for their analysis, and why the regression oracle approach in the paper works as well?   Yes."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the authors' efforts in addressing their concerns but also expresses some remaining confusion. The reviewer uses phrases like \"a little bit confused\" and \"unclear to me,\" indicating a neutral sentiment with a slight leaning towards concern.",
            "The reviewer explicitly states that the paper \"would be a good addition to the conference\" and recommends acceptance, indicating a positive sentiment.",
            "The review expresses several negative points, including concerns about the paper's readability, the reasonableness of the proposed scenario, the lack of experimental results, and issues with the theoretical exposition. The reviewer also questions the clarity and organization of the paper and its overall impact.",
            "The review highlights several strengths of the paper, including the creative use of techniques, non-trivial analysis, and relevant results. While weaknesses are also mentioned, the overall tone suggests a positive evaluation of the work.",
            "The review expresses overall positive feedback, stating \"Overall, I think this is a nice paper with fruitful results.\" While it raises concerns, the reviewer also acknowledges the paper's strengths and contributions.",
            "The review expresses positive aspects such as the usefulness of active learning, the importance of the core contribution, and the paper's clarity. While it points out weaknesses, the overall assessment is favorable."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review starts with appreciation (\"Thank you for the detailed response\") and acknowledges improvements. However, it also raises a specific concern about the realizability assumption and its implications for the paper's competitiveness, showing a balanced perspective.",
            "The reviewer thanks the authors, expresses hope that suggestions will be incorporated, and ultimately recommends acceptance. This demonstrates a supportive and encouraging tone.",
            "The review uses critical language such as \"starts this paper off on the wrong foot,\" \"not easy to read,\" \"somewhat lacking,\" \"Neither well written nor well organised,\" \"Language is imprecise and unclear,\" and \"impact of the paper however will be small.\"",
            "The review presents both strengths and weaknesses of the paper. It uses formal language and provides specific examples to support its claims, resulting in a balanced assessment.",
            "The review adopts a balanced approach by highlighting both the strengths and weaknesses of the paper. It uses constructive criticism and poses specific questions, indicating a desire for improvement rather than outright rejection. Phrases like \"In general, I like the results of the paper, but I have still some reservations\" exemplify this balanced perspective.",
            "The review adopts a balanced tone by highlighting both strengths and weaknesses of the paper. It uses constructive criticism by posing specific questions and requests for clarification, indicating a desire to help improve the work rather than simply dismissing it."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the authors' response and improvement while still maintaining a specific concern about the boundedness of $\\theta$ and the realizability assumption. The reviewer's points are logically connected and do not contradict each other.",
            "The review is consistent because the reviewer expresses gratitude for the authors' responses, anticipates improvements in the camera-ready version, and ultimately recommends acceptance, maintaining a positive and coherent stance throughout the review.",
            "The reviewer consistently expresses negative opinions about the paper. From the beginning, the reviewer expresses discomfort and suggests discarding their opinion, which is followed by a critique of the core idea.  The strengths mentioned are minimal, while weaknesses are numerous and significant, covering readability, scenario, experiments, and theoretical exposition.  The reviewer consistently points out flaws in quality, clarity, and significance, maintaining a negative stance throughout the review. The initial disclaimer does not contradict the subsequent critical analysis, but rather sets a tone of caution and potential bias, which is then followed by a detailed critical assessment.",
            "The review is consistent as it provides a balanced assessment of the paper by clearly separating strengths and weaknesses. The weaknesses are presented as constructive criticisms and suggestions for improvement, without contradicting the acknowledged strengths. The reviewer offers specific points for clarification and further discussion, maintaining a consistent and helpful tone throughout the review.",
            "The review is consistent because the identified weaknesses and questions logically follow from the strengths and overall assessment. There are no contradictory statements, and the critique is balanced and constructive.",
            "The review is consistent because the strengths and weaknesses are logically separated and do not contradict each other. The strengths highlight the positive aspects of the paper, such as the relevance of the topic and the clarity of writing, while the weaknesses point out areas for improvement, such as the need for a more concrete computational cost analysis and further intuition on the theoretical approach. These points are independent and do not create any contradiction within the review."
        ]
    },
    {
        "paper_id": "nips_2021_tDqef76wFaO",
        "paper_title": "Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression",
        "paper_abstract": "Modeling a system's temporal behaviour in reaction to external stimuli is a fundamental problem in many areas. Pure Machine Learning (ML) approaches often fail in the small sample regime and cannot provide actionable insights beyond predictions. A promising modification has been to incorporate expert domain knowledge into ML models. The application we consider is predicting the patient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. Pharmacological models describe the dynamics of carefully-chosen medically meaningful variables in terms of systems of Ordinary Differential Equations (ODEs). However, these models only describe a limited collection of variables, and these variables are often not observable in clinical environments. To close this gap, we propose the latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned Neural ODEs to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. We evaluated LHM on synthetic data as well as real-world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic.\n",
        "review_ids": [
            "77mZy4UASBR",
            "pnvYbxqvH99",
            "sRv_J-JNt1g",
            "yNqV5eQco4-",
            "xeSlvl60UOW",
            "VihnYAeVj1_",
            "85UdvI_snxG",
            "mgdbCjYVP9U"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Hello, I wanted to express my apologies for being absent from the discussion so far. I wanted to reserve time to re-read the paper as well as consider the response in context. I kept putting this off and for this I apologize. I appreciate the time the authors spent to address my concerns and I would recommend that the authors certainly follow through on their promises to clarify the paper. \n\nI wanted to quickly step in here and let you know that I've read the author's response as well as the other reviews and corresponding discussions. I feel that this work is a valuable contribution to the community and should help push forward the body of research looking to integrate expert and learned features in complex problem domains. I maintain my recommendation for acceptance.\n\nI have some reservations about how the paper centers its advances on assumptions of \"exact\" or \"correct\" expert models. This severely limits the type of possible extensions (focusing on health and pharmacology here) of this work given that so few of these kinds of expert models exist with any reliability. I do feel that the arguments that other reviewers about the technical novelty of this work being somewhat narrow to be warranted. The LHM has a sense of being a special case of the supposedly more general approaches. However, my concerns are not to the depth where I feel the need to lower my initial score. I do hope that the authors will adequately address these similarities (despite their own nuances and limitations -- as the authors pointed out in their discussion with reviewer yPRn) in their revised (and possibly final) version of this paper. ",
            " Dear authors,\n\nThank you very much for running this extra experiment. It's really appreciated. This helps addressing the concerns I have expressed. However, I think that the experiment would be more convincing if it would be done on the real dataset and potentially with a non linear (e.g. quadratic) ODE. I'd be happy to increase my score in this case.\n\nNevertheless, I'm positive about your work and think it's valuable for the community.\n\nBest Regards,",
            " Dear Authors,\n\nThank you for your answers. \n\nMy main concern still resides in the source of performance improvement. The experiments fail to convince that the improvement is due to the expert ODE or to a better behaved ODE (see my first comment that you did not address). I keep thinking an experiment with a mispecified expert ODE would be needed to better understand the source of improvement.",
            " Dear authors thank you for your response. I have a couple of comments. \n\nConsidering the relation to [1,2], I believe that the fact that these consider incomplete or imperfect physics model makes them more general than the approach the paper discusses which assumes perfect knowledge. In particular by switching off the components in these models that learn to complete for the missing knowledge one would get something that is pretty similar to the approach presented in the present paper, modulo the missing regularisation on how the domain knowledge \"latent\" variables are used (or not) by the model.\n\nWith respect to the use of independent priors for the two types of latent variables $z^e, z^m$ what is there to prevent $z^e$ from being independent from $z^m$, but completely useless? and have all the information pass through $z^m$? One way to check whether at least in the synthetic experiments whether the model is making something sensible is to see how well the $\\theta^e$ variables are learned and what is their error with respect to the real values used in the simulation. That would provide at least indirect evidence that the model is learning something sensible with respect to these variables. In any case the use or not of these variables should be studied more carefully, the improvement of performance between the vanilla version and the physics based version provides evidence that these are probably used, but this could be the result of some implicit regularisation, e.g. the number of $z^m$ latent variables being such, compared to the number of $z^e$, that the information that can flow through them is restricted forcing in that manner the use of the $z^e$ domain knowledge-based variables. \n ",
            "This paper proposes an architecture to mix Neural ODE models and more structured ODE coming from expert knowledge. They evaluate their approach on a dataset from COVID-19 patients using an expert ODE of the dexamethasone.  I think this is a good paper with an interesting idea to mix expert knowledge and ODE models. However, the experiments section focuses on a single ODE /dataset, on which the results are compelling. Importantly, to understand if the performance gain comes from the expert ODE or from a better behaved ODE, extra experiment is needed. \n\n**Model formulation**\n- Why is it beneficial to add the current treatment value as an input to the observation function. Indeed, the treatment is already encoded in the model through the ODE of the latent process (equations 3. and 5.)\n- $a(t)$ is considered continuous, so it requires some assumption to cast any type of treatment in this form. This would need to be discussed.\n\n**Related works**\n- Related work on mechanistic/pharmacology models embedded in ML architectures : I think you don't cite reference [1]. Maybe too recent but would be good to discuss the difference with your work.\n\n- I think your work is very much linked to this paper as well : [2]\n\n**Experiments :**\n\n- From the synthetic experiment, I would not be surprised that the Expert performs worse than LHM even if M=0, simply because the number of parameters is higher for LHM. It would be good to have expand the plots of Figure 3 with M=0, as this would give a better view of the MSE obtained for larger M.\n\n- Similarly as in the paper of Hussain et al. [1], it would be nice to assess the effect of a mispecification of the expert ODE. Indeed, it might be that part of the improvement over an unconstrained Neural-ODE is the fact that part of the ODE is well-behaved [3] (low Lipschitz constant). In practice this would just mean to encode in the model a \\emph{wrong} parametric form for $f^e$. This would help narrowing down the source of the performance gain.\n\n[1] Neural Pharmacodynamic State Space Modeling Zeshan Hussain, Rahul G. Krishnan, David Sontag\n[2] Linial, O., Ravid, N., Eytan, D., & Shalit, U. (2021, April). Generative ODE modeling with known unknowns. In Proceedings of the Conference on Health, Inference, and Learning (pp. 79-94).\n[3] How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization, Finlay et al. They have. I think one limitation is the source of improvement of the model, which would require an extra experiment.",
            "This paper introduces an approach to integrate expert information into learned ODE models of observed dynamic processes. Motivated by applications in healthcare (particularly in pharmacology and disease progression) the expert information is provided as a verified dynamics model of lab-measured features that may not be readily measured in the normal clinical practice. However, the inductive biases afforded on the physiological dynamics, conditioned on possible treatment interventions, are considered worthwhile in the task of predicting future patient measurements with the learned ODE model. The two models are brought together by inferring the latent variables of the unobserved \"expert\" measurements and how they may affect the latent state generating the observed measurements. The proposed hybrid approach is evaluated on synthetic as well as real ICU data, comparing with several contemporary nODE appraoches.  \n\n#### **Originality**\nWhile the use of expert models or inductive biases to accelerate or improve model performance is not new, the use of a factored latent space between joint ODEs is unique and interesting. The authors do a great job listing and comparing to contemporary methods and approaches in the literature. One omission that should be included and considered is that of Miller, et al (2020 MLHC; reference below) which learns an improved dynamics model of the Insulin-Glucose interaction dynamics from a pre-defined ODE and observed patient data. I would be interested to hear from the authors how they feel their proposed approach differs from this work.\n\n\n#### **Quality and Clarity**\nI found the paper to be very well written with an appropriate level of technical detail and formalization such that the construction of the proposed LHM was easy to follow. I particularly enjoyed the measured discussion that was included to discuss both potential benefits of LHM in practice (in particular, Section 3.5 was well formed) as well as the current limitations of the approach (e.g independence of noise terms vs. autocorrelation, possible expert model misspecification, etc). I did have a few questions however and look forward to the author\u2019s clarifications that they are able to provide.\n\nWas the claim on line 51 that the inferred expert variables provide additional insights to clinicians ever evaluated? If not, the hypothetical nature of this claim should be clarified. Or is the evidence of this statement provided in the analysis presented around Figure 4? \n\nIn the modeling of the expected measurements over the future time interval, does the length of that interval affect the confidence or uncertainty associated with the predictions? While these quantities are referred to toward the end of Section 2, the experimental results do not fully demonstrate the effect of prediction horizon on model confidence. It\u2019s unclear what the choice of $t_0$ has on the downstream performance of the predictions. Does the model appropriately extrapolate over long horizons? How do those predictions improve with longer histories ahead of the prediction interval? Is there a maximum future time window for which the predictions are reliable?\n\nThis line of questioning led me to wonder what the specified prediction time was for the ICU dexamethasone experiments included in Section 5.2? When were the predictions made? At the first treatment? At distinct time points?\n\nThe use of the term \u201cexpert variables\u201d is quite vague throughout the paper. More concrete examples of what these variables may be in the context of the conditions investigated in the paper would be really helpful to get a sense of what the proposed LHM is expected to learn. This would be most helpful in the early stages of the paper. One particular area could be at the outset of Section 3.2. \n\nIn the final paragraph of the \u201cDatasets\u201d subsection of Section 5.1, a range for $\\sigma$ is provided. However, in Section 3.2 this variable is included in the learnable parameters $\\Theta$? Are the experiments in Section 5.1 investigating the effects of a fixed $\\sigma$ parameter?\n\n\n#### **Significance**\nWhile I don\u2019t feel especially qualified to comment on the significance of this paper. I do feel that the contributions put forward by the work are potentially of high impact as far as the type of expert modeling needed for LHM is available and appropriately specified for the desired use case. \n\n\n##### **Additional references**\nMiller, Andrew C., Nicholas J. Foti, and Emily Fox. \"Learning Insulin-Glucose Dynamics in the Wild.\" Machine Learning for Healthcare Conference. PMLR, 2020.\n As mentioned above, the authors sufficiently address the current technical limitations of the proposed LHM approach. There are no expressed considerations or limitations in view of potential applications of this model in live situations. However, at this time this research appears to be largely establishing a proof of concept for circumstances when a fully validated (and trusted) expert model exists and LHM serves to only adapt and improve that model based on observed data (without considerations toward equitable performance across possible patient subgroups).",
            "In this work the authors proposed the latent hybridisation model to predict disease progression from physiological measurements. The proposed model combines ordinary differential equations designed by experts and the neural ODE approach to achieve prediction accuracy and the interpretability of the expert variables. The performance was evaluated on a synthetic data as well as a real-world dataset. Improved performance was demonstrated comparing with several competing models.  I think the authors demonstrated a nice idea to integrate Pharmacological models as prior knowledge for a generic machine learning model. The work is well-presented, and I enjoyed reading the article. \nThe implementation seems follow standard approach of variational inference and limited technical contribution was presented. The evaluation demonstrates improvement in performance comparing to several strong baselines on a real-word ICU dataset. It would be nice to have another dataset to validate the improvement. The authors also demonstrated the benefit in interpretability with qualitative analysis.\n\nDetailed comments:\n\nHow are the parameters in the expert ODEs determined? Can the authors provide a table for the values of ODE parameters?\n\nCan the authors provide interpretation on the values on RMSE, i.e, how good/off a predicted track is for RMSE ~ 0.6. It would be very helpful to plot a few time series comparing the predicted ones v.s. ovserved ones. Also, it would be helpful to provide the definition or reference for RMSE / CRPS in line 254-255.\n\nCan the authors provide details on the results presented in Fig 4? Specifically, are the selected trajectories from the training or test split? Are all observed time points used for reconstructing the time courses? I am confused that the blue curve in x1 looks like a perfect fitting to the datapoints with no prediction errors. \n\nAre the time courses of Z2 in figure 4 just the convoluted responses of drug treatments? If so, it seems not as informative as claimed.\n\nIs there any evidence supporting that z1 represents the real response of cytokine Type I IFN? The authors adequately addressed the limitations",
            "A VAE model in which the decoder includes an ODE describing the evolution of some non-directly observable quantities, and the encoder learns mappings from the observed measurements to the initial states of the non-observed quantities, as well as to the ODE parameters. The model is used for forecasting in interesting application that seeks to forecast response to treatment of COVID-19 patients. \n\nI have some concerns with respect to the novelty of the paper, since it seems to be a special case of recent work, as well with respect to how the model might choose to treat the ODE part of it. In the general case the latter might simply be ignored since there is no protection against something like that baked in the model.   The paper presents an interesting application in forecasting the results of different treatments on patients with COVID-19.\nIt proposes a learning architecture based on a VAE model where the decoder contains an expert model, a pharmacological ODE, \nthat describes the evolution of different quantities of interest, collectively denoted by $\\mathbf z^e(t)$, such as immune \nresponce to infection, concentration of the drug in the body, viral load. Typically these variables are not directly observable \nin the routine clinical setting, but only in laboratory.  Only the structure of the ODE is known but not its parameters $\\theta$.\n\nThe clinician has direct access only to a set of observable variables, $\\mathbf x(t)$, or more precisely to their noisy measurements, \n$\\mathbf y(t)$, measuring quantities such as temperature, heart rate, and more. In addition the clinician also provides the treatment\nwhich is also modelled as a temporal sequence $\\mathbf a(t)$.  Since the non-observable variables $\\mathbf z^e(t)$ \nof the pharmacological ODE explain only one part of the process that produces the observational data these are completed with an \nadditional set of variables $\\mathbf z^m(t)$. \n\nThe VAE model is structured as follows: \n* the encoder receives the observation data $\\mathbf y(t)$ and treatment $a(t)$ and encodes them to the parameters of the ODE, $\\theta$, and the \ninitial states $\\mathbf z^e(0)$  $\\mathbf z^m(0)$; these are then respectivelly passed through the pharmacological ODE and a learned ODE to produce \nthe complete sequences $\\mathbf z^e(t), \\mathbf z^m(t)$\n* the decoder receives the latent variables and reconstructs the input signal. \n\nIn forecasting time the model receives the $\\mathbf y(t)$ and $\\mathbf a(t)$ up to the present time, \nprovides the posterior, and the decoder will use the sampled initial states and parameters of the\npharmacological ODE, to conditionally generate the future observation trajectory given also a future \ntreatment plan.\n\n\nThe paper provides two sets of experiments one on syntetic data generated using the known pharmacological model and one on real world COViD-19 patient \ndata. The model is compared against a number of baselines that involve a variant in which the pharmacological ODE is removed (NODE), one which is using \nonly the pharmacological ODE (Expert), two that rely in combinations of them, and a additional number including autoregressive models. \n\n\nThe paper is quite well written and easy to follow. I have though a number of concerns. \n\nThe main one has to do with whether indeed the model is really using the pharmacological ODE and the resulting $\\mathbf z^e(t)$ latent variables in its generative\nprocess. Since the latent space has two components $\\mathbf z^e(t)$ and  $\\mathbf z^m(t)$ what is there that guarantees that all the information flow will not happen\ntrhough $\\mathbf z^m(t)$ and the decoder $g$ will learn to ignore it, reducing the model to the NODE baseline. The empirical results show that probably this is not \nhappening, since the proposed model outperforms systematically NODE. However there is nothing in the model architecture and objective function that would prevent that\nfrom happening in the general case. The end result would be that we will have a model that while we believe it is based in domain knowledge it has learned to circumvent\nthat knowledge. \n\nIn fact such a setting has been extensively studied in Takeishi, Kalousis, Physics-Integrated Variational Autoencoders for Robust and Interpretable \nGenerative Modeling, 2021, which seems to subsume LHM as a special case. In that work a number of regularisers have been proposed in an effort to guarantee that the \nODE will not be ignored and the additional non ODE latent variables will be used in as minimal way as possible. Moreover there is no assumption about the completeness \nof the provided ODE, i.e. the model can work with incomplete knowledge. \n\nVery relevant work, though not in a generative setting and thus no interpretable latent variables, is also Yin et al, Augmenting Physical Models with Deep Networks for \nComplex Dynamics Forecasting, ICLR 2021. That work learns a dynamics model to be used for forecasting by learning augment {\\em incomplete} ODEs. Its model has two dynamics \ncomponents; one directly relies on the incomplete ODE and one is data driven and additively completes the ODE based dynamics. Here too the model makes sure, through appropriate \nregularisation, that the data-driven component will only capture information that is not already contained in the physics component, making sure in that manner that it does \nnot overwrite the ODE based compelent.  \n\n\nExperiments: \nIn the synthetic experiments, since we have complete access to all data, including the non-observed latents, it would have been very informative:\n*  to show how well the encoder is able to predict the $\\theta^e$ parameters of the ODE, \n*  and compare the forecasted/completed curves of $\\mathbf y(t)$, $\\mathbf z^{e}$, produced by the model with the real ones. Such a figure, fig 4, is \nonly partially given for $\\mathbf z^{e}$ for the real data, which is expected since there we do not have access to the real $\\mathbf z^{e}$. However \nthere too it would be quite interesting to see how the forecasted $\\mathbf y(t), t>t_0$ values compare with the ones that actually occur in the real data. \n* the paper also mentions a forecasting horizon of 1, 3, and 7 days, the results of which I found in the appendix,  but it is not clear to which horizon \nthe table 1 in the main paper corresponds to. Again some visualisations of the actual dynamics would have also been quite useful here, to see how the\nperformance scales as we move further to the future. \n\n\n Given the application domain, that of forecasting for medical purposes, it is rather important to make sure that the model operates as it is intended. This goes back to my comment above on whether indeed the model makes use of its ODE or simply chooses to ignore it, since as the model is there is no explicit control to avoid such a scenario. The end result would be a model that is not driven by the pharmacology model, the latent variables $\\mathbf z^e(t)$ would be meaningless. That does not necessarily mean that the forecasting performance would be poor, it would rather be that of a classical data driven model.\nAs a consequence one would need to include mechanisms to avoid such case from happening, see related work above, as well as mechanisms that are able to detect the extend to which such a case happens, e.g. some sensitivity analysis of $g$ with respect to $\\mathbf z^e(t)$ and $\\mathbf z^m(t)$. \n \nGiven the sensitive application domain such a discussion is more than relevant.  "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Neutral",
            "Positive",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses appreciation for the authors' efforts, recommends acceptance, and states that the work is a valuable contribution. While there are reservations, the reviewer maintains their recommendation for acceptance, indicating a positive overall sentiment.",
            "The reviewer expresses positivity towards the work, stating \"I'm positive about your work and think it's valuable for the community.\" They also appreciate the extra experiment.",
            "The reviewer expresses continued concern and doubts about the validity of the performance improvement claims, stating that the experiments are unconvincing and a crucial experiment was not addressed. The phrase \"fail to convince\" and the statement about a \"mispecified expert ODE\" needing to be tested to understand the source of improvement indicate a negative sentiment.",
            "The review contains both positive acknowledgment ('thank you for your response') and critical questioning of the paper's methodology and assumptions. The overall sentiment is neutral because the critical feedback is constructive and aimed at improving the paper, rather than outright dismissal.",
            "The reviewer states \"I think this is a good paper with an interesting idea\". Although they raise several concerns, the initial positive statement indicates an overall positive sentiment.",
            "The review expresses overall positive feedback, highlighting the paper's originality, quality, and potential significance. Phrases like \"very well written,\" \"appropriate level of technical detail,\" \"easy to follow,\" and \"potentially of high impact\" indicate a positive sentiment.",
            "The reviewer expresses enjoyment in reading the article and acknowledges the authors' nice idea and well-presented work. They also highlight the improved performance compared to baselines and the demonstration of interpretability.",
            "The review expresses several concerns about the paper's novelty and the model's ability to effectively utilize the ODE component. Phrases like \"concerns with respect to the novelty,\" \"might simply be ignored,\" and the suggestion that the model \"has learned to circumvent that knowledge\" indicate a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review expresses both positive feedback (valuable contribution, recommends acceptance) and constructive criticism (reservations about assumptions, technical novelty). The reviewer acknowledges limitations and suggests improvements, creating a balanced perspective.",
            "The reviewer uses phrases like \"Thank you very much\" and \"It's really appreciated.\" They also offer constructive criticism and express willingness to increase their score if improvements are made, indicating a supportive stance.",
            "The tone is critical because the reviewer points out a significant flaw in the experimental design ('The experiments fail to convince') and insists on a specific experiment that was not performed ('you did not address').",
            "The reviewer starts with a polite acknowledgement and then proceeds to raise specific concerns and suggestions. This balance of positive and negative feedback indicates a balanced tone.",
            "Balanced",
            "The review provides both positive feedback and constructive criticism. While praising the paper's writing, originality, and potential impact, it also raises specific questions and points out areas for improvement, such as the need for clarification on certain claims and the vague use of the term \"expert variables.\"",
            "The review acknowledges the strengths of the paper ('nice idea,' 'well-presented,' 'improved performance') but also points out limitations ('limited technical contribution') and raises specific questions and suggestions for improvement, indicating a balanced and critical assessment.",
            "The review uses critical language to point out potential flaws in the model's design and the paper's lack of novelty. The reviewer questions the model's reliance on the pharmacological ODE, suggests improvements with specific examples from related work, and emphasizes the need for more rigorous validation, particularly given the sensitive application domain. Phrases like \"main one has to do with whether indeed the model is really using\", \"nothing in the model architecture and objective function that would prevent that\", \"sensitive application domain such a discussion is more than relevant\" convey a critical perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses initial apologies for late participation, then clearly states their positive assessment of the paper's contribution and maintains their recommendation for acceptance. While they raise reservations about the paper's assumptions regarding 'exact' expert models and acknowledge concerns about technical novelty, these are presented as points for improvement and discussion rather than fundamental flaws that would contradict their overall positive evaluation and acceptance recommendation. The reviewer consistently conveys a message of acceptance with suggestions for refinement.",
            "The reviewer expresses overall positive feedback, appreciating the authors' efforts and the value of the work. While suggesting further improvements (using real dataset and non-linear ODE), these are presented as ways to strengthen the work, not as criticisms that contradict the positive sentiment. The reviewer explicitly states being \"positive about your work\" and \"happy to increase my score\" if suggestions are implemented, indicating a consistent positive and constructive stance.",
            "The review is consistent because the reviewer maintains a single, clear concern throughout the text: the source of performance improvement. They explicitly state this concern remains after the authors' response and reiterate the need for more convincing experiments to clarify whether the improvement comes from the expert ODE or a better-behaved ODE. The suggestion of an experiment with a mis-specified expert ODE directly supports this consistent line of questioning.",
            "The review presents a consistent line of reasoning, starting with a comparison to related works, then questioning the independence of latent variables and suggesting ways to validate the model's behavior. The reviewer's points are logically connected and do not contradict each other.",
            "The review is consistent because it starts with a positive overall assessment, acknowledging the interesting idea of the paper. Then, it delves into specific areas for improvement, focusing on the need for more rigorous experimental validation to understand the source of performance gain. The reviewer consistently points out the lack of clarity regarding whether the improvement comes from the expert ODE or simply from a better-behaved ODE, and suggests specific experiments to address this. The questions and suggestions in 'Model formulation', 'Related works', and 'Experiments' sections all align with this central concern, making the review internally consistent and focused on improving the paper's clarity and robustness.",
            "The review is consistent in its assessment, highlighting both the strengths and areas for improvement in a constructive manner. The reviewer appreciates the novelty and clarity of the paper while raising valid questions for clarification and further consideration, without contradicting their overall positive impression.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the nice idea and improved performance, while also pointing out areas for improvement and asking clarifying questions. The reviewer provides constructive criticism without contradicting their overall positive assessment of the work's potential.",
            "The review is consistent because it raises a central concern about whether the proposed model truly utilizes the pharmacological ODE component or if it might learn to bypass it. This concern is consistently reiterated throughout the review, supported by arguments about the model architecture, references to related work that addresses similar issues, and concrete suggestions for experiments and analyses to investigate this potential weakness. The reviewer's arguments and suggestions are logically connected and contribute to a coherent critique of the paper."
        ]
    },
    {
        "paper_id": "nips_2022_dO11Niyc225",
        "paper_title": "A Non-asymptotic Analysis of Non-parametric Temporal-Difference Learning",
        "paper_abstract": "Temporal-difference learning is a popular algorithm for policy evaluation. In this paper, we study the convergence of the regularized non-parametric TD(0) algorithm, in both the independent and Markovian observation settings. In particular, when TD is performed in a universal reproducing kernel Hilbert space (RKHS), we prove convergence of the averaged iterates to the optimal value function, even when it does not belong to the RKHS. We provide explicit convergence rates that depend on a source condition relating the regularity of the optimal value function to the RKHS. We illustrate this convergence numerically on a simple continuous-state Markov reward process.",
        "review_ids": [
            "PooSwFNR2N",
            "LDmqG6fJ_A",
            "k7gKyLg1dfc",
            "3uOZl6pYbBU",
            "dFwiUsE6doS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for the detailed response. Some of my concerns are still not fully addressed and I intend to keep my score.\n\nI understand that you are subtracting $\\lambda V_{n-1}$ in the entire update as opposed to just inside the temporal difference. However, I believe introducing $\\lambda$ this way also has some impact on the effective discount factor of the problem. Since Proposition 2 provides an upper bound on the difference, I think this is ok and am satisfied with the authors\u2019 response on this point.\n\nThere are some papers that address the difficulty of dealing with algorithms under $\\ell_\\infty$-norm contraction operators. For example, the authors of [] have shown that with some additional effort the norm-square function can also be used as a Lyapunov function to study the associated ODE. As for the stochastic algorithm, [] introduced a smooth version of sup-norm square so that the decent lemma holds with the new potential function. \n\n[1] Borkar, V. S., & Soumyanatha, K. (1997). An analog scheme for fixed point computation. i. theory. IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications, 44(4), 351-355.\n\n[2] Chen, Z., Maguluri, S. T., Shakkottai, S., & Shanmugam, K. (2020). Finite-sample analysis of contractive stochastic approximation using smooth convex envelopes. Advances in Neural Information Processing Systems, 33, 8223-8234.\n\nI understand that [30] studies off-line LSTD while this paper studies the online version. However, given that on-line and off-line TD in the tabular setting and under linear function approximation are well-studied in the literature, and [30] studied off-line non-parametric TD, the challenges in extending the results to online nonparametric TD are still not entirely clear. What is the major technical challenge in this work and what is the novel idea that is proposed to overcome the challenge? \n",
            " This paper studies the convergence of the regularized non-parametric TD(0) algorithm with RKHS approximation. For both the IID and Markovian noise settings, convergence rate bounds have been obtained.  Numerical results have been given to support the theory. This paper is quite original. The quality is good. The paper is also well written.\n\nStrengths:\n1. Originality: This paper is original. The analysis is new and novel.\n\n2. Quality: The contributions are very solid. Theory for both IID and Markov noise cases have been discussed. Numerical results are also provided.\n\n3. Clarity: The paper is well written.\n\n4. Significance: The contributions are significant. The analysis is very interesting. \n\nWeaknesses:\n  The connections to the following relevant papers are missing, and some clarifications/discussions are needed. \n[Cai2019] Q. Cai, Z. Yang, J.D. Lee, Z. Wang. Neural temporal-difference learning converges to global optima. NeurIPS2019.\n\n[Hu2019] B Hu, U Syed. Characterizing the exact behaviors of temporal difference learning algorithms using Markov jump linear system theory. NeurIPS 2019.\n\n[Durmus2021] A Durmus, E Moulines, A Naumov, S Samsonov, H Wai. On the stability of random matrix product with Markovian noise: Application to linear stochastic approximation and TD learning. COLT 2021.\n\nSpecific suggestions are given as follows:\n\n1. [Cai2019] has discussed some results for neural network approximation case. In the introduction, the authors mentioned that studying the RKHS case could bring us closer to understanding what happens with other universal approximators used in practice, like neural networks. Hence it seems quite relevant to discuss the existing convergence theory for the neural network case. \n\n2. [Hu2019] has given some exact analytical formulas for the TD error for linear approximation on countable state space (under both IID/Markov assumptions). Is it possible to obtain similar exact formulas for the RKHS approximation on general state case? Some discussion will be helpful. \n\n3. [Durmus2021] has addressed the TD error for linear approximation on general state space. It will be interesting to compare the assumptions in [Durmus2021] with the Harris ergodic assumption in this paper. In the linear approximation case, can the analysis method in this paper be used to get some improvements over [Durmus2021]?\n\n\n\n\n\n\n\n\n\n\n\n\n 1. [Cai2019] has discussed some results for neural network approximation case. In the introduction, the authors mentioned that studying the RKHS case could bring us closer to understanding what happens with other universal approximators used in practice, like neural networks. Can the authors comment on the connections between their paper and [Cai2019]?\n\n2. [Hu2019] has given some exact analytical formulas for the TD error for linear approximation on countable state space. Is it possible to obtain similar exact formulas for the RKHS approximation on general state case? \n\n3. [Durmus2021] has addressed the TD error for linear approximation on general state space. It will be interesting to compare the assumptions in [Durmus2021] with the Harris ergodic assumption in this paper. In the linear approximation case, can the analysis method in this paper be used to get some improvements over [Durmus2021]? Yes, the authors have discussed the limitations.",
            " This paper analyzes the kernel based (on-policy) TD learning. Specially, they consider the case where TD learning is performed with the value function in a reproducing kernel Hilbert space (RKHS) (eq 3, 4). They provide convergence guarantees when the true value function V* does not belong to the RKHS under a so-called source condition (A2). They also provided non-asymptotic convergence rate for the algorithm under i.i.d setting and the Markovian setting (where the state action sequence is sampled from a fixed policy).\n Strength:\n1. The paper is well written and structured. The authors gradually build the machinery from dynamic programming, RHKS, stochastic approximations (and etc) so as to introduce their main results and analysis.\n2. As far as I know, the main technical contributions of the paper is the analysis of kernel based on-policy TD learning setting in the framework of RKHS. The general framework of analysis is similar from the analysis of TD-learning with linear function approximation, the results in terms of RKHS is technical.  To my knowledge, theoretical guarantees of TD-learning with general nonlinear function approximation is still lacking. Kernel based TD learning could be a step forward from existing TD-learning with linear function approximation.  And such results can provide more insight and support into those kernel based methods.\n\nWeakness:\nOne potential drawback could be that the main framework of analysis bears some resemblance to the analysis of TD learning with linear function approximation. But again, performing such analysis using tools from RHKS is still technical.\n\nBased on the results in ref[0], the projection step for the Markovian case for TD-learning may not be necessary.\n\nref:\n[0] Finite-Time Error Bounds For Linear Stochastic Approximation and TD Learning, R. Srikant, Lei Ying.  The paper analyzed the so-called regularized TD learning (eq 12). My understanding is that this $\\lambda$ is for the use of the representer theorem. But this parameter needs to be tuned in practices. In the main theorem (theorem 1), the analysis needs $\\lambda > \\lambda_{\\theta}$. Is there any insight on the parameter $\\lambda_{\\theta}$ here, does it depend on $\\gamma - 1$? Yes",
            " The paper studies the temporal difference algorithm for estimating the value function of a Markov decision process, assuming that the control policy is already applied and the resulting Markov chain is homogenous and stationary. Under certain assumptions, the rates of convergence of the weighted averages of the value functions provided by a regularized temporal difference algorithm are shown, averaging over the stochasticity, as well as the state space. Strength:\nThe framework seems technically solid. The presented results are explained rigorously, and the setting is abstract enough to include non-tabular Markov decision processes, as long as the feature space is an RKHS.\n\nWeaknesses:\nThe setting is a little artificial. \nThe experiments are not clear to be generalizable. \nThe presentation is too compact and a little hard to follow, and is also unclear in some places. \nImportance of the problem is not sufficiently motivated as the policy is already applied, the transitions are mixed and have reached to the stationarity, and now the goal is to find the value function. This, as well as the next, restrict the applicability of the proposed approach for being used as the evaluation step of a reinforcement learning policy. \nThe results are in the form of average-case analysis. Intuitively, that means that if we repeat the setting many times, we can learn the value function nicely. However, the more interesting analysis is one that can establish accuracy of the learned value function based on a single trajectory. So, in some sense, the policy evaluation is analyzed in an offline fashion, while for offline reinforcement learning policies, evaluations are not the main obstacle. \nFinally, I do not think that Section 4 fits well in the framework as restarting for many times, together with the fact that the expected learning error is studied, defeat the purpose and limit the practicality of the approach. I would like the authors to address the points discussed under Weaknesses, and also explain how they can improve the presentation. The latter seems necessary as there are unclarity and lack of enough explanation in some places. \n\nIn the abstract, 'source condition' is unclear.\n98: 'but ...' makes ambiguity. \n106: define a 'nonnegative' reward.\nIn (1), it is unclear what is known and what is not. The main interest is in the case that the Markov transitions are unknown, but from the rest of the presentation, it does not seem to be the case as some of the quantities need the transition to be computed. Note that as the processes is assumed to be stationary and/or mixed, the authors need to argue why the known distribution does not provide the transition.\nHarris ergodicity, especially its regeneration set, need to be defined, and the discussion in these lines need more explanations.\n132: it is unclear what is n, and how these computations relate to the setting.\nIntuitions of Lemma 2 that the operator is like a contraction are required, as well as implications of such a fact.\nThe first paragraph of Section 3 is unclear. \nOn proposition 1, I think we are interested in the solutions of (12), and not those of (13).\nI am not convinced about necessity and/or usefulness of regularizing the TD. Mentioned in Weaknesses. ",
            " This paper focuses on solving the policy evaluation problem in reinforcement learning using non-parametric TD-learning. By introducing a regularization parameter $\\lambda$, the authors derive (1) the convergence rate of the associated ODE, and (2) the convergence rate of non-parametric TD-learning under either i.i.d. or Markovian sampling. Numerical experiments agree with the theoretical findings. This paper is well organized and well written. The authors start with the ODE associated with the deterministic variant of TD-learning, and use the Lyapunov function there to study the non-parametric stochastic TD-learning algorithm. While this is a highly technical paper, the structure makes the paper easy to follow.\n\nMajor Comments:\n\n(1) What is the motivation of introducing the regularizer $\\lambda$? Is it because $\\Sigma$ is not necessarily invertible but $\\Sigma+\\lambda I$ is guaranteed to be invertible? The Bellman operator is also a contraction with respect to the $\\ell_\\infty$-norm, regardless of whether the Markov chain has a unique stationary distribution or not. Introducing the regularization parameter is to some extend equivalent to solving a different MDP with a smaller discount factor, and I feel it should be avoided if possible.\n\nThe TD-learning algorithm with the regularizer $\\lambda$ is different than the original one. What is the updated algorithm? I do not think it is simply subtracting another $\\lambda V_{n-1} (x_n)$ in the temporal difference.\n\n(2) Regarding the convergence rate of the ODE. Suppose we exploit the $\\ell_\\infty$-norm contraction instead of the weighted $L_2$-norm contraction. Do we always get geometric convergence?\n\n(3) The need for projection in TD-learning with Markovian sampling is somewhat problematic. First of all, assuming there is an oracle that gives the right projection set is not realistic. Second, the estimate of the size of the projection set depends on unknown parameters of the MDP. In [55], the authors provide a way of analyzing TD-learning with linear function approximation without a projection. Is it possible to remove the projection using similar techniques?\n\n(4) The authors listed existing literature studying non-parametric RL, but did not compare the results and the techniques with them. I briefly checked [30], which is not for fitted Q-iteration (as claimed in this paper), but studies kernel based non-parametric LSTD and seems to be closely related to this work. The technical novelty is not entirely clear in the current manuscript.\n\nMinor Comments:\n\n(1) \"Deadly Triad\" refers to bootstrapping, off-policy, and function approximation (which does not have to be nonlinear). This should be made clear in the paper.\n\n(2) TD-learning with either tabular representation or linear function approximation has been studied extensively in the literature, and the convergence rate there is $1/\\sqrt{n}$. Some discussion seems needed to clarify why non-parametric TD has a slower convergence rate.\n\n\n\n\n\n My main concerns are the need for the regularizer and the technical novelty compared to existing literature studying kernel-based RL. This paper does not have any potential negative societal impact."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the authors' response but states that some concerns remain. They express satisfaction on one point but raise questions about the novelty and challenges addressed in the paper, indicating a mixed sentiment.",
            "The review highlights several strengths of the paper, including originality, quality, clarity, and significance. While it also points out weaknesses, the overall tone suggests a positive assessment of the work.",
            "The review expresses a generally positive assessment of the paper, highlighting its strengths, such as being well-written, structured, and making a technical contribution to the field. While acknowledging a potential drawback, the reviewer emphasizes the technical merit of the analysis.",
            "The review expresses several weaknesses in the paper, including artificiality of the setting, unclear generalizability of experiments, poor presentation, lack of motivation for the problem, limitations of the analysis, and unsuitability of a section. The reviewer also points out specific unclear points in the paper.",
            "The review raises several major concerns about the paper's motivation, technical novelty, and the practicality of certain assumptions. Phrases like \"problematic,\" \"not realistic,\" \"not entirely clear,\" and the reviewer's main concerns about the regularizer and technical novelty point to a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review exhibits a balanced tone. The reviewer acknowledges the authors' efforts ('I thank the authors for the detailed response') and expresses satisfaction with a specific point. However, they also maintain some reservations and pose critical questions about the paper's novelty and technical challenges, resulting in a balanced assessment.",
            "The review presents both strengths and weaknesses of the paper, offering specific suggestions for improvement while also acknowledging the paper's positive aspects. The language is professional and constructive, indicating a balanced perspective.",
            "The tone is balanced because it acknowledges both the strengths and weaknesses of the paper. Phrases like 'The paper is well written and structured' and 'technical contributions of the paper' indicate a positive assessment, while 'One potential drawback' introduces a critical aspect. The reviewer maintains a neutral and objective stance throughout the evaluation.",
            "The reviewer uses phrases like \"The setting is a little artificial,\" \"The experiments are not clear to be generalizable,\" \"The presentation is too compact and a little hard to follow,\" \"Importance of the problem is not sufficiently motivated,\" \"I do not think that Section 4 fits well in the framework,\" and \"I am not convinced,\" indicating a critical evaluation of the paper's content and presentation.",
            "The tone is critical, as evidenced by direct questioning of the authors' choices and assumptions, such as \"What is the motivation of introducing the regularizer?\" and \"The technical novelty is not entirely clear.\" The reviewer also points out limitations and potential flaws in the methodology."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent. The reviewer acknowledges the authors' response to some concerns and expresses satisfaction on a specific point regarding the discount factor. While maintaining a neutral or slightly negative stance by keeping the score unchanged, the reviewer's arguments and questions are logically connected and do not contradict each other. The reviewer is seeking further clarification on the novelty and technical challenges of the work, which is a valid and consistent line of inquiry.",
            "The review is consistent. It praises the paper's originality, quality, clarity, and significance in the strengths section. The weaknesses section points out missing connections to relevant papers and suggests specific improvements. The weaknesses and suggestions are constructive and do not contradict the strengths. The review provides a balanced assessment, highlighting both positive aspects and areas for improvement, which is consistent with a constructive peer review.",
            "The review consistently praises the technical contribution of the paper, particularly in the RKHS setting, while acknowledging similarities to linear function approximation. The identified weakness and questions are valid points for improvement and clarification, not contradictions to the overall positive assessment of the paper's technical merit and contribution.",
            "The review is consistent because it identifies both strengths and weaknesses of the paper without contradicting itself. It praises the technical soundness and rigor while criticizing the setting's artificiality, presentation clarity, motivation, and practical applicability. These points are distinct aspects of the paper, and the reviewer's comments on each are logically consistent with each other, forming a coherent overall assessment.",
            "The review is consistent because it presents a balanced view, acknowledging the paper's strengths in organization and writing while raising specific, well-reasoned concerns about the methodology, novelty, and clarity. The criticisms are logically connected and contribute to a coherent overall assessment without any self-contradictions."
        ]
    },
    {
        "paper_id": "nips_2021_ZEoMBPtvqey",
        "paper_title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers",
        "paper_abstract": "We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.\n",
        "review_ids": [
            "uHiPLgD7B0_",
            "dtT9_4XLLo",
            "jFpINqjUBLq",
            "bwKKPdJkIFk",
            "ZdVFTpvX64Y",
            "gg9kgJdMe62"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a method to reconstruct dense surface geometry of a static scene from a monocular RGB video. \n\nThe method combines implicit functions and transformers for achieving the final reconstruction. Specifically, images in the video are encoded by a 2D image encoder. After that, for each voxel in a 3D feature volume, the feature was calculated by: first bilinearly sampling per-view features by projecting the voxel on the 2D feature maps in different views, and then, merge the per-view features from different views together using a multi-head transformer. After calculating the 3D feature volume, 3D convolutions are used to directly refine the features in it. Finally, for each query point in 3D space, an MLP-based occupancy decoder was introduced to decode the occupancy value given the corresponding feature of the query point (which is sampled in the refined 3D feature volume). \nTo further improve the reconstruction efficiency, the whole reconstruction pipeline was conducted in a coarse-to-fine manner. \n\nThe main contribution of the paper lies in the transformer-based feature merging stage. By utilizing the strong self-attention performance in the transformers for aggregating features from different views, the proposed method achieves plausible and more complete surface reconstruction results than previous methods.   Reconstructing dense surface geometry using a monocular RGB camera is a very challenging task. Accurate and complete surface reconstruction will benefit a lot of areas like AR/VR, semantic understanding, etc. \n\nThe paper writing is clear and easy to follow. The results are plausible, and the experiments are thorough. \n\nMy major concern is the technical novelty of the proposed method. Since there already have a lot of works that use transformers for feature aggregation (as mentioned in the related work section), simply transferring this technique to static scene reconstruction is not that a breakthrough, despite the fact that the way of using a transformer is relatively straightforward in the paper. \nMoreover, as shown in Fig.1 (1st row) of the supplementary material: the blue color views share similar image quality and viewing direction with the red color views. However, they contribute quite differently to the final results according to the output of the transformer, which is confusing. I would like to suggest the authors explain the reason for this phenomenon in detail.   ",
            "The paper presents a fully end-to-end trainable deep 3D reconstruction pipeline taking monocular RGB video sequences as input. The proposed pipeline works in an online fashion and at interactive frame rates.\nIn the first stage, the method extracts 2D features from input RGB images. In a next stage, 3D features are generated by projecting a 3D location into the images, pooling the corresponding 2D features and fusing them through a transformer network. After refining the 3D features with 3D convolutions, the geometry is predicted through a neural implicit decoder and finally extracted through marching cubes.\nNotably, the method uses a two-level hierarchy of features for improved quality and runtime performance. Furthermore, the self-attention weights are reused to only keep the most important observations for each 3D point at any given time.\nFinally, the authors compare their pipeline to several state-of-the-art methods and perform an ablation study to validate the individual components.\n  #Originality: \nThe idea of integrating self-attention into online 3D reconstruction pipelines and using the attention weights for view selection is novel and interesting, although not ground-breaking.\nUsing feature hierarchies or coarse-to-fine level approaches in order to reduce run-time or improve quality by incorporating larger perception fields is a well tested concept (e.g. Chibane et al. [5]; Saito et al. 2020 PIFuHD; NeuralRecon [39]) and not really new in the field of 3D reconstruction.\nAs such, this paper can be considered more a work which combines well-known techniques (Self-attention, Image Feature Extraction with 2D convolutions, using Feature Hierarchies, 3D Convolutions for \u201cRefining\u201d/Propagating 3D Features and using neural implicit functions to predict geometry) rather than a theoretical contribution but is nonetheless valuable and interesting.\nIn contrast to previous work the presented method is online and fast enough to run at interactive frame-rates. There exist other deep learning based online 3D reconstruction approaches (e.g. RoutedFusion, Weder et al.) but they work on depth inputs which is an easier task than RGB-only input. \n\n#Quality:\nFrom a qualitative perspective, the work feels well thought through and complete and the paper makes sense from a technical standpoint. The performed experiments and evaluations are extensive and one can see that the authors put a lot of effort into establishing a fair comparison with multiple SotA methods and baselines. The ablation study is very informative and valuable and proves that every proposed component is useful. Specifically, during the read-through, I was asking myself how much influence the viewing ray input had and was glad to find the corresponding ablation study in the supplementary material.\n\n#Clarity:\nThe paper is well written, is clear and well organized. The most relevant details are present and the pipeline should be reproducible with help of the supplementary material. The most important related work has been adequately cited.\n\n#Significance: \nQuantitatively, the paper only slightly outperforms previous work (Atlas) and is roughly on par with SotA (NeuralRecon). Qualitatively, they seem to yield better results, even better than NeuralRecon. Therefore, the results are interesting and show that at least some level of improvements can be expected when using self-attention. I believe that others will build upon the idea to incorporate self-attention into deep learning architectures for 3D reconstruction pipelines and therefore think that exploring self-attention for 3D reconstruction pipelines is valuable. Besides the quantitative results being not overly impressive, I believe that the results provided are valuable to the community.\n\n#Post-rebuttal:\nI agree with the findings of reviewer mh3Q that the novelty of the paper is of an incremental nature. Nevertheless, I believe that the improved results and the technical contributions offset this and justify accepting the paper.\nNeuralRecon can be viewed as parallel work, given that it has been published at CVPR 2021 (after NeurIPS deadline) and therefore I consider coincidental similarities in the pipelines not as a valid reason for rejecting the paper (although in my opinion, there are enough technical differences in the two pipelines, e.g., TransformerFusion projects 3D points into views, while NeuronRecon uses raycasting).\n\nThe observations of reviewer 4TZ2 regarding Fig. 1 of the supplementary were concerning at first but the answer of the authors makes sense, i.e. that out of several similar views only one is assigned the highest weight in order to remove redundancy. However, I suggest, the authors include their answer in the paper and/or the supplementary.\n\nFor the above mentioned reasons, my initial rating persists. The potential negative societal impact has been addressed adequately.\nLimitations have been addressed somewhat. For me it is not clear yet where the lack of details in qualitative results stems from. Is it only due to occlusions? Could it be that the refinement step (3D convolution) smooths out a lot of details? Could this be addressed with another self-attention mechanism in the refinement step?\n",
            "This paper presents an RGB 3D reconstruction algorithm with monocular RGB input, using transformer network. The transformer architecture is utilized for temporal feature fusion from multi-view images. Coarse-to-fine hierarchical scheme was designed for efficient surface detection and real-time reconstruction. Also, attention weights from the transformer is used in order to select the most dominant (feature-abundant) frames so that more interactive scene reconstruction can be available. By doing so, the proposed algorithm shows comparable performance on the benchmark experiment, compared to CNN-based previous methods.  This paper introduced a way of successfully utilizing transformer architecture inspired from NLP to the 3D scene reconstruction problem in computer vision. It is well-written and easy to follow. Also, the paper ablates well on their design choices.\n\nThe followings are my concerns and questions.\n1. While I do think that utilizing transformer architecture in the 3D reconstruction field alone can be a meaningful contribution, the overall pipeline looks very similar to NeuralRecon [39], except for the substitution of GRU Fusion unit with the ViT-based transformer module. Please elaborate on the additional difference or stand-outs that the proposed algorithm has over [39]. It will be really helpful to reconsider my rating towards acceptance.\n\n2. In figure 2, it seems like the coarse features are extracted from the 4th layer of the image encoder, and the fine features are from the 1st. Assuming the basic architecture of the ResNet, the former will have lower spatial resolution but the features will be more deeply embedded, and vice-versa. I wonder if the channel size of the encoded feature can also be a factor in choosing whether it is more appropriate for coarse/fine pipeline. What would happen if the 1st layer feature is utilized in T_c(Coarse Transformer) and the 4th layer feature is used for T_f(Fine Transformer)?\n I appreciate the authors for mentioning the limitations of their algorithm on the lack of details in scene reconstruction results.\n\nAdditionally, considering the capture rate of the conventional RGB cameras, I do not think 7 FPS satisfies as \u2018online\u2019 or \u2018real-time\u2019. \n",
            " I thank the authors for their in-depth comparison between this paper and NeuralRecon, providing much more insights on the advantage of utilizing a transformer network.\n\nI see that all other reviewers are also pointing out the lack in technical novelty, especially comparing it to NeuralRecon.\nAlthough the overall pipeline is still similar to previous works, I believe that this paper can be credited for its analysis on what can we expect from using a transformer network in 3D reconstruction field.\n\nTherefore, I change my initial rating to more positive decision.",
            " Thank the authors for the reponse. After reading the authors' responce and other reviewer's comments, most of my concerns are well addressed. \n\nDetailes feedbacks are shown below:\n\n- **About significance**. Most of the reviews show their concerns to the incremental novelty of this submission which uses Transformer for temporal multi-view feature fusion in monocular reconstruction task.  I acknowledge the nolvelty of this idea but feel it less supportive to stand alone. \nAs mentioned by second paragraph in my previous 'Incremental contribution' comment  and Reviewer Ert2, the Transformer as well as other modules such as surface filtering, refinement, intiail attention weight for view selection are bundled together which leads to the final performance. The improved description of contribution and more investigation of applying these modules to other pipeline can be strengthened in the paper and future work.\n\n- **Data Generalization and Numerical Comparisons**.\nThank the authors to conduct extra experiments on TUM datasets and the results show that it works reasonbly well under variontion of camera intrinsics and capturing devices. The abaliation study of temporal fusion using CNNs is adequate to show the advantage of Transformer on this task.\n\n- **MLP**.\nAdd more discussion and exploration on its incapability to catch fine structures is quite meaningful. In addition to the MLP structure, will the pre-defined voxel resolution at coarse and fine scales largely affect the results?\n\nOverall, I think most of the concens are well addressed by the author response. Although I have some slight concern on the significance, I think this submission is interesting and can be good to the area on multi-view visual information fusion and reconsturction.\nTherefore, I personally tend to accept this submisison and keep my original positive score.",
            "This paper proposes to use vision Transformer in a coarse-to-fine manner as a new learned multi-view feature fusion scheme in monocular reconstruction task. Additional blocks such as spatial refinement and C2F filtering help the method generate coherent geometry and improve computational efficiency.\n\nCompared to existing approaches, the full TransformerFusion system shows competitive results on 3D reconstruction quality using ScanNet benchmark with detailed ablation study.  There are many hanging fruits of applying Transformer in vision tasks. In this paper, the idea of using Transformer to adpatively select which frames to contribute more is well motivated and technically sound. As vision transformer naturally fits the task of temporal fusion of sequential video image data. Both quantitative and qualitative results shows the effectiveness of proposed system.\n\nPros:\n - Paper is well written and the ablation experiments address several concerns of architecture design.\n - The introduction of transformer in multi-view feature fusion is well motivated.\n - The carefully designed pipeline reach competitive performance with reasonable computational cost and experiments are  well-designed.\n\n\n\nCons:\n- Incremental contribution.\n\n    - Given the general pipeline of TransformerFusion is similar to existing approaches such as NeuralRecon, the claimed two unique contributions of this paper come from the adoption of Transformer and C2F refinement to make Transformer practically applicable. In my understanding, these two contributions is in principle a single contribution, i.e., bringing Transformer into monocular reconstruction and make it applicable. The contribution is a bit weak and incremental from this point of view. \n \n    - As mentioned above, the designed modules look very tailored to Transformer-based fusion. To strengthen the contribution and its generalisation, though not the main focus of this paper, it would be good to see if proposed modules like C2F surface filtering is beneficial to other exisiting systems as well.\n\n- Data Generalisation\n\n    - Current evaluation is tested using ScanNet alone and a common concern remains on the generalisations of trained TransformerFusion to other unseen scenes like 7-scenes or real-world exemplar scenes. \n\n- Numerical Comparison\n\n    - From Table 1 and Figure 2 in supplementary material, the refinement modules shows the strongest influence to reconstruction performance, even larger than the contribution from Transformer itself. Given the noisy qualitative result without refinement, is it caused by the lack of inductive biases in MLP-based Transformer fusion?\n\n    - Based on previous point, in Table 1 author compares the ablation experiments without Transformer fusion but a plain MLP with learned or equal weights during fusion. For me it is more interesting and fair to see how convolutional-like fusion structure performs here. In addition. given inductive biases in convolutional structure, will the benefits from refinement module become less pronounced?\n\n    - More explanation of these factors would be appreciated and help better understand what is the pros and cons of using Transformer here. \n\nSome detailed comments and questions:\n\n(1) How to select frames in video at test time? Are all frame-rate frames fed into the network or a pre-sampled subset is input due to efficiency consideration?\n\n(2) An implicit MLP decoder is used to finally predict the occupancy values. However, as mentioned in the limitation,  \nfine structures are prone to missing which is a slightly against the advantages of using continuous MLP. Is this result related to the pre-defined voxel resolution in coarse and fine scale or due to the limited resolution of fused feature? \n\n(3) In section 3.1, a 3D point in world coordinate is projected to multiple camera to query 2D features, is depth information of each view/point used here during projection? Counterpart approaches such as Altas or NeuralRecon emit a ray traversing the whole 3D voxel as no depth information is given. Could the author clarify this?\n\n(4) How about using more than 16 frames as only fewer frames are tested. Is there a trend to saturate the performance given more views and redundancy or the performance consistently improves?\n\nOverall, I think this is a well-motivated paper and is well written, therefore tend to accpet this paper (between 6 and 7). The quality of this paper is technically sound, though the contribution is a bit incremental and weak in my point of view, considering that applying Transformer to this task is the key uniqueness. Transformer shows its merits in multi-view fusion but there also comes with other related issues such as lack of inductive bias, computational cost due to point-wise query. More in-depth analysis of the pros and cons of using transformer in mono recontruction is appreciated, especially compared with other learning-based fusion approach using convolutional structure.\n\n The authors have well addressed the potential negative societal impact of their work in the main paper."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review acknowledges the paper's clarity, plausible results, and thorough experiments, but expresses a major concern regarding the technical novelty of the method. It also raises a specific question about an inconsistency in the results, indicating a balanced perspective.",
            "The reviewer expresses overall positive sentiment by stating the work is 'valuable and interesting,' 'well thought through and complete,' 'clear and well organized,' and that the results are 'interesting' and 'valuable to the community.' They also agree to accept the paper post-rebuttal.",
            "The review acknowledges the paper's strengths (well-written, ablations), but also raises concerns about novelty and real-time performance, leading to a neutral overall sentiment.",
            "The reviewer expresses a change to a more positive decision, acknowledging the value of the analysis provided in the paper.",
            "The reviewer states that 'most of my concerns are well addressed' and that they 'personally tend to accept this submission and keep my original positive score'. This clearly indicates a positive sentiment.",
            "The reviewer states the paper is well-motivated and well-written, and they lean towards accepting it (between 6 and 7). They acknowledge the technical soundness and the merits of using Transformers for multi-view fusion."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses a mix of positive and critical language. It praises the paper's clarity and experimental results (\"The paper writing is clear and easy to follow. The results are plausible, and the experiments are thorough.\") but also expresses concerns about novelty and raises a specific question about the experimental results (\"My major concern is the technical novelty...\", \"However, they contribute quite differently to the final results according to the output of the transformer, which is confusing.\"). The suggestion to the authors indicates a constructive approach.",
            "The review provides both positive and negative feedback, acknowledging the paper's strengths (clarity, completeness, valuable results) while also pointing out limitations (incremental novelty, only slightly outperforms previous work). This balanced approach is evident in phrases like 'although not ground-breaking' and 'Besides the quantitative results being not overly impressive, I believe that the results provided are valuable to the community.'",
            "The reviewer uses phrases like \"my concerns and questions\" and directly challenges the paper's novelty by comparing it to existing work [39]. The reviewer also questions design choices and labels the frame rate as not 'real-time', indicating a critical perspective.",
            "The reviewer uses phrases like \"I thank the authors\" and \"I believe that this paper can be credited\" which indicates a supportive tone. The reviewer also explicitly states a change to a \"more positive decision.\"",
            "The reviewer uses phrases like 'Thank the authors', 'most of my concerns are well addressed', 'interesting and can be good to the area', and 'personally tend to accept this submission' which demonstrate a supportive tone.",
            "The review presents both positive aspects ('Paper is well written', 'well motivated', 'carefully designed pipeline reach competitive performance') and negative aspects ('Incremental contribution', 'Data Generalisation', 'contribution is a bit weak and incremental'). The reviewer also poses questions and requests clarifications, indicating a balanced assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the strengths of the paper, such as clear writing, plausible results, and thorough experiments, while also raising a valid concern about the technical novelty of the method. The reviewer's points are logically connected and do not contradict each other. The critique about novelty and the question about Fig.1 are presented as areas for improvement or further explanation, not as contradictions to the positive aspects mentioned.",
            "The review maintains a consistent stance throughout, acknowledging the paper's strengths in quality, clarity, and practical value while also being realistic about the incremental nature of the novelty and the quantitative improvements. The reviewer consistently justifies a positive evaluation and acceptance recommendation based on the overall contribution and potential impact, even after considering the rebuttal and addressing concerns.",
            "The review is consistent because the reviewer acknowledges the potential contribution of using transformers in 3D reconstruction and praises the paper's writing and ablation study. However, the reviewer raises valid concerns and questions regarding the novelty compared to existing methods, the rationale behind design choices (feature layer selection), and the claim of real-time performance. These points are logically connected and aim to seek clarification and improvement of the paper, rather than presenting contradictory statements.",
            "The review is consistent because it acknowledges the lack of technical novelty, a common concern among reviewers, but appreciates the paper's in-depth analysis of using transformer networks in 3D reconstruction. The reviewer's change to a more positive decision is justified by the value of this analysis, despite the similarity to prior work like NeuralRecon. The review weighs both the weaknesses and strengths and arrives at a consistent conclusion.",
            "The review is consistent because it starts by stating that most concerns are addressed and ends with a positive recommendation for acceptance. The detailed feedback points are presented as minor suggestions for improvement or acknowledgements of addressed concerns, rather than major criticisms that would contradict the overall positive sentiment. The reviewer explicitly states they 'tend to accept this submission' and maintains a positive tone throughout.",
            "The review is consistent because it acknowledges the strengths of the paper such as being well-written, well-motivated, and having competitive results, while also pointing out weaknesses like incremental contribution and limited generalization. The reviewer recommends acceptance (between 6 and 7) despite the weaknesses, indicating a balanced and consistent evaluation."
        ]
    },
    {
        "paper_id": "nips_2021_P-if5sUWBn",
        "paper_title": "Deformable Butterfly: A Highly Structured and Sparse Linear Transform",
        "paper_abstract": "We introduce a new kind of linear transform named Deformable Butterfly (DeBut) that generalizes the conventional butterfly matrices and can be adapted to various input-output dimensions. It inherits the fine-to-coarse-grained learnable hierarchy of traditional butterflies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network compression. We apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity-accuracy tradeoff arising from the myriad deformations of a DeBut layer also opens up new rooms for analytical and practical research. The codes and Appendix are publicly available at: https://github.com/ruilin0212/DeBut.\n",
        "review_ids": [
            "IlLLtvbBxy",
            "SJznt-bBj-S",
            "3S_J8znhfwZ",
            "68kIMcqliwy",
            "0ox8CpUpC5R"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper proposes \"Deformable Butterfly\" (DeBUT) a generalization of butterfly transforms (the linear operator behind the FFT), which can support non power-of-to inputs/outputs. \nIt studies how to plug such transforms into various architectures (LeNet, VGG, ResNet), from the perspective of filter design, initialization, parameter savings and accuracy loss.\nOverall it is demonstrated that DeBut can achieve a favourable tradeoff compared to previous butterfly-style approaches such as \"Buterfly\" [5,6] and \"Adaptive Fastfood\" [35].\n  Overall the paper is very well written, with clear figures demonstrating the concepts of interests. The\npresented generalization of butterfly matrices seems very natural, and addresses the following challenges:\n\n- How to ensure that there are no \"dead parts\" in the chain of linear transforms.\n- How to go increase or decrease the number of channels (or doing both in so called \"bulging\")\n- How to initialize the transforms with an alternative least squares (ALS) method\n- How to fine tune the resulting network\n\nThe experiments are relatively extensive, and the results in Tab 4.-6. are favorable to the proposed method.\n\nMy main concerns are the following:\n\n- The relationship between the generalized butterfly transform and generalized FFT.\n It is well known the FFT (e.g. the Cooley\u2013Tukey FFT algorithm) can recurse over any factorization of the input size, not just powers of two (POT). Is there a relationship between this and the proposed DeBUT transforms?\n- The design of the transforms seems rather complicated. It would be more convincing if the authors could present a simple rule on how to transform layers.\n- Related: there is a constraint p/r = q/s that needs to be satisfied. This implies the channels need to be prime factorizable if you want to transform them. I don't see any discussion about this.\n\n- Inference times are presented in Tab. 6, without any details. Is this exploiting the structure of the transform? Why is the baseline (original network) missing from the benchmark? \n\n- Does not seem to work without ALS initialization for large networks? I did not find the results for this, although they were said to be found in Table A3. in the appendix.\n\n- in l.104 it is said DeBut \"replaces\" the circulant structure of CNN convolution. This sounds wrong to me, as the doubly circulant structure emerges from the same matrix multiplication being applied to to each patch extracted from the im2col operation. DeBut is also doing this, so the circulant structure remains (you could say with a nested butterfly structure).\n\n\nFinal update: Given the rebuttal and the other positive reviews, I am keeping my score.\n None",
            " Thank you to the authors for their responses.\n\nI remain unconvinced regarding the contributions of this paper. As I stated previously, the changes over the standard Butterfly technique aren\u2019t particularly large and the empirical results don\u2019t seem to show much of an advantage over the baselines.\n\nI\u2019m skeptical that the comparisons of training and inference times in Table 6 are accurate given the fact that Adaptive FastFood is running in sklearn on CPU (sklearn does not provide GPU support). Given the butterfly methods are using a GPU I think the comparison is misleading, especially given Adaptive FastFood shows better theoretical compression for a given accuracy in Table 5.\n",
            "In this work, the authors propose a new way named deformable butterfly (DeBut) to represent the linear transformation between feature layers in a deep neural network. It is based on a chain of butterfly factor matrices that can be used together to form a densified matrix for linear transformation. The experiments show the proposed method can reduce the model size while maintaining similar test accuracy.  Strengths:\n- Compared to the prior work [5,6], there is an extension from square matrix to non-square matrix for linear transformation.\n- The runtime complexity can be lower than O(nlogn).\n- The number of parameters can be reduced.\n- The idea of DeBut looks quite novel to me.\n- The paper is well-written and the complicated idea is well illustrated with figures and explanations.\n\nWeaknesses:\n- It seems that the accuracy may drop a little bit with DeBut, although the model size is reduced. In Table 4, the Top-1 accuracy is lower than ResNet-50.\n- Is there any example to demonstrate the difference DeBut-mono and DeBut-bulging? I am confused by these two designs. Why the authors propose these two designs and what are the pros and cons?\n- Are the results shown in the experiments by training a model with DeBut from scratch (or fine-tuning) or approximating a pre-trained model with DeBut?\n- Why DeBut does not improve the model accuracy, compared to Conv and FC?\n- It seems the model with DeBut runs slower than the baseline (e.g., ResNet50)?\n\n\n\n The limitations should be better explained, and I do not see negative societal impact.",
            "The paper present a generalization to butterfly matrices used for FC layer compression that allows flexibility in the dimensionality of the map. This removes the power-of-two limitation. Using that, the authors compress not only FC layers but also conv layers. In addition, the DeBut chain can be made flexible and serve as an addition design parameter. The results confirm that expressivity remains high under considerable compression ratios with better inference rates than competing approaches.   Strengths:\n* I wish to compliment the authors on a very well written paper. It was clear and pleasant to follow. \n* The proposed method is elegant and practical. The experimental section shown convincing results on standard benchmarks. \n* The hierarchical structure imposed by the DeBut chain is interesting to investigate, and the different chains that can be formed allow a nice flexibility to the model design with possible trade-offs between expressivity and model size. \n\nWeaknesses:\n* I would like to see a more principled comparison of different chains and their effect on performance.\n* when applied to convolutional layers the hierarchy effectively imposes a spatial structure to grouping neighboring pixels. Could you show those patterns? would it make sense to shuffle the column vectors resulting from im2col to control these patterns?\n* In the context of compression, many other methods exist but they're not shown in the paper.  limitations are not discussed in depth",
            "The authors propose \u201cdeformable butterfly\u201d, a variant of the butterfly transform that does not require power-of-two size butterfly factor matrices. The authors show how to adapt pre-trained models to use DeBut in place of convolution and fully-connected layers and also show experiments training models with DeBut chains from scratch. The authors provide experiments on a range of CNN architectures and datasets and compare to existing transforms like Adaptive Fastfood and Butterfly.  The paper is well written and organized. I found the work very interesting, but my main concerns are novelty and unique contribution over existing work. As I understand it, the primary contribution in methodology is that DeBut breaks the power-of-two (POT) constraint required in standard Butterfly matrices. This is certainly practically useful, but I don\u2019t think this change is novel enough to merit publication for this contribution alone. Additional contributions like empirical gains over Butterfly matrices would\u2019ve helped, but the advantage over Butterfly matrices seems to be marginal. Both techniques still do not appear to be simple substitutes for existing layers without accuracy loss. For example, 0.65% accuracy drop (almost 2x increase in error) training from scratch when replacing 3 out of 5 layers in LeNet and 1.5% decrease in top-1 accuracy using ALS with ResNet-50 with DeBut applied to only 9 layers in the model.\n\nComments\n1. Some of the language in this work is very sensational. For example, \u201crenders a new paradigm for network compression\u201d.\n2. In practice, convolution is rarely implemented with im2col + GEMM because of the memory bandwidth cost and memory footprint of the im2col. Modern high-performance convolutions fuse the im2col + GEMM into a single kernel with clever indexing (called an \u201cimplicit-GEMM\u201d convolution, as opposed to \u201cexplicit-GEMM\u201d where im2col is used).\n3. The authors should include the baseline accuracy in their result tables to make it easier for the reader.\n4. I don\u2019t see how DeBut \u201cunif[ies] and homogenize[s]\u201d convolution and FC layers more than they already are. DeBut using an im2col transform and then approximating the GEMM portion of the convolution is just exploiting what was already known.\n5. The statement \u201cwhere other fast linear transform schemes fail\u201d is an exaggeration. The empirical gains over Butterfly appear to be limited and Adaptive FastFood produces much better theoretical efficiency (although it appears to be expensive in practice).\n6. I\u2019d like to see more details on the implementations benchmarked in Table 6. Given all techniques listed are not mainstream I doubt the implementations are highly optimized and I\u2019d like to understand how much this contributes to the performance discrepancies. For example, I\u2019m not sure why DeBut would be much faster than Butterfly given they are essentially the same computation. I did not identify potential negative societal impacts in this work."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Neutral",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review acknowledges the paper is well-written, the generalization of butterfly matrices is natural, and the experiments show favorable results. The reviewer also states they are keeping their score after the rebuttal and other positive reviews.",
            "The reviewer expresses being 'unconvinced' about the paper's contributions and remains 'skeptical' about the accuracy of the results. These are clear indicators of a negative sentiment.",
            "The review presents both strengths and weaknesses of the paper, indicating a balanced and objective assessment. There is no overwhelmingly positive or negative language.",
            "The review expresses overall positive feedback, highlighting the paper's clarity, the elegance and practicality of the proposed method, and the interesting hierarchical structure. The reviewer also compliments the authors on the well-written paper and acknowledges the convincing experimental results.",
            "The review expresses concerns about the novelty and unique contribution of the work, stating that the primary contribution is not novel enough to merit publication. It also points out accuracy drops when using DeBut and criticizes the language as sensational and exaggerating claims."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Balanced",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "While the review expresses positive aspects such as the paper being well-written and demonstrating favorable results, it also raises several concerns and questions. This balance of positive feedback and critical questioning suggests a balanced tone.",
            "The review uses phrases like 'I remain unconvinced,' 'aren\u2019t particularly large,' 'don\u2019t seem to show much of an advantage,' and 'I\u2019m skeptical' which indicate a critical tone. The reviewer also questions the validity of the experimental setup.",
            "The review uses a mix of positive and critical feedback, presenting both strengths ('The paper is well-written', 'The idea of DeBut looks quite novel') and weaknesses ('It seems that the accuracy may drop a little bit', 'I am confused by these two designs'). The tone is objective and constructive.",
            "The reviewer uses phrases like \"I wish to compliment the authors,\" \"very well written paper,\" \"clear and pleasant to follow,\" \"elegant and practical,\" and \"interesting to investigate.\" These phrases indicate a supportive and encouraging tone. While the review also contains criticisms, the overall sentiment is positive and encouraging.",
            "The reviewer uses phrases like \"I don\u2019t think this change is novel enough,\" \"accuracy drop,\" \"language in this work is very sensational,\" \"statement is an exaggeration,\" and \"I doubt the implementations are highly optimized.\" These phrases indicate a critical assessment of the paper's contributions and claims."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the strengths of the paper (well-written, natural generalization, favorable results) while also raising specific and valid concerns (relationship to FFT, design complexity, constraints, missing details on inference time, dependence on ALS initialization, and a point about circulant structure). The reviewer maintains a balanced perspective by highlighting both positive and negative aspects without contradicting themselves. The final statement of keeping the score further reinforces consistency, suggesting a stable evaluation despite the raised concerns.",
            "The review is consistent because the reviewer maintains their negative stance on the paper's contributions, reiterating previous concerns about the limited novelty compared to the Butterfly technique and the lack of empirical advantage. The skepticism about Table 6 further supports this consistent negative assessment by questioning the validity of the comparisons presented.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper without any self-contradiction. The reviewer appreciates the novelty and clarity of the approach while also raising valid concerns about the accuracy drop, lack of clarity on certain design choices, and performance compared to baselines. These points are all valid and contribute to a balanced and constructive review.",
            "The review is consistent because it highlights both the strengths of the paper (well-written, elegant, practical, convincing results) and areas for improvement (need for more principled comparison of chains, analysis of spatial structure in conv layers, and discussion of limitations and comparison with other compression methods). The reviewer provides constructive criticism without contradicting the positive aspects of the paper.",
            "The review is consistent because the reviewer's comments and criticisms are all aligned with the central argument that the paper lacks novelty and sufficient empirical gains to justify publication based on the claimed novelty. The reviewer acknowledges the paper's strengths (well-written, interesting) but consistently emphasizes the weaknesses related to novelty and empirical results throughout the review. The specific comments further support this central argument by pointing out exaggerations, lack of clarity, and areas for improvement, all contributing to the overall assessment of limited novelty and impact."
        ]
    },
    {
        "paper_id": "iclr_2021_Eql5b1_hTE4",
        "paper_title": "Robust early-learning: Hindering the memorization of noisy labels",
        "paper_abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.",
        "review_ids": [
            "VIojxqDxeMz",
            "i_fZV9ttgbw",
            "wtHgi5ZzzX",
            "5OuwXvLJRUD"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes a method for deep learning with noisy labels, which distinguishes the critical parameters and non-critical parameters for fitting clean labels and updates them by different rules. The method is easy to implement and the empirical results are promising. Experiments on both simulated and real-world datasets show it reaches new state-of-the-arts results.\n\nQuestions:\n- I don\u2019t think the ablation is convincing \u2013 why does the proportion of non-critical parameters is assumed to be the same as the noise rate? The ablation only shows the method is insensitive when the estimation of the noise rate is not precise. However, why do we need to estimate the noise rate? What if the proportion of non-critical parameters is assumed to be a constant number? In other words, what will the performance be like if the estimation is largely different from the real noise rate? \n- Since the validation set is also noisy, why does the early stopping criterion adopts the minimum classification error on it? ",
            "------Overall------\nThis paper utilizes the memorization effects of deep models and aims to improve their robustness to noisy labels before early stopping. As the deep models fit training data with clean labels in the early stage of training, the authors propose a novel method to identify those more important parameters for fitting clean labels. They then deactivate the unimportant parameters to reduce side effects brought by noisy labels, which enhances the fitting to clean labels implicitly. I think this paper is interesting and makes sense. The major comments and issues are as follows: \n\n------Major comments------\n1. Different from other complex methods for learning with noisy labels , this work discusses that standard cross entropy loss can achieve competitive performance with early stopping. We can therefore focus the training stage before early stopping to handle noisy labels. The authors skillfully allow the optimality to be checked by a scalar, and then judge the importance of the parameters by analyzing the influence of the parameters on this scalar. The idea of this paper is novelty and meaningful. \n\n2. The paper is very well-written. The description of its motivation and technical details is clear and flows smoothly, which makes it easy for readers to understand the core idea of this paper and follow its implementation details.\n\n3. The experimental results are convincing. The authors provide a very detailed description of experimental settings. Besides, this paper exploits multiple methods for comparison and considers various noise settings to verify the effectiveness of the proposed method. The experimental results on synthetic and real-world datasets are convincing. The authors also perform an ablation study to present the proposed method is insensitive to the estimation of noise rate. \n\n------Issues------\n1. I only find the illustration of comparison between CE and CDR in the case of noisy CIFAR-100. This paper aims to reduce the side effect of noisy labels before early stopping, thus CE is an importance baseline in this paper. Can the authors add illustrations of the experimental results in other cases like Figure.2? \n\n2. The baselines and experimental results are sufficient. Could the authors add some introduction for the baselines and more detailed discussion for experimental results. \n\n3. The authors may need add some explanation for Eq.(2) and Eq.(5). The proposed method makes use of the memorization effects of deep models. The authors directly write \\tilde{S} rather than S in Eq.(5). However, this may be easy to misunderstand. I suggest that the authors can emphasize it or change it. \n\n4. Some typos need to be corrected. (1) \u201cThe underlying issue of directly using the gradient of......\u201d; (2) \u201cRobust positive update uses the gradients to update the critical ones......\u201d.\n\n5. Some minor comments. (1) The experimental results in Table 1 are too dense. (2) The figures are not readable, especially the title is small for me. This makes it a little hard to match the figures with specific cases. (3) The parameter (noise rate) $\\tau$ still needs to be estimated, which may be challenging. It will be promising to automatically set this parameter during training. Thus, a more in-depth analysis is worthy of further learning. \n\nI hope the authors can address these issues carefully to improve this work. ",
            "This paper aims to exploit the early stopping method to solve the problem of learning with noisy labels. Specifically, this paper finds that only partial parameters (critical parameters) are important for fitting clean labels and generalize well; while the other parameters (non-critical parameters) tend to fit noisy labels and cannot generalize well. Based on this observation, this paper proposes to divide all parameters into the critical parameters and non-critical ones, and perform different update rules for the two types of parameters, in each iteration. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the effectiveness of the proposed method.\n\nThis paper has the following advantages:\n1.\tThis paper is well-written and the motivation is very clear. This paper has clearly explained the two types of parameters, i.e., critical parameters and non-critical parameters.\n2.\tThis paper proposes a novel method with an interesting idea. Unlike many existing methods that are aggregations of multiple techniques, this paper only focuses on one concept. It is simple but effective. The view of updating different parameters by different rules is quite novel for learning with noisy labels. I think this view may bring some new insights to the area of learning with noisy labels.\n3.\tExperiments are quite thorough and results on both synthetic and real-world datasets validate the effectiveness of the proposed method.\n\nThis paper also has some minor issues:\n1.\tI would suggest the authors to carefully check the notations used in the paper. For example, $\\mathbf{w}$ denotes all the learnable parameters of the model in the paper, while $\\mathbf{w}$ usually means a vector. So I would suggest the authors to use another symbol to denote the set of all the learnable parameters of the model, e.g., $\\mathcal{W}$.\n2.\tIt is not well justified why this paper chooses the $\\ell_1$ regularizer. Is there any consideration to use the $\\ell_1$ regularizer except that could be associated with weight decay?\n3.\tThe hyper-parameter $\\tau$ (noise rate) needs to be known. It may not be a big issue as the ablation study in this paper demonstrates that the proposed method is insensitive to this hyper-parameter.\n\nOverall, I think this is a good paper with an interesting idea and convincing experimental performance. So I prefer to accept it.\n",
            "This paper tackles the problem of learning with noisy labels and proposes a novel method CDR which is inspired by the lottery ticket hypothesis. In particular, the proposed method categorizes the parameters into two parts, including critical parameters and non-critical parameters, and applies different update rules to these parameters. Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels.\n\nPros.\n1. The proposed method is interesting in its design. The authors provide an alternative interpretation for the optimality criterion, which reveals that the value of the parameter can also be used to check the optimality. It is novel and of significance. Meanwhile, the proposed method is easy to implement. This method can also be applied to existing algorithms to further improve their robustness.\n2. Overall, this paper is very well written and well organized. The technical details are easy to follow, and Algorithm 1 helps understand the procedures. \n3. Extensive experiments are performed to verify that the proposed method indeed helps over the baselines at fighting noisy labels. I like the details of experimental settings, which really can help reproduce these experimental results.\n\nCons.\n1. The new interpretation for the optimality criterion is important, but lacks detailed explanation. I find that there is only simple analysis. Perhaps the authors could add some details or citations for better understanding.\n2. Though the baseline S2E uses AutoML, it seems that S2E is an improvement on co-teaching or co-teaching+? It is not suitable to place it at the end. \n3. For some baselines such GCE and Joint, there are hyperparameters to consider. The authors should explain how to set their values for a fair comparison. For APL, there are multiple combinations of loss functions. In this experiment, which one did you choose? I suggest that the authors add such explanation in Section 4.2, which will make the results more convincing. \n4. The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. After early stopping, noisy labels still affect the performance. How to reduce the side effect during the whole training by using this idea? I personally think this is an interesting and meaningful direction. The authors can regard this as future work to improve this paper. \n\nOverall, I think this paper makes sense in learning with noisy labels. The proposed method is novel and effective. I recommend to accept this paper, and hope that the authors can address the above issues carefully."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the method's ease of implementation, promising empirical results, and state-of-the-art performance. These positive aspects outweigh the concerns raised in the questions.",
            "The review expresses overall positive feedback, stating the paper is \"interesting and makes sense\" and praising the novelty of the idea, the writing quality, and the convincing experimental results.",
            "The reviewer states \"Overall, I think this is a good paper with an interesting idea and convincing experimental performance. So I prefer to accept it.\"",
            "The review expresses a generally positive opinion of the paper, highlighting its novelty, significance, clarity, and experimental validation. The reviewer concludes with a recommendation to accept the paper."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The review starts with positive feedback, highlighting the method's strengths. However, it then transitions into critical questions about the ablation study and validation set usage, indicating a balanced perspective that acknowledges both the merits and potential weaknesses of the paper. The use of phrases like 'I don't think the ablation is convincing' shows a critical stance, but the initial positive remarks prevent the review from being purely critical.",
            "The tone is supportive, offering constructive criticism and suggestions for improvement rather than outright rejection. The reviewer uses phrases like \"I think this paper is interesting\", \"The paper is very well-written\", \"The experimental results are convincing\", and \"I hope the authors can address these issues carefully to improve this work.\"",
            "The reviewer uses positive language such as \"well-written\", \"very clear\", \"novel method\", \"interesting idea\", \"simple but effective\", \"quite novel\", \"thorough\", \"convincing experimental performance\", and \"good paper\". They also express a preference to accept the paper.",
            "The reviewer uses positive language such as \"interesting in its design,\" \"very well written,\" \"easy to follow,\" \"I like the details,\" \"novel and effective,\" and \"I recommend to accept this paper.\" While providing constructive criticism, the overall tone is encouraging and supportive of the authors' work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review raises valid concerns about the ablation study and the early stopping criterion, without contradicting itself.",
            "The review provides a generally positive assessment of the paper, highlighting its novelty, clarity, and convincing experimental results. The issues raised are specific and constructive, focusing on areas for improvement in presentation, explanation, and further analysis. There are no apparent contradictions within the review.",
            "The review consistently praises the paper's clarity, novelty, and experimental validation while also pointing out minor issues like notation and hyperparameter justification. The overall recommendation to accept aligns with the positive assessment of the paper's strengths.",
            "The review provides both positive and negative feedback on the paper, with suggestions for improvement. The reviewer ultimately recommends acceptance, indicating an overall positive assessment despite the identified weaknesses. There are no self-contradictory statements."
        ]
    },
    {
        "paper_id": "iclr_2020_HJgS7p4FPH",
        "paper_title": "Accelerating Reinforcement Learning Through GPU Atari Emulation",
        "paper_abstract": "We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms.  CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at [hidden URL].",
        "review_ids": [
            "ryl7uG2atH",
            "ByxxYQ73FS",
            "rJgL80qkqr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The work contributes a library emulating Atari games in GPU in parallel and allowing to speed-up the execution of reinforcement learning algorithms.\n\nI see that the paper qualifies to the conference; in particular there is listed the topic:\n\n- \u201cimplementation issues, parallelization, software platforms, hardware\u201d\n\nHowever, this is not a research paper, and I do not really see how I should asses it. What I can say about it is that it is considerable amount of work, not only implementing the simulator but also looking at what RL methods need, and how to optimize the allocation and exchange of the data so that everything would work on GPU more efficiently.\n\nFrom the practical perspective, I am somewhat confused. The speed-up factors in the experiments are rather modest: about 4x for simulating and rendering frames, 2.5x for full RL, on a single GPU. Better with scaling to multi-GPU systems. In Table 1 the total training time per resources used differs dramatically. However if I look at the lines with A2C it is about the same time with 100-200 CPU cores + 1 GPU versus 12 cores + 1 GPU. So this is about factor 10 in the resources, versus CPU parallelization probably suffering overheads.\n\nIt appears that the maximum steed-ups are achieved for a particular type of the reinforcement learning algorithms, and using it in a general case would give a modest improvement.\n\nThe paper itself consists of introduction, related work, 1 page overview of what it means to simulate the Atari games, and experiments. So it is mostly about measuring the speedups, with several implementations / platforms.\n\nI tend to think that this work will not very much boost the research for new RL methods. It is limited to Atari games, mostly helps to sample-inefficient RL methods and if it helps, the speed-up factors are not of the order that would make experiments by the researchers otherwise impossible. \n\nI would also give priority to theoretical contributions at ICLR. In the end, we all are using CUDA and cnDNN, but presentations about how they implement things are rather given at GPU computing conferences. \n\n\n",
            "This paper introduces a CUDA port of the Atari Learning Environment. The paper goes into detail examining the benefits that come from a GPU-only implementation, including much better per-GPU utilization as well as no need to run a distributed system of CPUs. They show this hardware scaling can be taken advantage of across a variety of state of the art reinforcement learning algorithms and indeed create new batching strategies to utilize their framework and the GPU better. \n\nThe paper is well written and goes into some detail describing the implementation of CuLE as well as various design decisions taken, as with splitting the emulation process across several kernels. Finally, the paper is very explicit about a number of optimizations that are not being exploited by the new framework and serve as markers for future work.\n\nA question that arises and which is not addressed in the experiments is how the authors verified their port is faithful to the original version; there is no mention of correctness in the paper.",
            "This paper describes a port of the Atari Learning Environment to CUDA, reports on a set of performance comparison, and provides a bottleneck analysis based communication bandwidth and various throughputs required to saturate them for training and inference.\n\nMy first reaction to this paper was, \"So what?\"; but as I read more, I like the paper more and more.  It was the bottleneck analysis that changed my mind.  It was done very thoroughly and it provides deep insight in the challenges that RL faces for both learning and inference in a variety of settings.  I especially liked the analysis of the advantages and limitations of GPU emulation.  I also thought the Discussion section was well written.\n\nThe paper would be better if:\n1) The figure fonts were larger throughout the paper.\n2) The gaps in Table 1 were explained.\n\nMinor issue:  Change \"feed\" to \"fed\" on page 3.\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses doubts about the paper's contribution to the broader RL research field, stating that it \"will not very much boost the research for new RL methods\" and that the speed-up factors are not significant enough. They also question its suitability for a theoretical conference like ICLR.",
            "The review highlights several positive aspects of the paper, such as its clear writing, detailed explanation of the implementation and design decisions, and explicit acknowledgment of potential future improvements. While it raises a concern about correctness verification, the overall tone is appreciative of the work presented.",
            "The reviewer initially had a negative reaction but states they liked the paper more and more as they read it. Positive aspects mentioned include the thorough bottleneck analysis, deep insight into RL challenges, the analysis of GPU emulation, and the well-written discussion section. Although there are some suggestions for improvement, the overall sentiment is positive."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like \"rather modest,\" \"confused,\" and \"I tend to think that this work will not very much boost the research.\" The reviewer also draws a parallel to GPU computing conferences, suggesting the paper is more suitable there than at ICLR, implying a lack of theoretical depth.",
            "The reviewer uses positive language like \"well written,\" \"goes into some detail,\" and \"very explicit.\" The review emphasizes the benefits of the implementation and acknowledges future research directions, indicating a supportive stance despite raising a concern about correctness.",
            "The review starts with an initial negative reaction (\"So what?\") but transitions to a more positive and appreciative tone. It offers both praise and constructive criticism, suggesting improvements while highlighting the strengths of the paper. This mix of positive and negative feedback indicates a balanced tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently expresses concerns about the limited speedup achieved and questions the paper's contribution to advancing RL research, particularly in comparison to theoretical contributions typically prioritized at ICLR.",
            "The review consistently praises the paper's clarity, detail, and exploration of optimizations, while also pointing out a significant omission regarding verification of the port's correctness. There are no self-contradictory statements.",
            "The review expresses a positive overall assessment, highlighting the value of the bottleneck analysis and the discussion section. The suggestions for improvement (figure fonts, table gaps, minor typo) are consistent with a generally positive evaluation."
        ]
    },
    {
        "paper_id": "nips_2021_4YlE2huxEsl",
        "paper_title": "Beltrami Flow and Neural Diffusion on Graphs",
        "paper_abstract": "We propose a novel class of graph neural networks based on the discretized Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with  positional encodings derived from the graph topology and jointly evolved by the Beltrami flow,  producing simultaneously continuous feature learning, topology evolution. The resulting model generalizes many popular graph neural networks and achieves state-of-the-art results on several benchmarks. \n",
        "review_ids": [
            "xi87pAv-d3a",
            "nzFEt_fpvs",
            "gGbiY7Brgg1",
            "eKuVL_mApqy",
            "VNUTow_K2MR",
            "tCBAXwDxRn",
            "ZOYVmm-qoMe",
            "DMhUbyNBi38",
            "aOHLN5rtAzr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper proposes a new graph neural network based on a continuous time flow. Each node is given a pre-processed position from an off-line node embedding method, then both the position and a vector of node features (together called Z) are changed during the flow. The flow changes Z by the average difference of Z of adjacent nodes, weighted by a scaled dot product attention. Additionally, a node-wise initial encoder and final decoder is used. The flow is computed via explicit adaptive or fixed integration and the backward pass via the adjoint method. The authors propose various extensions, including having dynamic adjacency via kNN and using hyperbolic positional space. The authors claim their method is motivated as a discrete analogue of a Beltrami flow.  ### Strengths:\n- The method performs well on smaller data sets with a small number of parameters.\n- The method explores some interesting new ideas for graph neural networks.\n\n### Weaknesses:\n- I am not quite convinced by the motivation of the proposed method as a discrete analogue of the continuous Beltrami flow. The \u201cstructural assumptions on the diffusivity\u201d $a$ seem to not be satisfied scaled dot product attention in BLEND. What is the point of all the theoretical motivation if the actual construction violates the assumptions of the theory?\n- Currently, I find it unclear which aspect of the proposed method makes it perform well. Is it the position that is added as a pre-processing step, the continuousness of the flow, the particular integrator, or the particular flow equation (6)? The ablation analysis in the appendix only partially answers this question. Further experiments could include: augmenting GAT with positional encodings; using BLEND with Euler steps; using GAT with continuous integration.\n- The method does not seem to get state-of-the-art results on larger sized data sets. The GAT baseline uses more parameters, but would BLEND improve if it used as many? I expect that BLEND has much higher training and inference time than GAT, even with the smaller model, because of the continuous integration. Concrete run-times are not given, so I can\u2019t say for sure.\n\n### Further suggestions for improvement:\n- I\u2019d be very interested in the performance of including channel mixing in the flow, referred to as Onsager diffusion, as currently, the fact that the channels only interact via the attention seems limiting. The same holds for time-dependent diffusivity.\n- Include (some idea of) the structural assumptions in thm 1 in the main paper.\n- Include results on BLEND-kNN on ogb-arxiv or explain why this result is missing.\n- Include some actual run-times of BLEND(-kNN) vs other methods.\n- In the appendix clarify why (9, suppl mat) is the obvious discrete analogue of (6, suppl mat). I see some notational similarity between (10, suppl mat) and (6, suppl mat), but that looks rather superficial, besides it being one possible generalization of the classic Dirichlet energy. In fact, the paper doesn\u2019t seem to be using this additional generality and only shows the classic example in which $\\tilde\\psi$ is constant. What does this additional $\\psi$ generality add?\n- Could the authors clarify the step from (8, suppl mat) to (1)? Where does the time come from?\n\n### Typos:\n-\tEqn 4, $x(x,0)$ should be $x(v, 0)$\n-\tIn Table 3, the score for GCN on CiteSeer is bold.\n-\tThe colouring in Table 3 seems incorrect. The BLEND-kNN performs on par with CGNN.\n-\tThe eqn under line 48 in the suppl mat would be clearer if parenthesis would be added to indicate that the partial derivative only applies to Z.\n-\tLine 153, missing reference\n\n\n### Conclusion:\n- Originality: The work is original. It tries to connect differential geometry to continuous flows in graphs in a way I hadn\u2019t seen before.\n- Quality: I have doubts about the correctness, as I question whether the presented theory applies to the proposed model. Additionally, important ablations are missing.\n- Clarity: The paper is clearly written.\n- Significance: The paper can be significant to all people researching graph neural nets and open exploration into continuous flows on this domain.\n- Score: 5, marginally below acceptance threshold. If the authors can convince me the theory does apply to their model, I will increase my score.\n- Confidence: 4. I read the paper in detail.\n The fact that their theory does not seem to be applicable to the used model, is not honestly mentioned in the limitations. To the contrary, the vagueness of unspecified 'structural assumptions', that are only given in the appendix, makes this theoretical limitation hard to find.\n\nI think the authors underestimate the current use of graph neural networks in industry. They are used widely. As such, some more elaboration on potential negative societal impact of graph neural networks in general could be given.",
            " I'd like to thank the authors for their additional baselines and theoretical clarification. I've increased my score to 7.",
            " Thank you for your extensive comments to my questions. I think I agree that your motivation of (5) in the above reply is reasonable and I think it would be good if these comments would be also stated in the final version of the paper. In particular that (5) has been generalized to imitate attention.",
            "I have read the authors' responses, comments from the other reviewers, and the discussion here. The authors have answered my questions satisfactorily. Hence, I am increasing my score to 7.\n\n\nThis paper derives a new class of GNN using ideas from differential geometry. It proposes discretization of the PDE associated with the Beltrami flow for the design of the GNN. Although the explicit discrete approximation of the Beltrami flow on a graph has many interesting similarities to existing attention mechanisms, this framework can be more general with the use of sophisticated differential equation solvers, as presented by the authors. The paper also brings out interesting connections between their model and existing GNN architectures, which in itself, is a nice contribution of this work.\n\n  The paper is generally well written except for some typos (possibly), for example,\n\n1) In eq. 4, should there be a $(4\\pi t)^{-d/2}$ in the RHS? Without it, if $\\alpha \\rightarrow 0$, eq.4 does not become the convolution in line 74.\n\n2) In the same equation, should $x(\\mathbf{x}, 0) \\rightarrow x(v, 0)$, and $x(u), x(v) \\rightarrow x(u, 0), x(v, 0)$?\n\n3) In eq. 6, should $U(0)= \\alpha U$ be $U(0)= U$ based on the description in section 2? \n\n4) In section 4, should $Y = \\psi(Z(T))$ be $Y = \\zeta(Z(T))$ based on the subsequent discussion?\n\n5) In eq. 8, $Q^{(k)} \\neq A^{(k)} - I$, it's actually $Q^{(k)} = \\tau A^{(k)} - (\\tau -1) I$. However, the definition of $q_{ij}^{(k)}$ is correct.\n\n6) In line 153, \\ref to a section is not working.\n\nOther questions/ comments:\n\n1) Is $\\alpha$ learned or treated as a hyperparameter? If it is learned, how is the criterion $\\alpha \\geq 0$ satisfied? If it is a hyperparameter, what is its effect on the classification accuracy?\n\n2) Can the authors compare the explicit scheme in sec. 3.2 with their implementation using numerical solver? This will allow the readers to understand the actual source of the performance improvements.\n\n3) Can the authors discuss the novelty in this work in comparison to existing ODE-based GNN models (reference 69, 96, 100). I appreciate that they conduct experimental comparisons with these related works, but some comments about the methodology as well would be welcome.\n\n4) In the supplementary material, the authors discuss several choices for the diffusivity function modeling but do not report any experimental comparisons. \n\nOverall, my impression is that this work has some novelty in terms of methodology and strong numerical results. But the writing could be significantly improved and the current version is not entirely satisfactory to be published. The authors discuss the limitations and societal impacts adequately.",
            " Thank you for your clarification. I now better understand that there are two points of view and that you claim that (5) is a graph discretisation of the Laplace-Beltrami operator in (2) for an arbitrary metric tensor G - which we'll let arbitrarily depend on the value of the field. I think it'd be good to add to the paper explicitly as a limitation of your theory that the theorem and the SM does not apply to your BLEND flow.\n\nStill, then I don't quite understand why (5) is reasonable. As the determinant of the metric is a scalar field, why is $a$ a function of both $z_i$ and $z_j$? The approach taken by [1, eqn 1.3] does depend on a node-wise weight, not an arbitrary edge-wise weight. Can you comment on the difference between your discretisation of the Laplace-Beltrami operator and the one in [1]?\n\n[1] Burago, Dmitri, Sergei Ivanov, and Yaroslav Kurylev. \"A graph discretization of the Laplace\u2013Beltrami operator.\" Journal of Spectral Theory 4.4 (2015): 675-714.",
            "** score updated 6->7 after rebuttal**\n\nThe paper proposes an interpretation of various GNN related architectures as Beltrami flows on a 2-manifold of position and feature embeddings, using this perspective to formulate a GNN architecture which uses feature encoders to create starting points in the 2-manifold and learned attention kernels (in their experiments, shared across time/layers) to give the update of the embeddings. The authors propose that this unifies various architectures as well as graph-rewiring techniques developed in the literature, evalaute against GCN,GAT,GAT-ppr,MoNet,GS-{mean/maxpool},CGNN and G(O)DE on cora,citeseer,pubmed,coauthor-CS,computer,photo and ogb-arxiv, showing their method matches or outperforms all other methods when taking into account parameter efficiency.  The authors follow the NODE approach of viewing residual layers as ODE steps, allowing a continuous depth when training with differential equation solvers and parametrizing an implicit layer. \n\nThe paper is well written (except for some typos, see below) and the perspective of graph rewiring through a continuous  process is intriguing. However, I have some questions before I can give  a higher rating\n\n-  you present various extensions in 3.4, what would be the expressive power hierarchy? I assume it goes deepsets \\subset Beltrami with time independent attention(w.t.i.a)  \\subset Onsager w.t.i.a \\subset Onsager Onsager w.t.d.a \\subset onsager with generic time dependent nonlinear equation == full attention network with enough layers?\n- how does this work compare with e.g.  https://arxiv.org/abs/1909.12790 and  G(O)DE/CGNN specifically? I think it's worth highlighting commonalities and differences, especially since G(O)DE/CGNN seem competetive with your method in table2  and 3 \n-  since you are using attention on the node embeddings, this means this method will be quadratic in cost w.r.t to number of nodes? as opposed to the sparsity amortization afforded by e.g. MPGNN?\n- you highlight the expressive power in the abstract, but if I undertand correctly, your method can only ever be equivalent to MPGNNs with rewiring techniques/full attention right? Or do you have a proof/example that says otherwise?\n- did you perform tuning with raytune for *all* methods not taken from the Pitfalls of GNN paper? It might be worth marking which is taken from the paper with a cross or something, and state explicitly whether the others were tunes fairly\n- following up on the complexity question, I think adding training curves or some details about training costs and parameter counts to the appendix would be beneficial to putting the work in context. in general, NODEs offer a tradeoff, not a free lunch\n- did you apply the jacobian/kinetic regularisation to G(O)D/CGNN as well? how does your method perform without the regularisation? \n\nTypos:\nI assume in the equation between 212 and 213 Y is meant to be decoded with \\xi, not with \\psi, which is referred to as encoder in 213?\n I think the authors are understating the negative effect  social network analysis can have in terms of surveillance and manipulation, as well as understating the benefit that can come from better architectures for relational tasks expressible as graphs.  VLSI,logic programming etc. can all benefit from this, as can the design of automated propaganda techniques, LAWs and police states. \n\nAs for limitations, as I said, computational expense should be discussed in the paper as well, or noted as a bottleneck. ",
            " Thank you for your clarifications. I heavily disagree with your take on the societal impact, even if you do not *study* it, if your method gets better results *in general* and it's mainly applied to socially dubious things, it warrants thinking about mitigation, impact and acknowledgement at least. The very least we can do when developing methods that might be abused is to be ashamed of it. So I urge you to at least mention them in the societal impact section (as well as, on a technical level, mentioning the fact that a study of the general expressiveness of GNN+rewiring methods remains an open work)\n\nAs per the the other remarks, thank your for the elaboration here and for the ones you will include in the paper. Given that \n\n- the run time seems to be \"not too bad\" in terms of scaling (i.e. similar or better scaling than GAT looking at your experiments, with a larger constant factor) \n- the results are either the same (modulo noise) or strictly better\n- my other questions seem to be answered\n\nI'll upgrade my score\n",
            " Thank you for your comments and the additional experimental results.\n\nI'm still confused why equation 5 is the obvious discrete version of equation 2, which has a very particular form of the diffusivity. It looks to me more that equation 5 is a very general form, which is a discrete version of equation 2 only if some assumptions on $a$ are satisfied. Can the authors motivate their choice of calling equation 5 *the* discrete Beltrami flow for any choice of $a$, and not an equation that only has some analogy to the continuous Beltrami flow?\n\nThanks",
            " Dear authors,\n\nYou say that in the comparison to explicit Euler, you use time step-size 1. For how long do you then integrate? In other words, how many 'layers' are there? Did you experiment with other time step-sizes? I wouldn't be surprised if that may significantly affect performance.\n\nThanks\n\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Negative",
            "Neutral",
            "Positive",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the validity of the theoretical motivation, clarity of the method's performance drivers, and the absence of state-of-the-art results on larger datasets. The reviewer also points out missing ablations, unclear explanations, and potential issues with the applicability of the theory to the model. The final score of 5, marginally below the acceptance threshold, and the statement about the theory not being applicable contribute to the negative sentiment.",
            "The reviewer expresses gratitude and indicates an increased score, signaling a positive assessment.",
            "The reviewer expresses agreement with the author's motivation and suggests incorporating the comments into the final version of the paper, indicating a positive evaluation.",
            "The review acknowledges the work's novelty and strong results but also points out issues with writing and clarity, indicating a mixed sentiment.",
            "The review expresses confusion and questions the validity of the approach, indicating a negative sentiment. Phrases like \"I don't quite understand why (5) is reasonable\" and \"Can you comment on the difference\" suggest the reviewer has concerns.",
            "The review presents both positive and negative aspects of the paper. It acknowledges the paper's well-written nature and intriguing perspective, but also raises several questions and concerns regarding expressive power, comparison with existing methods, computational cost, and experimental setup. The reviewer suggests improvements and clarifications, indicating a neutral stance overall.",
            "The reviewer states they will 'upgrade my score' after the authors addressed their concerns. While the reviewer expresses disagreement on societal impact, the overall assessment of the paper's quality and the authors' response leads to a positive final sentiment.",
            "The reviewer expresses confusion and questions the authors' justification for a specific claim, indicating disagreement and doubt.",
            "The review asks clarifying questions without expressing strong positive or negative opinions."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Balanced",
            "Critical",
            "Critical",
            "Neutral"
        ],
        "tone_reason": [
            "The review uses phrases like \"I am not quite convinced,\" \"I find it unclear,\" \"doubts about the correctness,\" and \"question whether the presented theory applies.\" These phrases indicate a critical evaluation of the paper's methodology, results, and theoretical foundations. The reviewer also points out specific weaknesses and suggests improvements, further highlighting a critical tone.",
            "The reviewer uses phrases like \"thank the authors\" and \"theoretical clarification,\" indicating a supportive and appreciative tone.",
            "The reviewer uses phrases like \"Thank you\", \"I think I agree\", and \"it would be good\", which convey a supportive and encouraging tone.",
            "The review identifies several specific errors and areas for improvement, such as typos, inconsistencies in equations, and requests for clarification and comparison with other methods. The phrase \"the current version is not entirely satisfactory to be published\" is a direct critical statement.",
            "The tone is critical, questioning the reasoning behind equation (5) and highlighting a potential discrepancy with existing literature. The reviewer uses direct questions and challenges the author's approach.",
            "The tone is balanced, as it acknowledges the strengths of the paper while also raising critical questions and suggesting areas for improvement. Phrases like \"intriguing perspective\" show appreciation, while questions about expressive power and computational cost demonstrate critical evaluation.",
            "The reviewer initially expresses strong disagreement ('I heavily disagree') and urges the authors to consider societal impact, indicating a critical stance. However, the tone shifts towards supportive as the reviewer acknowledges the authors' clarifications and states they will upgrade the score.",
            "The reviewer uses phrases like \"I'm still confused\" and \"It looks to me more that\" to express disagreement and challenge the authors' reasoning. The question \"Can the authors motivate their choice...\" directly challenges the authors' justification.",
            "The tone is polite and inquisitive, using phrases like \"Dear authors\" and \"I wouldn't be surprised if that may significantly affect performance.\" It avoids strong criticism or praise."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the weaknesses and conclusion consistently highlight concerns about the theoretical motivation and empirical validation of the method, while the strengths are limited and do not contradict these concerns. The overall sentiment and score align with the identified strengths and weaknesses.",
            "The reviewer expresses gratitude for the authors' improvements and indicates a score increase, which are consistent with each other.",
            "The review is consistent as it expresses agreement with the author's motivation and suggests incorporating the discussion into the final paper, without any contradictory statements.",
            "The review is consistent in acknowledging the paper's strengths, such as novelty and strong numerical results, while also pointing out areas for improvement, primarily in writing clarity and addressing specific technical details. The reviewer's initial satisfaction with the authors' responses leads to a score increase, but the detailed comments and overall impression indicate that the paper still requires revisions before being fully satisfactory for publication. There is no contradiction as the reviewer appreciates the core contributions but highlights necessary improvements.",
            "The review is consistent as it acknowledges the author's clarification and builds upon that understanding to raise specific questions and suggestions for improvement. The reviewer is not contradicting themselves but rather seeking further clarification and justification for specific aspects of the work in a logical progression.",
            "The review is consistent as it presents a balanced perspective, acknowledging the paper's strengths (well-written, intriguing perspective) while raising valid and related questions and concerns about expressive power, computational complexity, comparisons to other methods, and experimental details. The reviewer's points logically follow from the paper's description and claims, and there are no self-contradictory statements.",
            "The reviewer expresses a concern about societal impact but decides to increase the score based on other positive aspects of the paper, such as runtime, results, and answers to previous questions. This is consistent as the reviewer is weighing different aspects of the paper in their evaluation and prioritizing technical improvements for the score upgrade while still maintaining the importance of societal impact consideration.",
            "The review is consistent because it raises a single, focused question about the justification for calling equation 5 *the* discrete Beltrami flow, questioning the generality of equation 5 as a discrete version of equation 2 without further assumptions on 'a'. The reviewer's points are logically connected and do not contradict each other.",
            "The review is consistent as it raises valid questions about the experimental setup, specifically concerning the time step-size used in the comparison with the explicit Euler method. The reviewer seeks clarification on the integration duration and suggests exploring different time step-sizes to potentially improve performance. There are no contradictory statements or inconsistencies within the review."
        ]
    },
    {
        "paper_id": "nips_2022_3uj_8G7fxgs",
        "paper_title": "Multi-objective Deep Data Generation with Correlated Property Control",
        "paper_abstract": "Developing deep generative models has been an emerging field due to the ability to model and generate complex data for various purposes, such as image synthesis and molecular design. However, the advance of deep generative models is limited by the challenges to generate objects that possess multiple desired properties because: 1) the existence of complex correlation among real-world properties is common but hard to identify; 2) controlling individual property enforces an implicit partially control of its correlated properties, which is difficult to model; 3) controlling multiple properties under variour manners simultaneously is hard and underexplored. We address these challenges by proposing a novel deep generative framework that recovers semantics and correlation of properties through disentangled latent vectors. The correlation is handled via an explainable mask pooling layer, and properties are precisely retained by the generated objects via the mutual dependence between latent vectors and properties. Our generative model preserves properties of interest while handles correlation and conflicts of properties under a multi-objective optimization framework. The experiments demonstrate our model's superior performance in generating objects with desired properties.",
        "review_ids": [
            "v-1OuWhUlq",
            "vUIthqPDPTj",
            "i4BDPd-kdgP",
            "m3Mgv9w3Xtt",
            "smBe3JwDx2b",
            "GU0VImi3IEs"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " My comments have been properly addressed.",
            " I understand the argument, but on the other hand, results obtained by beta-vae [a] and subsequent works [b,c] tends to indicate that it is not so difficult to encode pos_x and pos_y into a single value each without supervision at all.\n\nBut this is only a minor concern.\nAfter reading through the submission again to reassess it, I'm left with a question: how do we properly evaluate the task?\nIndeed, the authors suggest that they are happy with properties that are *changeable, diverse, or even unseen before in training data*. But certainly, they still want the generated properties to be somewhat close to the training data, or else they would be no point to a data-driven approach at all.\n\nAn other way to look at it is that, some constrains are hard-set by the experimenter (positions in the example), and others (like shapes) are soft constrains influenced by the dataset and the deep model.\nGiven that the later are allowed to some degree to be unseen from the training data, how can we tell if a given level of adherence to the dataset in the generated samples is a desirable feature or an hindrance?\nIt seems to me that to evaluate those methods, we would need either an actual downstream task, or that the model provides a control over the degree of adherence to the dataset.\n\nI realize that this comes very late in the discussion process, but I would appreciate if the authors could comment on these views.\n\nIn any case, I also understand that this potential problem is inherent to the definition of the task and not easily solved.\nIn the meantime, I updated my rating from 3 to 6.\n \n--- \n[a] beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. Irina Higgins et al. ICLR 2017.\n\n[b] Isolating Sources of Disentanglement in Variational Autoencoders. Tian Qi Chen et al.  NeurIPS 2018.\n\n[c] Disentangling by Factorising. Hyunjik Kim and Andriy Mnih. ICML 2018.",
            " I'd like to thank the authors for the very substancial rebuttal.\n\nI believe my concerns have been adequatly adressed. Crucially, the answers corrected a miss-conception from my part as to the goals and motivations of the paper. The additional figures completed the previous results nicely, providing more information about the different components of the model.\n\nI still have a minor comment regarding response #3: \n*(2) The fact that \"x position\" and \"x+y position\" attributes are diluted into two variables indicates \u201cx position\u201d and \u201cx+y position\u201d are not 100% colinear and are also not 100% independent. This is why we see each of them has two variables in w, where one variable is shared between them while the remaining one of each of them is not shared.*\n\nI agree with this statement. What I meant that it would be very intuitive to encode \"x position\" as one value $w_i$, and \"x+y position\" as a pair of value that includes $w_i$ (or vice-versa). Wouldn't this configuration be better in terms of loss?\n",
            " Generating data with multiple constraints on its correlated properties is a critical task. The authors address this task by designing a new mask pooling layer to identify and control correlated properties using independent latent variables. These latent variables are bond to properties based on the mutual dependence. Then these latent variables are optimized to generate data with desired properties under a multi-objective optimization framework. The effectiveness of the proposed model has been shown on the molecule and image datasets in the evaluation session. Strengths:\n1. The tasks approached by the paper are challenging but practically important. The framework proposed by the authors is novel.\n1.1 The proposed framework includes a mask pooling layer learned to capture and control correlated properties via the independent latent variables. This also to some extent adds interpretability to the model.\n1.2 In addition, the framework employs a set of bridging latent variables w to aggregate information from w to predict properties. The exact recovery of w from properties can be achieved via an invertible constraint of mutual dependence.\n1.3 The authors formally propose a multi-objective optimization framework for simultaneously control correlated properties of generated data. It comes with the generality in terms of accommodating different optimization goals and constraints, which looks reasonable and interesting for controllable data generation.\n\n2. The experiments show the effectiveness of the model. \n2.1.\tThe mask pooling layer works well given the results shown in Figure 5 under the setting that x position and x+y position should be correlated with each other. \n2.2. Figure 3 shows the effect of disentanglement of the latent variable w. Correlated properties x+y position can be well controlled by the corresponding latent variable given by the results of mask pooling layer.\n2.3. As shown in Table 1, CorrVAE can handle correlated properties x+y position better than other comparison models on dSprites dataset. Table 2 shows the effectiveness of CorrVAE on molecule datasets on MolWeight and logP properties.\n\n3 The code of the paper is well packaged and published.\n\nWeakness:\n1. In Table 1, although the proposed method dominating the others, it would be great to provide more analysis about the results why sometimes the degree of superiority is very large while sometimes its performance is close to the others.\n2. The white text on Figure 3 and Figure 4 can be larger to be more readable. The fond size of Table 1 and Table 2 should be aligned.\n3. Missing references: For example [1] below can be discussed in the Related works session.\n\n[1] Xie, Yutong, et al. \"Mars: Markov molecular sampling for multi-objective drug discovery.\" arXiv preprint arXiv:2103.10432 (2021).\n 1. Provide more discussion about the results in Table 1. Why CorrVAE has worse performance than CSVAE on two independent variables x position and scale?\n2. Discuss the missing reference [1] in the Related works session.\n The paper discusses the limitation when pointing out the potential future works in the Conclusion. \nThat the work does not have potential negative social impact.",
            " The paper tackles the problem of controllable data generation when the controlled attributes y are correlated.\nTo do so, it proposes to separate the latent code in a disentangled VAE into two parts, z and w, where w contains the variables that are correlated to y, and z the variables independent of w (and hence to y). The disentangled variables w are combined into correlated variables w' via a learned mask M that indicates for each value of w', which values of w contribute to it. The values of w' are mapped one-to-one to the attributes y using an invertible network.\nThe model is used for controlled generation, to identify correlations between attributes, and for generation using multi-objective constrained optimization. Strengths:\n- The paper address the problem of attributes correlation that is often ignored in controlled generation in a principled way.\n- Experiments have been conducted on real molecular data, in addition to synthetic images.\n\nWeaknesses:\n- The experiments fail to sufficiently back the claims made in the paper. Crucially, the paper is framed as a data generation method but the proposed experimental protocol do not assess the quality of the generated data, only the perservation of a few attributes. -  The samples shown in Fig3 and Fig4 for dShapes are very bad, even in disentanglement VAE models standard. The shape attribute, that should be encoded in the independent z variables, are not only not conserved with attribute manipulation, but also they do not seem to be shapes that are in the dataset. This drastically limits the usefulness and significance of the method for data generation. Can the authors provide reconstruction errors and FID for generated data, for both seen and unseen combination of attributes?\n- The authors discuss in section 5.4 and show in Fig 3 latent traversals of w. However, it would appear that the w space is not that interesting, as they do not align well with w' and y. For instance, both the \"x position\" and \"x+y position\" attributes are diluted into two variables (figure 5). This is especially surprising since it increases the KL term in Eq3, and the mask sparcity loss. Meanwhile, the arguably more interesting variable w' that would be used for attribute manipulation is barely investigated. Can the authors provide qualitative samples for latent traversal of w', and quantitative figures on how well the attributes are retained?\n- I would also like to see the full mask M in Figure 5. What about \"y position\" for instance?\n- While the authors claim that the ablation CorrVAE-1 that use ground truth masks are achieving better performance than CorrVAE, it is not clear in the result Table 1. This could also indicate that the model is not working as expected and that different variables in the model do not exactly capture the information they are intended to get. Why would CorrVAE-1 would be slightly worse than CorrVAE on some task? How significant is the difference?\n- Important implementation details are in Appendix C. At the very least, the fact that the mask is encouraged to be sparse should be mentioned in the main paper. I would argue that approximations and relaxation of the problem, such as Monte-Carlo, Gumbel-SoftMax and Spectral Normalization should also be mentioned in the main paper when used as they are not exact implementations of the provided formula.\n\nOverall, I believe the paper overclaims what is the proposed model able to do. It indeed preserve the controlled attributes, arguably better than the baselines, but seemingly at the cost of loosing the other attributes, even some as fundamental as shape or orientation.\nI could change my opinion if the authors can provide evidence that this assessment is wrong (by answering the questions for instance), or if they can show that the trade-off is a desirable feature.\n\n\nMinors typos: \n\nl199: thrid\n\nl183 w_i^T \u00b7  w_j  : w_i \n\nl326-327: it seem it should be CorrVAE-2 insteand of CorrVAE-1?\n\nVertical spacing after subsection titles are very unusual. The paper do not mention its limitations.\nI believe the weaknesses raised above do qualify as limitations in term of data generation and should be adressed in the paper.",
            " The paper tries to address conditional generation where conditioning properties are correlated.\nThey point out that existing works that tackle the controllable property generation do not consider the correlation between multiple properties, as it is difficult to identify property correlation. Therefore, they propose a deep generative model that uses invertible mapping to map correlated properties to latent independent variables. During learning, they learn to encode the property and other information through two different encoders and use a novel mask pooling layer on the latent representation corresponding to the property encoder to learn the correlated properties. \n\n\n strength: The problem they try to solve is beneficial for the community and the proposed method is interesting\n\nweakness:  the presentation of the paper could be improved, some parts are bit confusing. They did not compare with the model where they explicitly model the dependency between the properties using hirerichical model or with those who do unconditional generative model coupled with Bayesian optimization to do conditional generation.  \n\nThe proposed model seems really interesting but I had some confusion:\n\n1. corresponding properties in figure3 and 4 are not readable. \n2. It is not very clear to me how  this conditional property is simplified: q(w,z|x,y) = q(w,z|x)\n3. I am not sure why we need the J matrix in w'=wJ^TM , is not the masking matrix M learn if an element of w contribute to w' or not and how big the contribution is?\n4. I did not really understand assumption 2 where they derive x and y are independent given z and w.\n5. In equation (2), from the first line to the next where they assume y_i can be learned from w'_i, where w_is are independent, is this assumption always holds? so each w'_i is independent, each y_i is derived from one w'_i, but y_i are correlated. So I am a bit confused that if each y_i is only derived from one w'_i, and w'_is are independent, how can they produce a set of y_i that are correlated? This leads to my next question the learned latent w itself stated that elements in w are disentangled from each other, so what is the intension behind why we introduce another set w' that are also disentangled. \n The proposed idea is very interesting, however, for property-controlled generation, each time it needs to solve the optimization problem to find the proper w for the target property, this could be expensive, also the paper did not compare with the approaches that do bayesian optimization in the latent to find a best patent variable that can generate the data with the target properties. "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states that their comments \"have been properly addressed,\" indicating satisfaction with the revisions.",
            "The review expresses a concern but acknowledges its minor nature, appreciates the authors' work by increasing the rating from 3 to 6, and frames the remaining issue as inherent to the task definition rather than a flaw in the implementation. The language is generally constructive and respectful.",
            "The reviewer explicitly thanks the authors and states that their concerns have been adequately addressed. They also acknowledge a misconception on their part and appreciate the additional figures.",
            "The review expresses overall positive feedback, highlighting the novelty and practical importance of the work, the effectiveness of the model in experiments, and the well-packaged code. While it points out weaknesses, they are framed as suggestions for improvement rather than fundamental flaws.",
            "The review identifies several weaknesses, including insufficient experimental backing, poor sample quality, and unclear variable alignment. The reviewer believes the paper overclaims the model's capabilities and that it loses important attributes. The reviewer expresses a willingness to change their opinion if the authors can address these concerns, but overall, the tone suggests a negative evaluation.",
            "The review expresses confusion and questions the validity of some assumptions and methodologies. Phrases like 'some parts are bit confusing,' 'It is not very clear to me,' 'I am not sure why,' 'I did not really understand,' and 'I am a bit confused' indicate a negative sentiment."
        ],
        "tone": [
            "Neutral",
            "Balanced",
            "Supportive",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The statement is a simple, direct acknowledgment. There's no strong emotion or stylistic element to suggest a particular tone beyond neutral.",
            "The review presents both concerns and positive aspects. It starts with a disagreement ('I understand the argument, but...') but then downplays it ('only a minor concern'). It raises a critical question about evaluation but also acknowledges the difficulty of solving the problem and ultimately increases the rating, showing a balanced perspective.",
            "The reviewer expresses gratitude (\"I'd like to thank the authors\"), acknowledges their own misunderstanding, and frames their remaining comment as a suggestion for improvement rather than a criticism. The language is polite and constructive.",
            "The tone is supportive, as evidenced by phrases like \"challenging but practically important,\" \"the framework proposed by the authors is novel,\" and \"the code of the paper is well packaged.\" The reviewer focuses on the strengths and provides constructive suggestions for improvement.",
            "The review uses phrases like \"fail to sufficiently back the claims,\" \"very bad,\" \"drastically limits the usefulness and significance,\" \"not that interesting,\" and \"overclaims.\" These strong criticisms indicate a critical tone. The reviewer also poses several direct questions challenging the authors' methodology and results.",
            "The review adopts a critical tone by directly questioning assumptions and expressing doubts about the clarity and validity of the proposed method. Phrases like 'It is not very clear to me how,' 'I am not sure why,' 'I did not really understand,' and 'I am a bit confused' contribute to this critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it is a short, positive statement indicating that the reviewer's comments were addressed. There are no contradictory statements within the review.",
            "The review is consistent. It raises concerns about the evaluation of the proposed method, specifically regarding the balance between generating diverse/unseen properties and maintaining closeness to the training data.  However, the reviewer acknowledges that this is an inherent problem in the task definition and not easily solvable, and despite this concern, they increase their rating, indicating an overall positive assessment while highlighting a valid point for discussion and potential improvement.",
            "The reviewer expresses that their concerns have been adequately addressed by the authors' rebuttal and additional figures. While they still have a minor comment, it is framed as a suggestion for improvement and does not contradict their overall positive assessment that their initial concerns were resolved.",
            "The review is consistent as it highlights both strengths and weaknesses of the paper in a balanced way. The weaknesses are constructive suggestions for improvement rather than fundamental flaws, and they do not contradict the acknowledged strengths of the paper. The reviewer points out areas for further analysis, presentation improvements, and missing references, all contributing to a consistent and helpful review.",
            "The review is consistent because the weaknesses raised are well-supported by specific observations and questions. The overall assessment that the paper overclaims is logically derived from the detailed weaknesses regarding experimental validation, qualitative results, and clarity of explanations. There are no contradictory statements or conflicting opinions expressed within the review.",
            "The review is consistent as it identifies both strengths and weaknesses of the paper, and the questions raised are relevant to the methodology and clarity of the paper. The reviewer appreciates the idea but points out areas of confusion and suggests improvements, without contradicting themselves."
        ]
    },
    {
        "paper_id": "nips_2021_sFyrGPCKQJC",
        "paper_title": "Optimizing Reusable Knowledge for Continual Learning via Metalearning",
        "paper_abstract": "When learning tasks over time, artificial neural networks suffer from a problem known as Catastrophic Forgetting (CF). This happens when the weights of a network are overwritten during the training of a new task causing forgetting of old information. To address this issue, we propose MetA Reusable Knowledge or MARK, a new method that fosters weight reusability instead of overwriting when learning a new task. Specifically, MARK keeps a set of shared weights among tasks. We envision these shared weights as a common Knowledge Base (KB) that is not only used to learn new tasks, but also enriched with new knowledge as the model learns new tasks. Key components behind MARK are two-fold. On the one hand, a metalearning approach provides the key mechanism to incrementally enrich the KB with new knowledge and to foster weight reusability among tasks. On the other hand, a set of trainable masks provides the key mechanism to selectively choose from the KB relevant weights to solve each task. By using MARK, we achieve state of the art results in several popular benchmarks, surpassing the best performing methods in terms of average accuracy by over 10% on the 20-Split-MiniImageNet dataset, while achieving almost zero forgetfulness using 55% of the number of parameters. Furthermore, an ablation study provides evidence that, indeed, MARK is learning reusable knowledge that is selectively used by each task.\n",
        "review_ids": [
            "GaOUj6rVaW1",
            "0ihgesfZxbP",
            "lWoQDywX8bL",
            "gbfY-mih7S-",
            "vMJXoJj01T5",
            "9hj1A-WewZf",
            "cu0cQZgVPW_"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper investigates how to dynamically select reusable knowledge from past experience via meta learning and trainable masks.\nThe main contribution is the proposition of a meta learning mechanism for enriching the sharing knowledge and trainable masks for selecting reusable parameters.\nThe experiments on 20-Split CIFAR-100 and 20-Split MiniImageNet have been conducted to validate the performances of the proposed method.\n\n  Originality:\n  This paper proposes a new method to increase sharable parameters between related tasks via a meta learning mechanism and to dynamically select reusable parameters using trainable masks.\n  Relationship to previous works has been explained and relevant literature has been cited appropriately.\n\nQuality:\n  The work is relevant to audience at Neurips and provides reasonable details for reproducibility.\n  The authors have conducted ablation study to evaluate the effectiveness of the proposed method, and have done experiments on CL benchmarks to compare the performances between the proposed method and the best performing methods.\n\nClarity:\n The paper is overall clearly written and the method is adequately described.\nIt provides reasonable details for reproducibility.\n\nSignificance:\n The proposed method outperforms the SOTA CL methods in terms of average accuracy by over 10%.\n\n[After Rebuttal]\nAfter reading other reviewers' comments and the authors responses, I'd like to keep my rating 6 because the authors have addressed an important question in CL and most reviewers' concerns in their responses. They also provide enough details/codes for readers to reproduce the results. Yes",
            " After reading other reviewers' comments and the authors responses, I'd like to keep my rating 5 because the authors did not address my question before, and the algorithm has strong assumption that tasks share a similar domain.",
            "This paper mainly solves the catastrophic forgetting problem in task-based continual learning. In particular, this paper proposes a new meta-learning based method, MARK, which uses knowledge base to store some useful knowledge and uses meta-learning to update Knowledge Base. In addition, the knowledge of KB is selected through a set of masks. Finally, the experimental results of this paper show that MARK better than existing methods on two datasets. The main innovation of this work is to use meta-learning to update the knowledge base to simulate human learning methods.  The idea of using meta-learning to update the knowledge base and combining with mask for feature selection is not novel enough. The main innovation of this paper is to propose \"meta-learning to update the knowledge base\", but the method of updating KB only with the data of the new task still has CF for KB. Therefore, I don\u2019t know that the use of meta-learning in this paper can get highly general features by KB. \n\nIn content: Refer to the question in Originality, hoping to get a clearer explanation. An additional suggestion is that if you use part of the data of the previous task (caching or generation) when updating the knowledge warehouse, will you achieve better results (e.g., use the replay based method)? \nExperimentally: \n1. Please show the experimental results of GPM on CIFA-100? As far as I know, the GPM code is very easy to run CIFAR-100. \n2. Because of meta-learning, how different is the training time between MARK and baseline? \n3. There are relatively few baseline models for experimental comparison. CONTINUAL LEARNING WITH HYPERNETWORKS. ICLR 2020. Continual Learning by Using Information of Each Class Holistically. AAAI 2021 \nClarity:\n\n This paper is clearly written. But I suggest that section 2 should be a subsection of section 3.\n The potential negative societal impact should be addressed clearly.",
            " After reading other reviewers' comments and the authors responses, I'd like to raise my rating to 5 because the authors have addressed my questions before, while the novelty of the proposed method is not great enough for a positive score.",
            " After reading other reviewers' comments and the authors responses, I'd like to keep my rating 6 because the authors have addressed an important question in CL and most reviewers' concerns in their responses.",
            "This paper proposes a mechanism for continual learning under discrete tasks with known task boundaries. The method is based on learning a shared feature extractor, called Knowledge Base (KB) in the paper. Given the KB, task adaptation occurs by training a mask over the KB\u2019s activations and a final linear classifier for that task. As far as I know, the proposed method is novel and empirical results on standard benchmarks are encouraging.  While the paper could do with a little polish, overall it is clearly written and the high-level idea of the method is easy grasp. While the idea of using parameters as a so-called knowledge base has been explored in the past, for instance in the HAT paper as the authors note, I find the method proposed in this paper interesting and the results are convincing.\n\nIn particular, given current parameters of a feature extractor (the KB), the authors first tune a task-specific mask and final classifier on the task, with the rationale of forcing the classifier to use existing knowledge. They then tune the KB, holding the mask and final classifier fixed, by a meta-learning strategy; the idea being that this lets the KB absorb new information that generalises across tasks. Finally, they perform some additional tuning of the mask and classifier to take advantage of the new information. \n\nInterestingly, rather than simply adapting the KB to the task, the authors construct a meta-learning problem out of the current task, by randomly sampling classes and observations into query and support sets. The KB is then trained with a variant of Reptile that, as opposed to taking a simple average, weighs the gradient contribution of each mini-task by their final accuracy. On paper, I wouldn\u2019t expect this to work, because the mini-tasks are all constructed from the same task. However, the authors show in an ablation that this meta-learning procedure really does help prevent forgetting. \n\nDetailed comments and questions:\n\nL 49: all continual learning methods that share parameters store shared knowledge in their parameters, claiming this as a novel contribution is a bit strong.\n\n\nL 61: you mention that meta-learning approach learns discriminative features, such as claws and fur. This is a strong claim that needs to be verified. Does it really learn distinct sub-concepts?\n\n\nYou add a task index to the problem description, but I do not believe that the task ID is actually used by the method?  \n\n\nL 97: I\u2019m not sure training on mini-tasks from one task necessarily \u2018bias the learner to improve its ability to generalize\u2019. If the task is very specific (say, MNIST images), so will all mini-tasks be, and there is no guarantee that they will actually generalize to new tasks.\n\n\nL 109: it would be helpful to say up front what the encoder F is, rather than leaving the reading wondering for 3 pages.\n\n\nL 163: a mask typically means something that takes on values 0,1. A more common choice would have been a sigmoid activation, or parameterising a Bernoulli distribution. I think this choice need stronger motivation, and you should also qualify that what you mean by mask isn\u2019t quite the typical definition.\n\n\nEq. 1, if the mask functions is a linear map from the feature space of a convnet to the all hidden activations in the KB, the number of parameters in the mask should be very large. It would be nice to see a comment on the size of the masking function.\n\n\nL 182. The update you use is essentially the same as [24], which should be cited.\n\n\nEq. 3. This seems rather unmotivated. Could you provide an ablation to show that weighting the batches makes a difference? One thing to note is that this biases the meta-update towards *easier* tasks, which may not be what you want - easier tasks are things the KB already knows. Hard tasks are things it needs to learn.\n\n\nL 239. What does the \u2018amount of weight updates\u2019 mean? The norm of the parameter delta? Or the number of non-zero gradient elements? Or something else?\n\n\nFigure 3. It seems odd to me that the baseline is at almost 100% on task 1, when the KB isn\u2019t. Since the KB is trained in the standard fashion on task 1, shouldn\u2019t it be equal to the baseline on task 1?\n\n\nAblations: I would move the baseline+mask ablation to the main text to have a complete ablation analysis.\n\nYou're missing a discussion of the limitations of this method. No discussion, this needs to be added. For instance, here are a few question I have: What would break the KB? For instance, what if tasks are more distinct that what is uses in the experiments? My guess is that the meta-learning strategy would break down, since the mini-tasks would become too specific. Are there ways to prevent that? Did you consider adding task replay to the meta-update step?",
            "This paper introduces a novel method MARK which utilizes the knowledge base (KB) that can facilitate learning a novel task and reducing the forgetting issue in continual learning and a mask function that selects proper reusable features in KB. The learning procedure consists of four steps: 1) Obtain feature vectors to train the mask function, 2) train a mask function and classifier to learn a novel task, 3) update KB using meta-learning, 4) fine-tune the mask function and classifier to utilize the information of updated KB. When updating the KB, the dataset for a novel task separated to many subtasks, and using the adaptation technique proposed in [24], KB meta-learns the features that can generalize to different tasks. In experiments, the proposed methods achieves the state-of-the-art accuracy in CIFAR and mini-ImageNet dataset. By ablating the main components (KB and mask functions), authors also show the importance of each component.  Pros\n\n- This paper proposes a novel method that utilizes the knowledge base which contains well adaptable features to a novel tasks and the knowledge for old tasks. In experiment, by showing the robustness of weight changes with respect to the sequential learning, using knowledge base can effectively resolve the stability and plasticity dilemma in continual learning\n\n- The proposed method outperforms other baselines with much fewer parameters than others.\n\nCons\n- In my opinion, I wonder MARK achieves the state-of-the-art performance. It would be great to compare MARK with a recently proposed method CTN[Ref 1] which also achieves great performance in CIFAR and mini-ImageNet dataset in task-incremental learning setting. \n\n- Though authors show that the deviation of weights in KB is much smaller than baseline, it maybe effective only in similar task distributions because there is no clear components  for reducing the forgetting issue in KB. Therefore, it would be great to show the effectiveness of KB in other continual learning scenario which consists of different distribution of tasks (e.g. sequence of 8 tasks proposed in HAT[7]) by comparing the results of MARK with other baselines (e.g. HAT)\n\nQuestions\n- Does MARK can applicable to RL? If it does, it could give a great impact on continual learning in RL.\n\n- If KB continuously updates its representation, is there any possibilities for mismatch between the current KB and the mask function for previous tasks?\n\n[Ref 1] \"Contextual Transformation Networks For Online Continual Learning\", Pham et al., ICLR, 2021 None"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses overall positive feedback, highlighting the paper's originality, quality, clarity, and significance. The final statement confirms a positive stance after considering the rebuttal and other reviewers' comments.",
            "The reviewer is maintaining their original rating due to the authors' failure to address their concerns and the algorithm's strong assumptions, indicating dissatisfaction.",
            "The review expresses concerns about the novelty and effectiveness of the proposed method. Phrases like \"not novel enough\", \"still has CF for KB\", and \"I don\u2019t know that the use of meta-learning in this paper can get highly general features\" indicate a negative sentiment. The reviewer also raises questions about the experimental setup and baseline comparisons, further contributing to this sentiment.",
            "The reviewer explicitly states they want to raise their rating to 5, indicating a positive shift in their assessment.",
            "The reviewer explicitly states they will \"keep my rating 6\" and acknowledges that the authors have addressed an important question and most reviewers' concerns.",
            "The review expresses overall positive sentiment, highlighting the novelty of the approach and encouraging empirical results. Phrases like 'interesting' and 'convincing' further reinforce this positive outlook.",
            "The review expresses both positive aspects ('novel method', 'outperforms other baselines') and negative aspects ('wonder MARK achieves the state-of-the-art performance', 'maybe effective only in similar task distributions'). It also poses questions for further clarification."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Critical",
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"relevant literature has been cited appropriately,\" \"provides reasonable details for reproducibility,\" \"clearly written,\" and \"outperforms the SOTA CL methods\" which suggest a supportive and encouraging tone. The explicit statement \"I'd like to keep my rating 6\" after the rebuttal reinforces this positive and supportive stance.",
            "The reviewer uses phrases like 'did not address my question' and 'strong assumption' which convey a critical perspective on the authors' response and the algorithm itself.",
            "The reviewer uses direct questions and critical statements to point out weaknesses in the paper. For example, \"the idea...is not novel enough\" and \"I don\u2019t know that the use of meta-learning in this paper can get highly general features by KB\" are critical assessments of the work. The suggestions for improvement are framed as necessary corrections, indicating a critical tone.",
            "The reviewer acknowledges the authors' efforts in addressing their questions and expresses a willingness to increase the rating, indicating a supportive stance. Phrases like 'authors have addressed my questions' suggest a positive and helpful attitude.",
            "The reviewer uses phrases like \"addressed an important question\" and \"most reviewers' concerns\" which indicates a supportive tone towards the authors and their work.",
            "The review provides both positive feedback and constructive criticism. It acknowledges the paper's strengths (novelty, encouraging results, clear writing) while also pointing out weaknesses and areas for improvement (strong claims needing verification, missing citations, lack of discussion on limitations). This balanced approach indicates a fair and objective assessment.",
            "The review presents both 'Pros' and 'Cons' sections, indicating a balanced assessment of the paper's strengths and weaknesses. The language is generally objective and analytical, avoiding overly enthusiastic or harsh criticism."
        ],
        "consistency": [
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive across all aspects (originality, quality, clarity, significance) and after rebuttal. There are no contradictory statements or negative feedback.",
            "The reviewer states that the authors did not address their question and points out a strong assumption of the algorithm as reasons. These points are typically considered negative aspects that would lead to a lower rating. However, the reviewer decides to keep a high rating (rating 5), which is inconsistent with the provided negative reasons. It is not explained why these negative points do not warrant a rating decrease.",
            "The review is consistent in its critique. It acknowledges the paper's contribution but raises concerns about the novelty and effectiveness of the proposed method. The reviewer's suggestions for improvement, such as including more baselines and addressing potential weaknesses like catastrophic forgetting in the knowledge base, are logically connected and aimed at strengthening the paper. There are no self-contradictory statements or conflicting viewpoints within the review.",
            "The reviewer is raising the rating because the authors addressed their questions, which is a positive aspect. At the same time, they are pointing out that the novelty is still not strong enough for a positive score, which is a separate concern about the contribution of the paper. These two points are not contradictory but rather represent a nuanced evaluation where the authors have improved the paper by addressing questions, but the fundamental issue of novelty remains a limitation.",
            "The review is consistent because the reviewer provides a clear and logical reason for maintaining their rating. They explicitly state that after considering other reviews and author responses, they are keeping their rating because the authors have addressed an important question and most reviewers' concerns. This is a consistent and rational justification for their decision.",
            "The review is consistent in its constructive criticism, starting with an overall positive assessment and then providing specific, actionable feedback and questions to improve the paper. The reviewer acknowledges the strengths while pointing out areas for clarification and justification, maintaining a consistent tone throughout.",
            "The review is consistent because it acknowledges the strengths of the paper in the 'Pros' section, such as novelty and performance, while also raising valid concerns and suggesting improvements in the 'Cons' section. The reviewer's questions are relevant and further explore the method's potential and limitations. There are no contradictory statements within the review; it presents a balanced and constructive critique."
        ]
    },
    {
        "paper_id": "iclr_2021_IgIk8RRT-Z",
        "paper_title": "CompOFA \u2013 Compound Once-For-All Networks for Faster Multi-Platform Deployment",
        "paper_abstract": "The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware and latency constraints. To scale these resource-intensive tasks with an increasing number of deployment targets, Once-For-All (OFA) proposed an approach to jointly train several models at once with a constant training cost. However, this cost remains as high as 40-50 GPU days and also suffers from a combinatorial explosion of sub-optimal model configurations. We seek to reduce this search space -- and hence the training budget -- by constraining search to models close to the accuracy-latency Pareto frontier. We incorporate insights of compound relationships between model dimensions to build CompOFA, a design space smaller by several orders of magnitude.  Through experiments on ImageNet, we demonstrate that even with simple heuristics we can achieve a 2x reduction in training time and 216x speedup in model search/extraction time compared to the state of the art, without loss of Pareto optimality! We also show that this smaller design space is dense enough to support equally accurate models for a similar diversity of hardware and latency targets, while also reducing the complexity of the training and subsequent extraction algorithms. Our source code is available at https://github.com/gatech-sysml/CompOFA",
        "review_ids": [
            "zbZCPnpS8ue",
            "oh5N63Hhpik",
            "8zn1FpJH1YV",
            "Hrd5pZSH08h"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper presents a compounding strategy for constraining the search spaces for once-for-all (OFA) network training framework. This is motivated by observations in prior work (particularly the EfficientNet by Mingxing Tan and Quoc Le) that certain compound relations exist between network dimensions (widths, depths, resolutions) for achieving optimal accuracy-latency trade-offs. In this work, network depths and widths are compoundly coupled. The resulting method called CompOFA reduces the search spaces significantly from 10^19 candidates to 3^5 (CompOFA) or 10^10 (CompOFA-Elastic) candidates, thus cutting the training costs by 50% or 30%. The models found with CompOFA/Elastic for specific hardware latencies are also more accurate than those found with OFA, despite requiring less training costs.\n\nPaper's strengths\n- Both smaller search spaces and the avoidance of progressive shrinking further reduce the training costs of an already efficient framework (OFA) while not sacrificing prediction performance is impressive. This makes it more practically useful.\n- The proposed compounding strategy is well-motivated by existing findings and nicely integrated to the OFA framework. It also clearly demonstrates that (human) prior knowledge is still very relevant in automated search for efficient networks.\n- The models searched by CompOFA match the performance of (or in some cases, outperform) OFA models, both in terms of best models given certain latencies and sampled models at population levels.\n- The paper is generally well-written and includes sufficient references to prior work.\n\nPaper's weaknesses\n- The pre-defined coupling configurations [D:2, W:3], [D:3, W:4], [D:4, W:6] are heuristically chosen based on the default search space originally proposed by the OFA paper. CompOFA works well because potentially these numbers happened to quite effective. In the more realistic settings (e.g., other datasets, tasks) where we do not have access to the prior numbers, it may be difficult to achieve good performance with a small number of coupling configurations, and OFA may work better to explore different combinations in a larger search space.\n\n- The compound coupling only considers network depths and widths, and only a small modification to the design space is proposed to achieve that. If the proposed method considered image resolution, the paper would be more complete in the context of compound scaling and it would have a stronger contribution.\n\n- In the OFA paper, smaller models are generated by shrinking the larger models, such that the smaller models (partially) retain the weights of the larger models. In this paper, all the smaller models are generated and trained at the same time. However, there is no information in this paper on how they are derived from the teacher network, e.g., whether they partially share the weights with teacher or they are distinctly initialized.\n\n- While it is interesting that CompOFA models could outperform OFA models in some cases, this paper does not make it clear why this is the case. CompOFA's search space is supposedly a subset of OFA's search space. The models trained by CompOFA should be covered by OFA. \n\n- Code is not provided in the submission for reproducibility and there is no promise of code release.\n\nMinor comment\n- Sec 5.2 mentions \"by using just one stage of training after the teacher model\" but the paper should explicitly mention that this applies only to CompOFA (fixed kernel). CompOFA (elastic kernel) in Table 4 actually still requires multiple stages.\n\nThis paper's ideas and contributions are nice-to-have but they are far from being groundbreaking.\n\n##post-rebuttal##\n\nI share the concerns of others reviewers that this paper has a limited novelty and narrowed scope but I think the authors have addressed other issues/concerns quite well. In my opinion, the key contributions mostly come from the insight and experimental results, and less so on the heuristic itself. Thus, I would adjust my rating to 7.",
            "Deploying models on multiple target end points with different hardware spec requires training model variants of an underlying architecture that meet the latency and other resource constraints on the device. OFA is a framework that helps this search for architectures like mobilenetv3 by training a model with large components and then training multiple instances of smaller models wherein components have reduced size so as to fit multiple target devices. Due to the large state space of parameter sizes for subcomponents, this search can take a while.\n\nThis paper proposes a heuristic to bring down the size. The main premise here is that searching for various subcomponents size parameters as if they were orthogonal design choices is not optimal. Good choices for parameters, for example the depth and the width are correlated. Therefore in place of searching as though these were orthogonal, the authors propose a correlated search to reduce the number of possibilities. This halves the overall search time, which is substantial saving given the time of the overall search. \n\nThe experiments are comprehensive on backing up the premise for the limited choice of data set and architecture considered. They show consistent improvements in overall training time  while showing the quality of resulting architectures is comparable to OFA methodology. This is done by showing the accuracy and size distribution of models obtained with this method vs OFA. \n\nMy concern with this paper is that the heuristic, while useful in this case, is not particularly interesting. Nor does the paper establish that this is applicable to other architectures. The result is that the scope of this paper is narrow. I would like more evidence to demonstrate that there are generalizable principles here. ",
            "#### Summary\nThis paper proposes to reduce the search space of OFA (a training scheme for obtaining networks for various deployment requirements) by scaling or shrinking the depth and width dimensions in NAS search space together. Because of the reduction, the multiple training phases of OFA is also less important and can be simplified to reduce training time.\n\n\n#### Technical\n* (+) The key observation made in this paper makes sense to me that the search space of OFA is over-sufficient and might be unnecessary for the target problem -- deployment in various environment with different latency/hardware constaints\n* (+) the experiments of this paper looks good, the improvement over OFA in terms of training speedup and energy saving is impressive.\n\n* (-)  I do appreciate the good engineering and intensive empirical results revealed by this paper but the technical novelty of this paper seems to me very increamental compared to OFa, though the arguments made in section 3.2 makes total sense me. For examples, the authors argued the model dimensions are not orthogonal, and the latency requirements only need to be satisfied in a way (granularity) that are below many thresholds. I think a better way to materialize these arguments is to explicitly characterize how these two observations should be incorporated into the architecture space, e.g., establishing direct correspondences between the design of the architecture space and the latency requirement, or by revealing how different dimension of architecture space are coupled (either theoretically or empirically ) and affect the design of the space. However, the paper ends up with a (not so intuitive) approach  -- that slightly modifies the space by coupling of two dimensions of the space, and starts discussing that by coupling these two dimensions, progressive multil-phase training can be eliminated to improve training.\n\n\n",
            "Paper Overview:\nThis paper is aiming at optimizing the OFA method in neural network model searching and training. Though CNN models perform well for many prolems, there is a serious shortage of this method. Researchers need to build and train a new model for every new problem, which will cost a lot of time and resources. A recent work, OFA, proposed a new method to partly solve this problem by training a family of models at a same time by parameter sharing. However, the OFA still have some problems. The searching space of OFA is too huge so the training process still takes too long and cost lots of resources. Thus this paper addressed a new solution, CompOFA, to speed up the training and try to get a balance between the accuracy and latency by building constraints between dimensions of model searching space and \"progressive shrinking\" approach.\n\nStrengths:\n1. This paper gives out obvious evidence of their basic insight. They use a heatmap to show the trade-off relationship between the width and depth of different models.\n2. This paper uses real data and SOTA works to do evaluation, makes their result more convincing.\n\nWeaknesses:\n1. The design idea and motivation of this paper is not well addressed. How do you decide to build the constraints? Why the trade-off is necessary? As far as I know, most of AI models care most about their accuracy, not latency, Unless the model is really too huge and slow (obviously the evaluation part of this paper only includes very small models with ms level latency). Because the inference speed can be easily increased by adding more GPUs. Since the inference job does not need to do synchronization, its is easily to achieve the linear scale out ratio. The paper just use a very arbitrary way to determine the constraints between these dimensions. How do you know that this is a good trade-off that researchers want?\n2. The design part is too brief. The first subsection just tell us \"We decide to build a constraint like this\", but why? The second subsection about speeding up the training is too ambiguous. Using some pseudo-code or program chart can help readers to understand your work's logic and innovations.\n3. The evaluation part is not solid enough to support this papers claim at the introduction. It claims to reduce searching space from 10^19 to 243. However, it still costs 50% training times compared to the original OFA method. And the training results also does not show apparent gains. The accuracy gain in most cases is lower than 0.4% while the overall accuracy does not exceed 80%. It will be better to add a classical light-weight CNN to be compared with, such as AlexNet, VGG or ResNet.\n\nOther Comments:\nThis paper's idea is quite creative and valuable that adding some constraints to the searching space will help us to remove many unnecessary models and improve the overall efficiency. However, the current design of the solution is not mature enough. It is necessary to add theoretical proofs for important design choices and convince readers that the choice is reasonable. And the evaluation part could also be improved by adding some classic models to compare."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the authors' addressing of concerns and adjusts the rating upwards to 7, indicating a positive shift in their overall assessment.",
            "The review acknowledges the paper's contribution (reducing search time and comprehensive experiments) but also expresses concerns about the limited scope and generalizability of the proposed heuristic. The overall sentiment is therefore mixed, leaning towards neutral.",
            "The review expresses overall positive sentiment, highlighting the validity of the paper's key observation and the impressiveness of the experimental results. While it points out a lack of significant technical novelty, the appreciation for the engineering and empirical work suggests a positive evaluation.",
            "The review expresses several weaknesses in the paper, including concerns about the design idea, lack of clarity in the design and training process, and insufficient evaluation to support the paper's claims. Phrases like 'not well addressed,' 'too brief,' 'too ambiguous,' and 'not solid enough' indicate a negative sentiment."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review acknowledges both strengths and weaknesses of the paper. While initially critical of the novelty and scope, the reviewer appreciates the authors' response and highlights the value of their insights and experimental results.",
            "The reviewer uses phrases like \"not particularly interesting\", \"narrow scope\", and \"I would like more evidence\" to express concerns and criticisms about the paper's contribution and generalizability. This indicates a critical tone.",
            "The review uses a balanced approach by pointing out both positive aspects (e.g., \"The key observation made in this paper makes sense to me\", \"the experiments of this paper looks good\") and negative aspects (e.g., \"the technical novelty of this paper seems to me very increamental\"). The reviewer also provides constructive suggestions for improvement, indicating a balanced and critical evaluation.",
            "The review adopts a critical tone by directly pointing out flaws and shortcomings in the paper's design, motivation, and evaluation. Phrases like 'The design idea and motivation of this paper is not well addressed,' 'The design part is too brief,' and 'The evaluation part is not solid enough' demonstrate a critical assessment of the work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently points out the limited novelty and scope of the paper throughout the review and post-rebuttal. While the reviewer's rating improves after the rebuttal, the core assessment of the paper's limited novelty and scope remains unchanged. The reviewer acknowledges the strengths and weaknesses in both stages of the review, and the post-rebuttal section shows a shift in emphasis towards the value of insights and experimental results rather than a contradiction of the initial assessment.",
            "The review consistently acknowledges the paper's contribution in reducing search time and maintaining model quality, while also expressing a consistent concern about the heuristic's novelty and generalizability. The reviewer appreciates the experimental results but questions the broader impact and scope of the proposed method, maintaining a consistent line of reasoning throughout the review without contradictions.",
            "The review is consistent because the reviewer acknowledges the practical strengths of the paper, such as good engineering and impressive experimental results, while also consistently pointing out the weakness in technical novelty and theoretical depth compared to OFA. The reviewer appreciates the empirical contribution but finds the technical advancement incremental, which is a consistent viewpoint.",
            "The review is consistent because the reviewer acknowledges the potential of the idea but consistently points out the lack of justification, clarity in design, and insufficient evaluation to support the claims. The strengths and weaknesses are logically aligned, leading to a coherent overall assessment."
        ]
    },
    {
        "paper_id": "iclr_2022_46lmrnVBHBL",
        "paper_title": "Explanatory Learning: Beyond Empiricism in Neural Networks",
        "paper_abstract": "We introduce Explanatory Learning (EL), an explanation-driven machine learning framework to use existing knowledge buried in symbolic sequences expressed in an unknown language. In EL, the burden of interpreting explanations is not left to humans or human-coded compilers, as done in Program Synthesis. Rather, EL calls for a learned interpreter, built upon existing explanations paired with observations of several phenomena. This interpreter can then be used to make predictions on novel phenomena, and even find an explanation for them. We formulate the EL problem as a simple binary classification task, so that common end-to-end approaches aligned with the dominant empiricist view of machine learning could, in principle, solve it. To these models, we oppose Critical Rationalist Networks (CRNs), which instead embrace a rationalist view on the acquisition of knowledge. CRNs express several desired properties by construction, they are truly explainable, can adjust their processing at test-time for harder inferences, and can offer strong confidence guarantees on their predictions.",
        "review_ids": [
            "QSeKKdyrFl",
            "nA2I3_8y34d",
            "qs3d7g_Lg-R",
            "JuWRoCKfuG2",
            "C-6LEcN_gZ0",
            "tqDYuVHUrFS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " > We believe that this problem cannot be escaped when we want to create a machine capable of formulating new hypotheses for unexplained observations.\n\nWe have fundamentally diverging opinions on this point.  Injecting our own knowledge is what we do with our offspring in order to be able to bi-directionally communicate with them.  I don't see any downside to injecting bias into machines if this makes them more introspectable/interactable.  Similar points are in a recent paper:\n\n  Kambhampati, Subbarao, et al. \"Symbols as a lingua franca for bridging human-ai chasm for explainable and advisable AI systems.\" arXiv preprint arXiv:2109.09904 (2021).\n\naccepted as a blue sky paper at AAAI'22, if I remember correctly.  I tend to agree with the points raised therein.\n\nMoreover, I find it hard to believe that:\n\n> injecting our own knowledge of the semantics of a language would lead to machines that generate symbols that are meaningful only to human onlookers, not to the machine itself\n\nIt is hard to prove that a machine is less likely to \"understand\" a human language than it is to \"understand\" an invented language -- especially given that we cannot directly access the semantics of the latter.\n\n\nRegardless, this (very interesting) discussion is independent of the quality of the manuscript itself -- which I think is good, except for the flaws that I pointed out already.",
            " Thank you for updating the paper and apologies for the very late response.  I appreciate the changes you made to the manuscript.\n\nSome final observations below.\n\n> these appear to the agent just as mute sequences of symbols, like Japanese strings to a non-Japanese speaker.\n\nI understand.  My point is that in real-world applications the language that the explanation is encoded in will always be a human language, say English or Japanese.  It is not clear to me what is the advantage of treating it as if it were an alien language when it clearly is not.  I mean, we don't have resources like Wikipedia or pretrained embeddings for Alienese, but we do have them for Japanese.  Why not use them?\n\n> Indeed, as the Reviewer acknowledges, our contribution is conceptual in nature. We aim to offer a unified perspective on recent works in AI [1, 2, 3] and work on defining the Explanatory Learning framework around the core idea of learning an interpreter from observations. Yet, these works show the large set of problems and applications that can be tackled by EL: such as image classification [1] or Math Word Problems in NLP [2, 3]. In all these problems the above works are the current SOTA.\n\nUnfortunately, I am not sure that I fully appreciate the leap forward.\n\n> Can the reviewer kindly elaborate on the semantics of the sentences implicitly supplied? Assuming that a semantics exists, how can it not be supplied at least implicitly in the dataset?\n\nRight, I stand corrected.\n\n> Yes, we absolutely agree that the terminology \u201cconcept\u201d and \u201cdescription\u201d would make sense. However, we prefer \u201cphenomena\u201d and \u201cexplanations\u201d for three reasons:\n\nI tend to disagree, for two reasons:\n- Explanations are often partial and/or approximate, while here the whole point is (approximately) guess a complete, exact description.  The supervision is complete and exact (albeit encoded in an unknown language).\n- Phenomena are linked to physics (and to subjective perception, for obvious reasons), which needs not be the case here.  \"Concept\" is a well-established term in AI and refers to all sorts of classes and groupings of elements.\nHence I still find the choice of terminology confusing and I estimate others in the ML community to face the same difficulty as I do.\n\n> Thank you for the detailed questions. The assumption mentioned at the beginning of the question is not actually necessary. We made it for simplicity, since in this way the goal is very clear\n\nThe text should be rephrased so not to mention this assumption or to explicitly lift it once it is no longer useful, otherwise it is simply confusing (it definitely confused me, after all).\n\n> The models discussed in the paper (, Emp-R, Emp-C) are all built through vanilla encoder-decoder transformers trained as usual. We do not add any intricacy except for the test-time prediction pipeline of the CRN that practically implements the rationalist knowledge acquisition process, which we describe step by step in the pseudocode algorithm in the left of Figure 3.\n\nThe reason why I pointed this out is that the lack of a proper formal description of the architecture in the main text hurts readability.  As a reader, I am always disappointed when I am forced to look elsewhere for a key piece of information -- it is a waste of my precious (to me) time.  I expect other readers to think the same.  I think that a formal description would benefit the main text.  Still, I appreciate the more detailed figure.\n\n> Please note that this operation is fully parallelizable, thus several conjectures can be tested in the same GPU batch. Tables 3 and 4 in appendix C display the computational cost and the execution time in seconds, which in Odeen remains in the same order of magnitude with respect to the traditional end-to-end approach with t=300 generated conjectures. Moreover, CRNs exhibit a parameter  at test time which controls the trade-off between computational cost and performance. As shown in the inset figure in the \u201cAdjustable thinking time\u201d paragraph in section 5, very cheap models with  or even  still express a significant gap in performance vs the traditional empiricist models.\n\nThis is good to know.  Still, this does not address the fact that inference -- compared to a common feed-forward architecture -- is much more involved.  I expect future developments of this idea to revolve around a more direct procedure.\n\n> This is true, our contribution is conceptual in nature, future works will be more tailored on applications. Yet, in this paper, we have pointed to existing works on real data that embrace our core idea of learning an interpreter purely from observations, such as CLIP [1] and the ones on Math World problems [2,3].\n\nI believe that a more clear link between CLIP and novel applications would help immensely to motivate the proposed framework.",
            "The paper considers the problem of classification learning where each data point is accompanied by some explanation. The explanation is in some arbitrary language. The paper proposes a way to increase the performance of the classification task by learning to predict the explanation associated with a given data point, and then using that explanation along with the data point to proceed to make a prediction about the data point's label. The paper implements this pipeline using neural networks, and presents empirical results on a new benchmark dataset to demonstrate its performance.  \n Strengths:\n\n- Acknowledges the bilateral nature of explanations in building explainable AI/ML.\n- Offers a new benchmark problem.\n\nWeaknesses:\n\n- Does not fully explicate the assumptions that it is making in terms of the explanations.\n- Does not properly position itself with respect to relevant work in the literature.\n- The somewhat over-the-top philosophical discussion distracts from the essential point.\n\nGeneral remarks:\n\nThe problem acknowledges the need to have bilateral explainability; i.e., build machines that can explain, by first explaining *to* the machines. This is discussed, for example, by Michael, \"Machine Coaching\", IJCAI Workshop on XAI 2019, which seems to be rather relevant to the current paper, especially given that the paper makes an effort to connect to learning theory. Another line of work that seems to be relevant is Mozina et al., \u201cArgument Based Machine Learning\u201d, AIJ 2007, where data points are accompanied by an argument (i.e., an explanation) on why they are labeled as they are, as is the case in the current paper. Ignoring the obvious difference from the current work that these two works assume that explanations are in logic, the underlying theme of the cited papers and the current paper seems considerably close to ignore.  \n\nI found the philosophical positioning of the paper as a study in empiricism vs rationalism to be somewhat far-fetched. I believe it is instructive to take a step back and seek what are the underlying assumptions of the paper, and whether those bring something new to the table. \n\nIt is clear that the label of a data point is a function of the data point and the phenomenon, and not only of the data point. And since the phenomenon is essentially associated with an explanation, then the label is a function of the data point and the explanation. What the authors propose, then, as the pipeline for predicting the label is a form of chaining of learned pieces of knowledge, where one predicts the explanation, and then using that previous prediction one proceeds to predict the label at a second cycle of inferencing. Of relevance here is the work by Valiant, \"Robust Logics\", AIJ 2000, and Michael, \"Simultaneous Learning and Prediction\", KR 2014, as well as some follow-up papers by the authors, that establish the benefits of chaining.\n\nFrom a formal point of view, there is nothing in the explanations that makes them explanations. As the paper says, they are simply strings. Is it really the case that they are arbitrary strings, or should they be strings that are learnable as a function of the data points (or more properly, sets of data points)? That is to say, if each type of explanation for a phenomenon P_j was simply replaced by the number j, would then this still be considered an arbitrary language of explanations, despite having no structure and not being learnable? If indeed, explanations cannot be arbitrary strings, then one needs to carefully state what are the underlying assumptions on the language of explanations. If, on the other hand, explanations can be arbitrary (e.g., explanation j for phenomenon P_j) then this makes the term \"explanation\" rather mood; it is simply another signal in the data (at a meta-level, see my comment below). \n\nAnother aspect of the paper is the two levels of learning problems that exist, as mentioned in the last two paragraphs: the object-level learning problem of learning from a data point and its explanation the label of the data point; and the meta-level learning problem of learning from sets of data points to predict the phenomenon/explanation. Both of these two problems seem to follow the empiricist view, in the sense of the paper, which, as I said above, makes it unclear what the philosophical discussion on empiricism vs rationalism really add to the picture. The conjecture generator seems to be simply a meta-level classifier (albeit a stochastic one). Which brings me back again to the question on whether the explanations need to have some learnable structure.\n\nAdditional points:\n\nThe metric of NRS seems to include in its definition the identification of a nearest neighbor. Why? Shouldn't the goal be to predict the actual explanation? If the algorithm that makes the prediction wishes to use the nearest neighbor to reach that decision, then that would be fine. But the use of the nearest neighbor seems more natural to be part of the algorithm that attempts to solve the problem, not part of the evaluation metric for measuring success.\n\nThe proposed approach to solving the problem seems not to be accompanied by any formal guarantees on its performance. This would typically be compensated by an extensive experimental section, which is not the case here. \n\nI would have found a different narrative for this paper to be more convincing and impactful: the introduction of Odeen as a benchmark problem, and a deeper discussion of its features and parameters, and then present the particular approach, properly placed in the context of relevant work, as a suggested direction for what type of systems would presumably be useful in tackling the Odeen benchmark. \n\n------- After the Author Rebuttal -------\n\nI acknowledge that the authors have made an effort to engage with the points that I raised, and I have increased my score. I believe that the connections with learning theory are much more deep than the brief remarks offered in the revised version of the paper, and I hope that the authors will consider exploring them further in their future work as a formal underpinning of their empirical work. \n I find Odeen to be a useful contribution, and one that would raise awareness on the need of certain underused techniques in the machine learning literature. But, the rest of the paper needs to be more clearly and properly placed in the context of existing work in the literature.\n",
            "The authors introduce a novel learning framework that revolves around learning a map between concepts and their description, where the latter are expressed in a language unknown to the learner.  Two learning problems are defined: (i) learning a new concept from a description of that concepts as well as examples and descriptions of other concepts, (ii) learn a new concept from examples of that concept as well as examples and descriptions of other concepts.  The underlying assumption is that the map (interpreter) from sentences to sets of examples is shared.  The authors propose an environment to evaluate these tasks and a neural architecture to tackle them.   - The paper is a pleasure to read.\n\n\n- The choice of illustrative examples is excellent.\n\n\n- The contribution is conceptual in nature, which is good.  Alas, the motivation for pursuing this research direction is unclear.  Specifically:\n\n1) It is not clear why the machine should estimate *both* the link between sentences and data and between sentences and sentences *jointly*.\n\n2) It is not clear why the lanugage would be unknown and unstructured.\n\n3) It is not clear what (new?) real-world applications this setup is meant to capture.\n\n(Please note that this setup does not solve the \"semantic gap\" problem, because the semantics of the sentences are implicitly supplied by whoever designs the data set.)\n\nI would advise the authors to ground their motivation in concrete conceptual problems and/or applications.\n\nGiven that the learning framework is *the* key contribution of the paper (the value of the two other major contributions depends on whether the learning setting makes sense), the fact that motivation is lacking/unclear is a serious concern.\n\nThis makes it hard to evaluate the significance of the paper.\n\n\n- The terminology used in the paper feels somewhat misleading/inappropriate.\n\nAs far as I can see, \"phenomena\" are simply *concepts* and the \"explanations\" are intensional *descriptions* thereof, except in a language unknown to the machine (but known to the annotator).  I do not understand what is gained by using this terminology.  Indeed, I find the latter confusing and I expect other readers to be confused by it too.\n\nIn particular, I do not understand the link between \"description\" and \"explanation\", especially considering the causal connotations of the latter term (which are becoming more and more clear as work on causality is being merged into AI and ML.)\n\n\n- The work seems to rely on a rather strict assumption.  In particular, the fact that D_0 should be discriminative for P_0 in L is (as far as I can see) unlikely to hold in practice -- especially considering that D_0 is supposed to contain a \"small set of observations\" -- and it simplifies the learning problem considerably.  In what applications is it reasonable for this asumption to hold?  Is it necessary?  What happens if it doesn't hold in practice?  How costly is it to acquire a D_0 that explicitly satisfies this assumption?  Given that such a D_0 would not be IID, how would this impact statistical learning of P_0?  These issues are touched upon in the conclusions, but they deserves an actual discussion.\n\n\n- The proposed architecture is reasonable but surprisingly involved and the details are hidden in the appendix.  It would be more straightforward to explain in detail the various pieces that make up the architecture (how many, what they take as input and what they spit out) from the get-go, rather than relying on Figure 3, which lacks mathematical precision.\n\n\n- Inferring whether an instance x satisfies a concept P is surprisingly involved as it requires to generate a (presumably large) number of candidate descriptions for P and then counting, for each description, whether x satisfies it.  Presumably this scales poorly with language complexity.  Is this efficient at all?\n\n\n- The experiments are limited to an interesting but entirely synthetic (actually, quite toy) setting.  For instance, as far as I can tell no sub-symbolic inputs are present.  Experiments with real data would have been useful to evaluate the efficacy of the proposed pipeline.  CRNs are compared only against two simpler baselines.  I realize that the focus of the paper is in its conceputal contribution, but a more varied selection of experiments would have been welcome.\n\n\nMinor issues\n------------\n\n- Wouldn't it be more natural to define the interpreter as a map from descriptions to classifiers (indicator functions)?\n\n- It would be good to disambiguate the term \"explanatory learning\" from previous uses of the same term, see \"Explanatory interactive machine learning\" AIES 2019.\n\n- The \"communication problem\" shares some aspects with multitask learning, especially so if the language is compositional and the tasks form a hierarchically (or can be related logically to each other, e.g., P0 is the negation of P1 etc.).  It also shares aspects with few-shot learning.  It may be worth highlighting the connection. Potentially great paper with unclear motivation/applications",
            "The paper proposes approaching learning a language as a learning problem that is grammar, alphabet, etc. agnostic. Such formulation has resulted in the so called Explanatory Learning (EL) framework that is paired up with an environment to test it. The starting point for learning a language is to extract an interpreter based on a given set of observation and their explanation and then use the interpreter to determine whether an observation belongs to the language, in a binary classification setting.  Questions: \n- In Figure 1, why the first left sequence is correct according to the rule? \n- When tagging unseen structures based on the secret rule discovered by the user, how many unseen structures are required to completely rule out other rules that are marginally different from the secret one. Is this number always 1176 in the current setting? \n- Since some figures don\u2019t have caption, it\u2019s hard to refer to them\u2026 In the figure on the lest side of \u201cMetrics.\u201d How the predicted vector is derived based on various rules? \n- As the authors point out the goal here is analogous to that of IPL. Do I understand this correctly that the main advantage of EL to IPL is skipping the translation of the data to logic? If that\u2019s the case, one can learn the translation itself and eliminate the difficulty. Can you elaborate on the advantage of EL over IPL?\n- Can\u2019t T-Acc be changes to something auc based instead that takes care of the permissively problem? \n\n\nOther remarks: \n- This is a language specific problem, where order of the sequence does not necessarily matters. The current title does not  reflect this. Perhaps something like Beyond empiricism in neural language learning or something alike would be a better suit. \n- Some approaches to explanatory AI that aim at generating explanations as well as predictions such as \u201cTED: Teaching AI to Explain its Decisions\u201d are missed here and should be commended on.  - Overall, I quite like the idea of the paper due to the generality of the approach, namely learning an interpreter from observation. \n\n- \u201cproving\u201d the discovered rule is indeed the secret one by examining it in practice (i.e., being used to tag unseen structure) as opposed to revealing the rule directly in text is also a very neat idea.",
            "The paper proposes a new framework for studying explanation driven machine learning problems called Explanatory Learning. The goal is to learn an interpreter model from explanations paired with observations for a particular phenomenon. The explanations might be in an unknown language, but the explanations paired with observations can be used to learn a good interpreter. Once learnt, the interpreter should be able to follow new explanation for an unseen phenomenon. They refer to this problem as the communication problem. They also define the scientist problem where explanations for the unseen phenomenon is not available. The paper also introduces an Odeen dataset to facilitate experiments with Explanatory Learning. The authors propose a neural network architecture as a solution to the scientist problem. It has two components: a conjecture generator which generates a set of candidate English explanations, and a second interpreter model. Experimental results show better generalization compared to end to end neural systems.   Strengths: 1. The proposed framework forces us to think about explanations and generalization as first class citizens. \n\nWeaknesses: \n1. The idea sounds very familiar to explanation based learning. It will be good to contrast the two.\n2. My main criticism is that I am not sure we need an entirely new framework for targeting explanation based solutions. Many recent work in NLP try to frame problems in English to get cross task generalization. I would prefer this paper to be positioned as an improvement to such existing approaches, as this is tackling a harder problem class. \n3. The experimental results are all on the new game like dataset. It will be good to see performance of proposed methods on real datasets, or already existing synthetic datasets. The baselines are also weak, so its not clear if the proposed techniques will perform better than other cross task generalization methods.   Although I am all for explanation based learning solutions, I don't think the components introduced in this paper warrants introduction of a new learning framework. I think it will be better to place it as an improvement to existing approaches for cross task generalization. I also found the experimental results to be weak. It was not clear if the proposed techniques will perform better than other cross task generalization methods.  "
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Neutral",
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses disagreement with a core idea but acknowledges the discussion is interesting and states the manuscript is good overall. Phrases like \"I tend to agree\" and \"very interesting discussion\" indicate a positive sentiment.",
            "The review expresses both positive acknowledgement of the authors' efforts and specific criticisms regarding clarity, terminology, and the leap forward of the work. The reviewer uses phrases like \"I appreciate the changes\" but also \"Unfortunately, I am not sure that I fully appreciate the leap forward\" and \"Hence I still find the choice of terminology confusing\".",
            "The review identifies both strengths and weaknesses of the paper, and while it offers constructive criticism, it also acknowledges the value of the benchmark dataset introduced by the paper. The final statement indicates a mixed evaluation, not strongly positive or negative.",
            "The review expresses significant concerns regarding the motivation, terminology, assumptions, and experimental validation of the paper. Phrases like \"motivation is lacking/unclear is a serious concern\" and \"makes it hard to evaluate the significance of the paper\" indicate a negative sentiment. The reviewer also points out several unclear aspects and potential limitations, contributing to the negative assessment.",
            "The reviewer states \"Overall, I quite like the idea of the paper due to the generality of the approach\" and highlights the neatness of proving the discovered rule through practical application. These positive statements indicate a positive sentiment.",
            "The review expresses concerns about the novelty and necessity of the proposed framework, stating it sounds familiar to existing concepts. It also criticizes the experimental results as weak and questions the improvement over other generalization methods. The overall tone suggests the reviewer is not convinced of the paper's contribution."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer expresses disagreement with the authors' core premise, stating \"We have fundamentally diverging opinions on this point.\" and \"I find it hard to believe that...\". This demonstrates a critical tone despite the overall positive sentiment.",
            "The review provides constructive criticism regarding specific aspects of the paper, such as terminology, the lack of a formal description of the architecture, and the need for a clearer link to existing works. Phrases like \"I tend to disagree\", \"I still find the choice of terminology confusing\", \"The text should be rephrased\", and \"hurts readability\" indicate a critical but ultimately helpful tone.",
            "The review uses phrases like \"does not fully explicate,\" \"does not properly position,\" \"somewhat over-the-top,\" and questions the validity of certain aspects of the paper, indicating a critical tone.",
            "The review adopts a critical tone by directly questioning the paper's core aspects such as the motivation, terminology, assumptions, and experimental setup. The reviewer uses phrases like \"it is not clear why,\" \"terminology used in the paper feels somewhat misleading/inappropriate,\" and \"the experiments are limited\" to express their criticisms. The reviewer also points out specific weaknesses and suggests improvements, indicating a critical evaluation of the work.",
            "The review presents both questions and remarks, offering constructive criticism alongside positive feedback. The reviewer asks clarifying questions and suggests improvements while also acknowledging the paper's strengths, indicating a balanced tone.",
            "The review uses phrases like 'My main criticism,' 'I am not sure we need an entirely new framework,' 'baselines are also weak,' and 'I don't think the components introduced in this paper warrants introduction of a new learning framework.' These phrases indicate a critical assessment of the paper's ideas and results."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in expressing disagreement with the paper's premise about the problem of injecting knowledge into machines. The reviewer provides arguments and a reference to support their opposing view, maintaining a consistent stance throughout the review.",
            "The review is consistent because the reviewer provides a series of constructive criticisms and questions regarding clarity, terminology, presentation, and motivation of the paper.  The reviewer acknowledges corrections and maintains a consistent focus on improving the paper's readability and impact without contradicting their own points.",
            "The review is consistent in its assessment, highlighting both strengths and weaknesses of the paper. The reviewer consistently emphasizes the need for better theoretical grounding, contextualization within existing literature, and a more focused narrative, while acknowledging the value of the proposed benchmark and the concept of bilateral explainability. The feedback remains consistent even after the author rebuttal, with the reviewer acknowledging the authors' efforts but reiterating the need for improvements in contextualization.",
            "The review is consistent in its critique, primarily focusing on the lack of clear motivation and real-world applications for the proposed framework. While acknowledging positive aspects like readability and examples, the reviewer consistently emphasizes the unclear motivation as a serious concern that undermines the significance of the conceptual contribution. The criticisms about terminology, assumptions, architecture presentation, and experiments all stem from or are related to this central issue of unclear motivation and practical relevance.",
            "The review is consistent because it expresses an overall positive view of the paper's idea and approach, highlighting its generality and neat aspects. While raising several questions and suggestions for improvement, these are framed constructively and do not contradict the positive sentiment. The reviewer's feedback is focused on seeking clarification and suggesting enhancements rather than pointing out fundamental flaws or inconsistencies in the paper itself or in their own assessment.",
            "The reviewer consistently argues that the core idea is not novel enough to warrant a new framework, suggesting it is similar to explanation-based learning and should be positioned as an improvement to existing cross-task generalization methods.  The reviewer also consistently points out the weakness of the experimental results and the lack of strong baselines to support the claims of the new framework."
        ]
    },
    {
        "paper_id": "iclr_2020_ByexElSYDr",
        "paper_title": "Fair Resource Allocation in Federated Learning",
        "paper_abstract": "Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q-Fair Federated Learning (q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q-FFL, we devise a communication-efficient method, q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q-FFL and the efficiency of q-FedAvg on a suite of federated datasets with both convex and non-convex models, and show that q-FFL (along with q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency.",
        "review_ids": [
            "rJeNTLRhFr",
            "BJg7OyyM9r",
            "SJxeQHzEqH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "In this paper, the authors propose a new optimization objective for fair resource allocation. Furthermore, a new algorithm, q-FedAvg, based on the vanilla federated learning, is proposed to solve the new optimization in massive and heterogeneous networks. The paper is well written. Theoretical analysis is also provided to support the effectiveness of the proposed methods. The experiments show good performance.\nIn overall, I think this paper solves an important problem in federated learning, and I vote for acceptance.\nHowever, since my knowledge in fairness is very limitted, I think my review is an educated guess. If the other reviews vote for rejection, I will not champion this paper.\n\nI have question to the authors:\n\nIs the proposed algorithm robust to the estimation of the Lipschitz constant? In my opinion, the proposed algorithm highly relies on $L_q(w)$. Thus, the estimation of L will be very essential. It will be better if the authors can show some results where different estimations of L  is used, and compare these results to show the sensitivity to the estimation of L.",
            "[Summary]\nThe authors propose a protocol to encourage a more fair distribution of the performance across devices in a federated setting. In contrast with previous work, which protects a specific attribute, this paper aims to achieve the uniformity of the accuracy distribution.\n\n[Key Comments]\nThe paper is well-organized and clearly written. The claims are well-supported by theoretical analysis and experimental results. However, my main concern is that the paper offers an incremental improvement over the early work FedAvg (McMahan et al., 2017). It would be helpful for the authors to summarize their contributions if space permits.\n\n[Details]\n[Pro 1] This paper provides insights into fairness (a more uniform accuracy distribution) in federated learning, which appears to be well-motivated.\n\n[Pro 2] This paper provides an instructive method to estimate the upper-bound of the Lipschitz constants for ??? the local objective function (the objective function with clients' data) ???. It is an interesting idea to choose dynamic step-size depending on the global Lipschitz constants and fairness parameter q.\n\n[Pro 3] The evaluation fully considers various uniformity metrics, sampling strategies, and the chosen of q.\n\n[Con 1] I am confused about the difference between the proposed method and Newton's method. It would be helpful for the authors to clarify the limitation of the objective function (for example, the objective function should be second-order derivable).\n\n[Con 2] The authors note that \"It is not straightforward to simply apply FedAvg to problem (2) when q>0, as the F_{k}^{q+1} term prevents the use of local SGD.\" I found it difficult for me to follow this argument. Is it relevant to the parameter q? Given the communication-efficiency improvement in Section 3.3, few explanations are provided for the main improvement over previous work. Is it because of the local updating? Otherwise, more details about the convergence rate will strengthen the submission.",
            "The problem of fairness in federated learning (FL) is important given the popularity of the topic and its immediate impact on the society. Vanilla FL approaches may be subject to poor performance for clients whose data is under-represented across all participants. This paper proposes a new algorithm for federated learning to reduce variance in performance across clients. The inspiration for the algorithm comes from the problem of uniform  resource allocation in wireless networks.\n\nWhile the problem and the motivation for the algorithm are interesting on the high level, I think this paper does not deliver the key ideas in sufficient detail and clarity.\n\nOn the algorithms side, I am still unclear on how the Lipschitz constant L is estimated on the first run with q=0. Are the results for q=0 in the experiments reported for this run or is it repeated with the learned L? Further, this procedure suggests that the number of communication rounds is at least doubled for the end-to-end training. Tuning q, which seems to be necessary, may require even more communication rounds.\n\nWhile there are a lot of experiments in the paper (across main text and supplementary), none seem to be carried out sufficiently well. Understanding the complete experimental setup for at least one of them is also quite hard due to numerous supplementary references throughout the experiments section. I would recommend to focus on fewer experiments, but present more thorough results. Below are some suggestions.\n\nThe importance of resource allocation in FL appears to me to be directly related to the key FL aspects such as degree of data heterogeneity and number of clients. This submission is lacking experiments comparing FedAvg to the proposed method under these settings (which can be simulated using available datasets). To argue in favor of the proposed approach it is important to demonstrate failure modes of the existing algorithms under some realistic scenarios and present a solution using new algorithm.\n\nAccuracies in Fashion MNIST and Shakespeare experiments seem quite poor suggesting some problems with the setup. FedAvg paper reports 54% on Shakespeare, whereas this paper reports 52%. It also appears that the number of considered \"devices\" on Shakespeare is significantly smaller than in the FedAvg paper (31 vs 1146) - what is the reason for this?\nOn Fashion MNIST, AFL paper reports 80%+ accuracy while achieving 90%+ on the combined dataset seems relative easy based on the results mentioned on the Github repository of the dataset. This paper reports 78% for the proposed method and AFL. Why is there a discrepancy with AFL paper and what is the performance of FedAvg on this dataset (assuming some suitable CNN architecture)? Is there a reason to believe that this dataset is much harder for federated learning than MNIST, where FedAvg roughly matches full data training?\n\nThis statement is ambiguous \"uniform sampling is a static method and can easily overfit to devices with very few data points, whereas q-FFL has better generalization properties due to its dynamic nature.\" If there is a device with very few data points it is easy to overfit to it and q-FFL will essentially ignore that device since the loss on this device is very small. Why does this not lead to more severe overfitting behavior?"
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states the paper is well-written, solves an important problem, and votes for acceptance. They also mention theoretical analysis and good experimental performance.",
            "The review expresses both positive aspects (well-organized, clear, supported claims, insightful, instructive method, thorough evaluation) and negative aspects (incremental improvement, confusion about method, unclear argument). This balance results in a neutral overall sentiment.",
            "The review expresses concerns about the paper's clarity, level of detail, and experimental setup. Phrases like 'does not deliver the key ideas in sufficient detail and clarity,' 'unclear,' 'none seem to be carried out sufficiently well,' 'lacking experiments,' and questions about discrepancies in accuracy indicate a negative sentiment."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer expresses overall agreement and advocates for acceptance (\"I vote for acceptance\"). They also frame their concerns as questions to the authors, suggesting a collaborative approach to improvement.",
            "The review provides both \"Pro\" and \"Con\" points, offering a balanced perspective. It uses formal language and constructive criticism, indicating a balanced tone.",
            "The review uses direct and critical language to point out flaws in the paper. Examples include questioning the methodology ('I am still unclear on how the Lipschitz constant L is estimated'), pointing out inconsistencies ('Accuracies in Fashion MNIST and Shakespeare experiments seem quite poor suggesting some problems with the setup'), and directly challenging statements made in the paper ('This statement is ambiguous')."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses an overall positive assessment of the paper, recommending acceptance based on its contributions and quality. While the reviewer acknowledges limitations in their expertise and expresses a conditional stance based on other reviews, this does not contradict their initial positive evaluation. The question raised is a constructive suggestion for improvement and does not undermine the overall positive tone of the review.",
            "The review is consistent as it highlights both the strengths (clarity, well-supported claims, insightful methods, comprehensive evaluation) and weaknesses (incremental improvement, confusion about relation to Newton's method, unclear argument about FedAvg limitation) of the paper. The reviewer provides constructive criticism and suggestions for improvement without contradicting themselves.",
            "The review is consistent in its critique, highlighting concerns about the clarity of the algorithm description, the insufficient rigor of the experiments, the lack of comparison with baselines, and potential contradictions in the paper's reasoning. The reviewer consistently argues that the paper needs significant improvements in these areas to be convincing."
        ]
    },
    {
        "paper_id": "nips_2021_9PexctnBali",
        "paper_title": "Dueling Bandits with Team Comparisons",
        "paper_abstract": "Lee Cohen, Ulrike Schmidt-Kraepelin, Yishay Mansour",
        "review_ids": [
            "Pz9sSgfMCd6",
            "7zXU_VlJYCb",
            "XKHkilNolh0",
            "jTMUQ-QC596"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper considers a variation of the dueling bandits problem where teams of arms compete against each other rather than individual arms. The paper considers two different models for team-wise comparisons: (1) stochastic where each competing team wins with certain probability and the better team wins with probability >= 1/2; (2) deterministic where the better team always wins. This paper works in the pure exploration setting as the goal is to find the Condorcet winner rather than minimising regret. For the deterministic setting the paper shows that any solution for the classic dueling bandits problem can be used to solve the team-wise dueling bandits problem with bounded number of duels depending on a \\emph{gap} parameter. For the deterministic setting the paper gives an algorithm to find the Condorcet winner with a bounded number of duels with no dependence on gap.\n\n\n\n  1. Real world applications: The paper should expand a bit on the real-world applications where only team comparisons are possible. In my opinion the applications that suit this problem are based on team sports/gaming as one can only observe team comparisons here. In many other cases such as advertising one can even compare individual arms to each other which might be better from a sample complexity point of view.\n\n2. Consistency Assumptions: This assumptions roughly states that there is an ordering among arms, and if a is better than b then adding a to  any team S results in a better team than adding b to S. In my opinion this assumption destroys the essence of team comparisons as one can expect that player a can be more compatible with players in S, and b can be more compatible with players in S'. Hence, adding a to S will result in a better team, whereas adding b to S' will result in a better team. If there is such a linear ordering amongst teams, then why can't we just compare arms a and b directly based on skills etc? It would be easier to make this assumption if there is a real-world validation based on experiments. \n\n3. Dependence on \\Delta and computation complexity: In the stochastic setting the algorithm is essentially searching for a witness among O(n^k) potential witnesses and if there are a few witnesses (which may likely be the case) then it can take O(n^k) time and samples. However, this detail is hidden away in the definition of \\Delta (addressed briefly in Section 5). The paper mentions that the \\Delta in this setting is similar to the \\Delta in dulling bandits. I would argue that this is not the case as \\Delta in dueling bandits is a natural parameter and can be even constant, whereas \\Delta here is a result of the reduction to dueling bandits and will likely have a polynomial dependence on n.\n\n4. Experiments: The paper would benefit greatly through experimental validation of the theoretical results on synthetic/real data and comparisons to a few baselines. Understanding the dependence of \\Delta on n through experiments would also be helpful. The paper addresses societal impact in the broader impact section. ",
            "This paper studies a variant of dueling bandits with teams. There are n players, and your goal is to select the \u201cbest\u201d team of k of these players (more specifically, you want to select a team of k of these players that doesn\u2019t lose to any team of size k that you can form from the remainder of the n-k players). Each round you can pick two teams of size k, watch them compete, and see who wins. The probabilities that one team beats another are chosen in such a way to be consistent with an underlying ordering of the players, in addition to satisfying some other assumptions (e.g. strong stochastic transitivity).\n\nThe authors study this problem in both the stochastic setting and the deterministic setting (where all the probabilities are guaranteed to be 0/1). In the stochastic setting, they can identify an optimal team whp with O((n + klog(k))/Delta^2), where Delta is some notion of \u201cgap\u201d between the performance of the kth player and the (k+1)th player. They accomplish this by first showing how to compare two players (roughly, you do something like pick a random S of size (k-1) and compare S + {a} to S + {b}, but something a little more complex than this). They then use this to reduce the problem to the well-studied problem of top K identification under pairwise comparisons. \n\nIn the deterministic case (where Delta is usually 0, so the previous bounds don\u2019t apply), they give an algorithm which identifies a \u201cbest\u201d team with O(kn log (k) + 2^{O(k)}) comparisons. Furthermore, when the order is an \u201cadditive total order\u201d (i.e. generated by just adding values corresponding to specific members), they give an algorithm that only uses O(kn log(k) + k^5). These algorithms are complex combinatorial algorithms -- the main idea is showing that it is possible to identify useful comparisons somewhat quickly (especially under additive orders).\n  \nI think the problem of team selection in this sort of dueling bandits framework is an interesting practical problem (as the authors mention, it shows up in many different settings, especially in sports / games), and I think interesting progress on it would be welcome at NeurIPS. I also think that this paper is quite technically impressive (mainly the algorithms for the deterministic case). I do have some qualms about this paper, however.\n\n- One issue I have is that the Delta gap in the bound of the algorithm in the stochastic case is very \u201ctailored\u201d to the specific algorithm they present. What I mean by this is that, usually when gaps appear in regret bounds, they correspond to some semantically meaningful notion about the test data (e.g. the gap in expected reward between the best and second-best solutions). This Delta is somewhat connected to the gap between the kth and (k+1)th players, but the definition feels not at all natural. Is there a good reason to care about this specific choice of Delta?\n\n- The analysis of the deterministic case is definitely a combinatorial tour-de-force, but I think 1. it is unlikely to be applicable in practice (even the stochastic case has pretty strong assumptions that prevent it from being applicable in practice) and 2. I don\u2019t really see the result or techniques in the analysis proving useful in other areas / for other problems (they are very tailored to this setting). These facts combined make me concerned how interesting the deterministic results will be to the NeurIPS audience.\n\nThe paper was well-written and easy to read. \n No concerns here.",
            "The paper introduces a new problem in which a learner can choose k disjoint teams from a set of n players and observe a winner. The objective is to pick a team that would win against any other team with probability at least 0.5 (Condorcet winner) and to do with a minimal number of duels. Deterministic and Stochastic settings are considered and upper bounds on the number of duels are obtained.    -The problem seems relevant and an interesting generalization of the duelling bandits setting. Further, various settings are considered and the solutions seem well-thought out. \n\n-Can the notion of the Condorcet winner be generalized? i.e. winning against any other team with probability 3/4 or in general p>= 1/2? How would that look like and affect the algorithms?\n\n-Isn't the problem statement restrictive, specifically the learner is choosing teams of k players. Suppose instead that the learner can choose teams of up to k, would this change things significantly? This would enable pairwise relations between single players.  \n\n-Is the consistency assumption realistic? Since it ignores the interaction between different players. E.g., we might have a>b, but if c complements b much better than a, we could have {b,c} > {a,c}. \n\n-Although there are significant differences, I believe the problem of communications complexity in stable marriage could be relevant, see e.g. Gonczarowski et al \"A Stable Marriage Requires Communication\". I wonder if the authors see any connections or applications of their techniques especially from the deterministic setting, etc.  \n\n-Lack of experiments: Even for work that is heavily theoretical, there is usually an experiments section.  I think limitations/societal impacts were adequately addressed. ",
            "A new dueling bandit framework with team comparisons.  This paper presents an interesting and non-trival generalization of dueling bandits by considering the duel between teams rather than single player. I can see this is a solid work with new framework and insights but the current presentation needs improvement and makes me feel hard to read.\n\nThe intro and Section 2 are generally good. The description of the problem is clear to me. One question is about the consistency assumption. Is that kind of an implicit gap assumption on the performance of a single player? That means for play 1 should be much better than other players such that no matter what teammates it has, this team is always better. This should be not required for standard dueling bandits? One minor is Line 64, there is no definition of set S.\n\nI can understand what is included in Section 3 but it's better to clearly state why we need this. I think a standard flow might be stating the problem, defining performance metrices, having the algorithm and proving the theorem.  \n\nSection 4 starts to confuse me. Do you propose new algorithm eventually? Becuase you write any dueling bandit algorithm for top k identification can solve your problem. In orignial dueling bandits paper, they properly define the measure of regret. But in this paper, there is no such measure. I think it's better to comment on that. \n\nSection 5 is not well-organized. It's pretty long and dense. And it's hard to get the key point. Will not determinstic setting be a special case of stochastic setting? There are too many newly defined concepts in this section. If you have new algorithms, it's better to have an algorithm box here. And it looks like the algorithm and theorem always mix together.\n\nNo conclusion is included.    no"
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses several concerns about the paper, including the lack of real-world applications, the restrictive consistency assumptions, the computational complexity, and the absence of experimental validation. These concerns suggest a negative sentiment towards the paper's current state.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges the interesting problem and technical impressiveness, but also raises concerns about the practicality and generalizability of the results. The overall sentiment is therefore neutral.",
            "The review starts with positive feedback, stating the problem is 'relevant and an interesting generalization' and the solutions seem 'well-thought out.' While it raises several questions and suggestions, the overall impression is positive.",
            "The reviewer expresses difficulty in understanding the paper, stating \"makes me feel hard to read\" and \"Section 4 starts to confuse me.\" They also point out organizational issues and lack of clarity, indicating a negative assessment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"destroys the essence of team comparisons\", \"I would argue that this is not the case\", and \"The paper would benefit greatly\" which indicate a critical tone. The reviewer also questions the assumptions and the justification for certain design choices, further reinforcing this tone.",
            "The review presents both positive and negative feedback, using phrases like \"interesting practical problem,\" \"technically impressive,\" but also \"some qualms,\" \"unlikely to be applicable,\" and \"concerned how interesting the deterministic results will be.\" This balanced approach indicates a neutral and balanced tone.",
            "The review offers both positive comments ('relevant and an interesting generalization', 'solutions seem well-thought out') and critical questions/suggestions ('Isn't the problem statement restrictive', 'Is the consistency assumption realistic', 'Lack of experiments'). This balanced approach indicates a fair assessment.",
            "The review uses phrases like \"needs improvement,\" \"starts to confuse me,\" \"not well-organized,\" and \"hard to get the key point.\" These are direct criticisms of the paper's presentation and structure."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because all points raised by the reviewer consistently express concerns about the paper's assumptions, real-world applicability, and lack of experimental validation. Each point builds upon the general critique, questioning the motivation, assumptions, complexity, and practical relevance of the proposed approach. There are no contradictory statements or conflicting recommendations within the review. The reviewer consistently argues for more justification, real-world examples, and empirical support to strengthen the paper.",
            "The review is consistent because the reviewer acknowledges the paper's strengths (interesting problem, technically impressive algorithms, well-written) while also raising specific concerns (unnatural Delta gap, limited practical applicability and generalizability of deterministic results). The reviewer's critique is balanced and does not present contradictory viewpoints.",
            "The review is consistent as it starts with acknowledging the paper's strengths and then proceeds to raise constructive criticisms and questions for improvement. There are no self-contradictory statements, and all points logically contribute to a balanced assessment of the work.",
            "The review is consistent in pointing out areas of improvement needed in the paper, mainly focusing on clarity, organization, and presentation, while acknowledging the potential of the work."
        ]
    },
    {
        "paper_id": "nips_2022_zzDrPqn57DL",
        "paper_title": "BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework",
        "paper_abstract": "Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7% to 28.9% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. ",
        "review_ids": [
            "Hd6Ce57Ircd",
            "uVn6Us0aP_",
            "HNAOCsCSzZw",
            "B8g021IaaAm",
            "VrENUT1h6-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " The authors propose a method to fuse two source of information for BEV detection namely multi-view images and LIDAR data in such a manner that any data defects in one source of information do not effect the network of the other method. Previous methods have combined the information from the two sources at different stages of the network pipeline, but they are prone to getting effected in the inference results when the data is corrupted from either source. This paper delays the combination of information to even later part of the pipeline thereby\nmitigating the effect of bad/corrupted/unavailable data. They do so by generating a pseudo BEV point cloud just from multi-view cameras and combining that information with LIDAR BEV. The combination part is based on a dynamic fusion method which selects important fused features be it from camera based BEV or LIDAR based BEV. The results are shown where missing data in the LIDAR or camera image do not effect the detection unless both are missing for the same object in the scene e.g. car. Strength:\nThe paper addresses a problem which could be a real life problem in LIDAR or image based data capture, where the LIDAR data is missing due to 3D scene material/reflectance properties or the image could be missing from a video stream. These problems can cause existing networks to fail. This paper addresses this problem which can make the commercial deployment of such systems more doable. The paper is well written with clear explanation of the previous work. The results are detailed and show scenarios where they are better than previous best results in challenging situations.\n\nWeakness:\n1. The dynamic fusion module should be explained in more detail as its one of the contributions of this paper. It should be explained with a scenario where data is missing from either of the streams and how the formulation in Eq.1 and Eq.2 will still be able to select the BEV information which exists to feed the final detection result.\n2. Citations 44-45, 32-33, 57-58 are repeated citations. Please fix it.\n4. grammar: #92 start->started, #3 discover->discovered For training the camera stream, a camera based BEV 2D point cloud is created. How is this camera stream trained as this may require a ground truth with camera based BEV 2D points and detections on that?\n The reviewer didn't find any major limitations. The authors could discuss and show some failure cases of their work.",
            " This paper introduced a method for point cloud object detection based on LiDAR camera fusion.  The main contribution of this method is the fusion framework that combines the camera and Lidar stream. This fusion module is very simple because it mainly consists of the concatenation of LiDAR and camera streams and a typical feature selection with an average pooling and 1x1 convolution. The results show that this method slightly outperformed the other method for the comparison. Moreover, the robustness against camera or LiDAR malfunctions is shown in the results. The ablation study shows that each module employed in this method improves performance. \n Strength\n- The performance is slightly improved. \n- The methodology is very simple. \n\nWeakness\n- Considering the small performance improvement and the simple methodology, I would think that the contribution of this method is relatively limited.  I am wondering why this simple fusion method is better than the other methods compare in this paper. If there are some results and discussions that make sense would be helpful for the readers.  I would suggest showing some failure cases and discussions about them because it will contribute to the community.\n",
            " The paper proposes a framework for 3D detection from RGB and LiDAR inputs in autonomous driving scenes. The pipeline includes separate networks reasoning from RGB and LiDAR inputs independently, and uses a fusion network for refined detection when both sources are available. Also the paper considers situations of data corruption and proposed to boost the robustness in the model design. The paper is the first to identify and evaluate the problem that most existing methods do not consider situations where one or both sources are unavailable, and proposes a pipeline customized for this situation. The proposed method is evaluated in the standard settings of object detection and compared with baseline methods both qualitatively and quantitatively. Strength:\n\n[1] The task identification. As mentioned above, the paper is the first to identify the issue within the current literature and models, and proposes a pipeline accordingly which reasons from two sources independently and thus more robust when data unavailability occurs. In this sense, the task identification itself is valuable to the community in defining and bringing attention to the task.\n\n[2] Extensive design choices and evaluation. Although the proposed pipeline is mostly based on existing methods, the paper is able to evaluate various design choices to demonstrate the flexibility of the proposed framework, as well as provide extensive evaluation into the results, yields SOTA results with both sources, and robust result when only one is available.\n\nWeakness:\n\n[1] Novelty and model design. The paper is novel in identifying the problem, which is legit and valuable. However for the proposed method itself, it is mostly a combination of existing methods utilization single sources without much modification, thus diminishing the merit of the proposed framework. Also the design to handle one or two sources in the framework is naive, basically running the first stage network only if only one source is available, and running both stages when two are available. A more sophisticated design could be, when for example camera stream is dropped for a few frames, is there a chance to stick to the fusion detector, but utilizing temporal information to compensate the missing RGB data, instead of simply drop the RGB branch and the fusion, running the LiDAR branch alone, which will likely result in a sudden drastic change to the detections?\n\n[2] Simulation for data corruption. The paper proposes to augment the data to simulate possible data corruption scenarios, via dropping points and limiting FOV. However more effort can be done to boost the robustness: e.g. looking for real driving sequences in extreme weather or with bad data, and train/evaluate on those data. Please see the points in the Weakness section above. N/A",
            " Most existing camera-lidar fusion work decorates lidar points with image features and then performs detection in 3D/BEV space. This work leverages recent Lift-Splat-Shoot work for cameras, which allows one to map both camera and lidar inputs to BEV space, before fusing and applying the detection head.  Strengths: \n- The proposed idea and its realization makes sense, and I am not aware of such published work (even though there seems to be concurrent similar work, since this seems a logical next step given the existence of LSS [52]). \n- Details in the model seem well thought out. This include the extensions to LSS (Dual-Swin-Tiny architecture, ADP), as well as the layers in the dynamic fusion module. \n- The experimental results show that this work is close to SOTA on nuScenes and that it affords significant model robustness in the case of lidar information missing compared to existing methods. \n- The model details are pretty clearly explained. \n\nWeaknesses: \n- Related work section is confusing in a few places and can be streamlined further. Examples: \n1) The Camera detectors section contains a discussion of PointPillars, which is a purely Lidar method. \n2) Range images are not really Euclidean space (see line 88) \n3) 89: \"Recently, people start to exploit these two feature modalities to increase the representation power\" --> There is earlier work to do this, if I understand correctly the statement. E.g. [5] from the paper, or End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds, by Yin Zhou et al, CoRL 2019. \n4) 90: \" Another line of work is to exploit the benefit of the bird\u2019s eye view plane similar to the camera perception\" --> a lot of this work came before camera started exploiting the BEV view. \n\n- Intuitive explanations are lacking in a couple of instances: \n1) Work does not explain the intuition why the model needs to be trained in two stages. What happens if it's trained in a single stage? \n2) 14: \"Note that we do not conduct data augmentation when multi-view image input is involved, while data augmentation plays a critical part in other cutting edge methods.\" Is this a limitation of camera fusion methods in general or something specifically lacking in your case? Can you please clarify? \n\n- It is unclear whether the approach is SOTA on nuScenes or not. Can you please explicitly contrast your performance relative to the nuScenes leaderboard (at least for the published approaches). When exploring that leaderboard myself, I see mentions of a method called BEVFusion that is SOTA but seems to be a different method? Assuming that method is different and already on the leaderboard, your naming may be confusing / too generic. \n\n- nuScenes is a dataset with particularly poor lidar (compared to other public datasets, such as Waymo Open Dataset, Argoverse2.0 etc). Results on at least one more dataset with high quality and longer-range lidar are highly desirable. The core issue of missing lidar points may be a lot less pertinent for more modern lidars. Also, as range increases beyond ~40m to 70-200m, the approach here may actually underperform lidar-painting approaches, since BEV view can start containing errors > 10m in the camera case making fusion in BEV space difficult. To this effect, analysis of the method performance as a function of object distance, relative to SOTA fusion methods for long distances will help. \n\n\nLanguage: \nThere are minor language issues and typos in the paper, it would benefit from another proofreading pass. \n - Is the approach SOTA on nuscenes or not?  Can you please explicitly contrast your performance relative to the nuScenes leaderboard (at least for the published approaches). \n\n- Can you provide an analysis of performance as a function of distance to object, and compare to a standard lidar approach and TransFusion/DeepFusion etc? \n\n- Can you please provide results on at least one more dataset with high quality Lidar such as the Waymo Open Dataset? \n\n- What is the latency of the approach and how does it compare to the baselines?  See comment on weaknesses. Some core potential limitations of the existing method have not been fully explored. \n\nMy current rating is predicated on the assumption that a similar idea has not been published yet (not completely certain) and that I will receive reasonable responses to my questions. ",
            " Towards the problem of current methods tend to fail at situations where hardware malfunctions, this paper presents a simple yet effective LiDAR-Camera fusion framework, namely BEVFusion. By disentangling camera pipeline from LiDAR network and using a dynamic fusion module, BEVFusion achieves SOTA performance and shows robustness against LiDAR or camera malfunction at the same time. An effective modification on the camera pipeline is also proposed to boost the final performance. ## Strength\n1. The paper is well written and easy to read.\n2. Robustness of autonomous driving algorithms should be paid more attention to. This paper raises the issue and makes the attempt to addressing it.\n3. Thorough experiments are performed. Claims are well-supported. SOTA performance is achieved on both normal and robust settings of nuScenes.\n4. The clean design of the framework makes it easy to use any camera or LiDAR framework.\n\n\n## Weaknesses\n1. It is nice to see a simple yet effective module (dynamic fusion module) being proposed. But it would be nicer to provide some insights and analysis into the design itself. For example, by analyzing how would the fusion module work when facing incomplete LiDAR or camera inputs, we might gain some insights into the module design of CSF and AFS.\n2. The experiments section does not provide runtime analysis, like inference time and memory footprint, and its comparison with other methods. 1. How does the baseline method in Table 7 fuse the features?\n2. This does not affect my rating to the paper. Just out of curious, would BEVFusion still works when facing both camera and LiDAR malfunction? The potential negative social impact is well discussed. But the limitation should be discussed more. For example, would the late-fusion style misses the opportunity to fuse intermediate LiDAR and camera features, and thus makes the pipeline suffer potential performance drop?"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses overall positive feedback, highlighting the paper's strengths, such as addressing a real-life problem, clear writing, and detailed results. While weaknesses are identified, they are presented as areas for improvement rather than fundamental flaws.",
            "The reviewer expresses concern about the limited contribution of the method due to the small performance improvement and simple methodology. Phrases like \"relatively limited contribution\" and questioning \"why this simple fusion method is better\" indicate a negative sentiment.",
            "The review acknowledges the paper's strengths, such as identifying a valuable task and providing extensive evaluation, while also offering constructive criticism. The overall tone suggests a positive sentiment towards the paper's contribution despite its weaknesses.",
            "The review identifies both strengths and weaknesses of the paper. While acknowledging the novelty and well-thought-out details, it also points out significant areas for improvement, such as related work clarity, intuitive explanations, benchmarking, and dataset diversity. The final rating depends on whether the reviewer's concerns are adequately addressed.",
            "The review expresses overall positive feedback, highlighting the paper's strengths such as being well-written, addressing an important issue, thorough experiments, and achieving SOTA performance. The reviewer appreciates the clean design and effectiveness of the proposed framework."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review adopts a balanced tone by providing both strengths and weaknesses of the paper. It uses formal language and constructive criticism, suggesting improvements without being overly critical. Phrases like 'The paper is well written' and 'The results are detailed' indicate a positive assessment, while specific points for improvement are raised in a professional manner.",
            "The review adopts a critical tone by questioning the significance of the contribution (\"relatively limited contribution\"), expressing doubt about the method's superiority (\"I am wondering why this simple fusion method is better\"), and suggesting improvements by showing failure cases. The language used is direct and points out weaknesses.",
            "The review presents both strengths and weaknesses of the paper in a clear and objective manner. It uses phrases like 'valuable to the community,' 'extensive evaluation,' but also points out 'mostly a combination of existing methods' and suggests 'more effort can be done,' indicating a balanced assessment.",
            "The review provides constructive criticism by pointing out specific weaknesses in the paper, such as confusing related work, lacking intuitive explanations, unclear SOTA status, and dataset limitations. The reviewer uses direct questions and suggestions for improvement.",
            "The tone is balanced as it acknowledges both strengths and weaknesses of the paper. While praising the paper's contributions, the reviewer also raises constructive criticisms regarding the lack of insights into the fusion module and the absence of runtime analysis. The reviewer's questions are posed politely, indicating a balanced and constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it highlights both the strengths and weaknesses of the paper in a balanced way. The reviewer appreciates the paper's contribution and novelty while pointing out areas for improvement, mainly focusing on the clarity of the dynamic fusion module and minor editorial issues. There are no contradictory statements or conflicting opinions within the review.",
            "The weakness is derived from the strengths mentioned. The reviewer acknowledges the slight performance improvement and simplicity as strengths, but argues that these are not significant enough to constitute a strong contribution. This is a consistent line of reasoning.",
            "The review is consistent because it praises the problem identification and evaluation while constructively criticizing the novelty of the method and suggesting improvements for robustness. The strengths and weaknesses are logically separated and do not contradict each other. The reviewer maintains a consistent tone and perspective throughout the review.",
            "The review is consistent because the weaknesses and questions raised are logically connected to the strengths and overall assessment. The reviewer provides specific examples and justifications for their points, and the tone is constructive and aimed at improvement. There are no self-contradictory statements or conflicting feedback within the review.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper without any self-contradiction. The strengths praise the paper's clarity, relevance, experimental validation, and design. The weaknesses point out areas for improvement, such as providing more insights into the proposed module, adding runtime analysis, and discussing potential limitations. These points are all valid and do not contradict each other, forming a balanced and constructive review."
        ]
    },
    {
        "paper_id": "nips_2021_0OWwNh-4in1",
        "paper_title": "Towards Lower Bounds on the Depth of ReLU Neural Networks",
        "paper_abstract": "We contribute to a better understanding of the class of functions that is represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning tasks. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). This problem has potential impact on algorithmic and statistical aspects because of the insight it provides into the class of functions represented by neural hypothesis classes. However, to the best of our knowledge, this question has not been investigated in the neural network literature. We also present upper bounds on the sizes of neural networks required to represent functions in these neural hypothesis classes.\n",
        "review_ids": [
            "nJBarqGmGxH",
            "1vaOCXZgOjD",
            "WzLogsS8edw",
            "VOqDTwmRQNE",
            "nt_vgvy_CNv",
            "FgVt6Vp89Zf",
            "vsU9XC1SBFL"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for the response. I still am not convinced that the results in this work are strong enough and I choose to keep my score the same.",
            "EDIT: After reading authors' responses, I decided to keep my score as is.\n\nThe paper studies the problem of exact representation of continuous functions via neural networks with the ReLU activation. The literature on the approximation power of neural nets is large, however the paper here delves into the less studied question of whether the class of *exactly* representable functions strictly increases when adding more layers (with no restrictions on size).\n\nThe authors want to understand the function classes exactly represented by different architectures and a step towards this direction is to analayze the class of functions captured by a depth-d neural net (without width constraints) and how this class of functions changes as we get to depth-d+1 neural nets.\n\nIt is obvious that ReLU nets will output continuous piecewise linear functions (CPWL for short) and a non-trivial fact from previous works is that $\\log(n+1)$ hidden layers suffice to represent *any* CPWL in n dimensions, via a ReLU net. Let d is a parameter for the depth and and ReLU(d) is all functions representable exactly via ReLU nets of depth at most d. The paper tries to understand ReLU(d) as d goes from 0 to $\\log(n+1)$.\n\nThe authors put forth two equivalent conjectures about the relations between the class of functions ReLU(d) for different d. Conjecture 1.1 states that every additional layer will indeed be substantial in terms of the representational capabilities of ReLU nets up to $d\\le \\log(n+1)$ of course, at which point all CPWL are representable. The authors reformulate this with Conjecture 1.2 that is a simple statement about max functions. \n\nThe authors then show a special case of Conj. 1.2 that corresponds to showing that the max function on 5 variables cannot be representted with 2 or 3 hidden layers, with the caveat that they need a certain assumption on the breakpoints of the function represented by any intermediate neuron. Along the same lines,  the authors show that the class ReLU(k) contains more functions than just taking the max on $2^k+1$ variables. To achieve this they use the theory of polyhedral complexes associated with CPWL functions.\n\nFinally, the authors find upper bounds on the sizes of the networks needed for expressing arbitrary CPWL functions with p linear pieces, as given in Theorem 4.4, which basically involves depth O(logn) nets with width growing as $p^{n^2}$.\n\n\n  I like the overall theme of understanding the exact representation capabilities of neural networks, escaping the traditional approximation theory viewpoint. As most of the traditional approximation theory results, rely on large widths to approximately construct step functions to approximate a given function, all these techniques cannot work here and new ideas are necessary.\n\nThe main ideas in the paper involve how to relate the max functions with output of ReLU nets. Both conjectures provided in the introduction are very plausible (equivalent of course as the authors show) and the authors take some small first steps towards understanding them.\n\nOne of the main results is that \"there does not exist a 3-layer NN\" to compute max on 5 variables. One quick observation here however is that following the notaion in Conj. 1.2 I think there is a shift by +1 that is not correct (is k=2 or k=3 for the statement?). One annoying thing with this result is that it involves a somewhat technical condition on the breakpoints of intermediate neurons which I can't see how to prove. Furthermore, I believe the result is useful but not very surprising. It basically means that computing the maximum between inputs somehow requires a lot of depth in some sense. \n\nThe next result is about ReLU(k) being a superset of the class of max functions on $2^k$ variables. To establish this they use a specific construction involving max functions and compositions between max functions that can be written as a ReLU net but not as a max function on several terms. Although the proof is somewhat complicated, still the result seems not as surprising.\n\nThe perhaps more interesting part has to do with Section 4 where the authors derive upper bounds for the size of the net to be able to represent CPWL functions.\n\nOverall, the motivation of the work is interesting from a theoretical point of view, however the results presented are quite weak. Section 2 relied on a technical assumption and only proves a very special case of the (plausible) conjecture 1.1, Section 3 is a comparison between very special class of functions. Section 4 has some interesting techniques. I like that the authors have identified the relation to max functions and have built several ways to analyze the exact representation capabilities of NN, however I believe more and stronger results are necessary to make this a solid contribution.\n\nQuestions/Future directions to the authors:\n- Could most of these results also be stated for recurrent neural networks? As far as I know several important approximation theory results (e.g., Telgarsky's \"Benefits of depth in neural networks\") can also be viewed for RNNs instead of feedforward nets, and having the analogous theory for RNNs would be interesting, and can strengthen the overall message of your paper.\n- To get the separations for the different depth levels and also to separate ReLU(k) from MAX(2^k), the authors identify the max function as a \"source of complexity\" in some sense. In particular Proposition 3.2 relies on max of max functions. Why not taking this to the extreme? Specifically, is there another way of using repeated compositions of max functions with themselves in order to get a \"sufficiently complicated\" function? Notice that works that have exploited this repeated compositions trick include the seminal paper by Telgarsky and also follow-ups that extended Telgarsky's results to much broader family of functions using characterizations from dynamical systems (e.g., \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\"). The \"source of complexity\" in that case came from oscillations in the input, and not max alternations between the inputs as in your case. \n- Another interesting direction would be to get also tradeoffs for the depth/width or other aspects of the architecture needed to exactly represent certain functions. This again will be a counterpoint to many existing approximation theory results that derive depth/width tradeoffs instead of just approximability results. none",
            " I thank the authors for their reply and explanations. \nOn (iii), I apologize for the imprecision in my comment on the limitations (and a possibly subjective evaluation). I meant to argue that, next to the bound on the depth, the additional knowledge of the required width in order of the number of linear pieces itself does not provide much significant insight, since it is unclear how sharp the bound is (probably not sharp, I suppose).",
            "This paper studies the role of depth in exactly representing real functions by ReLU networks. \nUnlike the common ML setting, the focus is on neural nets that are equal in every point to the target function. \nSome partial results are given as well as a conjecture that networks of depth k+1 have strictly more expressivity than networks of depth k.    Pros: \nIt is an interesting question to study expressivity in the interpolation setting where pointwise equality is required. The main conjecture (that depth k+1 is strictly larger than depth k) is of interest and the partial results serve as a good intro to the kind of polyhedral theory that is likely to be involved in these problems. The paper is generally well written and conveys the questions well (although some improvements are possible: see below). I agree with the authors that  finding connections between well develop fields such as polyhedral combinatorics and integer programing to neural networks could yield interesting insights. \n\nCons: \nThe requirement for equality in all points is very strong and less useful in machine learning. One issue is that it is easy to come up with simple functions that cannot be represented in this model by a finite network. In general, it is easy to obtain strong lower bounds by considering functions with a very large number of linear regions and using the ideas of Montufar et al (2014). See also\n\"Size and Depth Separation in Approximating Benign Functions with Neural Networks\" by Vardi et al (2021). Therefore lower bounds in this model may be less informative with respect to the power or limitations of neural networks. \nIt would be good if the authors could further justify their representational assumption from a machine learning perspective. Perhaps it is related to memorization and interpolation in neural networks. \n\nThe authors do not impose any bounds on the width or size of the networks. It might be of interest to consider what happens if one restricts the size of the ReLU networks to be polynomial: arguably networks used in practice have strong limitation on their size. It could be that such a restriction makes the questions raised in this paper more difficult: see Vardi et al. \n\nWith respect to the width bound in section 4 it would be nice to have also some kind of a lower bound, or at least an estimate of what the right answer should be. \n\nI've found the first two paragraphs not very informative reiterating common knowledge that does not seem to be very related to the body of the paper. Perhaps the authors could stress more their novel contribution in the first two paragraphs. \n\nNote after rebuttal: I think the paper studies an interesting question and introduces new techniques that might yield interesting insights. \nI thereby change my review to accept.  Yes. ",
            "It is known from the universal approximation theorem that ReLU networks with one hidden layer can approximate any continuous function on compact sets arbitrarily well. Instead of approximations, this paper investigates  the set of functions that can precisely be described by a network of certain depth. For a given input dimension D, a ReLU network with L=ceil[ log2(D+1) ] (or more) hidden layers (and arbitrary width) can describe the entire set of piecewise continuous functions. Here, new insight is provided for networks with having between 1 and L layers. In particular, under an unproven assumption, the paper shows that L is a strict lower limit for the depth and that adding layers to a network strictly increases the set of describable functions. This is achieved by studying a natural candidate function to require a larger number of hidden layers. The candidate function suggests a conjecture on a nice description of ReLU network functions of finite depth, which is shown to not hold true. Finally, the paper derives a bound on the width and depth of networks that can describe any piecewise linear function with fixed number of linear pieces.  The paper is very well-written and the arguments are carefully put together. All proofs are described in detail.  This theoretical paper is mostly interesting from a mathematical viewpoint. With universal approximation at hand, one could argue that a better understanding of the precise description of the set of ReLU functions does not add much from a practical point of view. However, as the authors point out, a better understanding could also be useful for algorithmic advances. \n\nI found the story of the paper intriguing. The conjecture on the lower limit in depth that is necessary to describe all continuous piecewise linear functions on a D-dimensional input space is reduced to proving that the candidate function max{0,x1,x2,,..,xD} cannot be described with less than ceil[ log2(D+1) ] many hidden layers. This statement seems at first like a fairly easy statement to prove or disprove, but appears to be actually quite difficult. Starting from the candidate function, there is natural hope to conjecture that a ReLU network with L hidden layers can only describe linear combinations of maxima of 2^L terms and no more, but this is shown to be false, which adds to the story that the description of ReLU network functions of a certain depth is complicated. \n\nAs a result, the paper does not present a complete story by fully characterizing the functions implemented by ReLU networks of fixed depth. \n\nThe following describes the progress made in more detail and discusses its limitations:\n\n(i) The conjecture on the tightness of the known lower bound on necessary depth to describe all piecewise linear functions on a D-dimensional input space holds true for networks with two hidden layers, but only under an unproven assumption. That is, the result is limited to three-layer networks and is even incomplete in that case.\n\n(ii) There are maxima of D+1 terms that can be described by a network of depth less than L=ceil[ log2(D+1) ]. This result is interesting, but oneit also disproves a conjecture that was first mentioned in this same paper and which did not previously appear important.\n\n(iii) A bound on the order of depth and width necessary to exactly describe any continuous piecewise linear function with p linear pieces, saying that the required width is polynomial in p with exponent defined by the input dimension. This is a only a small improvement to previously known results.\n\nThe main weakness of the paper is that it misses to describe the complications that need to be overcome to extend these limited results. For example, what are the problems to prove the necessary (unproven) assumption for the result in (i)? If it is a natural assumption that should be believed to (probably) hold true for ReLU networks, then why is difficult to show that it does indeed hold? A discussion of the difficulty to prove this critical assumption would make it easier to appreciate the partial result. Similarly, what are the problems of extending the proof to layers with more than two hidden layers?\n\nA strength of the paper is that the proofs of the partial results use nontrivial methods and require a good understanding on the geometrical setting of linear regions. The proof of statement (i) from above is based on mixed integer programming (MIP) with a nontrivial setup of the MIP problem. This part of the paper also introduces so-called H-conforming functions, which form a natural subset of piecewise continuous functions for the study of ReLU networks and this viewpoint could be interesting outside the study in question. The proofs to (ii) and (iii) use theoretical insight on piecewise continuous functions and their associated polyhedra of linear pieces. These proof ideas and techniques provide some insight on their own. \n\nTaken together, the paper presents an intriguing, theoretically interesting question, but only resolves it partially without explaining well why the partial progress is substantial. Since the ideas involved in setting up the statements and proofs are quite interesting by themselves, I tend to support the acceptance of the paper.\n\n\n*****\n Two small suggestions for potential minor improvement in the presentation:\n\n- Line 66: Better: \"By definition, a continuous function is piecewise linear in case ...\u201c \n- Line 96: The sketch of the proof only states half the arguments necessary for the proof. It would just cost a single line or maybe two to state the other half: If the specific n+1 term maximum function can be written with k layers, then all n+1-term maximum functions can.\n The paper is carefully put together and describes the assumptions of all statements clearly.  As argues in the main part of the review, the paper would benefit from describing the limitations of the used tools to extend the result and to go beyond the partial results reported in the study.",
            "This paper considers the problem of characterizing exact representations for ReLU networks of a given depth, but any width. It was previously shown that the functions represented by ReLU networks of depth logarithmic in dimension is exactly equal to the set of all continuous piecewise linear functions. This work provides some results which suggest that this result is tight. That is, a depth logarithmic in dimension is necessary to exactly represent all piecewise linear functions. \n\nProposition 1.3 simplifies this conjecture by giving a simple equivalent condition in terms of representing max(0,x_1,\\dots,x_n) and proves the conjecture up to dimension 4 under the assumption 2.4 which is unproven. In section 3 it is shown that the set of functions representable by a depth k network is a strict super set of Max(2^k) - i.e, set of all functions which can written as a linear combination of max of 2^k affine functions. This provides further evidence in support of the conjecture. \nSection 4 then extends the results of Arora et. al 2018 to provide an upper bound on the width required for a relu network to exactly represent affine function with p pieces. This bound is $p^{O(n^2)}$.  Disclaimer: I am not an expert on tropical geometry and related topics.\n\nI would first note that this paper is about exact representations. The closure of $\\mathsf{ReLU}_n(k)$ with respect to uniform convergence over compacts contains the space of all continuous functions by universal approximation property. This work is about not taking the closure and considering the exact representations. I acknowledge that this is indeed a hard problem. \n\nThe scope of the work seems to be very limited. It is not clear why having exact representation is ever useful in practical machine learning. The authors do mention the learning algorithm by Arora et. al 2018, which I believe they should expand on. The results seem a bit weak. The main result in Section 2 which proves the conjecture only upto dimension 4,  conditioned on an unproven assumption. The bounds in Section 4 are super exponential in dimension, which seems very intractable from a computation perspective. It would be great to see some discussions about lower bounds for the width. Practically speaking, if indeed this is tight, what is the use of exact representations at all?\n\nThe paper is very well written but I am skeptical about the relevance of this work in a venue like NeurIPS. I think a bit more effort needs to be put in to highlight the exact limitations of the work.",
            "The authors study the class of functions that can be represented by a fully connected neural network with ReLU activations. First, They conjectured that for any $k\\in\\mathbb{N}$, $n=2^k$, the function $f_n(x)=\\max\\{0,x_1,\\dots,x_n\\}$ cannot be represented by a fully connected network with $k$ hidden layers, and prove this conjecture for $k=2$ under some mild assumption. Second, they proved the class of functions that can be represented by a $(k + 1)$-layer NN is strictly larger than the class of functions that are linear combinations of $2^k$-term max functions. Finally, they provided a bound on the width of the NN required to represent continuous piecewise linear functions.   Originality: The related works are adequately cited. The novelty of this paper is high. The three main results in this paper, as mentioned in the above summary part,  will certainly help us have a better understating of deep neural networks from a theoretical way. I have checked the technique parts and find that the proofs are solid. I think this is a significant contribution to deep learning immunity. My only concern is that, the proof of Conjecture 1.2 for $k=2$ case, relies on Assumption 2.4, which is not quite elegant. It will be interesting to prove Conjecture 1.2 for $k=2$ case without any further assumptions.\n\nQuality: This paper is technically sound.\n\nClarity: This paper is clearly written and well organized. I find it easy to follow.\n\nSignificance: I think the results in this paper is significant, as explained above.    Yes, the authors have adequately addressed the limitations and potential negative societal impact of their work."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states they are \"not convinced that the results in this work are strong enough\" and chooses to \"keep my score the same,\" indicating a negative assessment.",
            "The review contains both positive and negative feedback. The reviewer appreciates the overall theme and identifies interesting techniques, but also points out the weakness of the results and the reliance on technical assumptions. The language is balanced, acknowledging both strengths and weaknesses.",
            "The reviewer expresses gratitude and acknowledges a previous imprecision, indicating a neutral stance.",
            "The reviewer initially expresses mixed feelings ('Pros' and 'Cons' sections), but ultimately changes their review to 'accept', indicating a positive final sentiment. They also state that the paper 'studies an interesting question and introduces new techniques that might yield interesting insights.'",
            "The reviewer states they \"tend to support the acceptance of the paper\" despite its limitations. They also describe the paper as \"intriguing\" and \"theoretically interesting\". The reviewer highlights the paper's strengths, such as the nontrivial methods used in the proofs and the introduction of H-conforming functions.",
            "The review expresses skepticism about the relevance of the work, calling the results \"weak\" and the bounds \"very intractable.\" The reviewer questions the practical usefulness of exact representations and suggests the authors need to highlight the limitations of their work.",
            "The reviewer expresses positive sentiment by stating the paper is a \"significant contribution to deep learning immunity,\" the proofs are \"solid,\" the paper is \"clearly written and well organized,\" and the novelty of the paper is \"high.\""
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The reviewer expresses doubt about the strength of the results, using phrases like \"not convinced\" which conveys a critical perspective.",
            "The tone is balanced, presenting both positive aspects ('I like the overall theme', 'interesting techniques') and critical feedback ('results presented are quite weak', 'relied on a technical assumption'). The reviewer also provides constructive suggestions for future directions, indicating a balanced assessment.",
            "The reviewer is polite and acknowledges their own mistake ('I apologize for the imprecision'). They also offer a critical assessment ('does not provide much significant insight') but frame it as a suggestion for improvement.",
            "The review presents both positive ('Pros') and negative ('Cons') aspects of the paper, offering constructive criticism and suggestions for improvement. The reviewer also uses formal language and provides specific examples to support their points.",
            "The review provides both positive and negative feedback. While praising the paper's theoretical interest, careful construction, and intriguing question, it also points out limitations such as the partial resolution of the question and the lack of explanation regarding the challenges in extending the results. The reviewer uses phrases like \"main weakness\" and \"only a small improvement\" to indicate areas for improvement. The closing statement shows support for acceptance, balancing the criticism.",
            "The review uses phrases like \"scope of the work seems to be very limited,\" \"results seem a bit weak,\" and \"I am skeptical about the relevance of this work.\" These phrases indicate a critical evaluation of the paper's contributions and impact.",
            "The tone is supportive, using phrases like \"certainly help us have a better understanding,\" \"significant contribution,\" and \"easy to follow.\" The reviewer also acknowledges the paper's strengths and significance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer expresses a consistent opinion by stating they remain unconvinced about the strength of the results and therefore maintain their original score. There is no contradiction in their statements.",
            "The review is consistent. It appreciates the motivation and direction of the paper, but points out the weakness of the results and the technical assumptions needed. The reviewer's overall assessment is balanced, acknowledging the interesting aspects while also highlighting the need for stronger contributions. There are no self-contradictory statements.",
            "The reviewer expresses gratitude, clarifies a previous point, and apologizes for imprecision. The statements are logically connected and do not contradict each other.",
            "The review presents initial concerns and suggestions for improvement (in 'Cons' section), but ultimately shifts to a positive recommendation ('Note after rebuttal') after considering the authors' rebuttal. This change of opinion is a consistent evolution within the peer review process, not an internal contradiction within the review itself. The reviewer's final positive assessment aligns with their note after rebuttal, indicating a consistent progression of their evaluation.",
            "The review is consistent because it acknowledges both the strengths and weaknesses of the paper, providing a balanced assessment and a clear rationale for its overall judgment. The reviewer highlights the theoretical interest and methodological contributions despite the partial nature of the results, leading to a consistent recommendation for acceptance.",
            "The review is consistent in its skepticism about the practical relevance and significance of the work, while acknowledging the technical difficulty and quality of writing. The reviewer consistently points out the limitations of the results and questions the usefulness of exact representations in practical machine learning.",
            "The review is consistently positive about the paper, highlighting its originality, technical soundness, clarity, and significance. The reviewer expresses a minor concern about an assumption in one proof, but this is framed as a suggestion for improvement rather than a fundamental flaw, and does not contradict the overall positive assessment of the paper's contribution."
        ]
    },
    {
        "paper_id": "iclr_2018_rJBiunlAW",
        "paper_title": "Training RNNs as Fast as CNNs",
        "paper_abstract": "Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ",
        "review_ids": [
            "BJsMKkGgf",
            "SyjjOZ5gM",
            "HyMadv_bz",
            "B10wVlpGM",
            "rycUihjGz",
            "r1XmNCKkz"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This work presents the Simple Recurrent Unit architecture which allows more parallelism than the LSTM architecture while maintaining high performance.\n\nSignificance, Quality and clarity:\nThe idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense. \nThe idea is well explained and the experiments convince that the new architecture is indeed much faster yet performs very well.\n\nA few constructive comments:\n- The experiment\u2019s tables alternate between \u201ctime\u201d and \u201cspeed\u201d, It will be good to just have one of them.\n- Table 4 has time/epoch yet only time is stated",
            "The authors introduce SRU, the Simple Recurrent Unit that can be used as a substitute for LSTM or GRU cells in RNNs. SRU is much more parallel than the standard LSTM or GRU, so it trains much faster: almost as fast as a convolutional layer with properly optimized CUDA code. Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below).\n\nOn the positive side, the paper is very clear and well-written, the SRU is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace LSTMs in most cases but yield fast training. On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art. Some of this has been pointed out in the comments below already. As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21. But already a long time ago Wu et. al. presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762). While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation. In conclusion: the authors seem to avoid discussing the problem that current non-RNN architectures  could be both faster and yield better results on some of the studied problems. That's bad presentation of related work and should be improved in the next versions (at which point this reviewer is willing to revise the score). But in all cases, this is a significant contribution to deep learning and deserves acceptance.\n\nUpdate: the revised version of the paper addresses all my concerns and the comments show new evidence of potential applications, so I'm increasing my score.",
            "The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. \n\nIn essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.\n\nPros:\n- Fairly well presented\n- Wide range of experiments, despite underwhelming absolute results\n\nCons:\n- Quasi-RNNs are almost identical and already have results on small-scale tasks.\n- Slightly unfortunate naming that does not account for autoregressive CNNs\n- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.\n- I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline.\n\nI recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented.\n\n",
            "Thank you for the new version of the paper. It looks much better, and I misunderstood the comments about Transformer. Indeed, combining it with SRUs could bring the best of both worlds and improve results even more. I have no more objections to accepting this work and I see its big potential, adjusting my review.",
            "I am not sure how to interpret the comment about the Transformer architecture. There is a table with results in your paper that are far below SOTA and it doesn't even mention this -- it looks like clearly misleading presentation, and with your comment it starts looking like it's misleading on purpose. Thus I'm lowering my score until the presentation is improved. In particular, your results are below 21 BLEU which is very far apart from the 28 BLEU of the Transformer -- the suggestion you make in the comment (that architectures like Transformer may not be needed with SRUs) seems to be far from conclusive at this point. Please present your work fairly and compare to existing SOTA -- it's a very good work, but the presentation is misleading.",
            "In Tables 6 / 9: \nIt is not clear why SRU model capacity was increased in depth (to 12 layers) and not in width, which would give an even faster model I would think. As you mention for LSTM 5 layers appear to be optimal, so it is surprising that 12 were needed for SRU."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses overall positive feedback, highlighting the well-motivated idea, clear explanation, and convincing experiments. Phrases like 'well motivated,' 'makes sense,' 'well explained,' 'convince,' 'much faster,' and 'performs very well' indicate a positive sentiment.",
            "The reviewer expresses overall positive sentiment, stating the paper is \"very clear and well-written\", the SRU is \"a superbly elegant architecture\", and the results show it \"could be a significant contribution to the field\". The final statement, \"this is a significant contribution to deep learning and deserves acceptance\", further reinforces this positive sentiment. The update at the end confirms this positive sentiment.",
            "The review expresses significant concerns about the paper's novelty and methodology, highlighting the lack of comparison with relevant baselines and questioning the contribution of the gated incremental pooling operation. Phrases like \"unfortunate naming,\" \"major conceptual error,\" and \"underwhelming absolute results\" contribute to the negative sentiment.",
            "The reviewer expresses satisfaction with the revised paper, stating \"It looks much better\" and acknowledges their misunderstanding of previous comments. They also indicate that they \"have no more objections to accepting this work\" and see \"its big potential.\"",
            "The reviewer expresses strong concerns about the misleading presentation of results, indicating dissatisfaction. Phrases like \"clearly misleading presentation,\" \"misleading on purpose,\" and \"lowering my score\" convey negative sentiment.",
            "The reviewer expresses confusion and surprise regarding the model's architecture choices, using phrases like \"It is not clear\" and \"surprising that 12 were needed.\" This indicates a negative sentiment due to perceived issues with the paper's reasoning."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Critical",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is supportive, offering 'constructive comments' rather than harsh criticism. The reviewer acknowledges the strengths of the work and provides suggestions for improvement in a helpful manner.",
            "The review presents both positive and negative aspects of the paper. While praising the clarity, originality, and potential impact of the SRU, it also criticizes the baselines used for comparison and the lack of acknowledgement of state-of-the-art results. The reviewer uses phrases like \"On the positive side\" and \"On the negative side\" to clearly delineate these aspects, demonstrating a balanced assessment.",
            "The reviewer uses direct and critical language, pointing out specific flaws in the paper's approach and suggesting major improvements. Phrases like \"Lack of comparison,\" \"major conceptual error,\" and the overall focus on the paper's shortcomings demonstrate a critical tone. The reviewer also offers specific suggestions for improvement, indicating a desire for the paper to be revised.",
            "The reviewer uses encouraging language, such as \"Thank you,\" \"much better,\" \"best of both worlds,\" and \"big potential.\" They also explicitly state they are \"adjusting my review\" to be more favorable.",
            "The tone is critical due to the direct accusations of misleading presentation and the statement about lowering the score. Phrases such as \"far below SOTA,\" \"doesn't even mention this,\" and \"seems to be far from conclusive\" contribute to the critical tone.",
            "The tone is critical as the reviewer questions the authors' design choices and expresses skepticism about the necessity of 12 layers in the SRU model. The phrase \"It is not clear why\" and the suggestion of a potentially better alternative (\"which would give an even faster model I would think\") highlight this critical stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it praises the significance, quality, and clarity of the work while providing constructive suggestions for minor improvements in presentation, specifically regarding the consistency of terminology in tables. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent because it acknowledges both the strengths (novelty, clarity, potential impact of SRU) and weaknesses (presentation of related work, outdated baselines) of the paper.  The reviewer clearly distinguishes between these aspects, praising the core contribution while criticizing the presentation. The initial recommendation of acceptance despite the weaknesses, followed by an increased score after revisions addressed the concerns, demonstrates a consistent evaluation approach focused on the overall value and potential of the work while also emphasizing the importance of proper contextualization and comparison with state-of-the-art.",
            "The reviewer consistently points out the lack of comparison with autoregressive CNN baselines as a major weakness of the paper. This argument is present from the initial analysis of the method to the cons section and the final recommendation, indicating a consistent line of reasoning throughout the review.",
            "The reviewer expresses a positive change in opinion after reviewing the revised paper, acknowledging their previous misunderstanding and stating no further objections, indicating a consistent positive trajectory towards acceptance.",
            "The review is consistent because the reviewer clearly identifies the misleading presentation of results as the main issue. The reviewer consistently argues that the paper does not fairly compare its results to SOTA, particularly Transformer, and that this misleading presentation is the reason for lowering the score. All points in the review support this central argument without contradiction.",
            "The review is consistent as it questions the choice of increasing depth over width for the SRU model and relates it to the optimal depth observed for LSTM models, without presenting any contradictory statements."
        ]
    },
    {
        "paper_id": "nips_2021_d9FjReQr-q-",
        "paper_title": "Understanding Partial Multi-Label Learning via Mutual Information",
        "paper_abstract": "To deal with ambiguities in partial multilabel learning (PML), state-of-the-art methods perform disambiguation by identifying ground-truth labels directly. However, there is an essential question:\u201cCan the ground-truth labels be identified precisely?\". If yes, \u201cHow can the ground-truth labels be found?\". This paper provides affirmative answers to these questions. Instead of adopting hand-made heuristic strategy, we propose a novel Mutual Information Label Identification for Partial Multilabel Learning (MILI-PML), which is derived from a clear probabilistic formulation and could be easily interpreted theoretically from the mutual information perspective, as well as naturally incorporates the feature/label relevancy considerations. Extensive experiments on synthetic and real-world datasets clearly demonstrate the superiorities of the proposed MILI-PML.\n",
        "review_ids": [
            "a8uI-gD3uWl",
            "ickSDwRtBtx",
            "-U0i5vmb1s",
            "WmpkcDTV8Z"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper addresses the problem setting of partial multi-label learning (PML). The task is to select the ground-truth labels from a set of label candidates and disregard false-positive or noisy labels that may occur in the labelling process. The authors propose MILI-PML that exploits dependencies between labels and features for selection and relies on the assumption that ground-truth labels have a higher overall dependency between labels and features than false-positive ones. The paper provides theoretical analysis to show that the model can identify ground-truth labels from a candidate set correctly and propose a corresponding optimization procedure. Further, the authors conduct experiments to demonstrate the effectiveness of MILI-PML and claim that the method achieves superior performance compared to other SOTA methods.\n\n** Thanks for the response to this review.   The paper considers the problem setting of PML from an information-theoretic perspective and justifies the proposed method MILI-PML accordingly. With this perspective, the authors provide a contribution to the field of PML. The authors include recent methods and compare the performance of the proposed method against those in the experimental part.\n\nThe quality of the paper is not sufficient yet and needs improvement. Some details of the theoretical analysis need to be clarified, and the experimental part needs to be extended by additional assessments, especially concerning the limitations of the proposed method.\n\nThe paper makes two assumptions on which MILI-PML is based: (1) \"that the overall correlation between a ground-truth label $y_k$ and features should be stronger than the overall correlation between a false-positive label $y_{\\bar{k}}$ and features\" and (2) \"It is naturally hold that the KL-divergence value should be smaller when the ground-truth label $y_k$ is added to the existing identified label set $v$ than that of the false-positive label $y_{\\bar{k}}$\".\n\nThe first assumption seems plausible on an intuitive level. However, it is unclear to what extent this assumption is fulfilled in the datasets used. The reported results give little information about the assumption, as only the performance on different evaluation metrics is reported. Whether the assumption tends to be fulfilled within a dataset could be shown by providing measures of the dependency between individual features and labels, such as a suitable correlation coefficient. Further, it is unclear to what extent MILI-PML exploits dependencies between features and labels. Therefore, it would strengthen the paper to assess how the performance of MILI-PML to select or reject a label correctly is related to the level of dependency between the features and a ground-truth or false-positive label. This influence could be analyzed by generating artificial data in which the dependency between a label and the features is controllable and evaluating the performance of MILI-PML under different configurations. This might also indicate potentials limitations of the proposed method.\n\nRegarding the second assumption, I suppose that the relation between $D_{KL}(P(Y_v \\cup Y_{\\bar{k}} | X) || Q(Y_v \\cup Y_{\\bar{k}} | X,w))$ and $D_{KL}(P(Y_v \\cup Y_{k} | X) || Q(Y_v \\cup Y_{k} | X,w))$ depends on both the choice of model $Q$ and its parameterization $w$. I expect that depending on the choice of $Q$ and $w$, the relation of the two KL-divergences can take on any value. Please clarify why your assumption that in general $Q$ approximates the true probability distribution $P$ better when a ground-truth label is added to $v$ rather than a false-positive one holds.\n\nIn Chapter 6.2 (and Chapter 3 of the Supplementary Material), it is stated that the hyper-parameters of the baselines were set to the corresponding values of the original papers. Optimize the hyper-parameters for the baselines for a fair comparison with MILI-PML and state the range of hyper-parameters considered and methods used for selection.\n\nThe submission is generally well written and organized. Mathematical statements are presented in (great) detail and are mostly easy to follow. Please see section \u201eQuestions and minor remarks\u201c for minor issues. However, I would recommend a language review before publication.\n\nWith the provided information about the implementation and the experiments, it is not yet possible to reproduce the results. Therefore, please provide the following details to improve reproducibility:\n\nNumber of evaluation runs and calculation of error bars in the results tables.\nDetailed descriptions of how the first and third quantities are calculated in Eq. 21.\nDetailed descriptions of how the real world datasets are used in the experimental part. For example, the MIRFlickr dataset [1] contains Flickr tags (raw form and processed form) and annotations. It is unclear which of these are used as labels, as the relevant statistics do not match the information on the website.\nThe experimental section indicates promising results for MILI-PML and could be of significance for the area of PML due to both the performance improvement and the theoretical analysis. However, the experimental part does not yet evaluate the method thoroughly enough, and therefore the significance cannot yet be assessed finally.\n\nQuestions and minor remarks:\n\nCan you explain the change of distribution in Eq. 7 in which the joint probability distribution of features and labels $P(x,y)$ is exchanged with the joint probability distribution of disregarded labels and selected labels $P(y_{\\bar{v}, y_v})$?\n\nMinus is missing at the last line of Eq. 1 in the supplementary material.\n\nTypos or missing words in line: 56, 79, 136, 147, 151, 175, 184, 258, 278, 283.\n\n[1] http://press.liacs.nl/mirflickr/ The paper partly addresses the limitations of the proposed method by stating assumptions that need to be fulfilled. It does not address the potential negative social impacts of the application of the proposed method. The authors state in the theoretical analysis that the algorithm only works under given conditions and certain assumptions. Limitations due to configurations (for example, which predictive model $Q$ is used) are not described. The experimental part does not examine whether the central assumptions about dependencies in the data hold and to what extent MILI-PML exploits them. I would appreciate it if the authors would comment in the paper on the potential impacts of their method. For example, I would think the method could be of interest in the context of automatic filtering of \"noisy\" or \"false\" labels and could lead to bias or discrimination depending on the domain of the data (e.g. personal data). Moreover, there might be a risk considering the quality of the data used. For example, if a large amount of structurally misannotated data is included in the underlying data set, the central assumption could be affected.",
            "This paper tackles the partial multi-label learning problem from a probabilistic formulation with theoretical interpretation and derives a novel objective function based on the concept of Mutual Information (MI). To optimize the objective function, the authors propose an optimization algorithm to identify ground-truth labels and update classifier alternatively.  The idea of studying the PML problem with tools from information theory is novel. The training objective function derived by the authors is reasonable and can be well explained theoretically. The technique and theoretical parts are solid and seem sound to me, which can benefit the following research in PML. The contribution is highly related and important to the community.\n\nThe overall structure and writing are clear and easy to follow.\n\nI have some questions: \n- Why there are no related references following the statement \u201cthe state-of-the-art PML methods\u201d in the first section?\n- In the paper, the authors claim that y, y_v are the vectors, but describe that \u201cy_v is selected from y\u201d. I believe there are some notation abuses, and the authors should modify it to make the mathematical expression more strict.\n- The notation v is used as vector in section 3, but used as the identified ground-truth label set in section 4. Different notations are suggested to use.\n - Why there are no related references following the statement \u201cthe state-of-the-art PML methods\u201d in the first section?\n- In the paper, the authors claim that y, y_v are the vectors, but describe that \u201cy_v is selected from y\u201d. I believe there are some notation abuses, and the authors should modify it to make the mathematical expression more strict.\n- The notation v is used as vector in section 3, but used as the identified ground-truth label set in section 4. Different notations are suggested to use.\n",
            "This paper presents two fundamental questions about the assumption of existing PML research. The authors derive a novel PML method from a probabilistic formulation which meets the $\\epsilon$-identifiable definition. They provide theoretical analysis and algorithms to verify the proposed two questions. Thorough comparative experiments validate the superiorities of the proposed method.  Pros: \nThis paper provides a new insight into partial multi-label learning problem from mutual information perspective, which is novel and highly related to the NeurIPS community. The presentation of the paper is clear and the relation with prior work is well-explained. A new objective function for PML is derived from a probabilistic formulation and a novel definition to evaluate whether a PML method can identify the ground-truth labels precisely is presented. The theoretical analysis is sound and solid. Thorough comparative experiments have been conducted to validate the superiorities of the proposed method, which has good academic value. \n\nCons:\n1.\tFrom Table 3 to 5, we can see that the proposed method does not always perform the best, such as, onYeastBP and Eurlex-ed datasets. How do the authors view this?\n2.\tSome details need elaboration. How to calculate the probabilities in Eq. (18) and Eq. (21)? Besides, it is pointed out that LIBLINEAR with L2-regularized square hinge loss is employed to train the binary classifiers. How is the L2-regularized square hinge loss incorporated into the optimization algorithm designed in section 5?\n3.\tIn Algorithm 2, the while loop of the condition should be explicitly explained.\n4.\tThere are some typos, such as \u201cfundermental\u201d, \u201cojective\u201d. The authors should carefully proofread this paper and correct all the typos.\n 1.\tFrom Table 3 to 5, we can see that the proposed method does not always perform the best, such as, onYeastBP and Eurlex-ed datasets. How do the authors view this?\n2.\tHow to calculate the probabilities in Eq. (18) and Eq. (21)? \n3.\tHow is the L2-regularized square hinge loss incorporated into the optimization algorithm designed in section 5?\n4.\tIn Algorithm 2, the while loop of the condition should be explicitly explained.\n5.\tThere are some typos, such as \u201cfundermental\u201d, \u201cojective\u201d. The authors should carefully proofread this paper and correct all the typos.\n",
            "Partial multi-label learning has appeared to be more useful in many applications. This paper first investigates some basic problems existed in the current research, and proposes a new method based on the mutual information, which appears to be interesting to the community.  - This paper first investigates two basic issues existed in the current research of partial multi-label learning and provides the solid technical solutions with theoretical guarantee.\n\n- A new method based on mutual information is proposed in this paper. The motivation and derivation are clear. The idea is interesting, which could motivate the community to bring some new tools from other domains to address partial multi-label learning tasks.\n\n- The extensive experiments are conducted on various data sets to verify the superior performance of the proposed method.\n - Some state-of-the-art partial multi-label references are missing, such as 1) Partial Multi-Label Learning with Label Distribution 2) Noisy label tolerance: A new perspective of Partial Multi-Label Learning 3) Partial multi-label learning with mutual teaching.\n\n- The explanation of Theorem 1 is weak; the author should provide more explanations.\n\n- Can the author do the experiments on the image data set?\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's quality, stating that it 'is not sufficient yet and needs improvement.' It points out the need for clarification in the theoretical analysis and extensions to the experimental part. The review also highlights unclear assumptions and the lack of thorough evaluation, indicating a negative sentiment.",
            "The reviewer states that the idea is novel, the objective function is reasonable, the technique and theoretical parts are solid and sound, and the contribution is highly related and important. The reviewer also notes that the structure and writing are clear and easy to follow.",
            "The review highlights several positive aspects of the paper, including its novelty, clear presentation, sound theoretical analysis, and thorough experiments. The reviewer explicitly states that the paper provides 'new insight' and has 'good academic value'. While the 'Cons' section points out weaknesses, the overall tone suggests a positive evaluation.",
            "The review expresses overall positive feedback, highlighting the paper's interesting approach, clear motivation, solid solutions, and superior performance. While it points out missing references and suggests improvements, the core assessment is favorable."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like 'needs improvement,' 'unclear,' 'little information,' 'not yet evaluate the method thoroughly enough,' and poses direct questions challenging the assumptions and methodology. These elements indicate a critical tone.",
            "The reviewer uses positive language like \"novel,\" \"reasonable,\" \"solid,\" and \"important.\" The reviewer also offers constructive criticism in the form of questions and suggestions for improvement, indicating a desire to help the authors strengthen their work.",
            "The review adopts a balanced tone by presenting both the strengths ('Pros') and weaknesses ('Cons') of the paper. It offers constructive criticism and specific suggestions for improvement, using a formal and objective style.",
            "The review adopts a balanced tone. It praises the paper's strengths (interesting approach, clear motivation, solid solutions, superior performance) while also providing constructive criticism (missing references, weak explanation, suggestion for further experiments). This mix of positive and negative feedback indicates a balanced assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review maintains a consistent critical but constructive tone throughout. It acknowledges the paper's contributions and potential while clearly pointing out areas needing improvement, such as theoretical clarification, experimental validation, reproducibility, and discussion of limitations and social impact. The reviewer's arguments and suggestions are logically connected and do not contradict each other.",
            "The review is consistent because it acknowledges the strengths of the paper, such as novelty and solid theoretical foundation, while also pointing out specific weaknesses like missing references and notation inconsistencies. The reviewer provides constructive criticism without contradicting the overall positive assessment of the paper's core contributions.",
            "The review is consistent because it acknowledges the strengths of the paper in the summary and pros sections, while also pointing out specific areas for improvement in the cons section. The cons are constructive criticisms and do not contradict the overall positive assessment of the paper's novelty, theoretical soundness, and experimental validation.",
            "The review is consistent because it provides both positive feedback on the paper's contributions and constructive criticisms for improvement without any contradictory statements. It acknowledges the strengths of the paper while suggesting specific areas for enhancement."
        ]
    },
    {
        "paper_id": "iclr_2022_lnEaqbTJIRz",
        "paper_title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design",
        "paper_abstract": "Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining\": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities.\tThis theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations. ",
        "review_ids": [
            "VluDZlu8XBI",
            "sUPa1qI4yF",
            "921tlMcxv4l",
            "xykRokzk90y",
            "1OhTy4F6tsS",
            "O_F_Z7yCpXv",
            "S5GhA_Db7Fv"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper provides an argument for reconsidering how we construct a single training example during LM (pre)training. It gives a formal proof that dependencies between two texts are vanishingly weak when the texts are presented in separate training examples (but not when they are in the same training example). Intuitively, in order to make cross-textual inferences between texts in different examples, a model must be able to access information stored in its weights, but commonly used learning rates and model architectures ensure that the updates to the weights are too small to recover this information.\n\nThe paper suggests remedying this problem by changing the way in which we construct training examples. Rather than obtaining examples by separating texts into contiguous chunks, we can construct examples from multiple semantically related chunks found by computing the similarity of text embeddings in the training corpus. Empirical results are presented which show that task-adaptive (i.e. post-pretraining) training on such data leads to greatly improved performance on zero-shot semantic similarity. Further results show that doing pretraining with these kinds of examples improves zero-shot QA performance by a small margin (from an extremely low baseline). The main results of this paper are of broad interest. Questions about how the nature of the input to LMs affects learning are important and poorly understood. The claim that the within- and between-example is crucial is novel and thought provoking.\n\nThe arguments are highly technical and require a lot of expertise, (including familiarity in particular with Levine, 2020, on which the proof builds substantially) to understand. This is not necessarily a strength or a weakness---there is great value to developing techniques for making this kind of formal argument. However, I find it difficult to follow even the thread of the argument at an intuitive level, which is a shame because it's interesting to me. I suspect I'm not the only potential reader who will have this problem. The paper could reach a much wider audience (which I think would be an improvement) if it provided more intuitive paraphrases of the main results in Section 2.3. \n\nThere are a few unaddressed issues with the authors' main suggestion: that we can improve LMs\u2019 abilities to make cross-example connections by constructing examples out of interleaved texts. Assuming a fixed example length, this means portions of text from the same source that otherwise would have been in the same example in the standard chunking scheme would now be separated. However, it seems that, in general, dependencies within a text are more important than dependencies between random semantically related sentences from different texts. Another problem is that a language model trained on this kind of interleaved text would presumably also generate less coherent texts.\n\nI also have some doubts about the empirical results. The results about kNN-task adaptive pretraining seem to have a confound: The method for retrieving data for this intermediate training step involves finding sentences with high similarity, but the task on which this method is shown to lead to improvements is itself a semantic similarity task. The claim is that the method should be helpful for many kinds of tasks, but my intuition is that this \"coincidental\" alignment of the data collection and the evaluation task makes the result less generalizable.\n\nThe empirical results about zero-shot QA don't help much because the task is clearly too difficult. The improvement on the models trained with the new method is from F1 of ~0.001 to ~0.01. Do larger LMs succeed at this task? If so, then it seems that models tested in this paper are just too small or ineffective for the task. Is there an easier task where this method leads to strong performance but ordinary pretraining does not?\n\n This paper makes an intriguing claim about the effect of the input on LM learning, which is backed up by a formal argument. These kinds of arguments are valuable. However, the argument is difficult to follow for a non-specialist audience. Also, the empirical results the authors report have some confounds due to questionable task selection.",
            "The paper addresses the bias induced by the inconsistency between the pretraining examples and downstream examples. Specifically, the text is continuous during pretraining while could be non-neighboring in downstream tasks. The authors proposed two methods: kNN-TAPT and kNN-pre-training in Task Adaptive Pre-training (TAPT) step or general Pre-training step respectively, where each training sample is composed of a sentence and its semantically closed neighbors (by kNN search). The method is effective in sentence similarity tasks (in kNN-TAMP scenario) and closed book open domain QA tasks (in kNN-Pre-training scenario).  The paper also gives a theoretical analysis of the so-called in-context bias,  by quantifying the NLM\u2019s ability to model dependencies between two sentences that appear in the same training example (the in-context representation) and in different training examples (the sequential representation). Strength:\n1.\tThe authors analyze the in-context bias of the self-attention model, which could inspire some research works on designing training examples.\n2.\tThe authors propose to include related texts retrieved by the kNN method in a single training sample, which is proved effective in solving sentence similarity tasks.\n3.\tA theoretical analysis of the in-context bias.\n\nWeakness:\n1.\tThe introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning.  The paper said: \u201cthe pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.\u201d  Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1.\n2.\tThe theory is a bit complicated and not easy to follow. \n3.\tThe experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.\n The paper addresses the inductive bias of in-context learning. The studied problem is meaningful and could be a potential breakthrough for pretrained language model. However, the written of the paper is a bit hard to follow and the experiments is somewhat limited.",
            "The authors show empirically and theoretically that transformer based language models cannot effectively exploit dependencies between sentences in different inputs. For the theory part, they introduce a new measure called \\eps-sep rank. A function is said to have low \\eps-sep rank if it can be approximated up to error \\eps by a function with low separation rank. Bounding the measure shows that if learning rate is low, the information from different training instances cannot be well integrated. Empirically, they present experiments where the task specific pre training was altered to include non-task specific similar sentences in the context of the LM. They show that augmenting context of pre training inputs with related similar sentences from Wikipedia helped improve end performance on similarity and zero shot open domain QA tasks.  Strengths: 1. Some experimental results are interesting. Simply adding related sentences in the pre training input context helps end performance.\n\nWeaknesses: 1. I think the presentation of the paper needs to be improved. For the theoretical analysis, it was not clear to me what is the contribution of the current analysis compared to the Levine 2020 paper. I felt similar conclusions can be drawn from the results of that paper as well. It will be good to rewrite highlighting the contributions. I also noticed a lot of repeated text in section 2 from Levine 2020 paper; will be good to modify.\n\n2. The empirical part of the paper shows improved performance of adding similar sentences to the context of LM training. I felt this was quite separate from the theoretical analysis. It does not follow from the theoretical results that adding similar sentences will be a good thing. As a result, having these in the same paper looked incoherent to me.\n\n3. The experimental results are weak. The gains are not super high. I will be more convinced if evaluation is done on a wider range of tasks.     I was not clear about the impact of the theoretical analysis presented, compared to past work. It will be good to rewrite the section clarifying the contributions and novel conclusions from the analysis. The empirical section sounded disparate from the theoretical part of the paper, and the results are weak. I suggest evaluating on a wider variety of tasks. ",
            " Happy with explanations added, thanks.",
            " Thank you to the authors for your detailed response.\n\nI'll start by responding to (3), because I'm not totally convinced. I think it's great that you've replicated the NQ results on GLUE---I'm convinced that this performance bump is not an accident. However, I'm still hesitant to read into small changes in performance so close to chance. The models are clearly barely able to perform the zero-shot QA task, so it's not clear if these improvements would generalize to models that *can* perform the task (which are ultimately more like the models we care about). Some alternatives: \n- Maybe evaluate on LAMA or other LM test sets that are designed to not require task-specific training. \n- Try fine-tuning. Even though you are rightly concerned that fine-tuning tells us less about specific inter-textual dependencies learned *during* pretraining, a significant improvement in fine-tuning performance would suggest general benefits. \n\nAs for the SemEval results, the confound I brought up wasn't meant to cast doubt on whether semantic similarity genuinely improved with KNN TAPT. It was meant to cast doubt on whether this approach would be helpful for *any* other task. Let me put it this way: I have no reason to think KNN TAPT would help with solving NLI, or any number of tasks. If you could show meaningful improvements in some setting (fine-tuning, zero-shot, few-shot, etc.) on one or more unrelated tasks, I'd be convinced that this approach might be broadly useful. Otherwise, why would we believe that it's more than a hack to improve SemEval performance?\n\n1. Thanks for including the proof sketch---this clarify the outline of the proof, though I would still encourage you to include even more intuitive descriptions along the way, if space allows.\n\n2. Apologies for missing the 50/50 training approach. This seems to be well justified now. If you could demonstrate that KNN pretraining and regular pretraining yield similar PPL on a shared test set, this would be very convincing. Of course, the percent of KNN examples is now a hyperparameter which could be tuned in subsequent work.\n\nThanks for you work!",
            "The paper presents a theoretical analysis of the strengths of including similar input examples S1 and S2 into the same actual transformer input. The analysis contrasts using examples of concatenated S1;S2 with running the two in separate inputs. The paper then proves empirically that including related examples in the same input (via a method the article calls KNN-pretraining) allows transformers to learn much faster in cases when cross-document dependency is relevant. Strengths:\n * Clearly written paper\n * The theoretical contributions will have a high impact on training transformer-based models\n * The theoretical analysis is supplemented by experimental analysis\n\n Weaknesses:\n  * No major weaknesses\n\n  \nSome minor aspects:\n * \"correct on less than 50 questions out of 20, 000 in the evaluation set\" \u2013 Separating thousands by commas (American system) is ok. Separating them by space is also ok. Doing both is not.\n * \"correct on roughly than 250 questions in the evaluation set\" \u2013 \"than\" shouldn't be there\n * Equation 4 would have been easier to read if the right-hand side was expanded to S1 and S2's `{w_2^j}` notation from the previous line. Despite its theoretical focus, the paper is easy to read and to follow. The theoretical analysis is supplemented by empirical results.",
            "This paper formalizes the notion of an in-context bias---the pre-trained neural language models are better at modeling dependencies that appear within one contiguous pre-training text chunk, rather than those that appear in different pre-training text chunks. The authors perform a theoretical analysis of this phenomenon and link the inability to model dependencies between different examples with the models' low-pretraining learning rate. The authors then propose two methods for improving pre-training example design by adding sentences that are similar (KNN), but from different documents. They empirically show that this works better than reasonable baselines on sentence similarity tasks (e.g., STS and SICK), as well as NaturalQuestions. I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand what is going on with pre-trained language models, why they work well, and what might we be able to improve from theses insights.\n\nStrengths:\n- Thorough theoretical analysis that reveals the connection between (practically-necessary) small learning rates and inability to use dependencies across text chunks\n- Useful framing and discussion of the \"in-context bias\", where models are more likely to learn dependencies within text chunks seen during pre-training.\n- reasonable initial experimental results demonstrating some ways to help models better use cross-text-chunk dependencies (put them into a contiguous text chunk), providing some hope that these results could make models better.\n\nWeaknesses:\n- I felt like the empirical validation could have been stronger. The only two tasks examined are sentence similarity tasks (which seem a bit more like a sanity check), and NaturalQuestions. It's particularly surprising to me that this works so well on NQ, and I wish the authors had dug a bit deeper into this, but I also recognize that page limits exist.\n- Where else do you think the in-context bias could be useful? It wasn't intuitive for me that it'd be useful for NQ.\n- Do you have any initial experiments on the \"self-improving\" aspect of this technique? This is mentioned several times, but there are no initial results or anything suggesting that it might be a promising direction to pursue. I thought this paper was interesting and brings a new perspective to practical design decisions used in pre-training language models. I think this paper is relevant to the ICLR audience, and that said audience would be excited to know about it."
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Negative",
            "Positive",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive and negative aspects. It acknowledges the value and novelty of the argument but also points out issues with clarity, potential drawbacks of the proposed method, and concerns about the empirical results. The final summary balances the intriguing claim with difficulties in understanding and questionable task selection.",
            "The review acknowledges both strengths and weaknesses of the paper. It highlights the meaningfulness of the problem addressed but also points out issues with clarity and limited experiments, resulting in an overall neutral sentiment.",
            "The review expresses multiple weaknesses including unclear presentation, lack of coherence between the theoretical and empirical parts, weak experimental results, and unclear impact of the theoretical analysis compared to past work. The reviewer uses phrases like 'not clear to me', 'looked incoherent to me', 'experimental results are weak', and 'I was not clear about the impact'.",
            "The reviewer expresses happiness ('Happy') with the additions to the paper, indicating a positive reception.",
            "The review expresses both positive feedback (\"great that you've replicated the NQ results\", \"Thanks for including the proof sketch\") and critical concerns (\"still hesitant to read into small changes\", \"why would we believe that it's more than a hack\"). The overall sentiment is therefore neutral.",
            "The review expresses overall positive feedback about the paper. It highlights strengths such as clear writing, high impact theoretical contributions, and the supplementation of theoretical analysis with experimental analysis. The reviewer explicitly states 'No major weaknesses.'",
            "The reviewer expresses appreciation for the paper's thought-provoking nature, useful framing, and interesting perspective. They explicitly state the paper is relevant to the ICLR audience and that the audience would be excited about it."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The review adopts a balanced tone by acknowledging the strengths of the paper while also providing constructive criticism. Phrases like \"The main results of this paper are of broad interest\" and \"This paper makes an intriguing claim\" demonstrate appreciation, while phrases like \"I find it difficult to follow\" and \"I also have some doubts about the empirical results\" indicate critical evaluation.",
            "The review presents both positive aspects ('Strength', 'effective', 'meaningful') and negative aspects ('Weakness', 'not easy to understand', 'complicated', 'limited') of the paper in a structured manner, indicating a balanced tone.",
            "The review adopts a critical tone by directly pointing out flaws and areas of improvement in the paper. Phrases like 'presentation of the paper needs to be improved', 'I felt this was quite separate', 'experimental results are weak', and 'It will be good to rewrite' indicate a critical assessment of the work.",
            "The reviewer uses positive language ('Happy', 'thanks') and expresses appreciation for the author's work, creating a supportive tone.",
            "The review offers both praise and constructive criticism. Phrases like \"I think it's great\", \"Thanks for including the proof sketch\" demonstrate a positive aspect, while suggestions and questions like \"However, I'm still hesitant\", \"why would we believe that it's more than a hack\" indicate a critical and analytical approach, resulting in a balanced tone.",
            "The review uses supportive language by highlighting the strengths of the paper. Phrases like 'Clearly written paper' and 'theoretical contributions will have a high impact' indicate a supportive tone. The reviewer also provides constructive criticism in a polite and helpful manner.",
            "The reviewer uses phrases like 'I thought that this paper was very thought-provoking, and I appreciated the attempts to better understand...', 'Useful framing and discussion...', 'reasonable initial experimental results...', and 'I thought this paper was interesting and brings a new perspective...' indicating a supportive and encouraging tone. While they point out weaknesses, it is done constructively."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently points out both the strengths (novelty, theoretical contribution) and weaknesses (clarity, empirical validation, potential drawbacks) of the paper without contradicting itself. The reviewer acknowledges the value of the theoretical argument but raises valid concerns about the practical implications and empirical support.",
            "The review presents both strengths and weaknesses of the paper that are logically distinct and contribute to an overall assessment. The reviewer's points are focused on specific aspects like motivation clarity, theory complexity, and experimental scope, without contradicting themselves.",
            "The review consistently points out issues related to the clarity of the theoretical contribution compared to prior work, the disconnect between the theoretical and empirical parts, and the weakness of the experimental results. The reviewer's concerns are aligned and do not contradict each other.",
            "The review expresses a positive sentiment and appreciation for the added explanations without any contradictions.",
            "The reviewer consistently questions the generalizability of the proposed method (KNN TAPT) beyond the specific tasks presented and asks for more evidence to demonstrate broader applicability, suggesting evaluations on different tasks or fine-tuning. There are no self-contradictory statements in the review.",
            "The review is consistently positive. It highlights strengths such as clarity, high impact theoretical contributions, and experimental validation.  It explicitly states 'No major weaknesses'. The minor aspects mentioned are indeed minor and do not contradict the overall positive assessment of the paper's strengths.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper in a balanced manner. The reviewer appreciates the theoretical contributions and novel perspective while also pointing out areas for improvement in empirical validation and further exploration. There are no contradictory statements within the review; the criticisms are constructive and aimed at strengthening the paper, not undermining its overall value."
        ]
    },
    {
        "paper_id": "nips_2022_3AV_53iRfTi",
        "paper_title": "Perceptual Attacks of No-Reference Image Quality Models with Human-in-the-Loop",
        "paper_abstract": "No-reference image quality assessment (NR-IQA) aims to quantify how humans perceive visual distortions of digital images without access to their undistorted references. NR-IQA models are extensively studied in computational vision, and are widely used for performance evaluation and perceptual optimization of man-made vision systems. Here we make one of the first attempts to examine the perceptual robustness of NR-IQA models. Under a Lagrangian formulation, we identify insightful connections of the proposed perceptual attack to previous beautiful ideas in computer vision and machine learning. We test one knowledge-driven and three data-driven NR-IQA methods under four full-reference IQA models (as approximations to human perception of just-noticeable differences). Through carefully designed psychophysical experiments, we find that all four NR-IQA models are vulnerable to the proposed perceptual attack. More interestingly, we observe that the generated counterexamples are not transferable, manifesting themselves as distinct design flows of respective NR-IQA methods.",
        "review_ids": [
            "mqFGNaTiJAF",
            "wNPjyy810Qf",
            "clRhX0Q2AQm",
            "iT1Pm_rHgc9"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " This paper builds upon work from several fields (adversarial attacks, MAD competition, and Eigen-distortion analysis) which are analysis by synthesis methods used to generate small magnitude perturbations (by some pixel measure) that cause large change in model response (either in classification for adversarial attacks, or perceptual distance in MAD and Eigen-distortion).  The paper extends this to the field of no-reference image quality metrics, and compares their contribution with these other methods.  They successfully use this method to synthesize images that change the NR-IQA quality score significantly but are below human detection (with humans in the loop).  They use these images to quantify model to model performance, but also to elucidate how the construction of specific models leads to specific failure cases, which demonstrates one of the key values of analysis-by-synthesis. This is a very strong paper that builds nicely on several previous works, but meaningfully advances the work and extends it to an application that was not previously accessible. They also derive key insights from application of their method that could lead to followup work to improve NR-IQA models. This paper fits nicely within the NeuRIPS audience sweet spot, mixing knowledge and techniques from perceptual science, signal and image processing and machine learning.\n\nThere are a few details of the human experiments that should be elucidated further, these will be included in questions below.  I would encourage the authors to work in luminances when quantifying perceptual quality (in human experiments and in IQA models) and not fixed code values which are less generalizable to different displays and viewing environments.\n\n Table 1 is incomprehensible on first read and should be reorganized for more clarity.  It shouldn't require reading several pages past the table to fully digest what it is communicating.  \n\nDoes the performance of UNIQUE actually improve in the face of the Brisque attacks over naive performance? (Same for Cornia under Unique attacks). What does that imply?\n\nWhy did the authors retrain the networks (at all), and why each of them on different datasets? \n\nIn addition to the four FR-IQA models, it would be interesting to compare to pixel MSE as an isotropic (in the pixel space) baseline. This will tell you the most about the NR-IQA model in the specific (as all pixel directs are equally penalized), whereas the other results show you a combination of the effects of the FR-IQA model used as well as the NR-IQA model, which is still useful but requires more effort to properly interpret. \n\nMore details about the exact lux of the illumination and the brightness of the displayed images on the displayed monitor would be useful.\n\nFor your derived value R, is there a theoretical upper bound that would provide satisfactory robustness?\n\nNot a question, but this is a perfect encapsulation of the power of analysis by synthesis : \n\"The primary observation is that the difference in perturbations provides useful clues on how they extract quality-aware features.\"  The results presented in the final figure are insightful and very interesting to ponder.\n Yes the author's carefully place their work in the context of many other works and carefully point on the limitations and advantages of their contribution with respect to this work.",
            " This work presents a detailed analysis of the vulnerability of commonly used no-reference image quality assessment (NR-IQA) metrics to perceptual attacks. To this end, the authors include four NR-IQA metrics, BRISQUE, CORNIA, Ma19, and UNIQUE, into their analysis. The possibility of perceptual attacks is explored with four full reference IQA metrics, Chebyshev distance, SSIM, LPIPS, and DISTS. Based on the study, the authors have come up with several important observations that shed light on the design flows of the specific NR-IQA methods. Strengths\n\n- First of its kind work to study the perceptual attack vulnerability of multiple NR-IQA methods\n- The findings in the works point to the design flows of the existing NR-IQA methods while also giving insights into the need for developing NR-IQA methods that can well resist perceptual attacks.\n\nWeaknesses\n\n- The possibility of attacks that can simultaneously affect multiple NR-IQA metrics is not investigated\n 1. Based on the experimental observations, can the authors provide some suggestions on some easy tricks that one could adopt to mitigate the vulnerability of perceptual quality metrics?\n\n2. What about generating attacks that could simultaneously affect multiple NR-IQA methods? If that is not possible, can one rely on multiple metrics and hope for improved/complete resistance to perceptual attacks?\n Has been addressed to a satisfactory level.",
            " This submission proposes a Lagrangian formulation to study the perceptual attack of no-reference image quality. By using the Lagrangian relaxation between the prediction error of NR-IQA and the perceptual constraints with FR-IQA, the perceptually imperceptible counterexamples are generated. With the psychophysical experiments, the analysis of the perceptual attack of NR-IQA is conducted. Strengths:\n1)\tperceptual attack of NR-IQA is an interesting problem, which would benefit to the image quality assessment community.\n2)\tThe visualization of the perturbations to the different NR-IQA methods shows the different characteristics, which is also interesting.\n\nWeaknesses:\n1)\tIt states the connections with the existing methods, while it is better to validate the connections by experiments. \n2)\tIt lacks comparisons, making it hard to judge its advancement.\n3)\tIt is better to provide the statistic for the relationship between the Lagrange multiplier and the perceptual quality. \n4)\tThe original performance of R in Table 1 is positive infinity, and the following values are not informative. Equation 6 can be modified by adding a regular term to the denominator to make the original performance a positive real number.\n5)\tSSIM is a higher the better metric, and it should be stated in the text whether (1-SSIM) or -SSIM is used as the D\n 1\uff09What is the basis for choosing these 6 images and their corresponding distortions\uff1f N/A",
            " This paper aims to investigate the perceptual robustness of NR-IQA models. It proposes a Lagrangian relaxation of the existing adversarial attack formula. Then it gives the algorithm for perceptually imperceptible counterexample generation. The experimental results show some findings. Strengths: \nThe problem is novel.\nThe problem and its relationship with adversarial attacks in classification are well described. The Lagrangian relaxation and the algorithm for perceptually imperceptible counterexample generation are easy to follow.\n\n\nWeaknesses:\nThe motivations to investigate the perceptual robustness of NR-IQA models should be given. Why this problem is important?\nThe findings are a bit general. What are the insights for the application of NR-IQA metrics? Are there any suggestions that can be concluded from the findings to defend against the adversarial attack?\n 1. Why this problem is important?\n\n2. Are there any suggestions that can be concluded from the findings to defend against the adversarial attack?\n\n3. How do you make sure that the perturbation is below the just-noticeable difference? The examples computed by MAP estimation shown in Fig. 2 are clearly different from the original image. The examples of the proposed method are not given in the paper. So it is hard to judge whether the perturbation is noticeable or not.\n\n4. What is the relationship between the four values in Italics and the value in parentheses for each group?\n The limitations are not discussed in the paper."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Negative"
        ],
        "sentiment_reason": [
            "The review contains phrases like \"very strong paper,\" \"nicely advances the work,\" \"derive key insights,\" \"fits nicely within the NeuRIPS audience sweet spot,\" and \"insightful and very interesting to ponder,\" indicating a positive overall assessment.",
            "The review expresses overall positive feedback, highlighting the work's strengths as being the first of its kind and providing valuable insights. The weaknesses are presented as suggestions for improvement rather than critical flaws.",
            "The review identifies both strengths and weaknesses of the submission, presenting a balanced perspective without leaning heavily towards positive or negative feedback. The language used is objective and evaluative.",
            "The review expresses concerns about the paper's motivations, findings, experimental validation, and limitations. It raises several questions and points out weaknesses in the paper's presentation and justification."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review acknowledges the paper's strengths and contributions (\"very strong paper,\" \"nicely advances the work\") while also pointing out areas for improvement and posing clarifying questions (\"Table 1 is incomprehensible,\" \"Why did the authors retrain the networks?\"). This combination of positive feedback and constructive criticism suggests a balanced tone.",
            "The review presents both strengths and weaknesses, using objective language and constructive suggestions. The tone is professional and aims to provide helpful feedback for the authors.",
            "The tone is critical due to the identification of several weaknesses, including lack of experimental validation, missing comparisons, and issues with the experimental setup and metrics used. Phrases like 'It lacks comparisons, making it hard to judge its advancement' and specific points about the original performance of R in Table 1 contribute to this critical tone.",
            "The review uses phrases like \"Weaknesses,\" \"a bit general,\" \"hard to judge,\" and asks direct critical questions such as \"Why this problem is important?\" and \"Are there any suggestions...?\" This indicates a critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive about the paper's contribution and significance, while also providing constructive criticism and specific questions for improvement. It highlights the strengths of the paper and suggests areas for clarification and further development without contradicting its overall positive assessment.",
            "The review is consistent because the identified weaknesses are presented as suggestions for improvement and further research directions, rather than fundamental flaws that contradict the acknowledged strengths of the work. The reviewer appreciates the novelty and insights of the study while pointing out a limitation in scope, which is a constructive critique rather than a contradiction.",
            "The review is consistent because the identified strengths and weaknesses are logically distinct and address different aspects of the paper. The strengths acknowledge the interesting nature of the problem, while the weaknesses point to areas needing improvement in methodology, validation, and clarity, which are standard and valid points in a scientific review. There are no self-contradictory statements.",
            "The review is consistent because it clearly outlines both the strengths and weaknesses of the paper. The weaknesses are presented as valid points for improvement, focusing on missing motivations, generality of findings, lack of clarity on perturbation imperceptibility, and absence of limitation discussion. The questions raised directly support and elaborate on the identified weaknesses, seeking further clarification and detail in these areas. There are no contradictory statements within the review; the reviewer maintains a consistent critical perspective aimed at constructive feedback."
        ]
    },
    {
        "paper_id": "iclr_2022_beUek8ku1Q",
        "paper_title": "k-Median Clustering via Metric Embedding: Towards Better Initialization with Privacy",
        "paper_abstract": "In clustering algorithms, the choice of initial centers is crucial for the quality of the learned clusters. We propose a new initialization scheme for the $k$-median problem in the general metric space (e.g., discrete space induced by graphs), based on the construction of metric embedding tree structure of the data. From the tree, we can extract good initial centers that can be used subsequently for the local search algorithm. Our method, named the HST initialization, can also be easily extended to the setting of differential privacy (DP) to generate private initial centers. Theoretically, the initial centers from HST initialization can achieve lower error than those from another popular initialization method, $k$-median++, in the non-DP setting. Moreover, with privacy constraint, we show that the error of applying DP local search followed by our private HST initialization improves previous results, and approaches the known lower bound within a small factor. Empirically, experiments are conducted to demonstrate the effectiveness of our methods.",
        "review_ids": [
            "pr2luZWNAeg",
            "0apcTAJkTQE",
            "4MDtvNTy5u",
            "fjZ0W1_J0OL",
            "ur-c-r4aYCG",
            "tAtpqJuUTwg"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for your response and the details on the runtime and analysis.",
            "This paper considers the problem of finding good initial centers for the fundamental problem of $k$-median clustering using a randomized embedding of the original metric into a tree metric.  After setting the initial centers, a standard local search algorithm is applied to produce an improved solution.  This is explored in both the standard context of $k$-median clustering, as well as in the relevant context of differentially private clustering.  In the latter setting, the goal is to minimize the amount of additive error introduced by the algorithm subject to being $\\epsilon$-differentially private.  An extension to $k$-means is given in the appendix.\n\nIn the standard setting of $k$-median clustering, the main theoretical result is an initialization algorithm which is an $O(\\log(\\min(\\Delta,k)))$-approximation to the optimal k-median clustering.  This is an improvement over k-median++ (which gives $O(\\log k)$) when $\\Delta$ is small, e.g. for $\\Delta = O(d)$ and $d$ is small.  Using this as a seed results for a local search method results in an $O(1)$-approximation overall.  At a high level, their algorithm first constructs an embedding of the original metric into a hierarchically well-separated tree (HST).  From there, the initialization can be seen as finding an $O(1)$-approximate solution on the HST efficiently.  The overall guarantee follows from standard results about HST's\n\nIn  the differentially private setting, the main result is a similar guarantee on the quality of the initial solution and also a bound on the quality of the final solution when using a known private local search algorithm.  The quality of the final solution has $O(1)$-multiplicative error and $O(\\epsilon^{-1}k^2\\Delta\\log(n)\\log\\log(n))$ additive error.  This is an improvement over the additive error of $O(\\epsilon^{-1}k^2\\Delta\\log^2(n)$ due to Gupta et al. 2010.  The number of local search iterations is also improved from $O(k\\log n)$ to $O(k\\log\\log n)$.  The main idea for the initialization is similar to the standard setting, but here they use the structure of the HST to ensure the initial solution is private by injecting a different amount of noise at each level of the tree.\n\nAn empirical study is done on a class of synthetic graphs as well as the MNIST dataset.  For the synthetic graphs, the metric space is given by the weighted shortest path distance in each graph, while for the MNIST dataset the metric is given by either $\\ell_1$ or $\\ell_2$.  The authors compare both the initial costs and the final costs (after running a local search method) for several initialization methods in both the standard and differentially private settings.  The main observation is that the proposed initialization methods tend to have better initial cost and the proposed differentially private method often outperforms the other methods in both initial cost and final cost.\n\n\n\n The main strengths of this paper lie in the differentially private version of their algorithm.  Here we get both theoretical and empirical improvements over prior work.  I also appreciated the simplicity of the algorithms and the clarity of the presentation.\n\nIn terms of weakness, first there is some misleading language with how the result for the standard setting is presented.  On page 2, the paper claims that the proposed method provides an $O(\\log \\min(k,d))$-approximation, but later on page 6 this is clarified as being an $O(\\log(\\min(\\Delta,k)))$-approximation.  This achieves the former bound when the input data is bounded so that $\\Delta = O(d)$, but this caveat is not discussed in the beginning of the paper.  I recommend the authors to move this discussion up to the statement of their contributions to avoid misleading readers.\n\nNext, the main approach of the paper is to embed the input metric into a tree metric (via an HST) then efficiently compute an O(1)-approximation on this HST.  Metric embeddings (especially tree embeddings) are a standard technique in approximation algorithms, and this should be made clear in the discussion.  Additionally, it should be noted that there are polynomial time algorithms for computing an exact k-median solution on a tree metric (e.g. see [1,2]).  The proposed algorithm is a worthwhile contribution due to its simplicity, but these prior results should be discussed.  Additionally, it would be interesting to use one of these exact methods as a baseline in the experiments.\n\nIn the experiments, the authors run a fixed 20 iterations of local search after finding the initial centers, then report the k-median costs.  It might be interesting to also compare the runtime/iteration cost of reaching a locally optimal solution for the proposed methods and baselines, as well as the final cost of the locally optimal solutions found by each.  This sort of experiment seemed to be motivated by the discussion of the improved iteration bound for the differentially private method, but is missing.\n\nAs of now I lean more towards rejection, but would be inclined to increase my score of the above comments are addressed.\n\nReferences\n\n[1] - Shah, Rahul. Faster algorithms for k-median problem on trees with\nsmaller heights.  Technical report. 2003.\n\n[2] - Tamir, Arie.  An $O(pn^2)$ algorithm for the $p$-median and related problems on tree graphs.  Operations Research Letters. 1996.  \n\n--------------------------------------------------\n\nEdit after reading the author's responses:\n\nOne of the main benefits of the proposed method for clustering on a tree is that it can be adapted to the differentially private setting, which it is unclear how to do for other (dynamic programming based) methods.  I have raised my score to weak accept.\n\n\n This paper considers initialization methods for $k$-median clustering in both the standard setting and the differentially private setting.  The paper gives theoretical bounds for their methods in both settings and backs this up with an empirical study.  Given the misleading presentation of some of the results, a lack of discussion/comparison to prior work on k-median in tree metrics, and a lack of running time/iteration count comparison in the experiments, I am not okay with accepting this paper unless these points are addressed.",
            " Thank you for the detailed response.  I see that one benefit of the proposed k-means algorithm for HST's over exact dynamic programming on a tree is that it lends itself to the differentially private implementation better.\n\nIn terms of the running time of the proposed method, the revision states it is $O(dn \\log n)$ in the euclidean case.  What is the running time for general metrics?",
            "The paper proposes a new initialization scheme for the k-median problem on graph input (or general metric spaces) using metric embedding tree structure. The paper proposes an algorithm that finds initialization of good centers using HST that gets an approximation factor of O(log min{k,d}) if the data is in Euclidean space  where d is the number of dimensions. Then, the paper studies clustering with differential privacy guarantee and hows that the initialization method could be adapted to give a slightly stronger muliplicative and additive errors. The work complemented these theoretical findings with experiments and show that the proposed initialization imporves the performance of k-median++ initialization. Overall the paper makes some good contributions and adds to the communities' understanding of the k-clustering problem, espeically with privacy constraints. The paper proposed new algorithm designs and proved updated bounds for k-median with/without privacy constraints.  The experiment design also seems comprehensive and persuasive to me.\n\nThe writing in this paper is mostly smooth but could still be improved and be more clarifying in some ways. The paper did well in presenting the algorithm's framework and the ideas there are interesting. Although one can argue that the improvement in approximation ratio in classical k-median clustering is marginal, but as the authors noted, the design is new and it serves well for the setting of differential privacy constraints. I do think the authors could be more clear when talking about previous work on privacy clustering, the approaches used and the differences, the setting of Euclidean space v.s. general graph input. Right now the problem setting and the results seem a bit confusing to me.\n\nMinor comments:\n1. Does the number \"2\" really matter in the 2-HST that you are using, or can we replace it with something like $1+\\epsilon$?\n2. In the approximation ratio there is $min\\{k, d\\}$. Does that mean that we can only obtain this result with points in Euclidean space? \n3. The notation $N(v)$ and $N_v$ share the same meaning, right?\n3. When defining $score(v)=N(v)\\cdot 2^{h_v}$, is this the first time the notation $h_v$ appears? I couldn't seem to find a definition of it.\n4.  In my opinion the paper sometimes uses a new terminology and assume the reader knows what it is. I think it is better to introduce terminologies and include a short description of them just to make sure the reader is on the same page. For example, the major comparison, k-means/median++,  is never fully explained.\n5. The paper studies k-median clustering, I wonder if we know anything about what happens when we switch to k-means. Do the conclusions still hold? Mostly, I consider the contribution made in this paper to be meaningful to the clustering community, but it could be improved (at least in writing). It has a valid theoretical framework, but as a review from the broader clustering community, I find it hard to judge how significant these findings are.",
            "This paper introduces a new initialization scheme for the k-medians clustering problem in the general metric space setting. This is based on the construction of metric embeddings via 2-HST\u2019s (Hierarchically well-separated trees). The authors also extend this to the differential privacy (DP) setting. They prove approximation guarantees in both the non-DP and DP settings, improving upon the literature. Finally, they empirically validate algorithms against a number of baselines with both real world and synthetic datasets for multiple metrics. Strengths:\n- This paper gives improved k-medians bounds in the general metric setting (non DP) that improve over the literature in the d < k regime. It also gives the best known theoretical guarantee in the DP setting, and is worse than the lower bound by a small additive factor (k log log n)\n- The paper empirically compares their algorithm to a number of standard baselines, showing favorable results for multiple datasets and metrics, and especially for less separable data and when the input data an unbalanced subset of the universe.\n\nWeaknesses:\nA few comments on improving the paper:\n- The paper does not discuss runtimes of algorithms. A discussion of runtimes as well as comparison of runtimes in experiments would be useful\n- Regarding the results for k-means - moving the k-means results to the main paper, as well as a brief discussion and comparison to k-medians would be useful.\n\nWe give a number of suggestions to improve clarity:\n- \u201cHST tree\u201d -> \u201cHST\u201d\n- \u201csymmetric difference one\u201d -> \u201csymmetric difference of size one\u201d\n- (In Algorithm 1) \u201ccost(F - x + y)\u201d -> \u201ccost(F - {x} + {y})\u201d\n- \u201cwe will count levels from large to small\u201d -> \u201cwe will count levels in descending order down the tree\u201d\n- (Section 3.2 Intro) - \u201cSuppose T is an L = log \\Delta -level-2 HST\u201d -> \u201cLet L = log \\Delta and suppose T is an L-level-2-HST\u201d\n- (Algorithm 2) Mention Algorithm 6 in \u201cBuild a level-L 2-HST tree T based on input U\u201d\n- State Theorem 3.4 and Theorem 3.5 before lemmas\n- State that NDP stands for Non Differentially Private\n- Theorem 4.2: Constants (10) can be absorbed into big-oh notation\n This paper improves upon the literature for the k-medians clustering problem in the general metric setting as well as in the differentially private case. The approximation guarantees improve the best known results in this case. The experiments demonstrate the validity of the theoretical contributions as well.",
            "The paper suggests an algorithm for the metric k-median problem using ideas from Metric embedding theory. The suggested use of the algorithm is as an initialization routine for the local search based algorithm for k-median. The differentially private version of the algorithm is also given along with bounds on k-median approximation factor. Experiments are conducted over datasets such as MNIST and results compared against the k-means++ algorithm (a popular initialisation algorithm). The paper uses familiar techniques from metric embedding literature to suggest an algorithm for the k-median problem. The contributions does not seem to be very strong.\n- The Metric embedding based algorithm for k-median is shown to give approximation guarantee of log{min(k, d)} which is better than k-means++ which gives O(log k). However, there are other algorithms that give much better approximation guarantees. It is not clear why the comparison is done with k-means++ here. This is something that the paper does not elaborate. Perhaps the algorithm is being suggested as an initialisation routine and hence the comparison is done with k-means++ but then that cannot be the only reason since the other algorithms with better approximation guarantees can also be suggested as initialisation routines. The discussion seems to be lacking on this aspect in my opinion.\n- The improvement with respect to the Differentially Private seems to be minor over the previous work. In summary, the paper neither introduces new techniques nor obtains significant improvement over past results. My suggestion for improving the paper would be to add a discussion on why the suggested algorithm should be the right \"initialisation\" algorithm."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('Thank you') for the response and details provided, indicating a positive reception.",
            "The reviewer initially leans towards rejection and expresses concerns about misleading language, lack of discussion of relevant prior work, and missing comparisons in the experiments. Although the score is raised to 'weak accept' after author responses, the overall tone remains critical.",
            "The reviewer expresses gratitude for the detailed response, indicating a positive reception to the authors' previous actions. They also acknowledge a benefit of the proposed algorithm.",
            "The reviewer states that the paper makes 'good contributions' and adds to the community's understanding. They also find the experiment design 'comprehensive and persuasive'. The reviewer concludes that the contribution is 'meaningful'.",
            "The review highlights several strengths of the paper, including improved k-medians bounds, best-known theoretical guarantees in the DP setting, and favorable empirical results compared to baselines. The reviewer concludes that the paper 'improves upon the literature' and that 'experiments demonstrate the validity of the theoretical contributions'.",
            "The review expresses concerns about the paper's novelty and impact, stating that the contributions \"does not seem to be very strong,\" the improvement in the differentially private aspect is \"minor,\" and the paper \"neither introduces new techniques nor obtains significant improvement over past results.\""
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Neutral",
            "Balanced",
            "Supportive",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer's use of 'Thank you' indicates a supportive and appreciative tone.",
            "The review uses phrases like 'misleading language,' 'lack of discussion,' and 'not okay with accepting' to express concerns. It also provides specific suggestions for improvement, indicating a critical assessment of the paper's current state.",
            "The tone is neutral as the reviewer acknowledges the authors' work and poses a question about the running time complexity, without expressing strong opinions or emotions.",
            "The review offers both positive feedback ('good contributions', 'comprehensive and persuasive') and constructive criticism ('could still be improved', 'a bit confusing'). The reviewer also poses questions and suggestions for improvement, indicating a balanced approach.",
            "The reviewer expresses clear support for the paper's contributions, using positive language such as 'improved,' 'best known,' 'favorable,' and 'demonstrate the validity'. The suggestions for improvement are framed as 'comments on improving the paper' and 'suggestions to improve clarity,' indicating a constructive and supportive approach.",
            "The review uses phrases like \"not clear why the comparison is done,\" \"discussion seems to be lacking,\" and \"improvement with respect to the Differentially Private seems to be minor\" to point out weaknesses and shortcomings in the paper. The reviewer also suggests a way to improve the paper, indicating a critical but constructive approach."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is a short, positive statement expressing gratitude and acknowledging the provided details. There are no contradictory points within the review.",
            "The review is consistent in its assessment. It identifies both strengths and weaknesses of the paper, providing specific reasons for each. The reviewer's initial concerns are clearly stated and logically lead to a 'lean towards rejection' stance. The edit after reading author responses shows a reasonable shift in opinion towards 'weak accept' based on a specific advantage of the proposed method, demonstrating a consistent and evolving evaluation process.",
            "The review is consistent as it acknowledges the authors' response and a benefit of the proposed method while asking a relevant clarifying question about the running time for general metrics, without any contradictions.",
            "The review is consistent. It acknowledges the paper's contributions and strengths, such as proposing a new algorithm, proving updated bounds, and having comprehensive experiments. At the same time, it points out areas for improvement, mainly focusing on clarity in writing, better contextualization of prior work, and clearer definitions of terminologies. The reviewer's concerns are specific and constructive, aiming to enhance the paper's quality without contradicting the initial positive assessment of its contributions and validity.",
            "The review is consistent as it highlights the paper's strengths, such as improved theoretical bounds and empirical validation, and suggests constructive improvements, like discussing runtimes and clarifying notation, without contradicting its overall positive assessment of the paper's contributions and validity.",
            "The review is consistent in its assessment. It acknowledges a potential advantage over k-means++ but points out limitations regarding comparison to state-of-the-art algorithms and the marginal improvement in differential privacy. The reviewer consistently argues that the contributions are not significant and suggests improvements to strengthen the paper's justification."
        ]
    },
    {
        "paper_id": "iclr_2021_jQUf0TmN-oT",
        "paper_title": "SACoD: Sensor Algorithm Co-Design Towards Efficient CNN-powered Intelligent PhlatCam",
        "paper_abstract": " There has been a booming demand for integrating Convolutional Neural Networks (CNNs) powered functionalities into Internet-of-Thing (IoT) devices to enable ubiquitous intelligent \"IoT cameras\u201d. However, more extensive applications of such IoT systems are still limited by two challenges. First, some applications, especially medicine- and wearable-related ones, impose stringent requirements on the camera form factor. Second, powerful CNNs often require considerable storage and energy cost, whereas IoT devices often suffer from limited resources. PhlatCam, with its form factor potentially reduced by orders of magnitude, has emerged as a promising solution to the first aforementioned challenge, while the second one remains a bottleneck. Existing compression techniques, which can potentially tackle the second challenge, are far from realizing the full potential in storage and energy reduction, because they mostly focus on the CNN algorithm itself. To this end, this work proposes SACoD, a Sensor Algorithm Co-Design framework to develop more efficient CNN-powered PhlatCam. In particular, the mask coded in the PhlatCam  sensor and the backend CNN model are jointly optimized in terms of both model parameters and architectures via differential neural architecture search. Extensive experiments including both simulation and physical measurement on manufactured masks show that the proposed SACoD framework achieves aggressive model compression and energy savings while maintaining or even boosting the task accuracy, when benchmarking over two state-of-the-art (SOTA) designs with six datasets on four different tasks. We also perform visualization for better understanding the superiority of SACoD generated designs. All the codes will be released publicly upon acceptance. ",
        "review_ids": [
            "60SpRWrROA",
            "kbhy32PIto",
            "LF5INkkF36",
            "TIPG2MxCR5"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "#################################\n\nSummary:\n\nThe paper proposed to adopt differentiable network architecture search (DARTS) for the co-design of the sensor (a lensless camera) and the deep model for visual recognition tasks, so as to maximize the accuracy and minimize the energy consumption. The key idea is to include the sensor configuration, in this case the phase mask of a lensless camera modeled as 2D convolutions, as additional parameters in architecture search.  The proposed method was evaluated on simulated data for a number of vision tasks (image classification, face recognition and head pose estimation), as well as using fabricated masks on a real world camera. The results demonstrated significantly increase recognition performance given the same energy level.  \n\n#################################\n\nPros:\n* The high-level idea of using NAS for the co-design of the sensor and the deep model is quite exciting.\n* The experiments are extensive, including both several vision tasks in simulation and a classification task using real-world sensor implementation.\n \n#################################\n\nCons:\n* Sensor Design: Generality vs Specificity \n\nThe paper proposed to tailor a physical sensor and bundle it with a deep model for a target vision task. Once realized, it is unclear if the sensor is able to adapt to a different task or even a different model. Would the phase mask identified on one model / task generalize to different tasks/models? Generality is an important property of visual sensors (e.g., RGB / ToF cameras). The same camera can be used for different tasks (e.g., image classification, face recognition, etc). And the captured images can be examined by various models. It is probably not surprising that higher accuracy and lower energy can be achieved if a sensor is specifically designed for a single task and for a single type of model. The question is why do we need such a high specificity. It seems very limited even in the IoT setting. For example, what if the backend model needs to be updated. \n\n* Lack of modeling for fabrication error\n\nFabrication error is quite significant and is not modeled. For example, on CIFAR 10, fabrication error accounts for a over 4% drop in accuracy, canceling out most of the gains from the co-design (using a Gabor mask has 2-3% drop in accuracy in comparison to the co-design). Is it possible to model fabrication error and encode that into the loss function for NAS? For example, a good design should avoid those patterns that are likely to create issues in fabrication. \n\n#################################\n\nMinor comments:\n\nThe paper has a strong vision flavor and might be a better fit to vision / computational photography conferences. \n\n#################################\n\nJustification for score:\n\nOverall, this is a paper with a very interesting idea and solid experiments. Yet, the problem setting is limited and the modeling has some issues. I am more positive about this paper after reading the authors' response. \n",
            "Summary\n\nThis paper presents a method called SACoD to develop a more efficient CNN-powered Phlatcam. The proposed method optimizes both the PhlatCam sensor and the backend CNN model simultaneously.  That is, the coded mask in Phlatcam and neural network weights are regarded as learnable parameters. The coded mask (the optical layer) can be considered as a special convolution layer. As a result, it achieves energy saving, model compressing as well as good accuracy. Extensive experiments and ablation studies are presented to show the effectiveness of the method.\n\nOverall,  I think the paper is interesting and properly designed. I also believe that various experiments and analyses of this paper can influence future related studies. However, I hope many experiments using raw images obtained from the real-world lensless imaging system will be included.\n\nStrength\n\nThis paper is the first attempt to optimize both sensor and CNN-model at the same time. The derivation and formulation look interesting.\n\n\nComments and Weakness\n\nIn figure3, the input seems like a normal RGB image. Also, data in datasets such as CIFAR10 and Cityscape are common RGB images. Then, wasn't it applied to the real image obtained using the lensless imaging system? Even in Table 1, reported results were from common RGB images in CIFAR-10. In both optimized and experimental results, source images for testing are not real-world data from lensless imaging systems.\n\nIs there any comparative test of the three methods in Figure3? Is there a reason why they are mentioned? The reasons why design (a) was chosen are mentioned in the text, it is also better to show them experimentally.\n\nTo what level can the Flops and energy be lowered based on the same baseline CNN model, and is it enough to put it on the real-world IoT system?\n\nIt would be better to compare the proposed method with more than two baseline methods. For example, I wonder what happens when regular CNN without optical layers is used. Also, it would be good to have an experiment on various types of mask filters except for Gabor.\n\nAbout table2, What is the reason for the big difference in CIFAR-100 while there is not much increase in CIFAR-10?\n\nFont sizes in Figure4 and Figure 6 are too small to read.\n",
            "##########################################################################\nSummary:\n \nSACoD presents a novel attempt to integrate the computational capabilities of a lensless imaging system, PhlatCam, with the search for the optimal convolutional neural network design for a given task. SACoD provides a framework which enables joint optimization of sensor and CNN resulting in IoT devices that achieve higher task accuracy\u2019s with limited resource budgets of a typical IoT system. The authors present a new an optical layer design that enables above described features. Detailed experiments comparing SACoD sensor + CNN with other baseline models covering past papers, demonstrate the superiority of SACoD\u2019s accuracy/efficiency curve over that of separately optimizing CNN arch or sensor/CNN joint-optimizations that do not vary network architecture. Additionally, ablation studies and results from measurements from actual phase masks fabricated help breakdown the accuracy/efficiency benefits of SACoD while analyzing the noise limitations of mask fabrication process.\n\n##########################################################################\nReasons for score: \n \nThe paper presents a sound theoretical description of the SACoD approach along with their optical layer design. The results presented through experiments clearly show the superiority of SACoD approach over other baselines that do not utilize the cost-free computational capabilities of the PhlatCam sensor. With the concerns raised in the Cons, sections answered I recommend that this paper be accepted.\n \n##########################################################################\nPros:\n\n1.\tSection 2 presents a good overview of the different approaches attempting sensor/CNN optimization to improve accuracy for given hardware resources. The proposed solution is unique in attempting to utilize the computation capability of PhlatCam imaging system.\n2.\tSection 3 presents a good theoretical description of the SACoD framework including the steps involved in its training to estimate the optimal mask, network arch and weights.\n3.\tIt is commendable that as noted at the end of Section 3, authors try out both differentiable and reinforcement learning (RL) based NAS approaches and achieve similar accuracies. Highlighting, the effectiveness of SACoD irrespective of the NAS approach utilized.\n4.\tSection 4 describes the detailed experiments that the authors used to assess the accuracy/efficiency benefits of SACoD over two other baseline approaches for 6 datasets and 4 tasks. Figures 4/5/6 are clear and help demonstrate the superior accuracy vs efficiency curves plotted for SACoD.\n5.\tAblation studies discussed in Section 4.6 on mask flexibility influence and optical layer effectiveness are useful to illustrate the importance of joint optimization of sensor and network as well as the effectiveness of SACoD optical layer.\n6.\tFigures showing a comparison of the performance of different approaches on vision tasks are helpful to show the superiority of SACoD.\n \n##########################################################################\nCons: \n \n1.\tSection 3, The optical sensing frontend subsection, describes how the object that is being imaged is at a distance d to the camera while trying to formulate the output of the masks in terms of a 2D convolution operation. Further in this subsection, the authors note that the mask is fixed at a distance d from the sensor. It would be useful to rephrase either of these sentences to clarify what is the distances of the object from the mask and the sensor.\n2.\tAuthors make an effort to describe the impact \u03b1 on w* and m*, it is not obvious to me as to why the dependence changes for w* and m*. Perhaps an example illustrating the indirect influence of \u03b1 on m* would be help clarify this.\n3.\tThe experiments in Section 4, present a range of datapoints for SACoD and other baselines for each task/benchmark. In my understanding, these points are obtained by tuning the accuracy vs efficiency tradeoff. How does the size of network change with these experiments? I worry that the efficiency of the GPU utilized for Flops and energy measurements might have biased the results somewhat against SACoD. Testing on a hardware platform with smaller on-chip memory and parallelism might be interesting exercise to show SACoDs superiority.\n4.\tSection 4.5 is commendable for confirming through measurements that the desired PSF can indeed be generated through fabricated masks. Further the ablation studies discussed in section 4.6 and appendix attempt to model the noise added due to fabrication process in the masks and the resultant accuracy loss. \n5.\tIts not clear that the magnitude of normal noise assumed by the authors is sufficiently capturing the fabrication noise measured. Considering that the accuracy drops reported in Table 1 and Figure 10 are for different benchmarks. It would be great if authors could use the same benchmark to model the measured 4% reduction in accuracy and then compare with other baselines. Cause for CIFAR 10, the SACoD approach does not have enough margin to dominate Gabor mask baseline despite a 4% accuracy loss. \n6.\tThis dependence on the noise during fabrication is a critical weakness of the SACoD approach. Since it relies on the computations carried out by these potentially noisy masks. The authors should consider developing a full-proof solution to this problem by utilizing noise model based training for their models such that the final network can be immune to the fabrication noises.\n\n##########################################################################\nQuestions during rebuttal period: \n\nKindly address the concerns noted in the Cons section\n",
            "The paper addresses the practical application-level problem with joint optimization from front-end sensor to back-end CNN algorithms. Authors validate the proposed method's advantages with comprehensive experiments on different tasks, including image classification, face/pose detection, image segmentation, and image translation. Lastly, the authors claim to contribute its source codes to the research community upon acceptance. Overall, I think this research paper is well-written with no flaw identified; more importantly, I believe this investigation will substantially impact many real-world applications via pushing CNN to edge devices.\nOne suggestion is to specify the search space for NAS, as the search space is usually application dependent. As it is not specified in the paper, I assume you adopt the default search space, which contains only the 10 operations, and I wonder what is the best set of operations among them that best for the sensor data?"
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive aspects (exciting idea, extensive experiments) and negative aspects (limited problem setting, modeling issues). The reviewer's final statement indicates a more positive outlook after reading the authors' response, but the overall sentiment remains neutral due to the initial concerns.",
            "The reviewer expresses overall positive sentiment, stating the paper is \"interesting and properly designed\" and that the experiments and analyses \"can influence future related studies.\" While the review includes criticisms, the overall assessment is favorable.",
            "The reviewer recommends acceptance of the paper after the concerns raised are addressed. The language used is generally positive, highlighting the strengths of the approach and the clarity of the results.",
            "The reviewer states \"Overall, I think this research paper is well-written with no flaw identified; more importantly, I believe this investigation will substantially impact many real-world applications...\""
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review presents both \"Pros\" and \"Cons\" sections, addressing the strengths and weaknesses of the paper. The language is objective and constructive, avoiding overly harsh or enthusiastic language. Phrases like \"quite exciting\" and \"some issues\" demonstrate a balanced perspective.",
            "The review provides both positive feedback (strengths) and constructive criticism (weaknesses and comments). The reviewer acknowledges the novelty of the approach while also pointing out limitations and areas for improvement, resulting in a balanced tone.",
            "The review provides both positive feedback (Pros) and constructive criticism (Cons). The tone is objective and aims to improve the paper rather than simply praising or condemning it. The reviewer uses phrases like \"It is commendable\" and \"would be useful\" to offer suggestions and highlight strengths.",
            "The reviewer expresses enthusiasm for the paper's potential impact and calls the paper 'well-written.' The suggestion is framed as a question, indicating a desire to help improve the paper rather than a direct criticism."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it highlights both the strengths (exciting idea, extensive experiments) and weaknesses (generality of sensor design, lack of fabrication error modeling) of the paper. The justification for the score summarizes these points, presenting a balanced and consistent evaluation.",
            "The review is consistent in its critique, primarily focusing on the lack of experiments using real-world lensless imaging data and the need for more comprehensive comparisons with baseline methods. The reviewer acknowledges the strengths of the paper but consistently points out areas for improvement related to empirical validation and broader context.",
            "The review is consistent because it highlights both the strengths (Pros) and weaknesses (Cons) of the paper. The reviewer recommends acceptance contingent on addressing the concerns raised in the Cons section, indicating a constructive and balanced assessment. The Pros section praises the novelty, theoretical soundness, experimental design, and clarity of presentation, while the Cons section points out areas for clarification and improvement, particularly regarding experimental details and potential limitations. The recommendation for acceptance after addressing the Cons aligns with this balanced perspective, showing no self-contradiction in the reviewer's assessment.",
            "The review is consistently positive. It starts with an overall positive assessment, praising the paper's contributions and impact. The suggestion provided is constructive and does not contradict the initial positive evaluation. The reviewer identifies no flaws while still offering a point for potential improvement, which is a consistent and helpful review approach."
        ]
    },
    {
        "paper_id": "nips_2021_E8BxwYR8op",
        "paper_title": "HNPE: Leveraging Global Parameters for Neural Posterior Estimation",
        "paper_abstract": "Inferring the parameters of a stochastic model based on experimental observations is central to the scientific method. A particularly challenging setting is when the model is strongly indeterminate, i.e. when distinct sets of parameters yield identical observations. This arises in many practical situations, such as when inferring the distance and power of a radio source (is the source close and weak or far and strong?) or when estimating the amplifier gain and underlying brain activity of an electrophysiological experiment. In this work, we present hierarchical neural posterior estimation (HNPE), a novel method for cracking such indeterminacy by exploiting additional information conveyed by an auxiliary set of observations sharing global parameters. Our method extends recent developments in simulation-based inference (SBI) based on normalizing flows to Bayesian hierarchical models. We validate quantitatively our proposal on a motivating example amenable to analytical solutions and then apply it to invert a well known non-linear model from computational neuroscience, using both simulated and real EEG data.\n",
        "review_ids": [
            "ZRSFGOnF5G-",
            "bsNUpuE9VUa",
            "2ZXNB2KHj5D",
            "3AyBEaIU6a",
            "bm12xJWpyP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the response! I have increased my score based on your extended results covering a more comparable baseline for the NMM test case with matched network capacity and observation aggregation. I appreciate your inclusion of a test case involving real data, and it is indeed good to see such practical applications shown in the literature.\n\nFWIW, I also wanted to mention that renaming your method to HNPE as you suggested in one of your responses seems like a good idea to me and should make the connection to SNPE more apparent.",
            "The paper extends SNPE-C to the situation where multiple sets of parameters yield identical observations, but where the parameters can be split into local and global ones, which are specific to, or shared across all observations, respectively. It is shown how this additional information can be used to build a flow-based posterior estimator yielding sharper distributions.\n  The paper considers simulation-based inference in the presence of strongly coupled parameters leading to a non-injective likelihood function. This parameter estimation problem for such a system is ill-posed, but under additional assumptions this can be resolved with a hierarchical Bayesian model in which some parameters are shared across all observations.\n\nThe authors propose a practical realization of this solution in the form of a \"h-flow\" model, consisting of two normalizing flows modeling the posterior for the local and global parameters, and utilizing a deepset architecture for the global parameters to enforce permutation invariance. The network is optimized with KL divergence, and the authors show how to train it within the multi-round SNPE-C scheme.\n\nh-flow is then experimentally tested on a simple model dependent on the product of two parameters, and on a 4d neural-mass model (NMM) of a cortical column with 3 local parameters and 1 global one. Performance is comparable to a naive normalizing flow in the simple case, but a significant improvement is seen for the NMM. To further show practical applicability, the authors then use h-flow to estimate NMM parameters on real EEG signals recorded under eyes open/closed conditions, and show that this results in separated distributions for the two conditions, in line with expectations.\n\nThe paper is clearly written, provides simple and illustrative motivating examples, and keeps the most relevant information in the main text. The proposed method is practically useful, and easy to implement, which should lead to adoption among practitioners already using SNPE-C or similar SBI methods. The experimental results could be made a bit more convincing with additional models or more baseline methods in the case of the NMM.\n\n----\nScore revised +1 based on the authors' responses and the promise to report improved baselines in the NMM test case. No concerns seen here. ",
            "The authors propose a new likelihood-free inference method for Bayesian hierarchical models, called h-Flow, that exploits additional information provided by a set of auxiliary observations with shared, global parameters. h-Flow works by approximating two relevant posterior distributions, i.e. that of the global parameter beta and the local parameters alpha given beta, with separate normalizing flows, which are then multiplied to yield the joint posterior distribution. The normalizing flows are trained by minimising the KL-divergence between the true and approximated posterior distribution and the training data is refined over several rounds by means of SNPE-C (also known as APT).  The authors provide an approach to likelihood-free inference, which is a known and important task, for Bayesian hierarchical models. While the method is new, to me it appears to be a slight variation of SNPE-C that includes a second normalizing flow for global parameters that is conditioned on a set of auxiliary variables. I am thus mildly worried about novelty and significance. I would encourage the authors to emphasise their contributions and the difference to related work further.\n\nThe derivations are generally clear and technically sound. While the examples in their experiments are very interesting, I don't think they highlight the method very well. It is still unclear to me whether or not any improvements are due to the proposed method, or a) an increased flexibility or b) simply having more auxiliary observations that are correlated with new observations. The authors attempt to clarify this by using a naive approach that ignores the hierarchical structure, but this only consists of one normalizing flow with a lower capacity than the two flows of h-Flow. Similarly, as the authors said, N>0 generally yields better posteriors but this is naturally not very surprising. In my opinion, a fair baseline for e.g. the EEG experiments would have been a comparison of N=0 vs N=9 with both naive architectures and h-Flow, but a setting where both approaches have the same capacity.\n\nThe submission is generally clearly-written, at least until Section 3. I believe the experimental section could have been more concise with some explanations about the models and approaches being moved to the appendix, leaving space for, perhaps, one more experiment or more elaborate baseline comparisons. The authors did not comment on limitations of their method and the potential societal impact it may have. \n",
            "The paper addresses the inference of illposed models, where the likelihood is intractable, and proposes to utilize hierarchical structures in combination with neural density estimators (normalizing flows) to ameliorate identifiability of the parameters. It motivates the approach with a simple analytical tractable example, and validates the approach on modelled and real EEG time-series.  The problem the authors consider is interesting and important for scenarios, where models are parametrized ambiguously. However, I consider the present work as too immature for publication, because to me, the proposed approach (making use of additional observations and separating the parameters in global and local ones) is in my opinion not sufficiently justified explored. Some questions I would like to see answered are\n+ When is the separation off variables into local and global possible or desirable. E.g. when you remove the assumption that alpha and beta are upper bounded by 1 in the motivating example, the separation wouldn\u2019t help any more in my opinion. I do not say, that the motivating example isn\u2019t interesting, but I would like to have seen more discussion, on when it makes sense to have this separation. Hence, I would appreciate some general discussion, when such the proposed separation makes sense.\n+ The problems, that are considered, are very low dimensional, which is making me wonder whether the neural density estimators are really necessary here. Just demonstrating that the separation ameliorates the inference by using Gaussians (or mixtures of them, or other simple estimators) for the density approximation in Eq. (5), might be more insightful, and easier to train.\n+ Overall, I was missing details about how many parameters are used in the different models, how long it takes them to train, etc. So I couldn\u2019t make myself a picture, whether comparisons are fair or not.\n+ For the fact, that the paper doesn\u2019t provide any theory (except for a specific toy example), how this separation is helping model inference, I think the empirical results are too little, to consider the proposed method as helpful in general.\n\n### Update\n\nReading the comments of the other reviewers, and the response of the authors, I am reconsidered my score and raised it to 6 (though decreasing my confidence score). This is because,\n\n+ Reading the reviewers' comments, it seems I underestimated the contribution of the paper, leveraging information of the hierarchical structure of models in the context of likelihood-free inference.\n+ I mentioned originally, that I found the manuscript written too complicated and mentioned as examples invariance of the averaging operation to permutations, and that $\\phi_3$ is introduced, but seems to be never learnt. I understand for the first point, that though trivial, it is a crucial point for the construction of the neural posterior, so it makes sense to give the fact more space than needed on first sight. The fact, that $\\phi_3$ is not needed in the shown experiments (as I got from the response to reviewer fnEF), still poses the question, whether it is necessary to introduce it, or just to mention the possibility of having, but simplify notation.\n+ Experiments seem to be computational heavy to provide new baselines for the NMM example, so it is maybe too much to ask. + As mentioned above, I\u2019d like to see some additional and rigour discussion on when such separation of variables helps the inference.\n+ Please report number of parameters of models, what device was used to train them etc. to allow the reader to evaluate the showed results.\n+ In general, more experiments, that demonstrate the improvement achieved by the proposed method \n+ I sometimes feel, the paper is written too complicated. For example, in l134 it is written that $f_{\\phi^{(3)}}$ is \u201cinvariant to the ordering of the input\u201d, which is obvious, since the function in question is a sum over inputs (which is commutative). I thought quite a bit about this comment, and I am not sure whether I am not seeing something, or why it is important to mention here. Another example is, that $f_{\\phi^{(3)}}$ seems never to be used in the experiments, which makes me wonder, why is it introduced in the first place.\n+ Some typos:\n++ L129f: indices of phi are mixed\n++ There are several approximation signs in the appendix that should be equal, and some equal signs that should be approximations.\n++ L107: It should be \\beta not \\beta_0\n",
            "This paper proposes an approach to perform statistical inference for hierarchical models with intractable likelihoods. It extends an existing approach for simulation-based inference (SNPE-C) in order to use models that have global and local random variables. \n\nThe authors propose a scheme in which two conditional flows a trained: The first flow models a distribution over global parameters given an observation and $N$ permutation invariant auxiliary observations. Permutation invariance could e.g. be accounted for with a deepset architecture. The experiments do not make use of this flexibility, relying on an average operation instead. The second flow then models a distribution over local parameters given the observation and global parameters. They refer to the resulting inference scheme jointly as h-Flow, for hierarchical flow.\n\nA motivating toy example is provided, along with a relevant, more realistic simulator from Neuroscience (a Neural Mass Model; NMM). h-Flow is compared against a naive approach on both of these examples. The comparison turns out about equal on the toy example and favorably for h-Flow on the NMM. On the toy example, h-Flow is additionally compared against a hierarchical ABC variant and LFVI, comparing favorably to both of them.  # Main Review\n\n## Significance and Originality\n\nOverall, the paper is well motivated and introduces a strategy which can potentially be useful for many applications of SBI. The proposed solution might come at no big surprise, but in fact this can be viewed as a strength: As long as the proposed strategy of training two flows (see summary above) performs robustly and demonstrably better than what one would naively do with SNPE-C, and given that it outperforms existing methods, I would view this paper as being a valuable addition to the conference that should be presented. \n\n\n## Clarity\n\nThe paper is generally well-written and easy to follow. I especially enjoyed that the authors introduced a motivating example that then finds its structure reflected in the applied setting.\n\n\n## Quality\n\nMy reasons for not giving a higher score, and currently tending to rather reject the paper, are many open questions and some specific requests I have regarding the comparisons. I detail these concerns below and hope that the authors will address them, in which case I will revise my evaluation accordingly.\n\n### Comparisons to Naive Approaches\n\nMy primary concern is that the paper does not yet clearly demonstrate that the proposed method is better than naive approaches using SNPE-C with a single flow (instead of using two flows, as h-Flow does). In the toy experiment, no difference between h-Flow and the naive approach is found (Figure 2). On the NMM, a difference is clearly visible in Figure 3, but as of now I am unsure how to interpret this result exactly and how it was obtained. Some specific questions and requests that can hopefully help clarify this issue follow.\n\n**Q1**: In Figure 3, the difference between the naive approach and h-Flow _is biggest_ for $N=0$. I was surprised by this, since my understanding is that we expect improvements in resolving indeterminacies by additional observations, i.e. when $N>0$. In the toy example, which served as a motivation for the NMM, as $N \\rightarrow 0$ the posterior distribution converges to $p(\\alpha, \\beta | x_0)$. In other words, for the toy example, we do not expect a difference (and empirically don't find) a difference in performance when $N=0$. Could the authors comment on the difference in performance for $N=0$ on the NMM? \n\n**Q2**: Regarding the results presented in Figure 3 the authors write: \"We see that the factorization scheme yields uniformly better results than the naive architecture. This is mainly because the dimensionality of the context variables in the naive case gets too big when more observations become available, making the training of the normalizing flow in the non-factorized setting harder.\u201c. Why exactly do the context variables in the naive case get too big with more observations? I would have thought that for the naive scheme we simply aggregate the additional observations before passing them as context to the single flow. If we do not aggregate them, we assume that the ordering matters, which is would put the naive implementation at a big disadvantage and is not how one would naively apply SNPE-C with a single flow in this context. It would be important to comment and clarify this.\n\n**R1**: As a request for new experiments: The results in Figure 3 are reported in terms of Wasserstein distance between a dirac delta on ground truth parameters and the inferred posterior (median without error bars for 5 repeats). It would be important to extend this analysis for a much larger number of ground truth parameters and report results such that the variability can be evaluated (e.g. mean and standard errors). Ideally, a secondary metric to the Wasserstein distance was reported in the supplement, such as the more commonly used negative log probability of true parameters _averaged_ over as many ground truth parameters as feasible (this is crucial, otherwise this metric would be hard to interpret).\n\n### Results for h-ABC and LFVI on NMM\n\n**R2**: As a second request, it would be good to provide results for h-ABC and LFVI on the NMM model for completeness.\n\n\n## Additional questions\n\n**Q3**: In Figure 1 (right) we observe that the $N=10$ case is worse relative to $N=0$. To me this seemed counterintuitive at first and I think it would be good if the authors commented on it in the manuscript. My interpretation is that the density estimation problem gets more difficult when we have more observations (posterior gets more concentrated). So resolving indeterminacies leads to less accurate inference (as compared to the analytic solution). Would the authors agree with this interpretation?\n\n**Q4**: \"We did not carry out more experiments on this data-driven setting because of difficulties due to numerical instabilities in the training procedure when N increases and for certain choices of ground truth parameters.\". It would be important to investigate this issue further, if possible for the current publication. Why would this only occur for certain ground truth parameters and not for others? \n\n**Q5**: For the toy example the authors set $\\sigma=0$, so that $\\epsilon = 0$. Usually SBI is only applied in the context of stochastic simulators. Would results change in a meaningful way if the experiments were repeated for a small value of $\\sigma$?\n\n**Q6**: The NSF in Figure 1 does not seem able to handle the sharp transitions that occur when $N=10$ or $N=100$. Do the authors have any suggestions to remedy this? \n\n**Q7**: On the NMM, MAFs instead of the more expressive NSFs are used (whereas the toy example does use NSFs). What are the reasons behind this choice and how do results change for h-Flow and the naive strategy when using them instead?\n\n**Q8**: \"Although the number of additional observations (N) was fixed in our analysis and experiments, this parameter could be randomized and amortized during learning and enable the posterior approximation to be fed with sets of auxiliary observations of varying sizes making it more flexible for applications.\u201c $\\rightarrow$ My understanding is that the aggregation is simply done through the average in the experiments. What exactly would not work/require changes to have flexible N?\n\n**Q9**: Have the authors done any internal experiments in which $\\phi_3$ was learned as well? If so, were there any problems that they encountered that restricted the experiments to learning $\\phi_1$ and $\\phi_2$ only?\n\n\n## Name of the algorithm\n\nI see two potential disadvantages associated with with naming the algorithm h-Flow. They are:\n1. One of the strengths of SNPE is that it can be used with any conditional density estimator. In the future, perhaps entirely different classes of CDEs will be invented. I might be misunderstanding something (please correct me if I'm wrong) but to me it seems there is nothing flow-specific about the proposed algorithm. In particular I assume that we can use it with all kinds of CDEs as long as they work with embeddings/can be designed to be permutation invariant and we can evaluate log probabilities and draw samples. For example, we could just as well use this with MDNs, which might be more suitable depending on the application/knowledge we have about a problem.\n2. h-Flow in the paper title sounded to me as if a single architecture was trained, i.e. that this paper was about an entirely novel flow architecture. Instead, two flows are trained, and if the approach gets extended to multi-level architectures, training would involve more and more flows, not a single one. \n\n**Q10**: What are the authors thoughts on these remarks regarding their algorithm's name?\n\n## Minor\n\n- Fig. 3: There are display issues with this figure when opening the PDF using Preview on macOS. Most marginals are missing and the x-axis of the inset figure is cut off (3 instead of 30)\n- L11: validate quantitatively our proposal $\\rightarrow$ validate our proposal quantitatively\n- L103: have to be approximated $\\rightarrow$ we decide to approximate them\n- L178: $5 \\times 10^{\u22124}$ rather than $5.10^{\u22124}$?\n- L227: captures well the global parameter $\\rightarrow$ captures the global parameter well\u2028\n- L229: could well estimate local parameters $\\rightarrow$ could estimate local parameters well\n\n## Update\n\nI thank the authors for their reply and have updated my score. The authors state in their checklist that potential negative societal impacts of their work are not applicable. I agree that an extended discussion of negative impacts might not be strictly necessary, given the nature of this work is about statistical inference applied to a certain class of models (with hierarchical structure and intractable likelihoods). Thinking more broadly, the societal impact and potentially adverse effects will depend on the setting in which the algorithm gets deployed. However, I would suggest to at least briefly touch upon the general difficulties associated with diagnostics/criticism in the SBI context."
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses gratitude ('Thank you for the response!'), appreciation ('I appreciate your inclusion'), and approval ('good to see such practical applications'). The reviewer also indicates an increased score, which is a positive signal.",
            "The reviewer states the paper is \"clearly written\", the method is \"practically useful, and easy to implement\", and the results show \"significant improvement\". The score was also revised upwards.",
            "The review expresses concerns about the novelty and significance of the proposed method, stating it appears to be a slight variation of an existing method. It also questions whether improvements are due to the method itself or other factors like increased flexibility or auxiliary observations. The reviewer also points out the lack of discussion on limitations and societal impact.",
            "The review expresses concerns about the maturity of the work, stating it's \"too immature for publication.\" It raises multiple questions about the justification and exploration of the proposed approach, the necessity of neural density estimators, missing details about model parameters and training, and the lack of empirical results.",
            "The review expresses significant concerns about the paper's clarity and quality, particularly regarding the comparison with naive approaches. The reviewer explicitly states they are 'tending to rather reject the paper' due to 'many open questions' and specific requests related to the comparisons. While acknowledging the paper's motivation and potential usefulness, the reviewer's reservations and numerous questions indicate a negative overall sentiment."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses encouraging language like 'good to see' and offers a suggestion in a friendly and helpful manner ('seems like a good idea to me'). The expression of gratitude ('Thank you') also contributes to the supportive tone.",
            "The reviewer uses positive language such as \"clearly written\", \"practically useful\", and \"easy to implement\". They also explicitly state \"No concerns seen here.\" which indicates a supportive tone.",
            "The review employs phrases like \"mildly worried about novelty and significance,\" \"I don't think they highlight the method very well,\" \"It is still unclear to me,\" and \"The authors did not comment on limitations.\" These phrases indicate a critical assessment of the work's novelty, experimental design, and completeness.",
            "The review uses phrases like \"not sufficiently justified explored,\" \"making me wonder,\" \"missing details,\" and \"too little\" to express concerns and criticisms about the paper's methodology, presentation, and results.",
            "The review adopts a critical tone by posing numerous questions and requests for clarification and further experiments. Phrases like 'My primary concern', 'I was surprised by this', and 'It would be important to comment and clarify this' highlight the reviewer's critical assessment of the paper's current state. The reviewer also points out potential issues with the algorithm's name and identifies several minor errors, contributing to the critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and appreciative of the authors' revisions and additions, without any contradictory statements or conflicting feedback.",
            "The review is consistently positive and constructive, highlighting the paper's strengths such as clarity, practical usefulness, and ease of implementation. The reviewer suggests a minor improvement regarding the experimental section (adding more baselines), but this is presented as a suggestion for further strengthening the paper rather than a fundamental flaw. The final statement about score revision and 'no concerns' reinforces the overall consistent positive assessment.",
            "The review is consistent because the reviewer raises concerns about novelty and experimental validation, and all subsequent points logically support these concerns. The reviewer questions whether the improvements are genuinely due to the proposed method or other factors like increased flexibility or auxiliary data, which is a consistent line of critique. The suggestions for improvement, such as fairer baselines and a more concise experimental section, directly address the initial concerns about validation and clarity. There are no contradictory statements or shifts in opinion within the review.",
            "The reviewer initially expresses concerns about the justification and maturity of the work, suggesting it's too immature for publication. After reading other reviews and author responses, the reviewer re-evaluates and increases the score, acknowledging the contribution regarding hierarchical structures. While the reviewer softens some initial criticisms and appreciates certain aspects after the update, they still maintain some of the original concerns (like the need for more discussion on variable separation and more experiments). This indicates a consistent evolution of opinion based on new information rather than a contradiction.",
            "The review is consistent in its assessment. It acknowledges the potential value and motivation of the proposed method but raises specific concerns about the experimental validation, comparisons to baselines, and clarity on certain aspects. The reviewer's initial positive remarks are conditional on addressing these concerns, leading to a nuanced but consistent evaluation rather than contradictory statements."
        ]
    },
    {
        "paper_id": "nips_2021_rqjfa49ODLE",
        "paper_title": "Evidential Softmax for Sparse Multimodal Distributions in Deep Generative Models",
        "paper_abstract": "Many applications of generative models rely on the marginalization of their high-dimensional output probability distributions. Normalization functions that yield sparse probability distributions can make exact marginalization more computationally tractable. However, sparse normalization functions usually require alternative loss functions for training since the log-likelihood is undefined for sparse probability distributions. Furthermore, many sparse normalization functions often collapse the multimodality of distributions. In this work, we present ev-softmax, a sparse normalization function that preserves the multimodality of probability distributions. We derive its properties, including its gradient in closed-form, and introduce a continuous family of approximations to ev-softmax that have full support and can be trained with probabilistic loss functions such as negative log-likelihood and Kullback-Leibler divergence. We evaluate our method on a variety of generative models, including variational autoencoders and auto-regressive architectures. Our method outperforms existing dense and sparse normalization techniques in distributional accuracy. We demonstrate that ev-softmax successfully reduces the dimensionality of probability distributions while maintaining multimodality.\n",
        "review_ids": [
            "XbjDBiDZSb0",
            "cZEwfmEdeY5",
            "WwbrDr9kCR9",
            "jlhiFePh813",
            "TNQrWKldN77",
            "2v7Z8Tw9SHa",
            "ZmGpTi5h7sZ",
            "N-5hmcaKfwp",
            "P_TeBISjqrW"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thanks for the quick clarification. Your corrected statement seems accurate. For the record, note however that the \"theorem\" ev_softmax(x)_j = 0 => sparsemax(x)_j = 0 is not correct, and in fact the two counterexamples above show that the supports of ev_softmax and sparsemax are generally not a subset of each other:\n\nx = [2, 0.5, -1.5] => sparsemax(x) = [1, 0, 0], ev_softmax(x) = [>0, >0, 0]\n\nx = [0.8, 0.2, 0] => sparsemax(x) = [0.8, 0.2, 0], ev_softmax(x) = [>0, 0, 0]\n\nI don't think this compromises the main findings of the paper, but it should impact the discussion about the relation between the two transformations suggested by one of the reviewers.",
            " Dear authors, \n\nI don't think the \"theorem\" in your previous answer is correct. You say\n\n\"we can see by inspection that sparsemax(x)_j  = 0 iff x_j <= 0\" (assuming sum(x) = 1)\n\nI don't get how we can see this \"by inspection\", and in fact here is a counterexample:\n\nx = [2, 0.5, -1.5] => sparsemax(x) = [1, 0, 0]\n\nYour claim that ev_softmax(x)_j  = 0 => sparsemax(x)_j = 0 does not seem to be correct. \n\nHere's a counterexample:\n\nx = [0.8, 0.2, 0] => sparsemax(x) = [0.8, 0.2, 0]\n\nFor this x, since 0.2 <= 1/3 ev_softmax(x)_2 = 0 but sparsemax(x)_2 = 0.2 > 0.\n\nPlease let me know if there is something I'm missing.\n\nThanks.\n\n\n\n",
            " Thanks for your response. Re. the softmax/ev-softmax analysis, I think this could be helpful to include in the paper.\n\nI still don't have a great sense of why a direct comparison with Itkina et al is not provided in this work. This could have verified that applying evidential softmax at training time is valuable. Similarly, another comparison that could've been worth including would have been sweeping over different values of alpha for entmax, even just for one of the experiments. I think the paper could have benefitted by having these baselines.",
            " Dear authors,\n\nThank you for your response. Below I'm following up on the points that I have further comments/questions about:\n\n1. I understand that applying evidential softmax at training time is done with the goal of performance improvements. However, I think the paper is lacking discussion about why this outcome is expected. Can you characterize the differences in terms of the output attention distributions? Are they sparser / more multimodal compared to the post-hoc approach of Itkina et al?\n\n3./4. I think it might be valuable in understanding previous approaches like sparsemax and entmax a bit more. The authors mentioned that these normalization functions \u201ccollapse multimodality in smaller latent spaces\". Maybe I missed something but I'm not sure why this would naturally be the case, and the paper doesn't dig deeper in understanding this. Similarly, I think the authors could\u2019ve done a sweep over different values of alpha for entmax. The plot from Peters et al that the authors refer to is based on their MT experiment, but the trend could very well be different for other tasks. I also think the follow-up (from R1) the authors mention could have been really valuable in understanding the pros/cons of these methods in different settings.\n\n6. It seems that the improvements relative to Itkina et al and entmax are quite small on both image generation and machine translation. I really think significance testing is necessary across all settings, not only for the METEOR/ROUGE scores the authors reported above.\n\nOverall, I think the paper could be improved in understanding the shortcomings of previous work better, more clearly describing the benefits of ev-softmax at training time (both empirically and while motivating their approach) and significance testing their improvements. \nFor these reasons, I think I will keep my current score.",
            " After reading the other reviews and the respective rebuttals, I decided to keep my score.",
            "This paper proposes a sparse normalization function called evidential softmax (ev-softmax). Specifically, it extends the post-hoc sparsification approach of Itkina et al, to propose a normalization function that can be applied during both training and testing. Notably, the authors claim that ev-softmax preserves the multimodality of the distribution, while providing sparse output distributions. The authors argue that sparsity is likely to help with interpretability and also improve task performance, while multimodality is useful when the output space is multimodal. They also show experiments showing that previous sparse normalization functions collapse multimodality of distributions. \n\nThe formulation of the ev-softmax function is accompanied with relevant proofs showing that a continuous family of approximations exist for ev-softmax, which make it possible to train with NLL and KL divergence based losses. Experiments on image generation using a generative model (VAE) and machine translation (using an autoregressive model) show some improvements over previous sparse normalization techniques on distributional accuracy and classification performance.\n  The paper provides a clear description of the formulation of the ev-softmax operation and a comprehensive list of properties exhibited by this function. The argument for building sparse yet multimodal distributions is also convincing. However, I'm not sure I completely understand the advantage of applying evidential softmax at training time. It would be great if the authors could clarify. Further, I believe the authors could have still compared with only applying post-hoc sparsification using the method of Itkina et al just to provide a data point for comparison. I believe this could be discussed more in the paper to emphasize the novelty of the work.\n\nOne other comment I had about the description is that currently, it might not be giving enough intuition to the readers about why ev-softmax gives sparse & multimodal distributions. This might be because that this work is an extension of the work of Itkina et al and some details are redundant, but I still think it could be worth illustrating in more depth how this function behaves and why it fits the desired criteria.\n\nOn the experimental front, I think the paper could be improved. The improvements on some of the tasks (in tables 2, 4) seem very small and significance tests are missing. I also wonder if the authors could have used considered different values of alpha for the alpha-entmax baseline. Following are my questions/concerns about the experiments:\n- Re: the MNIST experiments in sec 4.1, both sparsemax and entmax-1.5 seem to have degenerate performance in this experiment but they perform reasonably well in the other image generation experiments. Do you know why this is the case? \n- Did you consider varying the value of alpha in entmax? If yes, does that help improve the multimodality of the distributions?\n- How well does the wide residual network used for evaluating the models in the exp in 4.3 perform? Instead of using a model for evaluation, did you consider performing human evaluation on a smaller sample?\n- The improvements on the machine translation experiments appear to be very small. Have you performed significance testing (perhaps using a paired bootstrap test)? Computing multiple automatic metrics such as ROUGE, METEOR and performing human evaluation would be useful too.\n- How can one guarantee that it is multimodality that is helping performance? I understand the # attended words is higher / latent space reduction is (sometimes) higher with ev-softmax. However, there is lack of evidence showing that multimodality helps performance.\n- How expensive is the ev-softmax in terms of time and space, relative to other sparse normalization alternatives?\n- One of the motivations posed in the intro is multimodality, especially in the context of machine translation where multiple reference words are likely. However, in the experiments, this effect hasn't really been validated. The function has instead been applied to the attention distribution.\n\n \nOther comments:\n- In Sec 2.1, could you expand upon your earlier claim that the Gumbel-Softmax estimator introduces bias?\n- Some citations / comparisons to other relevant work such as Zhao et al (Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection) are missing.\n- Line 226: appears to be a typo.\n\nOverall, I think the paper is written well and has good motivation. The experiments have some weaknesses highlighted above and performance improvements seem small. Also, the description of the intuition behind the approach (why it induces multimodality + sparsity) would be useful.\n I would appreciate if the authors could expand upon the discussion in section 7 about the possibility of bias amplification through sparsification. I also wonder if there is relevant work that can be cited to add on to this discussion.\n",
            "The paper introduces a normalizing function called evidential softmax (ev-softmax). Based on principles of evidential theory, ev-softmax is able to assign zero probability to classes that lack evidence of the data. It can be seen as a generalization of the work done by Itkina et al. (2020), the crucial difference being that ev-softmax can be applied at training time. The experiment section shows improved performance compared to other sparse methods, on a variety of tasks.  _Originality_: The submission introduces a novel and differentiable normalizing function that is a generalization of the method described in Itkina et al. (2020). It is clear how this work differs from that contribution, and related work is adequately cited, as far as I know.\n\n_Quality_: The methods described in this submission are well supported by evidential theory and the authors do a good job in their explanation. Furthermore, they use their method in a variety of tasks, in which they empirically show that their method has superior performance when compared to sparsemax, 1.5-entmax, and softmax.\n\n_Clarity_: This submission is very well written and well detailed. Results could be easily reproduced since the authors provided the code.\n\n_Significance_: The method proposed in this submission can be an asset to researchers and practitioners in the future as a promising sparse alternative to softmax, where the reason for this sparsity is theoretically grounded in evidential theory. The improved results of ev-softmax on four different tasks also show that the method has practical applications. I believe the authors have adequately addressed the societal impact of their work.",
            "The paper proposes a new sum-to-one activation function, namely evidential softmax, which is specialized for the sparse multimodal activation.\nWith the suggested transformation, the authors resolve the existing post-hoc process of evidential sparsification.\nThe authors provide various experiments to support their proposed sparse activation function.\n\n  \nOriginality:\nRemoving the post-hoc process and finding the equivalent form seems quite novel work.\nHowever, I'm a bit confused of the messeage of the paper.\nThe motivation in the introduction and the final purpose that can be seen in the experimental result section of the paper seems quite distinguished.\n\nQuality:\nThe paper sound okay technically, however, I'm not convinced with the experimental section and the related questions are separately listed below.\n\nClarity:\nI cannot say that the paper is well-written due to the overall configuration.\nI wonder why the discrete VAE came up in the first in the background.\nIt would be better if Section 5 come before Section 4.\n\nSignificance:\nThe paper is explained through theoretical backups.\nHowever, the overall performance is too marginal and Table 2 seems to be taken from [9] but there are no comment on this.\nAlso, there is no confidence interval in VQ-VAE experiment.\nAs a minor comment, it would be more convinced if the authors applied various \\alpha values for the entmax.\n\nQuestions:\n- In Table 1, compared to the other sum-to-one activation functions, EvSoftmax has less nice properties. How does the authors think that which property unsatisfaction gives better result?\n- How does the result change in Fig 1 if the ratio over the labels is unbalanced? (including the extremely unbalanced case)\n- What does decoder calls imply? And is it good or bad if the number is small?\n- SS-VAE experiment result shows little difference across various models. What becomes when using extremely small amount of labeld data? (for example, 1%) Also, how does the performance gap change when using more complex datasets such as OMNIGLOT or CIFAR?\n- How is the top-1 accuracy result in the VQ-VAE experiment?\n- How does overall performance change as \\epsilon differs?\n- I can't find any comparison on the post-hoc process [10] in the experiment section. How differ the performance are?\n The authors pointed out the limitation of their work, and the work seems that it does not have any potential negative societal impact.\n",
            "This paper shows a sparse normalization function that preserves the multimodality of probability distributions to  train deep generative models.\n\nThe authors apply their methods to different VAE architectures and demonstrate the effectiveness of their methods.\n  The overall framework of the paper is good. However, the motivation of the paper is not very clear. Compared to the previous works about VAE, this paper is not very innovative. The authors do not fully demonstrate the limitations of their work. In addition, they do not clearly give the experimental results to show the benefits of considering the multimodality and sparsity of the data distribution."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Neutral",
            "Neutral",
            "Neutral",
            "Negative",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review acknowledges the author's clarification and points out an inaccuracy in a related statement. While it identifies a flaw, it also states that it doesn't compromise the paper's main findings, resulting in a neutral overall sentiment.",
            "The reviewer expresses disagreement with the authors' theorem, provides counterexamples, and states that they don't understand the authors' reasoning. This indicates a negative sentiment towards the correctness of the work.",
            "The review expresses a mix of appreciation and suggestions for improvement. While thanking the authors, it also points out missing comparisons and potential experiments, indicating a neutral overall sentiment.",
            "The review expresses both appreciation and constructive criticism. While acknowledging the authors' response, it points out areas needing improvement, such as a lack of discussion, deeper understanding of previous approaches, and the necessity of significance testing. The final statement indicates the reviewer is maintaining their original score, suggesting neither strong satisfaction nor dissatisfaction.",
            "The review simply states that the reviewer is maintaining their original score after considering other reviews and rebuttals, indicating no strong positive or negative feeling.",
            "The review expresses concerns about the experimental results, stating that improvements are small and lack significance tests. It also points out missing comparisons, unclear intuition, and potential weaknesses in the evaluation methods. The reviewer uses phrases like 'experiments have some weaknesses,' 'performance improvements seem small,' and 'not sure I completely understand,' indicating a negative sentiment.",
            "The review expresses overall positive feedback, highlighting the paper's originality, quality, clarity, and significance. It uses positive language like \"novel\", \"well supported\", \"good job\", \"very well written\", \"asset\", and \"promising\".",
            "The review expresses confusion about the paper's message, questions the experimental section, and criticizes the clarity of writing. The reviewer also points out marginal performance, missing information, and lack of comparison, indicating a generally negative assessment.",
            "The review expresses concerns about the clarity of motivation, lack of innovation compared to previous works, insufficient demonstration of limitations, and unclear experimental results."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Neutral",
            "Critical",
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer directly points out an error (\"theorem\" ev_softmax(x)_j = 0 => sparsemax(x)_j = 0 is not correct) and provides counterexamples, which indicates a critical tone. The phrase 'note however' introduces a correction.",
            "The review directly challenges the authors' claims, using phrases like \"I don't think the 'theorem' is correct,\" \"I don't get how we can see this 'by inspection',\" and \"Your claim...does not seem to be correct.\" The provision of counterexamples further reinforces the critical tone.",
            "The reviewer uses phrases like \"I still don't have a great sense of why...\", \"This could have verified...\", and \"the paper could have benefitted by having these baselines\" which suggest a critical stance and highlight perceived shortcomings in the paper.",
            "The reviewer uses phrases like \"lacking discussion,\" \"I'm not sure why this would naturally be the case,\" \"the paper doesn't dig deeper,\" and \"improvements are quite small.\" These phrases indicate a critical assessment of the paper's current state and highlight areas where the reviewer believes the work falls short. The suggestion for significance testing also implies a questioning of the validity of the presented results.",
            "The language is factual and devoid of emotional expression. The statement is a simple declaration of the reviewer's decision.",
            "The tone is critical, as the reviewer identifies several shortcomings in the paper. They pose multiple questions and concerns about the experiments, such as the degenerate performance of baselines, the lack of significance testing, and the need for more evidence to support the claim that multimodality helps performance. The reviewer also points out missing citations and a typo, further contributing to the critical tone.",
            "The tone is supportive, evidenced by phrases like \"authors do a good job in their explanation,\" \"very well written and well detailed,\" and \"can be an asset to researchers and practitioners.\" The reviewer clearly appreciates the work and its potential impact.",
            "The reviewer uses phrases like \"I'm a bit confused,\" \"I'm not convinced,\" \"I cannot say that the paper is well-written,\" and \"overall performance is too marginal.\" These phrases indicate a critical evaluation of the paper's various aspects.",
            "The review uses phrases like \"not very clear,\" \"not very innovative,\" \"do not fully demonstrate,\" and \"do not clearly give\" to express critical feedback on the paper's aspects."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer clearly points out a specific inaccuracy regarding the relationship between ev_softmax and sparsemax, provides concrete counterexamples to support their claim, and then logically concludes that while this inaccuracy exists, it does not compromise the main findings of the paper but should be considered in the discussion. The reviewer's points are well-reasoned and flow logically without contradiction.",
            "The review is consistent because the reviewer provides counterexamples to disprove the author's claims about sparsemax and ev_softmax functions. The reviewer's arguments are logically sound and focused on demonstrating the flaws in the author's statements using concrete examples. There are no contradictions within the review itself; the reviewer consistently argues against the author's claims.",
            "The review is consistent in suggesting improvements to the paper by adding specific comparisons (Itkina et al. and alpha sweep for entmax) to strengthen the evaluation and validate the proposed method.",
            "The reviewer consistently points out areas where the paper needs improvement, such as providing more justification for the proposed method, deeper analysis of existing methods, and more rigorous evaluation through significance testing. The reviewer's conclusion to maintain their current score aligns with these consistent concerns about the paper's shortcomings in explanation and validation.",
            "The review is consistent as the reviewer makes a decision (keeping the score) based on further information (reading other reviews and rebuttals). There are no contradictory statements.",
            "The review provides a balanced assessment, highlighting both strengths and weaknesses of the paper without contradicting itself. The reviewer offers constructive criticism and suggestions for improvement, maintaining a consistent tone and perspective throughout the review.",
            "The review is consistently positive across all aspects (Originality, Quality, Clarity, Significance) and praises the paper for its novelty, theoretical grounding, empirical results, clarity, and significance. There are no contradictory statements or negative feedback within the review.",
            "The review is consistent in its critique, highlighting both positive aspects (novelty of approach) and negative aspects (confusion about message, weak experimental validation, lack of clarity, marginal performance). The reviewer's concerns and questions all point towards the paper needing improvement in clarity, experimental rigor, and justification of its significance. There are no self-contradictory statements within the review.",
            "The review acknowledges the good framework but points out weaknesses in motivation, innovation, limitations, and experimental results. These points are not contradictory, but rather highlight areas for improvement."
        ]
    },
    {
        "paper_id": "iclr_2018_HJJ23bW0b",
        "paper_title": "Initialization matters: Orthogonal Predictive State Recurrent Neural Networks",
        "paper_abstract": "Learning to predict complex time-series data is a fundamental challenge in a range of disciplines including Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) (Downey et al.) are a state-of-the-art approach for modeling time-series data which combine the benefits of probabilistic filters and Recurrent Neural Networks into a single model. PSRNNs leverage the concept of Hilbert Space Embeddings of distributions (Smola et al.) to embed predictive states into a Reproducing Kernel Hilbert Space, then estimate, predict, and update these embedded states using Kernel Bayes Rule. Practical implementations of PSRNNs are made possible by the machinery of Random Features, where input features are mapped into a new space where dot products approximate the kernel well. Unfortunately PSRNNs often require a large number of RFs to obtain good results, resulting in large models which are slow to execute and slow to train. Orthogonal Random Features (ORFs) (Choromanski et al.) is an improvement on RFs which has been shown to decrease the number of RFs required for pointwise kernel approximation. Unfortunately, it is not clear that ORFs can be applied to PSRNNs, as PSRNNs rely on Kernel Ridge Regression as a core component of their learning algorithm, and the theoretical guarantees of ORF do not apply in this setting. In this paper, we extend the theory of ORFs to Kernel Ridge Regression and show that ORFs can be used to obtain Orthogonal PSRNNs (OPSRNNs), which are smaller and faster than PSRNNs. In particular, we show that OPSRNN models clearly outperform LSTMs and furthermore, can achieve accuracy similar to PSRNNs with an order of magnitude smaller number of features needed.",
        "review_ids": [
            "ByofVOOgG",
            "HJzahgcgf",
            "rJujSJjgG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "I was very confused by some parts of the paper that are simple copy-past from the paper of Downey et al.  which has been accepted for publication in NIPS. In particular, in section 3, several sentences are taken as they are from the Downey et al.\u2019s paper. Some examples :\n\n\u00ab\u00a0provide a compact representation of a dynamical system\nby representing state as a set of predictions of features of future observations.\u00a0\u00bb \n\n\u00ab\u00a0a predictive state is defined as\u2026 , where\u2026  is a vector of features of future observations and ...  is a vector of\nfeatures of historical observations. The features are selected such that ...  determines the distribution\nof future observations \u2026 Filtering is the process of mapping a predictive state\u2026\u00a0\u00bb\nEven the footnote has been copied & pasted: \u00ab\u00a0For convenience we assume that the system is k-observable: that is, the distribution of all future observations\nis determined by the distribution of the next k observations. (Note: not by the next k observations\nthemselves.) At the cost of additional notation, this restriction could easily be lifted.\u00a0\u00bb\n\u00ab\u00a0 This approach is fast, statistically consistent, and reduces to simple\nlinear algebra operations.\u00a0\u00bb \n\nNormally, I should have stopped reviewing, but I decided to continue  since those parts only concerned the preliminaries part.\n\nA key element in PSRNN is to used as an initialization a kernel ridge regression. The main result here, is to show that using orthogonal random features approximates well the original kernel comparing to random fourrier features as considered in PSRNN. This result is formally stated and proved in the paper.\n\nThe paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF. Three data sets are considered (Swimmer,  Mocap and  Handwriting). \n\nI found it that the contribution of the paper is very limited. The connexion to PSRNN is very tenuous since the main results are about the regression part. in Theorems 2 and 3 there are no mention to PSRNN.\n\nAlso the experiment is not very convincing. The datasets are too small with observations in low dimensions, and I found it not very fair to consider LSTM in such settings.\n\nSome minor remarks:\n\n- p3: We use RFs-> RFFs\n- p5: ||X||, you mean |X| the size of the dataset\n- p12: Eq (9). You need to add \u00ab\u00a0with probability $1-\\rho$ as in Avron\u2019s paper.\n- p12: the derivation of Eq (10) from Eq (9) needs to be detailed.   \n\n\nI thank the author for their detailed answers. Some points have been clarified but other still raise issues. In particular, I continue thinking that the contribution is limited. Accordingly, I did not change my scores.",
            "The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which \nuses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions:\n- the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems\n- a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFs\n\nThe contribution to the practice of PSRNNs seems significant (to my non-expert eyes): when back-propagation through time is used, using ORFs to do the two-stage KRR training needed visibly outperforms using standard RFMs to do the KRR. I would like the authors to have provided results on more than the current three datasets, as well as an explanation of how meaningful the MSEs are in each dataset (is a MSE of 0.2 meaningful for the Swimmer Dataset, for instance? the reader does not know apriori). \n\nThe contribution in terms of the theory of using random features to perform kernel ridge regression is novel, and interesting. Specifically, the author argue that the moment-generating function for the pointwise kernel approximation error of ORF features grows slower than the moment-generating function for the pointwise kernel approximation error of RFM features, which implies that error bounds derived using the MGF of the RFM features will also hold for ORF features. This is a weaker result than their claim that ORFs satisfy better error, but close enough to be of interest and certainly indicates that their method is principled. Unfortunately, the proof of this result is poorly written:\n- equation (20) takes a long time to parse --- more effort should be put into making this clear\n- give a reference for the expressions given for A(k,n) in 24 and 25\n- (27) and (28) should be explained in more detail.\nMy staying power was exhausted around equation 31. The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form. \n",
            "This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space.\nWhile PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features.\n\nIn particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness. \n\nAuthors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM. They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training.\n\nQuestion:\n-\tWhat is the cost of constructing orthogonal random features compared to RF?\n-\tWhat is the definition of H the Hadamard matrix in the discrete orthogonal joint definition?\n-\tWhat are the hyperparameters values use for the LSTM\n-\tEmpirical evaluations seem to use relatively small datasets composed by few dozens of temporal trajectories. Did you consider larger dataset for evaluation? \n-\tHow did you select the maximum number of epochs in Figure 5? It seems that the validation error is still decreasing after 25 epochs?\n\nPros:\n-\tProvide theoretical guarantee for the use of orthogonal random features in the context of PSRNN\nCons:\n-\tEmpirical evaluation only on small scale datasets.\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses confusion and concern about plagiarism, stating that parts of the paper are \"simple copy-past\" from another paper. They also find the contribution of the paper \"very limited\" and the experiment \"not very convincing.\" The reviewer concludes that they did not change their scores, indicating dissatisfaction.",
            "The reviewer acknowledges significant contributions to the practice of PSRNNs and finds the theoretical contribution novel and interesting. Despite criticisms regarding the proof's clarity, the overall sentiment leans towards positive due to the recognition of the paper's value.",
            "The review provides a balanced assessment, highlighting both the strengths (theoretical guarantee) and weaknesses (small-scale empirical evaluation) of the paper. The language is objective and factual, without strong positive or negative emotional indicators."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review uses phrases like \"very confused,\" \"simple copy-past,\" \"contribution of the paper is very limited,\" and \"experiment is not very convincing.\" These phrases demonstrate a critical assessment of the paper's content and methodology.",
            "The review provides both positive feedback (acknowledging the significance of the contributions) and constructive criticism (pointing out areas for improvement in the proof's clarity and requesting more dataset results). This blend of positive and negative feedback indicates a balanced tone.",
            "The review adopts a balanced tone by presenting both positive aspects ('Provide theoretical guarantee') and negative aspects ('Empirical evaluation only on small scale datasets'). It also poses specific questions, indicating a constructive and critical approach. The language is professional and avoids overly enthusiastic or harsh wording."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently expresses a negative opinion about the paper's contribution, starting with the plagiarism issue and continuing through the evaluation of technical aspects and experiments, culminating in the statement that the score was not changed.  While initially mentioning plagiarism which is a strong reason to reject, the reviewer explains the continuation of the review and focuses on the limited contribution and weak experiments, maintaining a negative stance throughout.",
            "The review is consistent because the reviewer appreciates the contributions of the paper, both practical and theoretical, while providing constructive criticism and suggestions for improvement. The reviewer highlights the significance of the practical contribution and the novelty of the theoretical contribution, but also points out areas for improvement such as more experimental validation and clearer proof writing. There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent as it provides a summary of the paper, asks relevant questions, and lists both pros and cons without any contradictions. The questions are pertinent to the methodology and experimental setup, while the pros and cons section offers a balanced evaluation of the paper's contributions and limitations."
        ]
    },
    {
        "paper_id": "iclr_2019_r1ztwiCcYQ",
        "paper_title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY",
        "paper_abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\n      samples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\n      of parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\n      in the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\n      the critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.",
        "review_ids": [
            "HkgcijgT27",
            "rJlqb64qn7",
            "Hyl3G76N27"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Summary: Non-convex learning problems can have multiple solutions with different generalization properties, thus it is important to find solutions that generalize well. The goal of this paper is to derive an algorithm for finding a solution to the learning problem with the best possible generalization properties. This is achieved by using a Bayesian approach in which the parameters w (e.g., weights of a network) are random variables and the effective loss (integral wrt to w) is minimized in lieu of the usual loss. The paper assumes that each component of the weight vector w is Gaussian and derives a formula for updating the mean and covariance of said Gaussians (this is an SGD method). The paper claims that the resulting effective loss is convex for large variances (sigma > threshold), nonconvex for small variances (sigma < threshold), and converges to original loss as sigma goes to zero. The paper also claims that when sigma=0 there are trivial solutions that are unstable as data changes, but that when sigma=threshold (assuming this is what is meant by end of convexity) there are non-trivial solutions that are less sensitive to data changes and hence the most generalizable.\n\nComments: the goal of the paper (finding minima that generalize well) is an excellent one. But the paper is not clearly written and appears to oversell the contribution. In particular, the title speaks about SGD, dropout, generalization and critical points \u201cat the end of convexity\u201d. Naturally, a reader is inclined to think that the paper will study SGD and dropout for deep learning and analyze generalization properties of the solutions found by those methods. In reality, there is very little in the paper about SGD, dropout, and generalization. The connection with SGD is merely because the method for updating mu and sigma is an SGD method. The connection with dropout is mentioned in passing in one paragraph and it is not very clear. The connection with generalization is claimed but never quite explained (there are no generalization bounds in the paper). As far as understand, the paper considers the minimization of the effective loss, uses a Gaussian approximation for computing the effective loss, and focuses primarily on the characterization of convexity as a function of sigma as well as a characterization of the critical points depending on whether the effective loss is convex (sigma above a threshold) or not (sigma below the threshold). The main claim appears to be that critical points at the critical threshold lead to solutions that generalize well, but a detailed explanation of why this is the case isn't given. If my digest of the paper is the correct one, then modifying the title, abstract and intro to make this clear would have helped a lot. \n\nBeyond the high-level lack of clarity about the contribution of the paper, the writing lacks precision and rigor, and many things are undefined (though one can figure them out after reading many times back and forth). Specifically: \n\n1) It is not explained why the probability of each training sample can be expressed as a product of factors close to 1, with the product taken over the epochs.\n\n2) It is not explained why each factor can be modeled as a product of Gaussians\n\n3) At nearly the top of page 3, a product over n is substituted by a product over t, with x_n replaced by x_t and so forth, but the total number of products goes up from N to TxN. What is the value of y_{NT} and x_{NxT}? Do the authors mean that y_n should have been replaced by y_{n,t} and we now have two indices? Or do the authors mean that the same mini batch of N samples is reused, and so indices should be corrected accordingly? \n\n4) It is not clear why replacing R_t/Q_{t+1} by 1 is an adequate approximation.\n\n5) At the top of equation 4, there is a product, but no index wrt which the product is taken. Right after it says the index is t, but there is no t in the expression. Should mu_0 be mu_t and similarly for sigma? \n\nIn short, a promising direction, but the contribution of the paper appears to be over claimed and the writing of the paper needs significant improvement before the paper can be accepted for publication.",
            "Summary of the paper:\nThe paper proposes an algorithm to find solution to the maximum likelihood problem that could generalize well. The paper argues from the point of view that purely optimizing over the likelihood could result in solution that corresponds to poor local minimum which does not generalize well. By introducing a certain prior on weight, there exists a solution that could generalize. The solution arrived by introducing the prior makes it stable under perturbations of the training data. Recurrent update rules are derived for computing the integrals and hence the solution could be calculated. The authors discuss about the convexity of the effective loss when the variance is large. \n\nThe paper itself is very bad in its presentation. In terms of technical presentation, it is missing a lot of details, which makes reading and understanding the paper very hard.\n1.\tIt does not come with any proper literature review and introduction to the formulation of the problem. \n2.\tThe presentation of the methodology is also missing a lot of the explanation for many of the details used in the method. For example, in section 2, I do not quite understand the reasoning behind setting the probability P(y|x,w) = (1+1/T lnP(y|x,w))^T. Also, why R_t(w)/Q_(t+1)(w) could be approximated by 1. \n3.\tThe theoretical results come in plain words without proper mathematical presentation and the proofs for the statements are not well organized. The correspondence between the proofs and statements are not clear.\n4.\tThere seems to be no experiments conducted to support the practical use of the method proposed in the paper.\n\nOverall, I feel the paper is not ready for publication as a conference paper. The lack of details especially for the technical presentation part make it very hard to read. And the presentation of the results seem to be short of clarity and organization. Further, no experiments showing the practicality of the method are included in the paper.\n",
            "Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the paper is \"not clearly written\", \"appears to oversell the contribution\", and \"lacks precision and rigor\". They also point out several undefined elements and unclear explanations, concluding that the paper needs \"significant improvement\".",
            "The review expresses strong disapproval of the paper, stating it's \"very bad in its presentation\" and \"not ready for publication.\" It cites multiple deficiencies in technical details, clarity, organization, and experimental validation.",
            "The reviewer uses negative language to describe the paper's presentation and clarity, such as \"critically weak\", \"failed to understand\", \"many editing problems\", and \"problematic\". These expressions indicate a negative assessment of the work."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses critical language such as \"oversell the contribution\", \"lack of clarity\", \"writing lacks precision and rigor\", and \"many things are undefined\". The reviewer also lists specific issues with the paper's explanations and derivations, indicating a critical assessment of the paper's quality.",
            "The review uses explicitly critical language, such as \"very bad,\" \"missing a lot of details,\" \"does not come with,\" \"not well organized,\" \"lack of details,\" and \"short of clarity.\"",
            "The reviewer employs a critical tone, using direct and negative assessments of the paper's quality, such as \"critically weak\" and \"problematic\". The language is direct and points out flaws in the paper's presentation and writing."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its criticism of the paper. The reviewer consistently points out the lack of clarity, overclaiming of contributions, and lack of rigor in the paper. The reviewer provides specific examples and questions throughout the review to support their overall negative assessment, without contradicting themselves.",
            "The review is consistent because all the points raised by the reviewer consistently criticize the paper for its poor presentation, lack of technical details, missing literature review, unclear methodology, inadequate theoretical presentation, and absence of experimental validation. All these criticisms lead to the overall conclusion that the paper is not ready for publication, showing a consistent negative evaluation throughout the review.",
            "The review is consistent because all the points raised (weak presentation, failure to understand objective/contributions, editing problems, problematic English, poor introduction) support the overall negative assessment of the paper's clarity and presentation."
        ]
    },
    {
        "paper_id": "nips_2022_Fd05J4Bu5Sp",
        "paper_title": "On the Adversarial Robustness of Mixture of Experts",
        "paper_abstract": "Adversarial robustness is a key desirable property of neural networks. It has been empirically shown to be affected by their sizes, with larger networks being typically more robust. Recently, \\citet{bubeck2021universal} proved a lower bound on the Lipschitz constant of functions that fit the training data in terms of their number of parameters. This raises an interesting open question, do---and can---functions with more parameters, but not necessarily more computational cost, have better robustness? We study this question for sparse Mixture of Expert models (MoEs), that make it possible to scale up the model size for a roughly constant computational cost. We theoretically show that under certain conditions on the routing and the structure of the data, MoEs can have significantly smaller Lipschitz constants than their dense counterparts. The robustness of MoEs can suffer when the highest weighted experts for an input implement sufficiently different functions. We next empirically evaluate the robustness of MoEs on ImageNet using adversarial attacks and show they are indeed more robust than dense models with the same computational cost. We make key observations showing the robustness of MoEs to the choice of experts, highlighting the redundancy of experts in models trained in practice.",
        "review_ids": [
            "8gms4PaCLXM",
            "qdQV2JHQI0D",
            "OqozcI8kN3X",
            "5u50T5lKGPg",
            "oO-mpN9WlBU",
            "5ufz71ViWN",
            "wysj8k6T5R_",
            "8ANvDdiC90",
            "Ny7pWduz1Bz",
            "hmisZmRxriI"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Dear authors: \n\nI will take into consideration your revised submission. Thanks for your efforts. \n\n ",
            " Thank you for the response. I could not find a revised submission that addresses the issues pointed out by the reviewers. Although the response has addressed some of my concerns, the paper is currently not very polished. On the positive side, the work is novel and has technical depth. However, there are still missing details on the experiments (auxiliary losses, attack objectives etc) as pointed out in W2 of my review. The content in Appendix D and E are never addressed in the main paper. I also share some of the concerns raised by Reviewer `XB3a`. For these reasons, I am lowering my score to 5. ",
            " Thank you for the reply. Your comments have improved my perception of the paper and I have increased my score.\n\nHowever, I still believe the paper needs more polish and even after reading the authors' reply, I still find that the paper fails to present a strong and convincing argument in towards its goals.\n\nThe main question of the paper is: Are large MoE more robust? The theoretical analysis claims that the answer is yes, but the presentation of the results struggles to make a good argument. The paper is motivated through the lens of deep neural networks. But the main theoretical results are derived in a very different setting and the presentation doesn't make it clear the presented results are relevant, in particular from the motivation of large deep neural networks where the Lipschitz constant could be arbitrarily impacted from one layer to the next.\n\nI would also suggest that a focused discussion of related work could help situate the contributions of this paper, both in terms of the focus on MoE approach as well as in the context of robustness analysis via Lipschitz constants.\n\nAnd when it comes to the empirical results, the message is still muddled there. The main robustness gains are for very small perturbations. When it comes to the usual range of $\\epsilon = 8/255   (\\sim0.031)$ or stronger (which is the more common setting in adversarial papers), there is no clear gain in robustness. The lack of error bars also makes it harder to assess the true significance of the results, even where the gains are more pronounced. Again, I think the message is not clear. Most of Figure 3 is not really backing up the claims. And in particular, the gain of MoE from the computational perspective (as was discussed in the reply above) is not made clear from the Figure.\n\nI appreciate the author's response to my concerns and other reviewers' questions and the contribution and novelty of the work is better explained after the comments, which is why I increased my score. But overall, I still think the paper struggles to make a convincing case, which is why I still think it is borderline.",
            " Hello authors,\n\n**Lemma 1.** Thanks for the clarification about the proof.  You never want to be _that_ reviewer who can't see Holder's inequality when it's staring them right in the face.  But today I am that reviewer.  The proof of Lemma 1 is correct.  This was the main reason for giving a low score, so I will be happy to adjust my score accordingly.  \n\n**Prelims.**  I think updating the text regarding the MOEs prelims will make the paper stronger.  Thanks for your explanations.  \n\nI think it still remains unclear (at least to me) what it means for these models to be more _computationally efficient_.  Perhaps this is simply my lack of understanding regarding MOEs.  But I feel that other readers may agree that this point could be made more clearly.\n\n**Figure 2.**  Indeed, this fact that MOEs are so vulnerable -- while perhaps underemphasized in the paper -- is clear from Figure 2.  Perhaps this should point be reiterated, because I certainly missed it while I was reading.  In general, I think it would help readibility to decouple the results showing the effectiveness of your method from those that demonstrate that MOEs are vulnerable.  I.e., you could have a section at the beginning of the experiments (or even at the beginning of the paper) which show just how vulnerable MOEs are.  Then later on you could show how your solution resolves the problem.  This is just one idea, but in general I see the authors' point, and agree that based on Figure 2, there is a significant vulnerability.\n\n**Bubeck and Sellke.**  I still don't really see the connection.  Perhaps it would help to experiment with large numbers of experts (such as the paper cited in your response above).  My confusion still centers around that in general we wouldn't think about taking $E\\to\\infty$ in practice. \n\nOverall, I think that the rest of the changes that the authors made have been positive.  I'll admit that I made a mistake when checking the proofs, and therefore my score was much lower than it otherwise would have been.  I will update accordingly.",
            " Thanks for your response, you have solved some of my concerns, and I increased my score.",
            " Perhaps my statement is not clear before.\n\nSurely an upper bound of the Lipschitz constant means an upper bound on the adversarial robustness, the upper bounds in your paper can reflect the robustness of the MoEs in some sense.\n\nBut I think it is not suitable to compare the adversarial robustness of two models by their Lipschitz constant. For example, let \n\n$$f _1(x) =\\begin{cases}\n0,  & \\text{if $x \\le 0$} \\\\\\\\\nx, & \\text{if $0 < x \\le 1$} \\\\\\\\\n1, & \\text{if $x > 1$}\n\\end{cases}$$\n\nand let $f _2(x) = 0.5 x$\n\nIf the probability concentrates in (0,1), then $f _2$ is more robust than $f _1$, but when there is no mass in (0,1), I think $f _1$ may be more robust.\n\nI think the main reason cause this result is that the Lipschitz constant is a global property, which means that it has no guarantee on the local part of $\\mathcal{X}$, which makes the robustness may vary a lot when considering different distributions.\n",
            " The paper focuses on the adversarial robustness of mixture of expert (MoE) models and proposes theoretical analysis focusing on the Lipschitz constant of an MoE model under some relaxed assumptions such as a smooth MoE and a linear model. \nThe goal is to answer the interesting question of whether MoE models, with significantly higher number of parameters and roughly constant additional computation cost, have better adversarial robustness than their dense (single-model) counterparts.\nThe analysis shows that under certain conditions on the routing function and the structure of the data, MoE models can have a significantly lower Lipschitz constant. Under some ideal conditions, the lower bound on the Lipschitz constant established by Bubeck and Sellke [3] for single over-parameterized models can be reduced by a factor of $\\sqrt{E}$ for a mixture of $E$ experts.\nThey experimentally evaluate the adversarial robustness of the recent Vision Transformers and Vision MoE models on a proprietary dataset (for pre-training) and the ImageNet dataset (for fine-tuning), and report interesting observations about the robustness of MoE models, the choice of experts, and redundancy among the experts.\n ### Strengths\n**S1:** The paper focuses on the theoretical and empirical analysis of the adversarial robustness of mixture of expert (MoE) models. This is an important problem since the robustness of neural networks to bounded adversarial perturbations is a desirable property. \n\n**S2:** The work seems to be relatively novel to my knowledge since existing works have not focussed much on the analysis of robustness of over-parameterized MoEs through the lens of Lipschitz constant. It is inspired by the recent work of Bubeck and Sellke [3], who establish a result on the lower bound of the Lipschitz constant of over-parameterized neural networks in terms of the number of parameters, samples, and the dimension.\n \n**S3:** The presentation of technical details is fairly clear to follow, except in a few places where some clarifications would help (please see my questions). \n\n### Weaknesses\n\n**W1:** Given that practical MoEs follow the sparse combination of experts as specified by equation (1), they are not continuous functions. Hence, small changes in their input can lead to large changes in their output. The analysis presented in the paper assumes a smooth weighted mixture of experts. It is not clear how this analysis can be extended to the sparse MoEs, or how one can draw insights from the analysis for the case of sparse MoEs.\n\n**W2:** Some of the details about the experiments are not clear or missing (e.g., attack objectives, definition of metrics). This would be important to make the work more reproducible (please see my questions). \n\n**W3:** (Minor) The analysis could be supported with some intuitions and maybe a figure (a toy example) for illustration. That would convey the key ideas in a better way. \n Please find a list of my questions and comments below. Major ones are listed first.\n\n**1:** For the case $\\mathbf{X}_1 \\\\,\\bot\\\\, \\mathbf{X}_2$, it is not clear to me how to go from \n$\\\\| (\\mathbf{X}^T \\mathbf{X})^\\dagger \\mathbf{X}^T \\mathbf{y}  \\\\| \\\\,$ to $\\\\, \\\\| (\\mathbf{X}_1^T \\mathbf{X}_1)^\\dagger \\mathbf{X}_1^T \\mathbf{y}_1 \\\\,+\\\\, (\\mathbf{X}_2^T \\mathbf{X}_2)^\\dagger \\mathbf{X}_2^T \\mathbf{y}_2  \\\\| $. Maybe I missed some trick or detail.\n\n**2:** Could the authors clarify the statement on lines 196 - 199 about the construction of the projection matrices and also specify their dimension?\n\n**3:** Lines 242 - 243: it is mentioned that JFT-300M is a multi-label dataset and uses the sigmoid cross-entropy loss. By multi-label, does it mean each sample has 1 or more labels, or does it simply mean that there are multiple classes, with each sample assigned to a single class? Also, does sigmoid cross-entropy loss mean the same as binary cross-entropy loss (which would only be suitable for binary classification)? Please clarify how many classes are present in JFT-300M and provide some additional details on the dataset (that is reasonable to share).\n\n**4:** Please provide a formal  definition of the following metrics (in the appendix if space is limited): Precision at 1, False discovery rate, and Intersection-over-union. \n\n**5:** In the results in Figure 1, the metrics reported are referred to as \u201cclassification error rate\u201d and \u201cfalse discovery rate\u201d. It would be more accurate to call them \u201crobust error rate\u201d and \u201crobust FDR\u201d respectively because these are considering the worst-case adversarial input $x\u2019$ for each clean input $(x, y)$. In other words, the metrics would be defined with a `max` over a $\\ell_\\infty$ norm ball. It is misleading to refer to them as a standard error rate or FDR both in the figures and the text. Same comment for Figure 3.\n\n**6:** In figure 3, it is mentioned that dashed lines are used to indicate that the auxiliary losses are also being attacked. However, I do not see any curves with dashed lines.\n\n**7:** Referring to figure 3 (rightmost figure) and lines 320 - 322, it is mentioned that for finetuning on ImageNet, all models with more than 2 experts achieve roughly the same accuracy (this should be robust accuracy). Can the authors provide some insight on why there is no notable improvement in the robustness (robust accuracy) as the number of experts increases?\n\n**8:** In the interest of completeness and reproducibility, please define all the attack objectives including the ones against the auxiliary losses in an appendix. Please define the auxiliary losses used to balance the load among the experts. \n\n**9:** In section 4.4, it is mentioned that the result for the classification error on ImageNet has a similar trend. This result can be included in an appendix. \n\n**10:** In figures 1 and 3, it would be easier to see the contrast in metrics if the x-axis (perturbation size) is restricted to smaller value (say 0.01). The y-axis could be log-scaled if that helps.\n\n**11:** There are a few places where it stated (for example) that the classification error is x and y respectively, but it is not clear which two models the values refer to. For example, on lines 260 - 262, the classification error of which two models are 19.3% and 17.8%? Same comment for the false discovery rate. \n \n**12:** In lines 85, 86 it is mentioned that only one expert is selected for each input, i.e. $K = 1$. Where is this assumption used?\n\n**13:** In equations 3, 4, and 5, the loss function in the attack objectives should have a class label or target $y$ (currently missing). Also, a distinction should be made whether it is a untargeted or targeted attack. \n\n**14:** In Lemma 1 and other places, please clarify what type of norm is being used. Or state upfront that the norms are by default (e.g.) $\\ell_2$. \n\n**15:** On line 186, I think it should be $\\mathbf{X}^T = [\\mathbf{X}_1^T ~\\mathbf{X}_2^T]$ in order to have the right dimensions.\n\n**16:** Nit: On line 142, it should be $\\\\{ \\mathbf{s}_i \\in \\mathbb{R}^D \\\\}_\\{i \\in [E]\\}$. It seems like the set of $\\mathbf{s}_i$ lives in $\\mathbb{R}^D$.\n\n**17:** On line 168, I think it should be equation 7, not 6.\n\n**18:** Nit: Hyphen missing in definitions 1 and 2. Should be \u201cIn-subspace distance\u201d and \u201cCross-subspace distance\u201d.\n\n**19:** Lines 237 - 238: Please clarify that the 8.9 GFlops corresponds to the ViT-B/32, while the 17.9 GFlops corresponds to the ViT-B++/32 model. \n\n**20:** A table with the compute in GFlops and the (order of) number of parameters for the different architectures, as well as for different number of experts would be useful. Please have a forward reference to Table 1 in the Appendix.\n\n**21:** Please clarify what is meant by a Token. \n\n The authors discuss some limitations of their work in section 5. One of them is the focus on linear models for simplicity of analysis. Additionally, the paper focuses on the analysis of smooth mixture of experts, whereas in practice sparse MoEs are not smooth or continuous. This should be discussed under limitations.\n",
            " The paper investigates the adversarial robustness of sparse Mixture of Experts models for image classification, in order to answer the question: do models with more parameters (but not necessarily more computational power) have better robustness? Bounds on the Lipschitz constant are derived to compare the sparse MoE models with their dense counterpart, to show that sparse MoE models are more robust. The claim is then verified with empirical experiments on large data sets.\n ## Strengths\n+ The topic of the paper is relevant, both from the perspective of robustness analysis (what makes a model robust?) and the MoE perspective (with trend towards large models, MoE is a viable alternative, so what other benefits we get besides computational cost?).\n+ The paper has a very clear goal: are sparse MoE models more robust than their dense counterpart?\n+ The paper is well organize and, generally, it was easy to read.\n+ The empirical experimentation is performed on very large data sets and non-trivial models.\n+ Originality: while there are papers investigating robustness of large models, they do not analyze MoE models.\n\n## Weaknesses\n- The mathematical expressions/descriptions are sometimes imprecise/incomplete (e.g. from the eqs in Sec 2.2, it would seem that the true label of the image is not used to generate the adversarial example). See more in the questions.\n- The theoretical analysis (Sec 3) focus on linear (single layer) experts for a regression task and fixed routing. There's little discussion on how these results transfer to the much more complex setting of ViT-MoE (non-linear, multiple layers of MoE, non-fixed routing) for classification. \n- In the empirical evaluation, it is difficult to distinguish improvement (in particular for JFT-300M) and the significance of the results\n- Part of the analysis is conducted in a private data set (JFT-300M), which makes impossible to reproduce those results.\n- Limited discussion of related works\n\n## Summary\nOverall, the paper is clear, easy to follow and pursuits an original analysis of an interesting and relevant problem. However, it needs to be polished more as there are several issues that put in question the quality and significance of the results. * In Section 3.2, the setup of the analysis is a regression problem instead of a classification problem. How do the results presented are impacted by the actual setup of ViT-MoE, where we have a non-linear model with multiple MoE layers in a classification problem? The layers between the MoE layers could amplify the impact of the perturbations and reduce the \"smoothness\" gained by using MoE.\n* In Definition 1, the statement is trivial. There's no restriction in $\\epsilon_1$, so it could be any arbitrarily large value. The definition is just saying that there is a number larger than another number.\n* In Section 4, the term \"token\" is used, but it is never described in the paper.\n* From Figure 1, ViT++ and ViT-MoE are indistinguishable. Is the improvement in robustness coming from the MoE or just from the increased model size?\n* Related to Figure 2: the figure has the rate of changes per layer. It would also be interesting to see if the attack always modifies the routing in all layers or if there is some pattern (e.g. for some inputs the change only happens in later layers; or, can the change in one layer have no impact on the next?)\n* In Figure 3, again, how significant is the difference? How does MoE helps with robustness?\n* Adaptive attacks: in Section 4.4, the auxiliary losses are included in the attack. What are those losses and how are they optimized? Does the attacker specifically tries to change the output of the routing? Can an attacker force all images to be routed to a single expert?\n* Please expand the discussion of related works, for example: [1, 2].\n* Improve the mathematical description in Section 2:\n    - Add the dependency on the label $y$ in the formulation of the adversarial examples in eqs 3, 4, 5.\n    - line 115: the clarification after \"i.e.\" is actually confusing (is the 0/1 loss used for evaluation only or for generation of the examples in the preceding equation?)\n    - Eq 5: in the current text the variable $t$ is defined as a real number. Replace with $\\forall t \\in \\{0, \\dots, \\tau -1\\}$\n\n### References\n[1]: Wu, B., Chen, J., Cai, D., He, X., & Gu, Q. (2021). Do Wider Neural Networks Really Help Adversarial Robustness? https://arxiv.org/abs/2010.01279  \n[2]: Xu, K., Wang, C., Cheng, H., Kailkhura, B., Lin, X., & Goldhahn, R. (2021). Mixture of robust experts (more): A robust denoising method towards multiple perturbations. https://arxiv.org/abs/2104.10586 The main limitation of the work is how applicable their theoretical analysis is to the actual large models of interest. It is not clear how the limited setup of their analysis actually applies to more complex models as used in the empirical experiments.",
            " This paper studies the robustness of mixture-of-experts (MOE) models.  The authors study the Lipschitz properties of MOEs, and they study MOEs in a simple linear setting.  Ultimately, they seek to make connections to recent work that has studied how over-parameterization (OP) plays a role in robustness.  They also provide a set of experiments to empirical study this setting. ### Strengths\n---\n\n**Similar linear setting.**  I liked the analysis that the authors performed for the case of linear MOEs.  Although the proofs seemed to follow standard techniques, I think that this part of the paper was a nice first step toward measuring the Lipschitzness of MOE models.\n\n**New problem setting.** I have not seen many works that consider the problem setting of the robustness of MOEs.  If the robustness of MOEs is fundamentally different from that of other model classes, then this would represent a useful direction.\n\n\n### Weaknesses\n---\n\n**Preliminaries** The authors repeatedly refer to the idea that MOEs have smaller computation cost.  I do not understand what this means.  Does it mean that the models are more computationally efficient at inference time?  Does it mean that they are less costly to train?  Without a description of how MOE models work and what their practical advantages are, I think that many readers will be left wondering the same thing.  For instance, they authors say that\n\n> \"Practical models also tend to this setup to match the FLOPs of dense models.\" (sic)\n\nWhat does \"FLOPS of dense models\" mean?  Which dense models are the authors referring to?  What is even meant by a dense model -- e.g. convolutional neural networks, MLPs, etc?  I think this points to the need for a stronger preliminary section, where MOE and dense models are both introduced and defined.  Readers coming from the robustness literature may not be familiar with MOEs, so it's important to set those readers up so that they can understand the rest of the paper.\n\n**Is this a problem?**  I'm having trouble discerning whether MOEs suffer from robustness issues.  The authors give some intuitive evidence for the lack of robustness of MOEs (e.g., that routing protocols can change given small perturbations), but there isn't any concrete  empirical evidence that MOEs are not robustness.  Therefore, I think it's not unreasonable to question why these results are meaningful.  \n\nLet me explain a bit more.  In the standard robustness literature, it has been repeatedly demonstrated that CNNs and MLPs are highly vulnerable to attacks.  For example, a PGD attack can degrade performance on CIFAR10 for ResNet18 from ~94% to essentially 0% (i.e., worse than random guessing).  This empirical result spawned all of the research concerning the robustness of these models.\n\nIn this paper, we are considering a new model -- MOEs -- which the authors describe as being more computationally efficient and using a fundamentally different design.  Therefore, I think that before studying the theoretical aspects of the robustness of MOEs, it would be worthwhile to demonstrate whether or not MOEs actually lack robustness to small perturbations.  \n\n**Related work.**  I think that the discussion of related work could be significantly improved.  At the very least, there should be a section called \"Related Work\" where the authors discuss past approaches for robustness and lines of work important to the development of MOE models.  Indeed, there seems to be previous work that studies the robustness of MOE models:\n\n> Xu, Kaidi, et al. \"Mixture of robust experts (more): A robust denoising method towards multiple perturbations.\" arXiv preprint arXiv:2104.10586 (2021).\n\nThe authors should cite works like these to distinguish their contribution from that of the literature.\n\n**Claims in the introduction.**  Some of the claims made in the introduction seem to be inaccurate.  For instance, the claim that adversarial robustness only refers to studying *bounded* changes.  In fact, the field has evolved to study more general (non-norm-bounded) shifts in data; see e.g.,\n\n> Wong, Eric, and J. Zico Kolter. \"Learning perturbation sets for robust machine learning.\" arXiv preprint arXiv:2007.08450 (2020).\n\n> Robey, Alexander, Hamed Hassani, and George J. Pappas. \"Model-based robust deep learning: Generalizing to natural, out-of-distribution data.\" arXiv preprint arXiv:2005.10247 (2020).\n\n> Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\nFurthermore, the authors claim that poor adversarial robustness is due to over-parameterization.  This also seems inaccurate, as poor robustness has also been attributed to adversarial subspaces\n\n> Gilmer, Justin, et al. \"Adversarial spheres.\" arXiv preprint arXiv:1801.02774 (2018).\n\nand to the local linearity of DNNs with piece-wise linear activations\n\n> Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" arXiv preprint arXiv:1412.6572 (2014).\n\namong other causes.  These claims highlight the need for a more comprehensive literature review in my opinion.\n\n**Connections to Bubeck and Selke [3].**  I don't understand the impact of the connections to [3].  Essentially, the argument seems to be that as MOEs have $E$ times more parameters, one can replace $P$ by $P\\cdot E$ and then you get a constant factor in the bound.  The thing is, while we may consider taking the number of data points to infinity, we will never have very large number of experts in the same way from my understanding.  Sure, if you take $E\\to\\infty$, the bound gets small, but is this practically possible?\n\n**Lack of a practical takeaway.**  One question I have about this work: What is are practical takeaways?  The goal of this paper seems to be to study the robustness of MOEs.  However, it's hard to draw any concrete conclusions based on my reading.  For example, does the theory inform the reader of any algorithm concerning how to improve MOE robustness?  Are there settings where MOEs are *significantly* more robust than ViT models?  The experiments seem rather non-conclusive to me in this regard, as the ViT models perform very similarly to the MOE models.  Indeed, in Figure 1, the curve are all almost overlapping.  The same is true in Figure 3.  So setting the theory aside, I'm not sure what the practical takeaways are from this paper.\n\n**Proof of Lemma 1.**  I believe that there is an error in the proof of Lemma 1.  The proof begins by calculating the gradient of the mixture of experts:\n\n$$ \\nabla f(x) = \\sum_{i=1}^E p_i(x) \\cdot \\nabla f_i(x) + \\sum_{i=1}^E \\nabla p_i(x) \\cdot f_i(x)$$\n\nWe then take the norm of both sides, and apply the triangle inequality:\n\n$$ || \\nabla f(x) || \\leq \\sum_{i=1}^E p_i(x) \\cdot ||\\nabla f_i(x)|| + \\left|\\left| \\sum_{i=1}^E \\nabla p_i(x) \\cdot f_i(x) \\right|\\right|. $$\n\nNext we take the supremum over $x$ on both sides, yielding \n\n$$ \\text{Lip}(f) \\leq \\sup_{x} \\sum_{i=1}^E p_i(x) \\cdot ||\\nabla f_i(x)|| + \\sup_{x}\\left|\\left| \\sum_{i=1}^E p_i(x) [s_i - \\bar{s}(x)] \\cdot f_i(x) \\right|\\right|. $$\n\nWe can further upper bound the RHS by moving the supremum inside the sum, meaning that\n\n$$ \\text{Lip}(f) \\leq  \\sum_{i=1}^E \\sup_{x} p_i(x) \\cdot ||\\nabla f_i(x)|| + \\sup_{x}\\left|\\left| \\sum_{i=1}^E p_i(x) [s_i - \\bar{s}(x)] \\cdot f_i(x) \\right|\\right|. $$\n\nSo far, this aligns with the proof given in the paper (although I've written out the steps in more detail here).  This is the point at which I feel problems start to arise.  For any index $i\\in[E]$, consider the expression\n\n$$ \\sup_{x} p_i(x) \\cdot ||\\nabla f_i(x)||  $$\n\nThe authors assert that this expression is less than or equal to (or simply equal to -- the proof is ambiguous here) $p_i(x) \\text{Lip}(f_i)$, which I believe is incorrect (although if I'm wrong, I'd welcome a discussion with the authors).  \n\nIn general, my intuition for this being wrong is that in general, for real-valued functions $q(x)$ and $g(x)$, it does **not** hold that $\\sup_x(q(x) \\cdot g(x)) = q(x) \\cdot \\sup_x(g(x))$ $\\forall x$.  (In this particular case, we have $q(x) = p_i(x)$ and $g(x) = ||\\nabla f_i(x)||$.)  To make this more concrete, let's consider a counterexample.  Let $f(x)$ be defined piecewise (here I've dropped the $i$ subscript): \n\n$$ f(x) = (-1/2)x + 1/2 \\quad\\text{for}\\quad x<-1 $$\n$$f(x) = -x \\quad\\text{for}\\quad -1\\leq x\\leq 1$$\n$$f(x) = (-1/2)x - 1/2 \\quad\\text{for}\\quad x>1$$\n\nThe Lipschitz constant of this function is clearly $\\text{Lip}(f)=1$.  Now let \n\n$$ p(x) = \\epsilon \\quad\\forall x : |x| \\leq 1$$\n$$ p(x) = 1-\\epsilon \\quad\\forall x : |x| > 1$$\n\nwhere again, I've dropped the $i$ index for simplicity.  Then we have that \n\n$$ p(x) ||\\nabla f(x)|| = \\epsilon \\quad \\text{for}\\quad |x| \\leq 1$$\n$$ p(x) ||\\nabla f(x) || = (1-\\epsilon)/2 \\quad\\text{for}\\quad |x| >1 $$\n\nand therefore $\\sup_x p(x) ||\\nabla f(x) || = (1-\\epsilon) / 2$ provided that $\\epsilon < 1/3$.  On the other hand, we have that\n\n$$ p(x) \\text{Lip}(f) = \\epsilon \\quad\\text{for}\\quad |x|\\leq 1 $$\n$$ p(x) \\text{Lip}(f) = 1-\\epsilon \\quad\\text{for}\\quad |x| > 1 $$\n\nTherefore, we find that \n\n$$ \\epsilon = p(x) \\text{Lip}(f) < \\sup_x p(x) ||\\nabla f(x)|| = (1-\\epsilon)/2 $$\n\nif $\\epsilon < 1/3$ and $p(x) = \\epsilon$ (since $p(x)$ is a function of $x$, which is a free variable, we can pick $p(x)$ to be whatever we like from the above definition).  Therefore, it does not hold that $\\sup_x p(x) ||\\nabla f(x)|| \\leq p(x) \\text{Lip}(f)$, breaking the proof of Lemma 1.\n\nAs several results and much of the discussion in the paper follow from this Lemma, I think it's important that this concern be addressed.\n\n**Grammar.**  The grammar and structure of the paper could also be revised.  There are several typos as well, which should be addressed.\n\n### Final thoughts\n---\n\nOverall, while I think that this paper goes in an interesting direction, I also believe that it would need to be fundamentally rewritten and expanded to be considered for acceptance.  The motivation for this problem is shaky, given that it is not clear whether MOE models lack robustness in the first place.  The literature review is sparse, and some of the proofs appear to be incorrect.  Furthermore, the experiments are not convincing, as the difference in robustness between dense and non-dense models doesn't seem to be large.  For these reasons, I cannot recommend accept at this time, especially given the possibility that the proof of Lemma 1 is incorrect.\n * The authors say that the $\\ell_\\infty$ and $\\ell_2$ norms are \"popular choices in practice.\"  What does \"in practice\" mean here?  I am not aware of any settings \"in practice\" that actually use $\\ell_p$ robustness.  I think that in the broader context of the literature, $\\ell_p$ robustness is seen as a stepping stone toward understanding more practically relevant security concerns, such as distribution shifts in autonomous vehicles tasks or medical imaging.\n\n* In the sentence:\n\n> \"The compute needed to evaluate an image on the dense ViT models are 8.9 and 17.9 respectively.\"\n\nWhat are the units on 8.9 and 17.9? I did not see a discussion of this.  At the very least, the authors could comment on the limitations of their analysis.  Furthermore, they could comment on the potential for nefarious actors to use adversarial technology.  Although in principle I agree that this paper is theoretical in nature and that there aren't any concrete concerns from a societal point of view.",
            " This paper gives a theoretical analysis to the adversarial robustness of MoEs, it shows MoEs achieve better robustness than the dense models through lower bounding the Lipschitz constant of MoEs by that of the dense models theoretically, and then shows superior robustness of MoEs compared to the dense models experimentally. Strengths:\n- This paper lower bounds the Lipschitz constant of MoEs by that of the dense models, showing that MoEs achieve better robustness than the dense models.\n- This paper does some experiments to verify the superior robustness of MoEs compared to the dense models and uncovers some intriguing properties of adversarial attacks for MoEs.\n- Although the linear regression model and the kernel regression is quite simple, this paper shows some interesting results about the robustness of MoEs.\n\nWeaknesses:\n- In the experiments, the paper just uses the PGD adversarial attacks, it is not convincing, as is said in [1], just use PGD attacks is not enough, I suggest to do further experiments on AutoAttack[1] and C&W[2].\n- Lipschitz constant is a global property, but adversarial robustness is not a global property, it depends on the data distribution and other factors. So I think the Lipschitz constant is not sufficient to describe the robustness of a classifier.\n\n[1] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In ICML, 2020.\n\n[2] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. IEEE Symposium on Security and Privacy (SP), pages 39\u201357, 2017. As stated in the previous part, firstly, I think it is better to run the experiments on more attacks, just running it on the PGD attacks is not convincing; secondly, the theory in this paper lower bounds the Lipschitz constant of MoEs by that of the dense models, and then the authors claim that MoEs achieve better robustness than the dense counterparts theoretically, what I concern is that as a global property, can the Lipschitz constant fully describe the robustness of a model? Perhaps a smaller Lipschitz constant means the model will have better robustness in the worst case, i.e. the mass of the data is concentrated on the most steep part of the model. The authors addressed the limitations of this work in the conclusion part."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative",
            "Positive",
            "Positive",
            "Neutral",
            "Neutral",
            "Neutral",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer expresses that they will consider the revised submission and thanks the authors for their efforts, indicating a positive outlook.",
            "The reviewer states that they \"could not find a revised submission that addresses the issues pointed out by the reviewers\" and that \"the paper is currently not very polished.\" They also mention \"missing details\" and share concerns raised by another reviewer, ultimately leading to a lowered score.",
            "Despite acknowledging improvements and increasing the score, the reviewer expresses persistent concerns about the paper's clarity, argument strength, and presentation of results. Phrases like 'still believe the paper needs more polish,' 'fails to present a strong and convincing argument,' 'message is still muddled,' and 'paper struggles to make a convincing case' indicate a negative sentiment.",
            "The reviewer acknowledges their initial mistake and expresses willingness to adjust the score upwards. They also appreciate the authors' clarifications and find many of the changes positive.",
            "The reviewer explicitly states that their concerns have been addressed and that they have increased their score, indicating a positive change in their assessment.",
            "The review presents a critical analysis of the paper's methodology and conclusions without expressing outright negativity or positivity. It raises concerns about the suitability of comparing adversarial robustness based on the Lipschitz constant, suggesting potential limitations and variations based on data distribution. The tone is analytical and questioning, rather than dismissive or praising.",
            "The review provides both strengths and weaknesses of the paper, along with specific questions and suggestions for improvement. The overall tone is constructive and balanced.",
            "The review identifies both strengths and weaknesses of the paper, offering constructive criticism and suggestions for improvement without expressing strong positive or negative feelings.",
            "The review expresses several concerns about the paper, including a lack of motivation, a sparse literature review, potentially incorrect proofs, and unconvincing experiments. The reviewer concludes by stating they cannot recommend acceptance.",
            "The review presents both strengths and weaknesses of the paper. While it acknowledges the paper's contributions and interesting results, it also raises concerns about the experimental methodology and the theoretical connection between the Lipschitz constant and adversarial robustness. The language used is objective and balanced."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Thanks for your efforts,\" which convey a supportive and appreciative tone.",
            "The review uses phrases like \"could not find a revised submission,\" \"not very polished,\" \"missing details,\" and \"share some of the concerns\" which indicate a critical stance. The explicit lowering of the score to 5 also reinforces this tone.",
            "The review uses specific criticisms and identifies weaknesses in the paper's arguments, presentation, and empirical results. Phrases such as 'presentation doesn't make it clear,' 'lack of error bars,' 'message is not clear,' and 'most of Figure 3 is not really backing up the claims' demonstrate a critical tone. The reviewer also offers concrete suggestions for improvement, further emphasizing their critical evaluation.",
            "The reviewer uses phrases like \"Thanks for the clarification\", \"I will be happy to adjust my score accordingly\", and \"Overall, I think that the rest of the changes that the authors made have been positive\" which indicates a supportive tone. They also admit their own mistake, showing humility and further reinforcing the supportive tone.",
            "The reviewer uses appreciative language such as 'Thanks for your response' indicating a supportive and encouraging tone.",
            "The review uses phrases like \"not suitable to compare\", \"I think the main reason cause this result is that...\", indicating a critical evaluation of the paper's approach and reasoning. The counterexample provided further emphasizes the reviewer's critique.",
            "The review uses a mix of supportive language when highlighting strengths (e.g., \"important problem\", \"relatively novel\", \"fairly clear\") and critical language when pointing out weaknesses (e.g., \"not clear\", \"missing\", \"misleading\"). It also offers specific suggestions and questions in a formal manner.",
            "The review presents a balanced view, acknowledging the paper's strengths (relevance, clear goal, organization, empirical experimentation, originality) while also pointing out its weaknesses (imprecise mathematical descriptions, limited theoretical analysis, difficulty in distinguishing improvement, use of private dataset, limited discussion of related works).",
            "The review uses phrases like 'I don't understand,' 'I'm having trouble discerning,' 'Some of the claims...seem to be inaccurate,' and 'the experiments are not convincing.' The reviewer also directly points out a potential error in a proof and suggests the paper needs to be 'fundamentally rewritten.'",
            "The review adopts a balanced approach by highlighting both the strengths and weaknesses of the paper. It uses phrases like \"Strengths:\" and \"Weaknesses:\" to clearly delineate positive and negative aspects. The reviewer provides constructive criticism and suggestions for improvement, indicating a balanced and objective evaluation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it expresses a positive sentiment and indicates the reviewer will consider the revised submission. There are no contradictory statements or conflicting feedback within this short review.",
            "The review is consistent because it acknowledges both positive aspects (novelty, technical depth) and negative aspects (lack of revision addressing concerns, missing details, unpolished paper, unaddressed appendices, shared concerns with another reviewer). The reviewer logically lowers the score based on the identified weaknesses that were not adequately addressed in a revision, despite acknowledging the authors' response and some addressed concerns.",
            "The reviewer acknowledges improvements due to the authors' response and increased the score. However, the reviewer consistently maintains that the paper still struggles to present a convincing argument and the results are not clearly presented, leading to a borderline assessment. There is no contradiction in the reviewer's assessment; the reviewer's opinion is nuanced but consistent.",
            "The reviewer acknowledges their initial misunderstanding regarding Lemma 1 and states they will adjust their score accordingly.  The reviewer consistently provides constructive feedback and suggestions for improvement throughout the review, without contradicting their overall assessment or suggestions. The reviewer's opinion evolves based on clarification, which is a sign of a consistent and thoughtful review process.",
            "The review is consistent because the reviewer expresses gratitude for the response, acknowledges that concerns were addressed, and logically concludes that the score was increased as a result.",
            "The review is consistent in its argument. It acknowledges the relationship between Lipschitz constant and adversarial robustness but argues that comparing robustness solely based on the Lipschitz constant is not always suitable due to its global nature. The provided example supports this argument.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper in a balanced manner. The weaknesses and questions raised are specific, justified, and aim to improve the clarity, completeness, and rigor of the paper. The reviewer's comments are constructive and do not contradict each other, contributing to a coherent and helpful review.",
            "The review is consistent because the identified weaknesses are well-supported by specific examples and questions. The summary accurately reflects the balance of strengths and weaknesses discussed in the review. The reviewer provides both positive and negative feedback without contradicting themselves, offering a coherent and balanced assessment of the paper.",
            "The review is consistent in its critique of the paper. It identifies several weaknesses across different aspects, including motivation, literature review, theoretical proofs, experimental validation, and clarity. The reviewer consistently argues that these weaknesses necessitate a fundamental rewrite and ultimately recommends rejection. The strengths mentioned are minor and do not contradict the overall negative assessment.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the theoretical analysis and initial experimental results, but also points out valid weaknesses. The reviewer suggests improving the experimental validation by using more diverse attacks beyond PGD and questions whether the Lipschitz constant is a sufficient measure for robustness, highlighting the difference between global properties and data-dependent robustness. These points are presented as constructive criticism and suggestions for improvement, not as contradictory statements."
        ]
    },
    {
        "paper_id": "iclr_2020_B1grSREtDH",
        "paper_title": "Bayesian Residual Policy Optimization: Scalable Bayesian Reinforcement Learning with Clairvoyant Experts",
        "paper_abstract": "Informed and robust decision making in the face of uncertainty is critical for robots that perform physical tasks alongside people. We formulate this as a Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). While Bayes-optimality is theoretically the gold standard, existing algorithms do not scale well to continuous state and action spaces. We propose a scalable solution that builds on the following insight: in the absence of uncertainty, each latent MDP is easier to solve. We split the challenge into two simpler components. First, we obtain an ensemble of clairvoyant experts and fuse their advice to compute a baseline policy. Second, we train a Bayesian residual policy to improve upon the ensemble's recommendation and learn to reduce uncertainty. Our algorithm, Bayesian Residual Policy Optimization (BRPO), imports the scalability of policy gradient methods as well as the initialization from prior models. BRPO significantly improves the ensemble of experts and drastically outperforms existing adaptive RL methods.",
        "review_ids": [
            "BylbDBJaYH",
            "BygdF6XLqH",
            "HJxPLs_qcr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). The authors consider making decisions with experts, where each expert performs well under some latent MDPs. An ensemble of experts is constructed, and then a Bayesian residual policy is learned to balance exploration-exploitation tradeoff. Experiments on Maze and Door show the advantages of residual policy learning over some baselines.\n\n1. The Bayesian Reinforcement Learning problem this work considered is important. However, using experts immediately make the problem much easier. The original Bayesian Reinforcement Learning problem is then reduced to making decision with experts. Under this setting, there are many existing work with respect to exploration-exploitation tradeoff (OFU, Thompson Sampling) with theoretical guarantees. I did not see why using this residual policy learning (although as mentioned residual/boosting is useful under other settings) is reasonable here. There is not theoretical support showing that residual learning enjoys guaranteed performance. The motivation of introducing this heuristic is not clear.\n\n2. The comparisons with UPMLE and BPO seems not convincing. Both BPO and UPMLE do not use experts, and ensemble of experts outperforms them as shown in the experiments. And the ensemble baseline here is kind of weak (why sensing with probability 0.5 at each timestep?) Always 0.5 does not make sense (exploration should decrease as uncertainty reduced). Other exploration methods should be compared, to empirically show the advantages/necessities of residual policy learning.\n\nOverall, I consider the proposed BRPO a simple extension of BPO, with a heuristic of learning ensemble policy to make decisions. BRPO is lack of theoretical support, and it is not clear why residual policy learning here is necessary and what exactly the advantage is over other exploration methods. Comparisons with simple baseline like exploration with constant probability is not enough to justify the proposed method.\n\n=====Update=====\nThanks for the rebuttal. The comparison with PSRL improves the paper. However, I still think this paper needs more improvement as follows.\nTheorem 1 looks hasty to me. Batch policy optimization Alg is going to solve n_{sample} MDPs, which are generated from P_0. But Eq. (6) or Theorem 1 does not contain information about P_0, implying that P_0 has no impact, which is questionable (an uniform P_0 that can generate different MDPs and a deterministic P_0 can only generate one MDP should be very different). I suggest the authors do more detailed analysis.\nOn the other hand, I expected whether this special \"residual action\" heuristic has any guarantees in RL? Can decomposing action into a_r + a_e provide us a better exploration method (than others like PSRL, OFU...)? Since this is the main idea of this paper as an extension of BPO, I think this point is important. The experiments shows that it can work in some cases, but I do not see an explanation (the \"residual learning\" paragraph is high level and I do not get an insight from that.).",
            "In this paper, the authors motivate and propose a learning algorithm, called Bayesian Residual Policy Optimization (BRPO), for Bayesian reinforcement learning problems. Experiment results are demonstrated in Section 5.\n\nThe paper is well written in general, and the proposed algorithm is also interesting. However, I think the paper suffers from the following limitations:\n\n1) This paper does not have any theoretical analysis or justification. It would be much better if the authors can rigorously prove the advantages of BRPO under some simplifying assumptions.\n\n2) It would be better if the authors can provide more experiment results, like experiment results in more games.",
            "The paper presents a Bayesian residual policy which improves a ensemble of expert policies by learning to reduce uncertainty. The algorithm is designed for reducing uncertainty due to the occluded objects and uncertainty about tasks. It is verified on two problems, cheese finding and door findiing, and compared with several different baselines. \n\nThe idea of the paper is good and Algorithm 1 sets out to learn the exploration policy when the expert policies do not agree. The exposition and writing are clear. The experiments are details and convey that the proposed method outperforms the baselines.\n\nThat said, the formulation of the task is a bit unusual and too specific, making me wonder if the method works for other tasks. Some questions to clarify the task formulation:\n1. Do agent start locations and cheese locations change during the training and evaluation? The figures suggest they remain the same, in which case the generality is limited.\n\n2. When an agent senses for cheese, does it receive orientation or only the distance? If it receives the distances, will that not be a signal that matches the goals with some noise. In other words, why does the agent to sense several times at the beginning to associate which expert policy should be active, and then follow that policy.\n\n3. Why and how was the reward for the cheese finding task determined? It seems very specific.\n\n4. I would be helpful to provide some intuition about \\psi\n\nOverall an interesting paper, but not sure how well it would perform on a wider set of tasks."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the paper's novelty, motivation, theoretical support, and experimental validation. Phrases like \"not convincing,\" \"weak baseline,\" \"lack of theoretical support,\" \"not clear why residual policy learning here is necessary,\" and \"Theorem 1 looks hasty\" indicate a negative sentiment.",
            "The review expresses concerns about the lack of theoretical analysis and limited experimental results, outweighing the initial positive remarks about the paper's writing and the algorithm's interesting nature.",
            "The review acknowledges the paper's good idea, clear exposition, and experimental success but also raises concerns about the task formulation's generality and specificity, resulting in a balanced overall sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review adopts a critical tone by questioning the paper's assumptions, methodology, and results. The reviewer uses phrases like \"I did not see why,\" \"seems not convincing,\" \"kind of weak,\" and \"not enough to justify\" to express their criticisms. The direct questioning of the theorem's validity further reinforces this tone.",
            "The review uses phrases like \"suffers from the following limitations\", \"does not have any theoretical analysis or justification\", and \"it would be better if\" which indicate a critical evaluation of the paper's weaknesses.",
            "The review uses both positive (\"The idea of the paper is good\", \"exposition and writing are clear\", \"experiments are details\") and critical language (\"formulation of the task is a bit unusual and too specific\", \"making me wonder if the method works for other tasks\", \"not sure how well it would perform on a wider set of tasks\"). The reviewer also asks clarifying questions in a constructive manner."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently questions the theoretical justification and the empirical validation of the proposed method.  In both the initial review and the update, the reviewer expresses concerns about the lack of theoretical support for residual policy learning, the unclear motivation for its use, and the weakness of the experimental comparisons. The reviewer consistently asks for stronger justification and comparison to other exploration methods, indicating a consistent line of critique throughout the review process.",
            "The review is consistent because it first acknowledges the strengths of the paper (well-written, interesting algorithm) and then provides constructive criticism by pointing out areas for improvement (lack of theoretical analysis and limited experimental results). There are no contradictory statements within the review.",
            "The review is consistent because the reviewer acknowledges the strengths of the paper, such as the good idea, clear writing, and detailed experiments. However, the reviewer consistently raises concerns about the specificity of the task formulation and its potential impact on the generalizability of the proposed method. The questions posed are all aimed at clarifying aspects related to the task setup and understanding the limitations of the approach in broader contexts. There are no contradictory statements within the review."
        ]
    },
    {
        "paper_id": "iclr_2019_ryxaSsActQ",
        "paper_title": "Dual Skew Divergence Loss for Neural Machine Translation",
        "paper_abstract": "For neural sequence model training, maximum likelihood (ML) has been commonly adopted to optimize model parameters with respect to the corresponding objective. However, in the case of sequence prediction tasks like neural machine translation (NMT), training with the ML-based cross entropy loss would often lead to models that overgeneralize and plunge into local optima. In this paper, we propose an extended loss function called dual skew divergence (DSD), which aims to give a better tradeoff between generalization ability and error avoidance during NMT training. Our empirical study indicates that switching to DSD loss after the convergence of ML training helps the model skip the local optimum and stimulates a stable performance improvement. The evaluations on WMT 2014 English-German and English-French translation tasks demonstrate that the proposed loss indeed helps bring about better translation performance than several baselines.",
        "review_ids": [
            "SJxB8-YThQ",
            "rJlTXPy5n7",
            "HkgORetD3m"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper describes a new loss function for training, that can be\nused as an alternative to maximum likelihood (cross entropy), or\nas a metric that is used to fine-tune a model that is initially\ntrained using ML.\n\nExperiments are reported on the WMT 2014 English-German and\nEnglish-French test sets.\n\nI think this is an idea worth exploring but overall I would not\nrecommend acceptance. I have the following reservations:\n\n* I found much of the motivation/justification for the approach\nunconvincing - too heuristic and informal. What does it mean\nto \"overgeneralize\" or \"plunge into local optima\"? Can we say\nanything semi-formal about this alternative objective? \n\n* The improvements over ML are marginal, and there are a lot of moving\nparts/experimental settings in these models, i.e., a lot of\ntweaking. The results in tables 2 and 3 show a 0.36/0.34 improvement\nover ML using DSD. (btw, what is meant by \"DSD-deep\" or \"ML-deep\"? I'm\nnot sure these terms are explained?)\n\n* The comparison to related work is really lacking. The \"Attention is\nall you need\" paper (Vaswani et al.) reports 28.4/41.0 BLEU for these\ntest sets, respectively 3.4/5.96 BLEU points better than the results\nin this paper. That's a huge gap. It's not clear that the improvements\n(again, less than 0.5 BLEU points) will remain with a state-of-the-art\nsystem. And I think the paper is misleading in how it cites previous\nresults on these data sets - there is no indication in the paper that\nthese better results are in the literature.\n\nSome small things:\n\n* unplausible -> implausible\n\n* \"Husz (2015) showed that D(P || Q) is not identical to its inverse form\nD(Q || P)\" this is well known, predating 2015 for sure.\n",
            "This paper presents a new loss objective for NMT. The main idea is to optimize an interpolation of KL(P|Q) and KL(Q|P), which is Kulback-Liebler Divergence computed at the word-level for model distribution Q and true distribution P. The motivation is that KL(P|Q) finds a Q that covers all modes of the data whereas KL(Q|P) finds a Q that concentrates on a single mode. So optimizing on the interpolation gets the best of both worlds. In my opinion, this is a relatively simple and known idea in ML (but perhaps not in MT? I'm not sure.) On the other hand, the NMT experiments are well-implemented and convincingly shows that it improves BLEU on a WMT dataset. \n\nIn general, the experiments look solid. I applaud the multiple baseline implementations, in particular even including the SMT baseline. The lack of transformer/CNN models is not a demerit in my opinion, since the focus is on loss objectives and the LSTM models are just as reasonable. \n\nThe paper is clearly written, with a few exceptions. It is not clear why you have to first train with ML before switching to the proposed DSD objective. As such, Section 4.5 should be prefaced with a motivation. Also, Figure 3 is hard to read with the two kinds of plots -- maybe split into two figures? \n\nAn open question is: does your model capture the issues of mode covering as mentioned in the motivation? It would be helpful to include analyses of the word-level distributions to quantify the differences (e.g. word entropy) between ML and various KL/DSD solutions. Also I would recommend showing train/test set perplexity scores of the various proposed and baseline methods. \n\nAs a minor point for argumentation: it is not clear that your proposal addresses the sequence-level loss vs word-level loss issue. It is conceivable, but it seems indirect and there is no quantifiable connection between the word-level loss (such as DSD) and a sequence-level loss. Or is there? \n",
            "This paper describes an alternative training objective to cross-entropy loss for sequence-to-sequence models. The key observation is that cross-entropy is minimizing KL(P|Q) for a data distribution P and a model distribution Q; they add another loss that minimizes the inverse KL(Q|P) to create their dual-skew divergence. The idea is tested in the context of neural MT, using a model similar to that proposed by Bahdanau et al. (2015) with results on English-to-French and English-to-German WNT 2014. In the context of beam search, improvements are small (<=0.5 BLEU) but statistically significant.\n\nThis is an interesting idea, and one I certainly wouldn\u2019t have thought of on my own, but I think it is currently lacking sufficient experimental support to warrant publication. The paper feels strangely dated, with most experiments on two-layer models, and only two citations from 2017. The experiments compare against an in-house maximum likelihood baseline (varying greedy-vs-beam search and model depth), and against a number of alternative training methods (minimum risk, scheduled sampling, RL) with numbers lifted from various papers. These latter results are not useful, as the authors (helpfully) point out that the baseline results in this paper are universally higher than the baselines from these other papers. Furthermore, it feels like methods designed to address exposure bias and/or BLEU-perplexity mismatch are not the right comparison points for this work, as it does not attempt to address either of these issues. I would instead be much more interested to see a comparison to label smoothing (Szegedy et al., 2015), which perhaps addresses some of the same issues, and which produces roughly the same magnitude of improvements. Also, the literature review should likely be updated to include Edunov et al., 2017. In general, the improvements are small (though technically statistically significant), the baseline models are somewhat shallow and the deltas seem to be decreasing as model depth grows, so it is hard to get too excited.\n\nSmaller concerns:\n\nFor Table 1, it would be helpful to explain why Baseline is not equal to \\Beta=1. With some effort, I figured out that this was due to the alpha term modifying the cross-entropy objective when \\Beta=1.\n\nIt would also be useful to tell us what \u201cswitching point\u201d was used for Table 1 and Figure 2.\n\nChristian Szegedy, Vincent Vanhoucke, SergeyIoffe, Jonathon Shlens, and Zbigniew Wojna. 2015. Rethinking the inception architecture for computer vision. CoRR abs/1512.00567. http://arxiv.org/abs/1512.00567.\n\nSergey Edunov, Myle Ott, Michael Auli, David Grangier, and Marc\u2019Aurelio Ranzato. 2018. Classical structured prediction losses for sequence to sequence learning. In Proceedings of NAACL-HLT 2018."
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses reservations about the paper's motivation, the marginal improvements over existing methods, and the lack of comparison to state-of-the-art systems. The reviewer explicitly states they would not recommend acceptance.",
            "The review expresses overall positive sentiment, acknowledging the well-implemented experiments, solid results, and clear writing. The reviewer also 'applauds' the multiple baseline implementations.",
            "The reviewer expresses concerns about the paper's experimental support, datedness, and comparison methods. Phrases like 'lacking sufficient experimental support,' 'not useful,' 'hard to get too excited' indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses phrases like \"unconvincing,\" \"marginal improvements,\" \"comparison to related work is really lacking,\" and \"misleading.\" The reviewer also points out specific flaws in the paper, such as the lack of explanation for certain terms and inaccuracies in citing previous work.",
            "The review offers both praise and constructive criticism. It acknowledges the paper's strengths (well-implemented experiments, clear writing) while also pointing out areas for improvement (clarification of motivation, figure readability, addressing mode covering issues). The reviewer also poses questions to stimulate further exploration and justification.",
            "The review uses critical language to point out flaws in the paper's methodology, experimental setup, and literature review. Examples include 'The paper feels strangely dated,' 'These latter results are not useful,' 'it feels like methods...are not the right comparison points,' and suggestions for improvements are framed as shortcomings of the current work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer's points and reservations logically support the overall recommendation of rejection. The reviewer raises concerns about motivation, marginal improvements, comparison to state-of-the-art, and misleading presentation, all contributing to a negative assessment.",
            "The review is consistent because it provides both positive feedback (solid experiments, clear writing, good baselines) and constructive criticism (motivation for pre-training, figure clarity, deeper analysis of mode covering, connection to sequence-level loss) without any self-contradictory statements. The reviewer acknowledges the strengths while suggesting areas for improvement, maintaining a coherent and balanced perspective throughout the review.",
            "The review is consistent in its assessment. It acknowledges the interesting idea but consistently argues that the experimental support is insufficient for publication. The reviewer's points, such as concerns about dated experiments, weak baselines, irrelevant comparisons, and small improvements, all contribute to this central argument. There are no contradictory statements or conflicting opinions within the review."
        ]
    },
    {
        "paper_id": "nips_2022_Xg-yZos9qJQ",
        "paper_title": "Exploration via Elliptical Episodic Bonuses",
        "paper_abstract": "In recent years, a number of reinforcement learning (RL) methods have been pro- posed to explore complex environments which differ across episodes. In this work, we show that the effectiveness of these methods critically relies on a count-based episodic term in their exploration bonus. As a result, despite their success in relatively simple, noise-free settings, these methods fall short in more realistic scenarios where the state space is vast and prone to noise. To address this limitation, we introduce Exploration via Elliptical Episodic Bonuses (E3B), a new method which extends count-based episodic bonuses to continuous state spaces and encourages an agent to explore states that are diverse under a learned embed- ding within each episode. The embedding is learned using an inverse dynamics model in order to capture controllable aspects of the environment. Our method sets a new state-of-the-art across 16 challenging tasks from the MiniHack suite, without requiring task-specific inductive biases. E3B also outperforms existing methods in reward-free exploration on Habitat, demonstrating that it can scale to high-dimensional pixel-based observations and realistic environments.",
        "review_ids": [
            "SZV6S-r5yvL",
            "1M1wkPib_EH",
            "7zQHwAhkdAL",
            "xH0yBCZsiSH",
            "iy3rlnYOSV",
            "9Hzh34aJrNh",
            "Cmjqk5R2P6U"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I appreciate the authors for their reply. Here I would summarize my arguments with some updates:\n\n1. We agree that the elliptical bonus is suitable for continuous state space (let's put the issue of whether it is episodic aside for now), and naively using count-based bonus obviously does not make sense. I would restate that the elliptical bonus is widely studied both in theory and in practice, so I can not agree this is actually one of the contributions of the paper.\n\n2. Regarding the proposed episodic variant of the elliptical bonus, I would argue the example I mentioned above is actually just a very simple and general example without any specific adversarial construction, so I don't agree it is not representative and can be simply ignored. \n\n3. Regarding the author's claim on the importance of cmdp, I actually think this is the major source of our disagreement. The authors agree that in order for the episodic bonus to work, the problem has to enjoy a certain favorable structure. I think this is why previous papers such as [1] claim that their episodic reward is for \"procedurally generated environments\" instead of claiming to solve cmdp, and there seems no addressing of this limitation in the revised paper. \n\n4. I would like to bring up an issue of the ablation study that the authors mention. In the ablation study, the authors compare the episodic version and non-episodic version of their constructed bonus. However, according to the efficient computation of the covariance matrix that the authors propose in the paper, if they compute the non-episodic bonus in this way, the covariance matrix is aggravated by all previously learned features, which does not make sense anymore because one should use the latest feature and use all previous samples to recompute the covariance matrix from scratch. Please correct me if I understand the construction wrong.\n\nIn summary, I agree, as I mentioned in the previous threads, that the paper shows good empirical results. However, again, the novelty and contribution of the proposed method, compared with previous (learning feature + elliptical bonuses) methods, seems only the episodic version of the bonus for cmdp. The contribution seems limited to me, and I am not convinced episodic bonus is the solution to cmdp. Other factors (such as the changing feature as I brought up earlier) may also result in a boost in performance and I believe a more careful empirical analysis is required. I argue that regardless of the nature of the paper, one algorithm should match its claim at least on an intuition level but I am not convinced in the current form of the paper. Finally, I acknowledge that the wording of my current rating is overly harsh: thus I increased my rating by 1 but I can not assign a score higher than this because that would contradict my overall evaluation of the paper. \n\n[1] Ride: Rewarding impact-driven exploration for procedurally-generated environments.",
            " First I would like to thank the authors for their detailed response and also apologize for my late reply and certain overlooks in the original review (for example, the random network baselines). I would address some major issues in this followup:\n\n1. First I want to further discuss the contextual mdp setup. I am aware of the setup in my original review but I would provide some additional clarification since it causes some confusion during the discussion. I would argue that we should consider CMDP as a broader class of MDP, because for example, we could consider a singleton mdp as a contextual mdp whose context distribution only has one non-zero point-mass. I appreciate the authors' acknowledgment that my previous maze counter-example would make sense in the singleton mdp case, but we can easily extend it to cmdp case because we can assume everytime the dynamics are nearly identical each iteration (and deterministic), and the goal is sampled in some state that the policy never visited before, then I believe the counterexample should still hold. \n\n2. However, the above argument could be treated as an argument against episodic bonus overall. Here I thank the reviewer for the pointer to section 3 which shows that the episodic bonus is indeed helpful even for other algorithms. Thus it indeed seems like episodic bonus is helpful for practical cmdp problems that enjoy certain structures. \n\n3. I appreciate the authors for acknowledging that counting-based bonuses are ill-suited for large/continuous state spaces. I understand the motivation of section 3 for showing the failure of count-based methods, but I still can not appreciate the way the current motivation is formulated because the ineffectiveness of counting based bonuses for large/continuous state spaces is just too obvious. In addition, if we want to talk about small state/action space, I would also like to point out that **elliptical potential bonuses reduce to count-based bonuses in tabular case.**\n\n4. For the positive side, I am very impressed by the new experiments because the habitat benchmark seems a challenging environment.\n\n5. For a side note, one thing we have been ignoring is that the algorithm is *learning* the feature and the bonuses are constructed by the learning feature. Although again previous algorithms have been studying learning feature + elliptical bonuses, the practical advantage of learning such features on the fly seems still unclear. For example, if we use *ground-truth* feature + elliptical episodic bonuses in the above maze counterexample, it does not seem to work. However, using a shifting feature (even though it is incorrect!) seem to break the deadlock (even though it's not clear to me if it is in a good way or a bad way).\n\nIn summary, I would acknowledge that my major critique still remains, but mostly to the episodic bonuses. However, the most major difference between this work and previous (learning feature + elliptical bonuses) methods is the reset of the covariance matrix (thus episodic bonus), which as I argued above, remains not obvious why it would work without nice context generation property. While I highly appreciate the new experiment result, I would still like to keep my original evaluations.",
            " Thanks to the authors for addressing the concerns and updating the paper. I will stick with the current score. This is a nice paper.",
            " I would like to thank the authors for addressing the concerns that were raised, and actively improving the paper during the rebuttal phase. Considering the changes to Section 5.2, and the detailed clarifications, I have updated my score.",
            " The paper proposes a method that alternates between learning the feature mapping via the inverse dynamics model and using the learned feature to construct the elliptical bonus for exploration. The paper also analyzes several heuristic exploration bonus method by removing the count-based component in them and shows that they fail under such modification. The paper claims that the proposed algorithm contributes the most to the contextual markov decision processes (CMDP) setting, and shows its empirical competitiveness in the MiniHack benchmark.  ## Strength\n\n1. The proposed method, E3B, when evaluated in the MiniHack benchmark, which is a reasonable benchmark for CMDP, shows better performance than the previous empirical methods with heuristic exploration bonuses. The evaluation also contains some variants of the baselines to provide more insights into the results. \n\n2. The paper is easy to follow with several informative visualizations. \n\n## Weakness\n\n1. It is very surprising to see that the proposed algorithm, which resets the empirical covariance matrix at the beginning of the episode (according to Algorithm.1), can actually work. First, this should not work in theory, where your covariance matrix $C$ is constructed using the policy cover defined on all previous iterations. Intuitively, it is also not very clear why this can work effectively. Resetting $C$ at the beginning of the episode means the bonus construction has no memory of what states have already been visited before. Combined with that the algorithm performs policy optimization based on reward + bonus, consider the setting if we have sparse reward and short horizon: for example, if one wants to escape a maze in small time budget, the algorithm could first visit some dead end, but receives high bonuses due to resetting $C$, and update to visit the dead end with higher probability after the PG update, and visit the dead end again but still receives high bonus, and thus eventually stuck?\n\n2. The contribution of this work does not seem obvious, either. Using elliptical bonus in Deep RL or with neural network function approximation has already studied by some previous work. For example, PC-PG [1] (which this paper also mentions) actually uses PPO+elliptical bonus in their experiment. [2] even has a specific Deep RL version that uses deep RL + elliptical bonus and is evaluated in the deep RL benchmarks. If we also consider the representation learning part, [3] provides both theoretical and empirical results with neural network function approximation. In addition, [4] also justifies that a variant of [2] is actually performing feature learning in a noisy system. There seems no comparison against any of these works in this paper, given that the approaches are similar and the above baselines actually also show strong performance in practice.\n\n### references\n\n[1] Agarwal, Alekh, et al. \"Pc-pg: Policy cover directed exploration for provable policy gradient learning.\" Advances in neural information processing systems 33 (2020): 13399-13412.\n\n[2] Song, Yuda, and Wen Sun. \"Pc-mlp: Model-based reinforcement learning with policy cover guided exploration.\" International Conference on Machine Learning. PMLR, 2021.\n\n[3] Xu, Pan, et al. \"Neural contextual bandits with deep representation and shallow exploration.\" arXiv preprint arXiv:2012.01780 (2020).\n\n[4] Ren, Tongzheng, et al. \"A free lunch from the noise: Provable and practical exploration for representation learning.\" arXiv preprint arXiv:2111.11485 (2021). 1. In figure 1, why are the baselines not performing well even with their original constructions (with counts)?\n\n2. The motivation against using counting based method seems not very fair? \"if each state is unique\" (line 102) almost implies that the state space is continuous, then one obviously should never use naive count-based bonus, since count-based bonus only works for tabular MDP. \n\n3. In the feature learning part, it is known that the inverse dynamics model is valid only if it is conditioned on some conditional distributions of actions (for example, policies). In the proposed algorithm, is the conditioned policy being a mixture of all previous policies? Would it providing conflicting information, for example, at some state policy a goes left and policy b goes right? Since the algorithm aims to perform exploration.\n\n4. In result shown in Fig.6, is there any hypothesis why RND completely fails on all tasks?\n\n5. The ablation shown in section 5.1.3 is not using very strong baselines. For example, in [1,2,4], those method are using RFF kernels and random networks like RND as feature mappings, which show convincing empirical performance. \n\n6. In Fig.8, is there any possible explanation for why E3B is worse than the baselines in some task?  1. As mentioned in the previous section, resetting the covariance matrix seems a limitation of the proposed work. \n\n2. The results shown in Section 5.2 shows that the proposed algorithm achieves very limited improvement or even no improvement than previous algorithms on regular MDPs with large state space.  ",
            " The paper gives evidence of the importance of the pseudo count quantity used in existing exploration algorithms in RL, and proposes a new algorithm to extend the pseudo count idea to continuous state spaces by also proposing a method to learn a feature encoder. They give this evidence through various empirical experiments.\n\nThanks to the authors for putting in the effort in doing this work! Strengths:\n- I appreciate works that take a hard look at existing methods in the manner that is done in this paper and try to understand the fundamental flaws.\n- I like table 1 of distilling the exploration methods to their essence, and also I like figure 3.\n- The time-counter example of how states can appear unique is a nice one for intuition.\n- The paper is generally well-written and easy to follow.\n\nWeaknesses:\n- I view the paper as trying to solve two independent problems, but the paper makes it seem they are intrinsically tied together. The two problems are: 1) learning a feature encoder and 2) exploration. While the paper does include an ablation study showing how important both these parts are, it would be nice to get a better understanding of these two independent components. For example, how does using the learned encoder improve other existing count-based methods, or how does using off-the-shelf feature encoders affect the performance of the elliptical exploration algorithm?\n Questions/Suggestions:\n- How are differences between contexts defined? I see that there is a distribution over contexts but how extreme can differences between two contexts be? It doesn't appear to be the case here, but generally does the reward function change between contexts too?\nHow many samples are actually needed for fitting in the ellipse? \n- The paper also notes that \u201cany feature learning method\u201d could be used, but it proposes to use the inverse dynamics model. I am curious how off-the-shelf feature extractors perform with the elliptical exploration algorithm? It seems like this problem is an independent problem itself, and it would make sense to leverage existing works instead of re-inventing something new.\n- How sensitive are results to the regularization term in Equation (1)? In my experience, I have found this term to be sensitive.\n- I know the goal isn't explicitly lifelong learning, but I am curious what the authors think about getting E3B (non-episodic) from Figure 7 to work in that setting? In a lifelong setting where the agent is constantly interacting with no clear start and end to different contexts, I wonder how the proposed method would work then and how it compares to existing methods?\n- So we can see a clear performance improvement in Figure 6. But I am curious why ICM performs similarly to E3B on the high-dimensional Vizdoom domain, especially given that the motivation was current methods don\u2019t fare well in high dimensional/continuous state spaces?\n- How will the count-based methods in Table 1 improve or change if you use the proposed feature encoder (inverse dynamics model) as input into these existing exploration methods instead of using the proposed elliptical count-based algorithm?\n Yes, they have.",
            " This work highlights an important weakness for existing exploration methods relying on intrinsic motivation in CMDPS, namely their over reliance on a count-based episodic term. Since this count-based term is computed on hand-engineered features, or on the entire observation, such methods underperform in noisy or more realistic environments. The authors propose to replace this term with an elliptical episodic bonus, computed on learned features extracted by inverse dynamic modeling. This simple modification significantly improves the performance of intrinsically motivated agents in complex environment which differ across episodes. This is shown through experiments on the MiniHack suite, and potential for scaling to image-based tasks is presented on VizDoom. The authors also report some interesting nuances in environments, and clarify the importance of heuristic choices in existing methods. **Strengths**\n* The claims made through the paper are modest, but well argued and sufficiently supported by experimental results.\n* The paper is well written and easy to follow. The motivating weakness of existing methods is outlined well, and the idea of elliptical bonuses is clearly presented, and motivated from different point of views.\n* The experimental section is convincing: the metrics reported follow a statistically principled evaluation approach, baselines are satisfactory and their behavior is well explained.\n\n**Weaknesses**\n* While the motivation and the intuition of the method are clear, the main body of the paper does not explicitly describe the algorithm itself, which is relegated to the Appendix. As a result, this part of the paper is lacking in clarity.\n* The method is not particularly original, as it can be seen as an incremental generalization of count-based exploration bonuses to continuous settings. **Main comments**\n* The method's clarity can be improved. While the computation of the elliptical bonus is clear, its integration in the reward signal is not. This should be clearly discussed in the main body of the paper. Furthermore, to the best of my undestanding, E3B's reward only includes an episodic term, while RIDE, AGAC and NovelD also include a global term. While the lack of robustness in the episodic term is understandable, it is not clear why E3B also removes the global term.\n* I would suggest author to ablate the method by replacing the elliptical terms with a hash-based count term [1], which can be computed over the representations learned through inverse modeling.\n* The choice of MiniHack tasks is not well motivated. I am not fully informed on common evaluation protocols on MiniHack, but to my understanding the authors performed a selection of tasks. The reasoning behind this selection should be motivated, particularly in relation to prior works.\n* Figure 6 shows how RIDE does not achieve any reward signal in any of the tasks considered. Can the authors provide an intuitive explaination for this result?\n\n**Minor comments**\n* line 50: Prior works on elliptical bonuses should be cited before the Related Works section on the last page. I would suggest to add them here, in order to clarify that this idea is not novel.\n* lines 83-85: I would recommend adding references to NovelD and AGAC to better assist the reader.\n* Figure 1: What is the behavior of each agent when disabling the count-based episodic bonus? Is their explorative behavior less extended, or do they fail to move at all?\n* line 123: This experiment is only reported for NovelD, but the reasoning for this choice is only provided on line 243. I would recommend to move this explanation here.\n* line 176: Why is $n$ not fixed across all experiments? How is its value selected?\n\n**References**\n\n[1] Haoran et al. \"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning.\", 2017 The authors do not overstate their contribution, and the limitations of their work are properly presented."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses multiple disagreements with the authors regarding the novelty and contributions of the paper. Phrases like 'I can not agree,' 'I don't agree,' 'I am not convinced,' and 'contribution seems limited to me' indicate a negative sentiment. Although the reviewer increased the rating, they explicitly state it is still limited due to their overall evaluation.",
            "The review expresses both positive and negative feedback. It acknowledges the authors' response and appreciates the new experiments, but also maintains the original critique regarding the episodic bonuses and the motivation behind certain aspects of the work. The overall tone is balanced, pointing out both strengths and weaknesses.",
            "The reviewer explicitly states \"This is a nice paper\" and thanks the authors, indicating a positive sentiment.",
            "The reviewer expresses gratitude for the authors' efforts in addressing concerns and improving the paper. The statement \"I have updated my score\" implies a positive change in evaluation.",
            "The review expresses significant concerns about the paper's theoretical soundness, novelty, and experimental setup. The reviewer questions the algorithm's effectiveness due to the covariance matrix reset, points out the lack of comparison against relevant prior work, and raises several issues with the experimental design and motivation.",
            "The review expresses appreciation for the authors' effort and highlights several strengths of the paper, such as its insightful analysis of existing methods, the helpful distillation of exploration methods in Table 1, and the clarity of writing.",
            "The review acknowledges the paper's strengths, such as well-argued claims, clear writing, and convincing experimental results. While it points out weaknesses and suggests improvements, the overall tone indicates a positive assessment of the work's value and potential."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Supportive",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review adopts a critical tone by directly challenging the authors' claims and methodology. Phrases like 'does not make sense anymore,' 'seems only the episodic version,' 'contribution seems limited,' and 'I am not convinced' demonstrate a critical evaluation of the work. The reviewer also points out potential flaws in the ablation study and raises concerns about the paper's claims.",
            "The review uses phrases like \"I appreciate,\" \"I am very impressed,\" but also \"I still cannot appreciate,\" \"remains not obvious why it would work.\" This mix of positive and critical language indicates a balanced tone.",
            "The reviewer uses phrases like \"Thanks to the authors for addressing the concerns\" and \"I will stick with the current score\" suggesting a supportive and appreciative tone.",
            "The reviewer uses phrases like \"thank the authors,\" \"addressing the concerns,\" \"actively improving the paper,\" and \"detailed clarifications,\" which convey a supportive and encouraging tone.",
            "The review uses phrases like \"very surprising to see that the proposed algorithm... can actually work,\" \"the contribution of this work does not seem obvious,\" and \"the motivation against using counting based method seems not very fair?\" These phrases convey a skeptical and critical perspective.",
            "The review presents both strengths and weaknesses of the paper, along with specific questions and suggestions for improvement, indicating a balanced assessment. The reviewer uses phrases like 'I appreciate' and 'I like' for strengths, but also points out perceived issues with the paper's structure and potential areas for further investigation.",
            "The review presents both strengths and weaknesses of the paper, offering constructive criticism and suggestions for improvement while also acknowledging the paper's merits. It maintains a formal and objective tone throughout."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer consistently argues that the novelty and contribution of the episodic bonus are limited, especially in the context of CMDP.  They question the claim that the episodic bonus is a solution to CMDP and suggest that other factors might be contributing to the observed performance. The reviewer's points are logically connected and support their overall evaluation of the paper's contribution as limited, even while acknowledging good empirical results and increasing the rating slightly.",
            "The reviewer acknowledges the authors' response and new experiments, appreciating the practical benefits of episodic bonuses shown in the paper. However, they consistently maintain their original critique regarding the theoretical limitations of episodic bonuses, particularly concerning their effectiveness without specific context generation properties. The reviewer's points are nuanced and represent a consistent perspective of acknowledging practical improvements while still holding onto theoretical reservations.",
            "The reviewer expresses positive feedback about the paper and the authors' revisions while maintaining their original score, indicating a consistent evaluation.",
            "The review is consistent as it expresses a positive change in opinion based on the authors' actions and provides specific examples of improvements, such as changes to Section 5.2 and detailed clarifications.",
            "The review is consistent in its critique. It acknowledges the empirical strengths of the paper, specifically the performance on the MiniHack benchmark, but raises significant concerns about the theoretical justification of the method (covariance matrix reset), the novelty of the approach compared to existing literature, and the limited improvement in more general MDP settings. The weaknesses and questions raised are all aligned with these central concerns, creating a consistent critical evaluation of the paper's contributions and methodology.",
            "The review is consistently positive and constructive. The reviewer appreciates the strengths of the paper, highlighting specific aspects they liked, such as the analysis of existing methods, Table 1, Figure 3, and the time-counter example. While pointing out weaknesses, the reviewer focuses on suggesting improvements and further investigations rather than contradicting their initial positive assessment. The questions and suggestions are aimed at clarifying and strengthening the paper, not undermining its value.",
            "The review is consistent in its assessment. It highlights both strengths and weaknesses of the paper without any self-contradiction. The reviewer appreciates the clarity of motivation, writing, and experimental design, while pointing out areas for improvement such as algorithm description clarity in the main body, originality, and some experimental details. The feedback is constructive and aimed at improving the paper."
        ]
    },
    {
        "paper_id": "iclr_2019_SJMO2iCct7",
        "paper_title": "A NOVEL VARIATIONAL FAMILY FOR HIDDEN NON-LINEAR MARKOV MODELS",
        "paper_abstract": "Latent variable models have been widely applied for the analysis and visualization of large datasets. In the case of sequential data, closed-form inference is possible when the transition and observation functions are linear. However, approximate inference techniques are usually necessary when dealing with nonlinear evolution and observations. Here, we propose a novel variational inference framework for the explicit modeling of time series, Variational Inference for Nonlinear Dynamics (VIND), that is able to uncover nonlinear observation and latent dynamics from sequential data. The framework includes a structured approximate posterior, and an algorithm that relies on the fixed-point iteration method to find the best estimate for latent trajectories. We apply the method to several datasets and show that it is able to accurately infer the underlying dynamics of these systems, in some cases substantially outperforming state-of-the-art methods.",
        "review_ids": [
            "rJgBQzFjnX",
            "Bygvx6sc2X",
            "rJe98VxNhm"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper discusses a algorithm for variational inference of a non-linear dynamical models. In this paper model assumption is to use single stage Markov model in latent space with every latent variable Z_t to be defined Gaussian distributed with mean depends on Z_(t-1) and time invariant variance matrix lambda. The non linearity in transition is encoded in mean of Guassian distribution. For modeling the likelihood and observation model, the Poisson or Normal distribution are used with X_t being sampled from another Gaussian or Poisson distribution with the non-linearty being encoded in the parameters of distribution with variable Z_t.  This way of modeling resembles so of many linear dynamical model with the difference of transition and observation distribution have nonlinearity term encoded in them. \nThe contribution of this paper can be summarized over following points:\n\n- The authors proposed the nonlinear transition and observation model and introduced a tractable inference model using Laplace approximation in which for every given set of model parameter solves for parameters of Laplace approximation of posteriori and then model parameters get updated until converges\n\n-the second point is to show how this model is successful to capture the non-linearity of the data while other linear models do not have that capabilities \n\n\nNovelty and Quality: \nThe main contribution of this paper is summarized above. The paper do not contain any significant theorem or mathematical claims, except derivation steps for finding Laplace approximation of the posteriori. The main challenge here is to address effectiveness of this model in comparison to other non-linear dynamical system that we can name papers as early as Ghahramani, Zoubin, and Sam T. Roweis. \"Learning nonlinear dynamical systems using an EM algorithm.\"\u00a0Advances in neural information processing systems. 1999. \nor more recent RNN paper LSTM based papers. I think authors need to distinguish what this paper can give to community beside approximate posteriori of latent variables that other competing models are not capable of. If the aim is to have that posteriori, the authors should show what type of interpretation they have drawn from that in experiments.\nThere are lots of literature exist on speech, language models and visual prediction which can be used as reference as well.\n\nClarity: \nThe paper is well written and some previous relevant methods have been reviewed . There are a few issues that are listed below: \n\n1- as mentioned in Quality sections authors should be more clear about what is distinguished in this paper that other non-linear dynamical systems \n\n2- they used short form RM for Recognition model or FPI for fixed point iteration that need need to be defined before being used\n\n\n\nsignificance and experiments: \nThe experiments are extensive and authors have compared their algorithm with some other linear dynamical systems (LDS) competing algorithms and showed improvement in many of the cases for trajectory reconstruction. \nA few points can be addressed better, it can be seen for many of experiments exhaustive search is used for finding dimension of latent variable. This issue is addressed in Kalantari, Rahi, Joydeep Ghosh, and Mingyuan Zhou. \"Nonparametric Bayesian sparse graph linear dynamical systems.\"\u00a0arXiv preprint arXiv:1802.07434\u00a0(2018). That paper can use non-parametric approaches to find best latent dimension, although the paper applied the technique on linear system, same technique could be adopted to non-linear models. Also that model is capable of finding multiple linear system that model the non linearity by switching between diffrent linear system, for switching linear system, this paper can be named as well: Linderman, Scott, et al. \"Bayesian learning and inference in recurrent switching linear dynamical systems.\"\u00a0Artificial Intelligence and Statistics. 2017.\n\nIt is shown that the model can reconstruct the spikes very well while linear model do not have that power (which is expected), but it is interesting to see how other non-linear models would compare to this model under those certain conditions\n\nIt is desired and interesting to see how the model behave one step ahead and K-step ahead prediction. Please address why it cannot be done if there is difficulties in that.",
            "I'll start with a disclaimer: I have reviewed the NIPS 2019 submission of this paper which was eventually rejected. Compared to the NIPS version, this manuscript had significantly improved in its completeness. However, the writing still can be improved for rigor, consistency, typos, completeness, and readability.\n\nAuthors propose a novel variational inference method for a locally linear latent dynamical system. The key innovation is in using a structured \"parent distribution\" that can share the nonlinear dynamics operator in the generative model making it more powerful compared. However, this parent distribution is not usable, since it's an intractable variational posterior. Normally, this will prevent variational inference, but the authors take another step by using Laplace approximation to build a \"child distribution\" with a multivariate gaussian form. During the inference, the child distribution is used, but the parameters of the parent distribution can still be updated through the entropy term in the stochastic ELBO and the Laplace approximation. They use a clever trick to formulate the usual optimization in the Laplace approximation as a fixed point update rule and take one fixed point update per ADAM gradient step on the ELBO. This allows the gradient to flow through the Laplace approximation.\n\nSome of the results are very impressive, and some are harder to evaluate due to lack of proper comparison. For all examples, the forward interpolate (really forecasting with smoothed initial condition) provides a lot of information. However, it would be nice to see actual simulations from the learned LLDS for a longer period of time. For example, is the shape of the action potential accurate in the single cell example? (it should be since the 2 ms predictive r^2 shows around 80%).\n\nExcept in Fig 2, the 3 other examples are only compared against GfLDS. Since GfLDS involves nonconvex optimization, it would be reasonable to also request a simple LDS as a baseline to make sure it's not an issue of GfLDS fitting.\n\nFor the r^2=0.49 claim on the left to right brain prediction, how does a baseline FA or CCA model perform?\n\nWas input current ignored in the single cell voltage data? Or you somehow included the input current as observation model?\n\nAs for the comment on Gaussian VIND performing better on explaining variance of the data even though it was actually count data, I think this maybe because you are measuring squared error. If you measured point process likelihood or pseudo-r^2 instead, Poisson VIND may outperform. Both your forecasting and the supplementary results figure show that Poisson VIND is definitely doing much better! (What was the sampling rate of the Guo et al data?)\n\nThe supplementary material is essential for this paper. The main text is not sufficient to understand the method.\n\nThis method relies on the fixed point update rule operating in a contractive regime. Authors mention in the appendix that this can be *guaranteed* throughout training by appropriate choices of hyperparameters and network architecture. This seems to be a crucial detail but is not described!!! Please add this information.\n\nThere's a trial index suddenly appearing in Algorithm 1 that is not mentioned anywhere else.\n\nIs the ADAM gradient descent in Algorithm 1 just one step or multiple?\n\nMSE -> MSE_k in eq 13\n\nLFADS transition function is not deterministic. (page 4)\n\nlog Q_{phi,varphi} is quadratic in Z for the LLDS case. Text shouldn't be 'includes terms quadratic in Z' (misleading).\n\nregular gradient ascent update --> need reference (page 4)\n\nDue to the laplace approximation step, you don't need to infer the normalization term of the parent distribution. This is not described in the methods (page 3).\n\nEq 4 and 5 are inconsistent in notation.\n\nEq (1-6) are not novel but text suggests that it is.\n\nPredict*ive* mean square error (page 2)\n\nIntroduction can use some rewriting.\n\narXiv papers need better citation formatting.",
            "The paper presents a variational inference approach for locally linear dynamical models. In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,  enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO. Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.  \n\nQuality: The experiments appear to be well designed and support the main claims of the paper. \n\nClarity: The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately. It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way. I also struggled a little to understand what is the difference between forward interpolate and filtering. \n\nOriginality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted. In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.  \n\nSignificance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics. \n\nOverall, this appears to be a board-line paper with weak novelty. On the positive side, the experimental validation seems well done. The clarity of this paper needs to be strengthened.  \n\nMinor comments: \n- abstract: uncover nonlinear observation? -> maybe change \"observation\" to \"latent dynamics\"?\n\n"
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Neutral"
        ],
        "sentiment_reason": [
            "The review provides both positive feedback (e.g., \"The paper is well written\", \"The experiments are extensive\") and negative criticism (e.g., \"The paper do not contain any significant theorem or mathematical claims\", \"authors should be more clear about what is distinguished in this paper\"). The reviewer also offers suggestions for improvement, indicating a balanced perspective rather than outright negativity or positivity.",
            "The review contains numerous criticisms regarding the writing clarity, completeness, and rigor of the paper. It points out missing details, inconsistencies, and lack of proper comparisons, indicating a negative assessment of the paper's quality.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges well-designed experiments and potential applicability but criticizes clarity and novelty, resulting in a balanced, neutral overall sentiment."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The review adopts a balanced tone by offering constructive criticism alongside positive remarks. Phrases like \"The paper is well written\" show appreciation, while statements such as \"authors should be more clear about what is distinguished in this paper\" and suggestions for improvement maintain a critical yet helpful stance. The reviewer also uses neutral language to describe the experiments and results.",
            "The review uses direct and critical language, such as \"the writing still can be improved,\" \"harder to evaluate due to lack of proper comparison,\" \"the main text is not sufficient to understand the method,\" and numerous specific points of correction and questioning of the methodology. The reviewer also uses phrases like \"crucial detail but is not described!!!\" which conveys a strong critical tone.",
            "The review presents both positive aspects (well-designed experiments, potential applicability) and negative aspects (lack of clarity, weak novelty) of the paper in a relatively objective manner. Phrases like \"On the positive side...\" and criticisms are presented with specific justifications, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it maintains a focused critique throughout all sections, primarily questioning the novelty and significance of the paper in relation to existing non-linear dynamical systems literature. The reviewer consistently asks for clarification on the paper's unique contribution and suggests improvements to strengthen its positioning within the field.",
            "The review maintains a consistent critical but constructive tone throughout. It identifies areas for improvement in the manuscript, ranging from high-level concerns about experimental validation and completeness to low-level details like typos and equation inconsistencies. The reviewer provides specific examples and suggestions without contradicting themselves.",
            "The review is consistent in its assessment. It highlights both the strengths of the paper, such as well-designed experiments and potential applicability, and weaknesses, such as clarity and novelty. The reviewer consistently points out these aspects throughout the review without contradicting themselves. The overall assessment of a 'board-line paper with weak novelty' aligns with the individual comments on originality and clarity versus the positive comments on experiments and significance."
        ]
    },
    {
        "paper_id": "iclr_2021__ojjh-QFiFr",
        "paper_title": "Language-Mediated, Object-Centric Representation Learning",
        "paper_abstract": "We present Language-mediated, Object-centric Representation Learning (LORL), learning disentangled, object-centric scene representations from vision and language. LORL builds upon recent advances in unsupervised object segmentation, notably MONet and Slot Attention. Just like these algorithms, LORL also learns an object-centric representation by reconstructing the input image. But LORL further learns to associate the learned representations to concepts, i.e., words for object categories, properties, and spatial relationships, from language input. These object-centric concepts derived from language facilitate the learning of object-centric representations. LORL can be integrated with various unsupervised segmentation algorithms that are language-agnostic. Experiments show that LORL consistently improves the performance of MONet and Slot Attention on two datasets via the help of language. We also show that concepts learned by LORL aid downstream tasks such as referential expression interpretation.",
        "review_ids": [
            "hfib2bHPXhV",
            "1wdLnWl8bz",
            "-usPlmaWfw",
            "Zz05effWqk",
            "_x506rwOLvD"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "### Summary\n\nThis paper proposes to combine the neuro-symbolic concept learner for visual reasoning from language (NS-CL; Mao et al., 2019) with recent unsupervised approaches to learning object-centric representations such as MONet (Burgess et al., 2019) and Slot-Attention (Locatello et al., 2020). While NS-CL normally relies on pre-trained object-detectors (in a supervised fashion) to extract visual representations, the proposed combination (dubbed LORL) use MONet or Slot Attention for this. By additionally back-propagating error signals from language-driven visual reasoning tasks obtained via NS-CL into MONet/Slot-Attention, it is shown how LORL is better able at learning object-centric representations and perform instance segmentation. \n\n### Pro\u2019s / Con\u2019s / Justification\n\nOverall the paper is well written and easy to follow. Moreover, using other modalities such as language to improve visual perception is an interesting research direction and timely due to recent advances in object-centric representation learning\n\nHowever, the significance of this contribution is rather limited due to two main reasons:\n\nLORL is a very straightforward combination of MONet/Slot-Attention with NS-CL and therefore not very novel --- the encoder part of NS-CL is simply replaced by the MONet/Slot-Attention, while everything else remains pretty much the same. Note that I don\u2019t consider the \u201cobjectness scores\u201d to modulate the NS-CL reasoning process very novel, since it is essentially a straight-forward heuristic to cope with a limitation of MONet/Slot-Attention in that sometimes the object-centric representations may containing background information or are \u201cempty\u201d. Were object-centric representations correctly inferred (as in the original NS-CL), this heuristic would therefore also not be needed \n\nThe results are not very surprising: it is found that better object-centric representations can be learned by fine-tuning on a visual reasoning task that uses language. However, since this task is learned in a supervised fashion and the dataset contains questions of the form \u201cwhat is the name of the white object?\u201d (parsed by a pre-trained semantic parser using a custom DSL), this provides a substantial degree of supervision to the representation learning part. In general, it is then not very surprising that using supervised data for fine-tuning improves representation learning, which limits the significance of this contribution further. \n\nDue to these reasons, I can only recommend rejection at this point in time. \n\n### Potential Improvements \n\nI don\u2019t think that it is straightforward to improve the current submission, although I have some detailed comments that I would like to see addressed below.\n\nMore generally I would encourage the authors to see if visual reasoning _alone_ (i.e. running solely the fine-tuning stage, but without the perception loss) would cause object-centric representations to emerge. Firstly, in that case LORL would not have to rely so much on existing contributions from prior approaches like MONet/Slot-Attention (a perceptual loss for which we know it already yields reasonably good object-centric representations). More importantly, in this case it is less obvious that a supervised language-based approach would work, since there is no guaranteed separation into approximate object-representations due to using a perception loss. This would almost certainly make the contribution more significant.\n\nIt would also be interesting to analyze in what situations fine-tuning on VQA is helpful and whether there are certain types of questions that are particularly helpful in this case. I could image that questions focusing on individual object properties are more useful than those that focus on more global information (like the total number of ceramic objects). More generally it would be interesting to quantify the amount of supervision given to the representation learning and comparing this to the amount of improvement gained.  \n\n### Detailed comments\n\n* Regarding the objectness score an ablation is missing. What is the effect of this when learning object-centric representations? I expect it to be marginal, but it would still be good to demonstrate this. It would also be good to add the reasoning accuracy when the objectness modulation is not used to Table 4.\n\n* Similarly, is it correct that the baseline approach in Tables 1a and 1b includes the objectness score, and is thus used for the proceeding training stages to arrive at the results for LORL? If not then please add to these tables the performance of LORL after the perceptual training phase (and if yes, please add a baseline that does not include the objectness score as per my previous comment)\n\n* The standard deviation in Tables 1b and 2 is generally quite high, which makes it difficult to compare in some cases (although I agree that LORL generally outperforms the purely perceptual approach as is also expected). Therefore I would ask that you add additional seeds at least for Table 2 to allow for a better comparison.\n\n* The proposed metrics (GT Split and Pred Split) seem rather arbitrary and I don\u2019t think provide much additional insight. The ARI score is consistently in favor of LORL, and although the magnitude of the difference when comparing is sometimes greater when using GT Split and Pred Split it is unclear to me how to interpret that magnitude. More generally, the choice of assigning a mask to an object if it claims at least 20% currently seems arbitrary, while this could be an important hyper-parameter for these losses. For example, what do the results look like when using a threshold of 10% or 30%?\n\n* I don\u2019t find the results in Table 3 very surprising, since isn\u2019t this what LORL is exactly trained for given the kind of questions used for the fine-tuning stage? What else could explain the improved ARI and the results in Table 4 other than that the representations have become better for semantic retrieval? Perhaps I am missing some alternative interpretations.\n\n* I would appreciate a comparison to NS-CT in Table 4, even though the latter is supervised. It would help understand how good the score obtained by LORL-SA is. \n\n* Section 5.4 essentially feels like an after-thought and details are missing. It would be helpful if the baseline IEP-Ref approach could be explained in some more detail. \n\n* Regarding the conclusion, I don\u2019t see why LORL is a \u201cprincipled framework\u201d. What is the principle that is being applied here? And why is this desirable over other approaches?\n\n* I noticed in Appendix B that on PartNet-Chairs the second training phase (where only the reasoning part is trained) is skipped. What is the reason for this? What happens if this is also done for the Shop-VRB-Simple dataset? \n\n### Post Rebuttal\n\nI have read the other reviews and the rebuttal. I appreciate the extensive revision and response of the authors. Indeed, several of the minor issues/clarifications that I had raised have now been addressed.\n\nHowever, as noted in my initial review, my main concern with this work is the highly limited novelty and the significance of the findings:\n\n* LORL is essentially a simple application of unsupervised object-centric representation learners (like MONet, Slot-attention) to the language-guided visual reasoning framework proposed in MAO et al (2019), where the pre-trained vision module is interchanged. As I have previously argued, the objectness score is a heuristic only needed to overcome a limitation of the considered vision modules, which is not very interesting. Indeed, this score is not needed for segmentation (which is the primary measure of success that the authors have adopted).\n* The main finding, which is that providing some degree of supervision to purely unsupervised object-centric representation learners improves their performance, is not very surprising. Although one could argue that the visual reasoning task does not provide direct supervision on the object segmentations, the considered type of questions (and DSL) provide a sizable amount of supervision I believe (as is also evident from the observed fluctuations in Table 9).\n\nOne issue that I noticed in the revision is when comparing the results in Table 9 and Table 10. It can be seen how when training on the visual reasoning task using only 25% of the provided data (i.e. 22.5K as opposed to 90K) actually reduces segmentation performance, i.e. from 83.51 (image only) to 81.01. This is surprising, and perhaps somewhat concerning, since I would have expected any reasonable amount of supervision to be helpful and certainly not degrade performance. The authors do not provide an explanation for this behavior, while it appears to invalidate the main claim regarding the benefit of LORL in the general case.\n\nFor these reasons I remain in favor of a rejection.",
            "Focus of the work: The paper tries to tackle the problem of learning object centric representations using language. The authors note that most of the previous work which tries to use language  assume pre-trained object detectors to generate object proposals in the visual image. \n\nMethodology: The proposed method consists of 4 modules. \n1. Object Encoder, which tries to generate a modular representation of the scene. \n2. Object Decoder, a module to reconstruct masks corresponding to individual objects given the high level factorial representation from the object encoder.\n3. A pre-trained semantic parser: a pre-trained module to parse the input (for ex. a question) into semantically meaningful program which can be easily executed.\n4. Neural-symbolic Program: This modules takes a factorial representation (i.e., the output of the encoder), as well as some other intermediate information from module 2 and module 2 and outputs an answer to the question. \n\nThe paper mentions that this is a \"principled\" framework for object centric learning. By keeping the module 3, and module 4 fixed, the authors evaluate different methods for learning object centric representations like MONET and SLOT attention. The authors evaluate how inclusion of the language can improve the performance of MONET and SLOT attention.\n\nExperiments: The authors evaluate the proposed work on two visual reasoning datasets for image segmentation evaluation: Shop-VRB-Simple, and PartNet, as well as a subset of PartNet called PartNet-Chairs. The paper shows that inclusion of the language improves the performance of both MONET as well as SLOT attention, albeit the increase in performance corresponding to SLOT attention is more as compared to MONET.\n\nInteresting points:\n\n1. I like the problem which the proposed method hopes to achieve i.e., learning the representation of high level variables (i.e., objects), which are often associated with language.  \n\n2. It's interesting to note that the gain in performance for Slot attention is more as compared to MONET. This point was also emphasized in the work of RIMs (https://arxiv.org/abs/1909.10893), where they used a \"top-down\" representation to learn object centric representations.\n\n3.  I like the ablation in table 4 i.e., fine-tuning the entire model improves the performance of the model, as compared to separately training the different components. \n\nMajor Points:\n\n1.  I think the paper presents nice preliminary experiments for showing that incorporation of language can help learning object centric representations. Since the contribution of the paper is to actually evaluate different methods for object centric learning (MONET/SLOT Attention) and combining with the framework of neuro-symbolic learning. It would be interesting to evaluate the LORL + Slot Attention as well as LORL + MONET for more difficult tasks to see how well the learned representations transfer to new environments which share some common structure. It could be in the form of  language conditioned scene generation such that by using language one can generalize in a compositional way to new scenes or in the context of instruction following (instruction is in the form of language) where their are objects in the scene, and the goal is to put the objects in  a particular spatial configuration or just navigation (https://arxiv.org/abs/2003.05161).\n\n\nMinor Comments:\n\n\"We have proposed Language-mediated, Object-centric Representation Learning (LORL), a principled\nframework for learning object-centric representations from vision and language.\"\n\nI'm not sure about the use of the word \"principled framework\" as the proposed method is not really a framework. As authors note in the paper, the goal of the paper is to see how incorporation of the language can be used to learn or improve representations of the high level variables (i.e., objects).\n\n======\n\nAfter Rebuttal: I have read the rebuttal, as well as reviews by other reviewers. I very much agree with the authors that the problem is very interesting, but as of now more work needs to be done in terms of \"downstream applications\". \n",
            "=Summary\nThe paper proposes a framework for object-centric representation learning with additional language supervision such as e.g. questions and answers, denoted as Language-mediated, Object-centric Representation Learning (LORL). The authors combine two ideas from prior work, the unsupervised object-centric representation learning and the neural-symbolic concept learning, in one architecture. The model obtains object representations by learning to reconstruct the input image (as in MONet and Slot Attention). The learned representations are used as input to the neural-symbolic program executor, which learns to answer questions about objects. The entire model is trained in three stages: first the reconstruction objective, then the QA objective, and, finally, jointly. Experiments on two datasets demonstrate that the obtained object segmentations have better quality that those of the original unsupervised models. The learned representations are also shown to be effective in several other down-stream tasks.\n\n=Strengths\n\nThe intuitions are clearly conveyed and the method is well motivated: language introduces strong inductive biases, providing concept names which can in turn be grounded in images, helping the model understand what these concepts should look like (especially for heterogenous objects such as \u201ccoffee maker\u201d).\n\nThe authors have collected language descriptions for the PartNet-Chairs dataset.\n\nAdding LORL on top of Slot Attention leads to notable improvement in object segmentation performance on two datasets.\n\nThe learned representations (using Slot Attention) allow to retrieve similar objects that belong to the same object category, showing that they capture object semantics much better than the unsupervised models.\n\nIt is interesting to see that the learned representations can be effectively used for other tasks, e.g. referring expression comprehension w/o additional fine-tuning.\n\n=Weaknesses/High-level comments\n\nI am not sure how unexpected the main result really is (the improved segmentation quality), considering that LORL receives additional (language) supervision. Comparison of vanilla unsupervised models vs. LORL is not completely fair or informative, in my opinion. It would be interesting to compare LORL vs. some other forms of supervision (or even fully/partially supervised segmentation models) or show how VQA supervision compares to caption supervision (more on that below), or using attributes instead of language, etc.\n\nThe authors claim that \u201cLORL can be integrated with various unsupervised segmentation algorithms\u201d (P1). In practice almost all the experiments are carried out with just Slot Attention. The only reported result with MONet (Table 2) is somewhat weak (as also stated by the authors, P7). This seems to undermine the authors' claim. \n\nThere are no experiments on the popular CLEVR dataset, only on the two recent datasets introduced by the authors (ShopVRB-Simple is obtained by selecting easier scenarios from ShopVRB). Specifically, they could have used CLEVR for MONet, as MONet is not applicable to the ShopVRB-Simple data.\n\nIt is in unclear what the language task on the PartNet-Chairs actually is, it is only stated that the authors collected descriptive sentences. It is mentioned that the output can be True or False; does it mean that the task is to predict whether the sentence is true to the image? No\u201dnegative\u201d examples are included in the paper, nor is there any discussion on that. \n\nWhy have the authors decided against collecting question-answering data for the PartNet-Chairs, similar to the ShopVRB-Simple? Is there an advantage of using a specific form of supervision (e.g. question-answering supervision)? Some analysis on which form of supervision is more effective would have been interesting.\n\nThe overall framework mostly relies on existing components, the main technical novelty is bringing them together and an additional objectness score used in both objectives. It is not entirely clear whether the reported Slot attention and MONet performance is obtained with the vanilla implementations or after stage 1 training of the proposed model (which also includes the objectness scores). Overall, would be interesting to see an ablation of the proposed objectness score.\n\nWould be great to learn more about the referring expression comprehension experiment (what the test data looks like, how similar it is to the observed training data, how was the model adapted to the task, etc).\n\nThe authors show that the final training stage significantly improves the model\u2019s QA abilities, saying that it demonstrates the importance of joint training. I am not entirely sure about the purpose of this experiment. Would be also interesting to see how the obtained performance compares to the vanilla neural-symbolic approach.\n\nHere we have synthetic images with no background (relatively easy to segment) and synthetic (templated) language (easy to parse). I am somewhat skeptical about the generality of the proposed framework (which also concerns prior works on which this work builds), i.e. can we expect that this model will work with real cluttered images and real natural language? What do the authors believe is the main limitation of their proposed approach?\n\nNothing was stated about making the code available.\n\n=Detailed comments (P# - page number)\n- P1: perhaps \u201c(for) learning disentangled, object-centric scene representations\u201d?\n- Throughout the paper the authors say \u201creferential expression interpretation\u201d, while this task is more commonly known as \u201creferring expression comprehension\u201d.\n- The authors say \u201cobject\u201d to refer to both objects (in ShopVRB-Simple) and object parts (in PartNet-Chairs). I wonder if there is a better way to explain this, which encapsulates both scenarios.\n- Fig 1 is misleading as it misrepresents the task posed on the PartNet-Chairs dataset (it is not question-answering). \n- P2 ShopVRB should be ShopVRB-Simple.\n- P2 PartNet should be PartNet-Chairs.\n- P3: should o_i be z_i?\n- P3 white object Fig. 2 => in Fig. 2\n- P4: should {z_i} be {z_k}?\n- P4 which query the attribute => which queries the attribute\n- P4 All concepts appear => All concepts that appear\n- P4 \u201cWe use the same domain-specific language (DSL) for representing programs as CLEVR \u201c : in fact it is not the same but extended, as stated in the appendix.\n- P5 an filter => a filter\n- P5 \u201cwe find that it better highlights the quality of learned object-centric representations for various models.\u201d - not clear what this means\n- P7 \u201cThe quantitative results are summarized in Table 1.\u201d - make it clear that this is for the ShopVRB-Simple\n\n==================================================================\n\nPost-rebuttal comments:\n\nI thank the authors for an extensive response and the other reviewers for brining up many relevant questions!\n\n= Main positive additions: \n\nLORL was successfully combined with another unsupervised approach, SPACE, on the CLEVR dataset.\nLORL+SA outperforms IET, which has the same amount of supervision. It does lose to NS-CL slightly, which has access to pre-trained object detectors.\nI like the added analysis of the QA types, data efficiency etc.\n\n= Some of the remaining issues:\n\nThe positive impact of the objectness score on performance was not demonstrated. To show its benefit the authors had to propose yet another evaluation scheme (precision and recall of the reconstructed scene graph).\nThe training objective for PartNet-Chairs should be discussed in the main paper, not in the appendix. Also, perhaps I am missing something, but would not one still need some negatives to train it?\nMinor: Fig 1 in the revision is still wrong, i.e. the example for PartNet-Chairs dataset still illustrates the QA task.\n\nOverall, I like that the approach is intuitive and well motivated but I also think the overall technical novelty is limited. The authors have considerably expanded their evaluation, but at the same time have introduced another confusion (as pointed out by R3 during post-rebuttal discussion): it appears that using 25% of supervision leads to lower segmentation performance, contradicting the main claim of the paper. \nI therefore decrease my score to 5. I hope to see an improved version of the paper (with more exciting technical contributions) in a future venue!",
            "I thank the authors for taking time in writing a rebuttal, and running additional experiments.\n\n> \"\"However, our goal is not to improve existing methods, but to demonstrate how concept learning from language and unsupervised object discovery can bootstrap each other\"\n\nCurrently, I feel like the paper is not really written in a way which is aligned with the above goal.\n\nJust to reiterate I think, this is a great problem to study, as relation b/w different entities are normally sparse \n(just like dependencies in language), which can be very useful for downstream tasks.\n\nLet's just take the example of the abstract.\n\n\n> We present Language-mediated, Object-centric Representation Learning (LORL),\nlearning disentangled, object-centric scene representations from vision and language\n\nWhat is LORL ? LORL is not really a framework. The goal of the paper is to study how language and unsupervised learning of entities can bootstrap each other (as authors mentioned it themselves). In order to achieve this, the paper uses two methods for unsupervised object discovery. So, I think this is not an accurate description of what the paper is actually trying to do.\n\n> \"But LORL further learns to associate the learned representations to concepts, i.e., words for object\ncategories, properties, and spatial relationships, from language input. \n\nAgain, what really is LORL ? It's NOT a framework. \n\n> \"These object centric concepts derived from language facilitate the learning of object-centric\nrepresentations\"\n\nThis line from the abstract accurately justifies what the paper is about. But the rest of the paper is not really written \nin such a way.\n\n> \"Experiments show that LORL consistently improves the performance of MONet and Slot Attention on two datasets via the\nhelp of language.\"\n\nOne can write it: Our experiments show that concept learning from language can improve the performance of XYZ... But again, the paper is over-claiming.\n\n> \"We also show that concepts learned by LORL aid downstream tasks such as referential expression interpretation.\"\n\nSimilarly here..\n\nI like the goal of the paper, but according to me exposition of the paper is not accurate as of now. \n\n> \"Such ability naturally transfers to other domains such as object manipulation or instruction following if we integrate our model with other planning algorithms, which we leave as future works.\"\n\nIt would be interesting to see, if the ability \"naturally transfers\". Its not as easy of a problem as authors seem to imply.\n\nI hope authors are doing well in such times.",
            "This paper proposed an interesting idea that uses language to learn the concept and aid downstream tasks such as segmentation and referential expression interpretation. The authors combine the unsupervised segmentation method (MONet and Slot Attention) with neural symbolic concept learning (NS-CL). By joint training these two objectives, the authors show improvements in the object segmentation and several downstream tasks. \n\n1) What is the language objective for the PartNet-Chairs dataset? The dataset contains templated captions instead of questions, but the paper didn't mention any of the associate target or loss. \n\n2) A lot of the paper needs more clarification, for example, how to calculate the  \"Whiteness\" using the concept embedding? How many programs are there? I can not find that information even in the supplementary materials. It is hard to understand the exact algorithm of the proposed method.  \n\n3) The performance of pretrained semantic parser is not reported in the paper. Since there are only a few templates, will the model achieve 100 percent performance? \n\n4) The semantic parser info is necessary since the model will know the exact additional information of the image given the question and captions (especially captions), assuming there is no error in the parsing procedure. This will harm the overall novelty of the paper since the proposed method can not generalized to more complicated questions and captions. \n\n5) If the pretrained semantic parser can provide very high accuracies, for a fair comparison, it will be good the authors can show the performance of MONet and Slot Attention with additional supervision. For example, Instead of having a Neuro-symbolic program executor, simply use the concept embedding to filter the object-centric representations and then do image decoding. This will show the effectiveness of the Neuro-Symbolic Program Executor.\n\n6) Missing related work, [1] also learns the visual concept through question answering. The authors should discuss the difference between this work. \n[1] Yang et.al Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition (CORL)\n\nI really like the paper's idea, but a lot of critical information experiments are missing. I am happy to increase my rating if the authors can resolve my questions. "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Neutral",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer consistently expresses reservations about the paper's novelty and significance, ultimately recommending rejection. Phrases like \"significance of this contribution is rather limited\", \"results are not very surprising\", and \"I can only recommend rejection\" clearly indicate a negative sentiment.",
            "The review expresses overall positive sentiment due to phrases like \"I like the problem,\" \"interesting to note,\" and \"nice preliminary experiments.\" The reviewer also mentions agreeing with the authors that the problem is very interesting.",
            "The review acknowledges both strengths and weaknesses of the paper. While it appreciates the clarity and motivation behind the method, it also raises concerns about the novelty, experimental setup, and the generality of the framework. The reviewer ultimately decreases their score, indicating a mixed evaluation.",
            "The reviewer expresses concerns about the paper's exposition and alignment with its stated goal, using phrases like \"not really written in a way which is aligned with the above goal\" and pointing out inaccuracies and overstatements.",
            "The reviewer expresses concerns about missing information, lack of clarity, and potential limitations in the paper's approach. While acknowledging the interesting idea, the review highlights several critical issues that need to be addressed before the reviewer can recommend acceptance, indicating a negative overall sentiment."
        ],
        "tone": [
            "Critical",
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a critical tone, directly challenging the novelty and significance of the work. Phrases like \"very straightforward combination\", \"not very novel\", and \"results are not very surprising\" reflect this critical stance. The reviewer also questions the basis for the authors' claims, such as \"I don\u2019t see why LORL is a 'principled framework'.\"",
            "The review provides both positive feedback (\"I like the problem,\" \"interesting to note\") and constructive criticism (\"more work needs to be done in terms of 'downstream applications'\"). It maintains a professional and objective tone throughout.",
            "The review presents both positive and negative aspects of the paper in a measured way. It provides specific examples and justifications for its criticisms while also acknowledging the authors' efforts and contributions. The reviewer uses phrases like 'The intuitions are clearly conveyed' (positive) and 'I am not sure how unexpected the main result really is' (negative) to maintain a balanced perspective.",
            "The review adopts a critical tone by directly questioning the authors' claims and the accuracy of their descriptions, using phrases like \"what really is LORL?\" and \"the paper is over-claiming.\"",
            "The review uses phrases like \"needs more clarification,\" \"hard to understand,\" \"will harm the overall novelty,\" and \"critical information experiments are missing.\" These indicate a critical assessment of the paper's clarity, completeness, and novelty."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its negative assessment of the paper's novelty and significance, both in the initial review and after rebuttal. The reviewer provides clear and consistent reasons for recommending rejection, focusing on the straightforward combination of existing methods, the unsurprising nature of the results, and concerns about experimental findings.",
            "The review maintains a consistent stance throughout. It appreciates the problem being addressed and the initial results, while also constructively criticizing the need for more downstream application validation and questioning the use of the term 'principled framework'. There are no contradictory statements or shifts in opinion within the review, including the 'After Rebuttal' section.",
            "The review is consistent in its assessment, highlighting both strengths and weaknesses of the paper. The reviewer acknowledges the paper's motivations and some positive results, but also raises valid concerns about novelty, experimental comparisons, clarity, and generalizability. The post-rebuttal comments show a consistent evaluation process, acknowledging improvements made by the authors while still maintaining some criticisms and even lowering the score based on new concerns that emerged during the rebuttal process. There are no self-contradictory statements or abrupt shifts in opinion without justification.",
            "The review is consistent in arguing that the paper's exposition is not aligned with its stated goal of demonstrating the bootstrapping relationship between concept learning from language and unsupervised object discovery. The reviewer consistently points out that the paper overemphasizes 'LORL' as a framework instead of focusing on the core research question.",
            "The review is consistent in pointing out the paper's interesting idea but lacking clarity and experimental details. The reviewer raises valid concerns about missing information, unclear methodology, and insufficient experimental validation, all contributing to a consistent critique of the paper's current state."
        ]
    },
    {
        "paper_id": "nips_2022_9T0Bnap5-j7",
        "paper_title": "DeepFoids: Adaptive Bio-Inspired Fish Simulation with Deep Reinforcement Learning",
        "paper_abstract": "Our goal is to synthesize realistic underwater scenes with various fish species in different fish cages, which can be utilized to train computer vision models to automate fish counting and sizing tasks. It is a challenging problem to prepare a sufficiently diverse labeled dataset of images from aquatic environments. We solve this challenge by introducing an adaptive bio-inspired fish simulation. The behavior of caged fish changes based on the species, size and number of fish, and the size and shape of the cage, among other variables. However, a method to autonomously achieve schooling behavior for caged fish did not exist. In this paper, we propose a method for achieving schooling behavior for any given combination of variables, using multi-agent deep reinforcement learning (DRL) in various fish cages in arbitrary environments. Furthermore, to visually reproduce the underwater scene in different locations and seasons, we incorporate a physically-based underwater simulation.",
        "review_ids": [
            "1qzu-C3QR9n",
            "3VsO6V121fF",
            "PQmvPT0eV2",
            "sUNaOKASBR3",
            "tQQVPSTDyNu",
            "8H4AkDMnhc-"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for taking the time out to respond to my review. Having considered your response and manuscript, I have decided to update my rating. ",
            " I've taken measure of the updated comments, reviewed my feedback and updated it.",
            " The given work develops a fish simulation motivated by solving real world problems in the fish farming industry such as over and underfeeding. Through this work, they wish to replicate fish schooling behaviour with higher fidelity, by leveraging Deep Reinforcement Learning to model this. The authors argue that this resulting setup could then be used for training computer vision models to accurately count varied fish species in the cages. * Overall, the work is well written, well organized and clear to follow. It reasonably describes the past work in the field, stating how the contributions are distinct\n\n* The authors motivate the problem statement well, on how this is an important societal problem to solve.\n\n* However, the major contribution of the work focusses on designing this synthetic environment, which seems to be technically significant. What would add strength to this claim would the extension of the Simulation vs Real section, expanding on how this synthetic dataset translates to real world impact There is no major algorithmic novelty introduced. Details on the reward engineering could be expanded, since that seems an essential component of the work.\n\n* Additionally, discussion on how this work could be more generalizable to the Reinforcement Learning/Computer Vision community would be useful. There are no details if the key contributions could be adapted in other domains.  \n Section 4: Could details on the infrastructure to inject a Deep RL agent with the Unity environment be expanded? What is the latency of the system? Could researchers tune parameters of the simulation to allow for generalization across different scenarios?\n\nLine 127-161: On a high level, could there be a description of the weighting of the scalar reward term? At the moment they all are considered to be equal.\n\n   How is the upper bound of r_{t}{BC} computed in the scenario a collision takes place?\n\n   How is wBD empirically computed? Is it a static parameter for a given experiment instance?\n\n   It seems like a significant part of the intended behavior is being encoded through constraints.  Given the complexity of these   intricate behaviors, It would be interesting if this could be implicitly  learnt through forms of behaviour cloning, ranking relevant trajectories.\n\n\n\n\n Yes",
            " This paper proposes a multi-agent reinforcement learning algorithm to simulate the shoaling/schooling behavior of fish. The main application of the method is to control position and movement of fish in a 3D simulation which enables synthetic data generation for downstream computer vision tasks such as counting and sizing. This paper presents an interesting practical application of reinforcement learning - simulating animal behavior for synthetic data generation.\nIt is great to see Deep RL applied to a practical problem! The results in the paper also seem pretty convincing: some of the rendered images from the simulation seem to closely resemble real world images.\n\nMy first point of criticism is lack of support for the claims made in the paper. Section 1 suggests that using RL for synthetic data generation in this scenario improves the accuracy of downstream vision models. The accuracy of the system is evaluated only on three videos and is never compared to anything. Typically a machine learning paper that proposes a method should compare this method's results with a baseline of some sort to support the claim that it's a good method to use for such task. Here obviously RL algorithm would be expensive to run and tune and it's advantages are not apparent unless you compare the results with obvious baselines, such as for example running Foids/Boids algorithms and comparing performance of resulting models, or even placing fish in the simulated cage randomly.\nWhile using RL for this task definitely looks interesting, I'm not convinced that it will significantly improve the results of the final model (I would be convinced if such measurements were present in the paper).\n\nAuthors also mention that they \"attempt to automate fish counting and sizing\" although the sizing task is never mentioned in the paper again. I think it would have been more correct to say that this method can also be used for other task, rather than suggesting that something has been attempted but never showing the actual outcome of the attempt.\n\nTo avoid being too negative in my review, it must be said that the final results are absolutely fantastic. The supplementary video is absolutely great too. Appendix is very comprehensive and contains a lot of valuable information, especially domain expertise.\nI am conflicted in my evaluation, because this is clearly a high-profile and important work addressing an important ecological problem and this paper can have a great impact in its sub-area. At the same time it lacks some of the essential elements of a machine learning publication. I think the potential impact of this paper outweights its weaknesses therefore I'm leaning towards accepting this paper. What RL implementation was used? I don't think this was mentioned in the paper.\n\nHave the authors thought about domain randomization? I.e. randomizing textures, 3D models of fish and other parameters of data generation pipeline and how does it affect the final accuracy? Authors briefly addressed limitations in the conclusion - from the data presented it is hard to judge the applicability of this method beyond this one task. \n",
            " The authors present a method for augmenting computer vision task with simulation, with specific application to fish simulation. They create a simulation environment using deep reinforcement learning to mimic the behavioural properties of several fish species. The simulated behaviour is then rendered in a physically realistic computer-generated environment. Finally, computer vision modules are used downstream to, for example, identify and count fish.  The authors provide a comprehensive and detailed simulation of fish behaviour. This is then optimized using deep reinforcement learning. They apply this approach to problems in fish farming, which they sufficiently justify in their introduction. While this work is original, with respect to the combination of many different approaches to their task problem, it is not entirely clear what is novel in the learning methods. \n\nIn particular, the authors deal with three main problems which they address with machine/deep learning (simulation of fish behaviour, rendering fish behaviour in photo-realistic videos, identifying fish in videos). However, there is no attempt to unify learning across these problems in any significant way, so they remain relatively disjoint. Moreover, it is very difficult to judge the performance of the deep learning approach as the quantified assessment is done at the scale of fish identification, as discussed below. \n\nThe main result of this work is the accurate simulation of fish behaviour, using deep learning. However, the presented summarising features separating out different simulation environments, swarming behaviour and change in polarity and momentum over time, are compared only against some subsets as example. These are not sufficient to judge whether this work is successful at capturing fish behaviour. I suggest the authors consider quantified metrics for the ground truth data they have on fish behaviour and systematically compare this to the simulated behaviours. Is it possible to perform some clustering on the video data and compare clustered behaviours/features to those generated using the simulation? This would give a much easier way for assessing how successful this approach has been. \n\nThe use of simulated synthetic data to improve downstream video tasks could be a useful tool for assessing problem cases where data is sparse/collection if expensive. For example, the synthetic data could be used to identify adversarial examples where, in this example, the fish counting algorithm performs poorly. Have the authors considered this? \n\nIs it possible to closely couple the simulation with the available data, for example, through in-filling trajectories of fish videos given initial trajectories? However, I appreciate these agent-based simulators are highly stochastic and this may not be meaningful in practice.  This work is very closely centred on simulating fish behaviour, aided by deep learning. The manuscript is currently structured such that it is more relevant to modelling fish behaviour than advances in machine learning. While I believe that this work represents a significant advance in reproducing the collective dynamics of fish species, the authors should consider how to structure and emphasise this work such that it has more relevance to advances in deep reinforcement learning.  \n\n\n",
            " The proposed paper aims at defining a fish simulation with deep reinforcement learning to mimic the behavior of swarms of fish. Fish behavior is simulated based on a multi-agent deep reinforcement learning approach and a combination of different rewards. Additionally, an underwater simulation is proposed to define plausible scenes. To validate the method, the simulation is used to generate training data for common computer vision tasks (e.g. counting of fish).  Strengths\n\n- Simulating the behavior of fish with deep multi-agent reinforcement learning is novel and interesting. \n- The paper is very-well written and easy to follow. \n- The approach is technically sound and the results are impressive. \n- The use of synthetic data for training neural networks for downstream tasks (counting fish) is convincing.\n- The simulation is calibrated with field data. \n\nWeaknesses\n\n- The only negative point I have regarding this paper is that it does not quite seem a fit for a conference paper in terms of the page limit. A lot of interesting and important content is presented in the appendix (it is 14 pages long), so it seems the paper would be a better fit for a journal. However, given the high quality of the paper I think this should not be a reason for rejecting this work.  - L113: What is meant by multi-hot values? I assume this refers to a multi-one hot vector? Please clarify. \n- L116: What exactly are the 'visual observations'? The rendered images of three frames?\n- L123: 'Clamped by a small angle'? When I am not mistaken this should be 'clamped to'. \n- L152-154: While it is clear how the aggressors velocity is changed, I am missing a description of how the subordinates behavior is modeled. Put differently, what exactly is the 'chase mode' of the subordinate fish? While Sections 5 and 6 briefly touch on the limitations of some aspects of the method (the accuracy of Yolo is actually not a limitation of the proposed method), I would encourage the authors to add a more in-depth discussion on the limitations of the proposed simulation. What are the failure cases of the simulation? What kind of fish behavior cannot be captured? etc. A discussion on the limitations could initiate future work in this direction. L314-316 also touch on a few topics but in a rather shallow manner. Here more detail would be appreciated."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Neutral",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they have decided to update their rating after considering the response and manuscript, implying a positive change.",
            "The review simply states that the reviewer has updated their feedback based on previous comments. There's no indication of positive or negative sentiment.",
            "The review expresses overall positive feedback, highlighting the work as \"well written, well organized and clear to follow.\" While it suggests improvements, the core assessment is favorable.",
            "The review expresses both positive and negative aspects of the paper. It acknowledges the paper's potential impact and the quality of the results but also points out significant shortcomings in the methodology and validation.",
            "The review acknowledges the work's originality and comprehensive simulation but raises concerns about the novelty of the learning methods, the disjointedness of the learning across different problems, and the lack of quantified metrics for assessing the simulation's accuracy. The reviewer also offers suggestions for improvement, indicating a constructive but critical stance.",
            "The review highlights several strengths, including the novelty and interesting nature of the approach, the well-written and easy-to-follow style of the paper, the technically sound approach, impressive results, convincing use of synthetic data, and calibration with field data. While weaknesses are mentioned, the reviewer explicitly states that the high quality of the paper should outweigh concerns about page limits."
        ],
        "tone": [
            "Supportive",
            "Neutral",
            "Balanced",
            "Balanced",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The phrase \"Thank you for taking the time out to respond\" expresses appreciation, and the willingness to update the rating suggests a supportive stance towards the authors and their work.",
            "The tone is neutral because the statement is factual and lacks any emotional coloring. It's a straightforward report of actions taken.",
            "The review provides both positive feedback (e.g., \"well written, well organized\") and constructive criticism (e.g., \"Details on the reward engineering could be expanded\"). It maintains a neutral and objective tone while offering suggestions for improvement.",
            "The review provides both positive feedback (e.g., \"final results are absolutely fantastic,\" \"supplementary video is absolutely great\") and constructive criticism (e.g., \"lack of support for the claims,\" \"advantages are not apparent unless you compare the results with obvious baselines\"). The reviewer also uses phrases like \"To avoid being too negative\" to maintain a balanced approach.",
            "The review uses phrases like \"it is not entirely clear what is novel\", \"no attempt to unify learning\", \"very difficult to judge the performance\", \"not sufficient to judge whether this work is successful\", and \"manuscript is currently structured such that it is more relevant to modelling fish behaviour than advances in machine learning\". These phrases indicate a critical evaluation of the work's contributions and presentation.",
            "The reviewer expresses clear support for the paper, using phrases like 'very-well written', 'technically sound', 'results are impressive', and explicitly stating that the paper's quality should prevent rejection. Even while pointing out weaknesses, the reviewer offers constructive suggestions and encourages further development of the work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it presents a logical flow of action: acknowledging the author's response, considering the response and manuscript, and deciding to update the rating based on this consideration. There are no contradictory statements within the review.",
            "The review is consistent as it describes a logical review process.",
            "The review is consistent as it starts with acknowledging the strengths of the work (well-written, clear, good motivation) and then moves on to provide constructive criticism and suggestions for improvement. The reviewer points out areas where more details or explanations are needed, such as expanding on the real-world impact, reward engineering, generalizability, and specific implementation details. All points are aimed at improving the paper and are not contradictory.",
            "The review is consistent in its mixed evaluation. It acknowledges the strengths of the paper, such as the interesting application and impressive results, but also points out significant weaknesses, primarily the lack of baselines and validation for the claims. The reviewer is conflicted but ultimately leans towards acceptance due to the potential impact of the work, which is a consistent and reasoned stance. There are no self-contradictory statements or abrupt shifts in opinion.",
            "The review is consistent in its critique, focusing on the evaluation of the fish behavior simulation and the manuscript's emphasis. It acknowledges the strengths of the work, such as the comprehensive simulation and originality, but consistently points out weaknesses related to the lack of quantified evaluation metrics for the simulation itself and the unclear novelty in the learning methods. The reviewer's suggestions and concerns are all aligned with improving the evaluation and highlighting the machine learning aspects of the work.",
            "The review is consistent because the strengths are significant and positive, while the weaknesses are minor points for clarification or improvement. The reviewer explicitly states that the page limit concern should not be a reason for rejection, indicating an overall positive evaluation despite minor suggestions."
        ]
    },
    {
        "paper_id": "iclr_2021_9SS69KwomAM",
        "paper_title": "Solving Compositional Reinforcement Learning Problems via Task Reduction",
        "paper_abstract": "We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures. Code and videos are available at https://sites.google.com/view/sir-compositional.",
        "review_ids": [
            "3OwFwHv0jTj",
            "lqMvAFdXO2",
            "R8XdVWbPadl",
            "szHrlzWnrBO",
            "QPRiuFPuULK"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thank you for the rebuttal. \nIt is appreciated that sometimes different researchers will have different views on what a method represents.\nTo clarify, you propose a method that searches for a good task decomposition such that an intermediate task can be solved to simplify solving the final task. Even if the terminology is not used, this has a lot in common with curriculum learning (i.e. finding easier tasks to simplify learning harder tasks). And because of this similarity, a comparison with methods which include steps for finding simpler tasks (see e.g. the previous reference [1]), such as your algorithm does, would be natural. Instead this type of comparison is missing in the submission. While including any additional algorithm clearly constitutes a good share of additional work, here it is justified given limited baselines baselines.\n\nIn addition, the strongest benefits are obtained in the stacking task where highly important, additional information is only provided to the proposed method. Here, I appreciate the provision of the additional baseline with reduced privileged information. My suggestion would be to only include the additional curve (also in the main paper) and remove the results with privileged information as these mostly hold information about the quality of the additional information and not the proposed algorithm.\n\nThe outperforming of SAC by PPO still remains a big question and I expect that this could be addressed with sufficient hyperparameter tuning for the off-policy algorithm, but regarding the paper's focus this aspect can be seen as less important.\n\nOverall, this remains an interesting paper with a flawed experimental section, which however has been slightly improved by the additional baseline.",
            "#######################################################################\n\nSummary:\n\nIn this paper the authors propose a method for solving compositional tasks in the RL setting. The method (Self-Imitation via Reduction), is a 2-step method in which the agent first reduces the target task into two simpler tasks, and then solves the full task using as a demonstration the composite task uncovered in step-1.\n\n#######################################################################\n\nReasons for score:\n\nI am currently voting for a rejection based on what seem to be two key omissions from the paper:\n\n1. The way in which the set of 'library' tasks is selected, and\n2. A measure of how expensive this offline component of the algorithm is\n\nThe experimental comparisons could also have been stronger.\n\n#######################################################################\nPros:\n\n1. I think the paper is reasonable well written. I found just a few typos, and I think the key conceptual ideas are reasonably easy to follow. \n\n2. The 2-step implementation of some sort of \"policy reduction\" followed by an imitation learning step would seem to be a good in principle idea that merits further exploration\n\n#######################################################################\n\nCons:\n\n1. In Sec 3 the authors talk explicitly about a multi-task RL learning setting, and indeed a central part of the method is a search step over interim states $s_\\beta$ , but I do not see explicitly how this set is constructed. This is of course a critical consideration for a number of reasons:\n\n(1)  If this set of tasks is large, then you are likely to find a good $s_\\beta$, but the search space increases (as does the memory footprint)\n\n(2) If this set is small (and perhaps curated), then the subsequent result is weak (if the agent need only search over a small handful of interim tasks which already include moving the elongated box say, then of course the subsequent learning is rapid). \n\n2. The experimental results would be more compelling if comparisons were made to methods that similarly had some access to interim policies. The comparisons to SAC for example are valid in that they provide a lower bound, but there is additional information available to SIR. Similar it is unclear what the comparison to SAC with a dense rewards shows exactly.\n\n3. The authors mention that the method is extensible even though \"... tasks reduction only performs 1-step planning... SIR still retains the capability of learning an arbitrarily complex policy by alternating between imitation and reduction: as more tasks are solved, these learned tasks can recursive further serve as new reductions...\" - this is a nice idea, but I saw no evidence of this implementation in the paper.\n\n#######################################################################\n\nQuestions during rebuttal period:\n\nPlease address the concerns above. Also, you have some comparison with hierarchical policy algorithms in 9.a/b - is it possible to extend these to the other domains?\n\n#######################################################################\n\nSome general comments:\n\n- I think the results for these multi-step methods are often easier to digest and understand if the phases are presented separately:\n    - Phase 1, reduction:\n        - for each experiment, which task reduction was uncovered by the agent?\n        - as you know, in many cases there are multiple valid reductions, which does your algorithm find and why?\n        - since your reduction phase is 1-step and greedy, in which situations might it not work so well\n        - etc.\n    - Phase 2, imitation:\n        - comparison to other methods\n        - effect of parameter choices\n        - etc.\n- A little more justification for you particular experimental comparisons can be helpful. It seems like there are a few potential avenues you might have liked to explore:\n    - comparison to an information impoverished baseline (like SAC)\n    - comparison to different task distributions\n    - comparison to other subtask methods\n    - etc.",
            "The submission proposes an intuitive curriculum learning method which focuses on sparse reward tasks in RL and uses universal value function approximators. \nIt has 3 explicit steps: \n1. identifying a state to decompose the one task into two \n2. solve these new tasks \n3. solve the complete task by imitating the trajectories from both subtasks.\n \nOn the positive side, the paper is overall clearly written and easy to follow for most parts and performs commensurate or better to baselines on a set of simulated manipulation and locomotion domains.\n\nOn the negative side, the method often only performs commensurate or close to the baselines while introducing significant added complexity and additional hyperparameters. The stacking task, which demonstrates the strongest benefit for SIR, requires strong domain knowledge as the search for intermediate states is highly constrained.  Constraining the search for intermediate states to positions with blocks under the required stacking position is significant domain knowledge unavailable to the other methods. The minimum requirement for a fairer comparison would be to include a version of SIR without this constraint. \n\nMore generally, aspects regarding the specifics of the space in which we search for intermediate states and the baselines remain unclear (e.g. in terms of the search space since according to the appendix the space for states and goals is not the same and e.g. for stacking other constraints exist).\n\nThe final problem regarding the evaluation is that while presenting essentially a curriculum learning method, the paper does not compare against other work in curriculum learning as baseline (e.g. [1,2]).\n\nOther questions remain such as the surprising statement that off-policy SAC underperforms on-policy PPO on the navigation task. Statements that are counter to intuition and existing comparisons between SAC and PPO should be supported with experimental results.\n\nOverall, the introduced method follows a valuable direction for curriculum learning in RL but the submission demonstrates significant weaknesses regarding fair evaluation.\n\n[1] Florensa, Carlos, et al. Automatic goal generation  for reinforcement learning agents. In International Conference on Machine Learning 2018\n[2] Racaniere, Sebastien et al. Automated curriculum generation through setter-solver interactions. In International Conference on Learning Representations 2020.\n\n\n(Disclaimer: I have reviewed a previously submitted version of this work and a big share of critical points remains the same between both reviews including domain knowledge unavailable to baselines and comparison to other curriculum learning methods.)\n",
            "# Summary \nThis paper proposes a new method that combines task reduction and self-imitation learning for goal-based reinforcement learning problems. The idea is to decompose a hard task into two subtasks (subgoals) such that the solution to one of them is already known. Self-imitation learning is used to quickly learn to reproduce such successful trajectories. The experimental result shows that the proposed method outperforms the baseline SAC + HER and SAC + SIL as well as hierarchical architectures such as HIRO and DSC.\n\n## Pros\n* The idea is interesting and novel.\n* The empirical results are good.\n\n## Cons\n* Limited to (a subset of) goal-based RL problems.\n\n# Novelty\nThe proposed idea of decomposing a task into two easy tasks is novel and interesting. It would be worth citing and discussing a relevant prior work [1], which also proposes such a decomposition for goal-based RL. \n\n# Quality\n* The empirical results are good. Specifically, it is interesting that the proposed method outperforms hierarchical RL methods without being explicitly hierarchical.\n* At the same time, the proposed method seems very specific to a subset of goal-based RL problems, where searching the goal space is computationally tractable. \n* Though I appreciate the extension to visual domains using $\\beta$-VAE to remedy the aforementioned limitation, the U-Wall maze doesn\u2019t seem like visually complex, and the result (Figure 8) is not very strong. Either showing much better results on Figure 8 or showing results on complex visual domains would strengthen the claim. \n* It would be more convincing and interesting to show that the proposed method can do deeper planning by applying task reduction recursively.\n\n# Clarity\n* The paper is easy to follow, and the figures are well-presented.  \n* What is the rationale behind $V(s_a, s_b, g_a) = V(s_a, s_b) * V(s_b, g_a)$? This seems quite specific to \u201cgoal-reaching\u201d 1-or-0 reward structures. Do you have an idea how to generalize this to more general reward structures? \n* In Algorithm 1, do you generate a new trajectory to get $\\tau\u2019$? If this is the case, is this taken into account as the number of steps (x-axis) in the learning curves? Otherwise, they are not fair comparisons. \n* Just to check if they are apples-to-apples comparisons, do you use HER across all methods (yours and baselines)?\nIt would be good to mention that this paper considers goal-based RL problems early in the paper (in abstract or introduction). \n\n[1] Floyd-Warshall Reinforcement Learning: Learning from Past Experiences to Reach New Goals, Vikas Dhiman et al.\n",
            "This paper presents Self Imitation via Reduction (SIR) an approach to learning long-horizon tasks by successively reducing it to easy to solve tasks, generating solutions to these easier tasks and self-imitation on successful task solutions. This is done by training a goal-conditioned policy together with a universal value function. When given a task that cannot be solved via the current policy, SIR searches for an intermediate state that divides the task into two easily solvable sub-tasks; this search is carried out by maximising the (composed) value of the sub-trajectories under the current policy. The policy is then executed for these sub-tasks in sequence; success in both these sub-tasks means a solution for the full task is now found. This solution is used as a demonstration that the policy can use for self-imitation via an advantage weighted behavioural cloning objective. This is combined with the policy loss of standard actor-critic algorithms such as SAC and PPO in both the off-policy and on-policy settings respectively and shows significant performance improvements compared to baselines on several long-horizon tasks such as robotic pushing, stacking 3 blocks and a multi-room maze task. \n\nA few points:\n1. This approach tackles an important problem of effectively learning policies for long-horizon tasks taking advantage of compositional structure of sub-problems. It nicely combines several ideas from prior work such as self-imitation and universal value functions to present a method that is conceptual simple but performs well empirically on complex tasks.\n2. SIR seems quite related to the two-level architecture in standard hierarchical RL approaches \u2014 the planner for task reduction is the top level and the low level being the policy. There is a few key difference though: SIR uses this structure primarily for learning and over time the knowledge in this bi-level structure is distilled into the low level policy. It would be interesting if this is discussed a bit further in the paper.\n3. Unlike traditional sub-goal selection methods which recursively decompose the problem into sub-problems, SIR does a single reduction step to decompose the task into two sub-tasks. This works well under the assumption that the goal-conditioned policy has sufficient representational capacity to capture the variety of tasks in the environment. Is it possible to extend the current approach to a recursive decomposition for harder tasks?\n4. The paper is very well written. There is a clear motivation, contributions and a thorough overview of the related work in this area. The discussions are well structured and together with the appendix a lot of detail is provided on the experiments and methods.\n5. A crucial component of the proposed approach is the search for possible reductions. In the proposed approach this is highly structured and limited to very few dimensions of the actual observation space (e.g. only considering object translations in the pushing task, only considering moving unstacked blocks in the stacking task). This provides a key advantage to SIR compared to baselines as it significantly reduces the branching factor of search and consequently can lead to many successful reductions early on during training. This somewhat reduces the strength of the proposed results. As an additional baseline, it would be good to see the performance achieved by SIR when search is not structured and allowed to explore all dimensions of the state space. Does this reduce the performance and/or learning speed of SIR?\n6. The paper presents initial results on a vision-based task where a VAE representation is used as state. What is the dimensionality of this representation? As mentioned above, this can have a significant impact on the learning performance (while CEM should do better compared to random search it is not clear if this can mitigate the issue by itself).\n7. Another key limitation of the proposed approach is the reliance on arbitrary resets which is not feasible in the real world. While this is briefly discussed in the paper it is not clear how this can be mitigated easily. A more detailed discussion would be useful.\n\nOverall, the approach is quite nice and the initial results are encouraging. I would suggest a weak accept."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review acknowledges the paper's interesting aspects but also points out flaws in the experimental section. It appreciates the rebuttal and the addition of a baseline but suggests further improvements and expresses concerns about the experimental setup.",
            "The reviewer recommends rejection due to omissions and concerns about experimental comparisons.",
            "The review expresses several criticisms, including the method's limited performance gains relative to its complexity, reliance on domain knowledge, unclear experimental setup, and lack of comparison to other curriculum learning methods. The reviewer also mentions 'significant weaknesses regarding fair evaluation' and unresolved critical points from a previous review.",
            "The review expresses a generally positive outlook on the paper, highlighting the novelty of the idea and the good empirical results. While it raises concerns and suggests improvements, the overall tone suggests the reviewer sees value in the work.",
            "The reviewer expresses overall positive sentiment, stating \"the approach is quite nice and the initial results are encouraging\" and suggesting \"a weak accept\".  They also highlight the paper's strengths, such as tackling an important problem, combining ideas well, and being well-written."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review points out 'flawed experimental section,' mentions missing comparisons ('this type of comparison is missing'), questions results ('outperforming of SAC by PPO still remains a big question'), and suggests removing certain results. While polite, the core feedback is critical of the paper's methodology and presentation.",
            "The review points out specific weaknesses in the paper, such as missing details about task selection and unclear experimental comparisons. Phrases like 'two key omissions,' 'experimental comparisons could also have been stronger,' and 'I do not see explicitly how this set is constructed' indicate a critical tone.",
            "The review uses critical language, pointing out 'significant added complexity,' 'significant domain knowledge,' 'unclear' aspects of the method and baselines, and 'significant weaknesses regarding fair evaluation.' The repeated emphasis on shortcomings and the need for further clarification contributes to the critical tone.",
            "The review provides both positive feedback (Pros, clarity, good empirical results) and constructive criticism (Cons, limitations, suggestions for improvement). It maintains a neutral and objective tone while pointing out both the strengths and weaknesses of the paper.",
            "The review is formal and objective, using phrases like \"It would be interesting if this is discussed a bit further in the paper\" and \"As an additional baseline, it would be good to see the performance achieved by SIR when search is not structured and allowed to explore all dimensions of the state space.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the authors' rebuttal, identifies a key similarity of the proposed method with curriculum learning, and consistently argues for the inclusion of comparisons with curriculum learning methods. It also consistently points out the issue of privileged information in the stacking task and suggests improvements to the experimental section. The reviewer's points are logically connected and build a coherent critique of the paper's experimental setup while acknowledging the paper's overall interest.",
            "The review is consistent because the reviewer clearly states a rejection based on specific weaknesses (omissions and weak experiments) and provides detailed justifications for these weaknesses in the 'Cons' section. The 'Pros' section acknowledges some strengths, but these are minor and do not contradict the overall negative assessment leading to rejection.",
            "The review is consistent in its critique, pointing out both positive aspects like clarity and negative aspects primarily focused on the flawed evaluation methodology, unfair comparisons (domain knowledge), and lack of comparison to relevant baselines in curriculum learning. The reviewer consistently argues that despite the potential of the method, the evaluation is weak and needs improvement for a fair assessment.",
            "The review is consistent. It highlights the novelty and good empirical results as strengths, while also consistently pointing out the limitation of the method being specific to goal-based RL problems and suggesting improvements in areas like visual domain results and clarity. There are no contradictory statements within the review; the criticisms and suggestions are focused and build upon the initial assessment.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper, providing constructive criticism and suggesting a weak accept, which aligns with the overall tone and specific points raised."
        ]
    },
    {
        "paper_id": "nips_2021_Blq2djlaP9U",
        "paper_title": "Deep Conditional Gaussian Mixture Model for Constrained Clustering",
        "paper_abstract": "Constrained clustering has gained significant attention in the field of machine learning as it can leverage prior information on a growing amount of only partially labeled data. Following recent advances in deep generative models, we propose a novel framework for constrained clustering that is intuitive, interpretable, and can be trained efficiently in the framework of stochastic gradient variational inference. By explicitly integrating domain knowledge in the form of probabilistic relations, our proposed model (DC-GMM) uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as \\textit{pairwise constraints}. These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. We provide extensive experiments to demonstrate that DC-GMM shows superior clustering performances and robustness compared to state-of-the-art deep constrained clustering methods on a wide range of data sets. We further demonstrate the usefulness of our approach on two challenging real-world applications.\n",
        "review_ids": [
            "-wevLtj6vM",
            "TCUP1q7WEj3",
            "_tATHzBuqv",
            "WjHSn9En7tI",
            "a8pTX6ATE2f"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I think authors' response generally improves my understanding of the paper. But my general impression of the paper remains unchanged. I will keep my score.",
            "This paper proposes a deep generative model with latent variables drawn from a Gaussian mixture model.  Moreover, the prior over the components is conditioned on prior information that specifies clustering constraints, allowing the user to specify one of three cases: x_i and x_j (1) must be in the same cluster, (2) must not be in the same cluster, or (3) no information.  An ELBO is derived.  Experiments demonstrate clustering performance on a variety of data sets ranging from MNIST to STL-10 to heart ultrasounds.   Originality:  I find the paper novel, as I am aware of no other work that combines deep Gaussian mixture models with clustering constraints\n\nClarity:  I find the paper to be well-written, clear, and the results well-presented.\n\nSignificance:  Given that clustering is a pervasive methodology, there is potential for wide adoption.  However, I don't think that Gaussian mixture DGMs have found the popularity of other DGM types.  Moreover, while the combination is novel, there does not seem to be any outstanding originality or insight (i.e. this paper didn't make me view clustering or DGMs from a new perspective).\n\nQuality:  I find the paper's execution to be well done.  The model is reasonable and practical.  The results use sensible benchmarks and baselines.\n\n Addressed in Section 5",
            "Authors propose a probabilistic method for representation learning for clustering problem. Authors took a generative perspective and proposed an interesting algorithm for the problem at hand. Authors also evaluated their proposal on a set up problems.  Authors propose a probabilistic method for representation learning for clustering problem. Authors took a generative perspective and proposed an interesting algorithm for the problem at hand. Authors also evaluated their proposal on a set up problems.\n\nThe paper is well written and proposed method is novel. Intuitively integrating domain knowledge is making sense. However, I have several concerns about the method and problem setting. I have given my concerns below.\n\u2022\tWhy do we need domain expertise for deep learning? One can also design features.\n\u2022\tDomain expertise is implemented as a pairwise relation matrix i.e., W \\in R^{NxN} where N is the number of data points. To get full domain knowledge, one needs O(N^2) relations. Please note that this is quantity scale quadratically. I am aware that authors don\u2019t claim the need of full information however this will not decrease the complexity. In other words, if one increases the data set size for a given number of relations, the ratio will decrease quadratically. How will authors address this point?\n\u2022\tConflict between experts: Authors assume that there exists a panel of experts which can create W \\in R^{NxN}. In principle I agree that domain knowledge exits however one needs to address the conflicts between experts. Let\u2019s assume we have two experts and each supplied relations on k points (the same points). By chance they give opposite responses. How will the algorithm handle these cases? \n\u2022\tDoes W \\in R^{NxN} define a metric on data points? If yes, would you please elaborate how to check this? If no, can one use any function for generating W? \n\u2022\tAuthors assume the existence of Gaussian latent space. How realistic is this assumption for practical problems, i.e., Categorical distributions? \n\u2022\tIn the experiments, there are 6000 constraints used in all datasets. One can consider labelling data as domain knowledge or constraint. One can ask experts to label 6000 data points and use a semi-supervised method. What is the benefit of current setup compared to semi-supervised methods?\n\u2022\tCan the proposed algorithm scale to millions of points?\n\u2022\tCan the proposed algorithm scale to high dimensional data e.g. large resolution images ?\nIn general, I am positive about the method however I would like to some of my concerns to be addressed.\n Please see above.",
            "The authors propose a novel framework for constrained clustering via a Gaussian Mixture Model on the latent space of a Deep Variational Autoencoder. To impose the constraint information, the proposed paradigm incorporates instance-level clustering preferences via Bayesian prior probabilities with varying degrees of certainty. The authors have derived the Conditional Evidence Lower Bound in variational inference for their problem setup. They have also provided comprehensive experiments compared with state-of-the-art methods DEC, VaDE and IDEC on classic benchmark datasets of MNIST, FMNIST, Reuters and STL-10. They have also explored the performance of their method on a more challenging dataset, which is the Face image dataset.  The authors provide a novel framework for constrained clusterin via deep generative models by incorporating instance-level clustering preferences via Bayesian prior probabilities with varying degrees of certainty. The paper is technically sound and comprehensive experiments have been conducted for multiple classic benchmark datasets for clustering including MNIST, FMNIST, Reuters, STL-10 and more challenging cases such as the face image dataset. The proposed method has outperformed state-of-the-art clustering methods such as DEC, VaDE and IDEC. The paper is also well written and easy to follow.  Yes. The authors have adequately discussed the limitations of their method. ",
            "This paper deals with a constrained clustering problem which differs from the vanilla clustering problem in the extra pre-existing knowledge. We can also regard this problem as clustering with some supervision. The prior information known here is the pairwise relationship among input samples (a small portion). The method introduces a latent categorical variable of corresponding group. Experimental results show superior performance over the other baselines.  *originality*\n\nDesigning a mixture prior based on the given pairwise sample relations is pretty novel. The experiments include several real-world datasets as well as standard benchmarks like MNIST. The claims are also supported by proofs or experiments.\n\n*quality*\n\n- some notions like $\\delta_{c_ic_j}$ is not explained in the paper\n- How $W$ is defined specifically is very important in this method but it is not clarified. I don't quite understand why \"the values of |$W_{i,j}$| are set to $10^4$ for all data sets\". $W$ is very important in the optimization. How it is defined directly affects the training.\n- The objective function involves all the samples. Do you still use mini-batch training? I can see Eq. (12) is adopting the mini-batching. But what about other terms. \n- Eq. (11) is the final objective. I think the authors could give some insights or explanations why the Eq. (8) is decomposed to Eq. (11). The transition is not very smooth. Is it for the convenience of the computation?\n- Is K pre-defined? With a fixed K, the problem would generally be easier.\n- How is acc calculated? Do you directly use variable $c$?\n- Some examples could be given to better motivated and validate the use of $W$.\n\n*clarity*\n\nConstrained clustering problem is not quite the same as the standard clustering problem. Some supervision is known a priori. I think the authors should explain the problem more clearly, stating explicitly which is know and which is not. Besides, the evaluation protocol should be more clear, e.g. which variable is used for prediction and how you obtain the accuracy.\n\n*significance*\n\nThe real-world datasets are novel. So the technique might be useful in the healthcare domain. The method is also generally novel, taking advantage of the VAE framework. The authors mentioned some limitations in the \"Limitations & Future Work\" section. I agree that obtaining $W$ is complicated and might not be straightforward. The paper should improve the clarity of this part.\n\nBesides, the explanation of the objective function should be elaborated. Some insights and training details should be given."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Negative",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer states that their understanding improved but their overall impression and score remain unchanged, indicating a neutral sentiment.",
            "The review expresses overall positive feedback regarding the paper's originality, clarity, and execution. Phrases like \"I find the paper novel,\" \"well-written, clear,\" \"results well-presented,\" and \"execution to be well done\" indicate a positive sentiment.",
            "While the reviewer acknowledges the novelty and potential of the method ('interesting algorithm', 'positive about the method'), the review is dominated by a list of significant concerns and questions about the method's limitations, assumptions, and practical applicability. The reviewer repeatedly questions the method's design choices and experimental setup.",
            "The review expresses positive feedback, highlighting the novelty of the framework, technical soundness, comprehensive experiments, outperformance of state-of-the-art methods, and good writing quality.",
            "The review expresses both positive aspects ('pretty novel', 'superior performance') and negative criticisms ('not explained', 'not clarified', 'should be elaborated'). The overall sentiment leans towards neutral due to the balance of positive and negative feedback."
        ],
        "tone": [
            "Neutral",
            "Balanced",
            "Critical",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The language is objective and factual, without strong positive or negative indicators. Phrases like 'generally improves my understanding' and 'remains unchanged' suggest a neutral and balanced perspective.",
            "The review acknowledges the paper's strengths (originality, clarity, execution) but also points out limitations regarding the significance and potential impact. The reviewer uses phrases like \"However, I don't think that...\" and \"there does not seem to be any outstanding originality or insight\" to provide constructive criticism, resulting in a balanced tone.",
            "The tone is critical, as evidenced by the numerous questions raised, starting with 'Why do we need domain expertise for deep learning?' and continuing with detailed concerns about complexity, conflict between experts, the nature of the pairwise relation matrix, assumptions about the latent space, scalability, and comparison to semi-supervised methods. The phrasing of the questions indicates a skeptical and challenging stance toward the authors' approach.",
            "The reviewer uses positive language such as \"novel framework\", \"technically sound\", \"comprehensive experiments\", \"outperformed state-of-the-art\", and \"well written and easy to follow\" to express support for the work.",
            "The review provides both positive feedback on the originality and significance of the work, and critical feedback on the quality and clarity. It points out strengths and weaknesses in a constructive manner, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer states that their understanding improved due to the authors' response, but their overall impression and score remain unchanged. This is a consistent viewpoint as improved understanding does not necessarily imply a change in overall evaluation.",
            "The review is consistent because the reviewer provides positive feedback on originality, clarity, and quality, while offering a more nuanced perspective on significance without contradicting the positive aspects. The reviewer acknowledges the novelty and sound execution but suggests the work lacks 'outstanding originality or insight', which is a balanced and consistent assessment.",
            "The review is consistent because it starts with positive feedback, acknowledging the novelty and interesting approach of the paper. It then raises a series of valid concerns and questions regarding the method's assumptions, complexity, scalability, and practical applicability. Finally, it concludes with a positive overall sentiment while emphasizing the need to address the raised concerns. The reviewer maintains a consistent stance of appreciating the paper's strengths while critically evaluating its weaknesses and areas for improvement, without contradicting themselves.",
            "The review is consistently positive, praising the novelty, technical soundness, comprehensive experiments, and clarity of writing. There are no contradictory statements or negative feedback within the review.",
            "The review is consistent in its critique, focusing on the lack of clarity and insufficient details in the paper.  Across different aspects like Quality, Clarity, and Significance, the reviewer consistently points out issues related to missing explanations (e.g., notations, W definition, objective function derivation), unclear problem definition and evaluation, and the need for more insights and training details.  While acknowledging originality and potential significance, the core message remains consistent: the paper needs to be improved in terms of clarity and providing sufficient technical details."
        ]
    },
    {
        "paper_id": "nips_2022_303XqIQ5c_d",
        "paper_title": "You Only Live Once: Single-Life Reinforcement Learning",
        "paper_abstract": "Reinforcement learning algorithms are typically designed to learn a performant policy that can repeatedly and autonomously complete a task, usually starting from scratch. However, in many real-world situations, the goal might not be to learn a policy that can do the task repeatedly, but simply to perform a new task successfully once in a single trial. For example, imagine a disaster relief robot tasked with retrieving an item from a fallen building, where it cannot get direct supervision from humans. It must retrieve this  object within one test-time trial, and must do so while tackling unknown obstacles, though it may leverage knowledge it has of the building before the disaster. We formalize this problem setting, which we call single-life reinforcement learning (SLRL), where an agent must complete a task within a single episode without interventions, utilizing its prior experience while contending with some form of novelty. SLRL provides a natural setting to study the challenge of autonomously adapting to unfamiliar situations, and we find that algorithms designed for standard episodic reinforcement learning often struggle to recover from out-of-distribution states in this setting. Motivated by this observation, we propose an algorithm, Q-weighted adversarial learning (QWALE), which employs a distribution matching strategy that leverages the agent's prior experience as guidance in novel situations. Our experiments on several single-life continuous control problems indicate that methods based on our distribution matching formulation are 20-60% more successful because they can more quickly recover from novel states.",
        "review_ids": [
            "N356D1MDDz",
            "G3k2E3nkTzq",
            "eKcMAjUBAMu",
            "j44HPvaKBo7",
            "KBDgrML8Zqr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank you for the very thorough revision of the work and responses to the reviewers. I think the paper would be an interesting addition to the RL literature. I feel like my original score still reflects my opinion about the paper, therefore I've decided to keep it.",
            " This paper considers the non-episodic RL setting (no environment resets) where an agent is given experience data from a source task and needs to efficiently learn to maximize rewards. They give discussions on why current RL approaches like SAC and GAIL will struggle in this setting (mainly due to distributional shifts from a source task). They then propose QWALE, an \"adverserial imitation learning\" approach (similar to GAIL) that uses Q-values (pretrained) to weight a discriminator that is learned and used as auxiliary rewards during training. They show experimentally that previous works (including GAIL) indeed struggle in this setting and are outperformed by QWALE. This is a very interesting paper that proposes a novel algorithm (QWALE) to learn policies in non-episodic discounted settings. I believe this is very relevant to the RL community, and the contributions are significant relative to previous works. Mainly,\n- They propose the single life RL setting, which is a novel more realistic non-episodic setting where the agent is given data collected (and policies/value functions learned) from previous experience.\n- They provide intuitive explanations for why previous works can struggle in this setting.\n- They provide a number of experiments in multiple domains (including complex continuous domains) demonstrating how QWALE out performs previous RL methods like GAIL in the non-episodic setting.\n\nHowever, I have a couple important concerns on whether the claims made in the paper are well supported. If I understand correctly, the specific main contributions of this work are the new discriminator update rule (eqn 1) and the reward shaping approach (line 7 in algorithm 1). However \n- No theory is given about them. For example, how does the weightings added to the discriminator update rule affect the theoretical analysis of GAIL [26]. How does the proposed reward shaping affect optimality?\n- No explanation was given for the specific choice of the weightings ($exp(Q(s, a) \u2212 b$) and $exp(-(Q(s, a) \u2212 b))$. Why is there an exponential in there and what is the effect of b theoretically or experimentally?\n- b is claimed to incentivize the agent to move towards states with higher value than its current state. This was not supported theoretically nor experimentally.\n - Missing reference in line 248: \"Appendix ??\"\n- $q_\\theta$ appears in algorithm 1 and line 243, is critical to the approach, but is never defined. Did you mean $q_\\phi$?\n- How is $D_{online}$ updated in Alg 1?\n- In line 618 of the Appendix you give a different reward function used to speed up training (different from the one in Algorithm 1). What exact reward function was used for training? Was the same reward function used for the GAIL baseline?\n- In figure 5 (or the Appendix), please include the trajectories for SAC and GAIL (and compare them with QWALE)\n- How sensitive is QWALE to the hyper-parameters involved? A hyper-parameter sweep plot would be useful (since there are no theoretical contributions to strengthen the claims) The authors have adequately addressed most of the limitations (except where mentioned in my review above) and potential negative societal impact of this work.",
            " This paper introduces and tackles the single-life reinforcement learning (SLRL) problem, in which there are no episodic resets available, and the agent has to solve the task in one-shot. The agent has access to prior data which comes from a similar but not the same environment, and this data might not be expert data. The authors propose QWALE, Q-Weighted Adversarial Learning, which extends Adversarial Imitation Learning methods to the general setting in which  the data might not be optimal for the current environment, by weighting the experiences with a pretrained Q-function. Strength\n- It introduces and describes single-life reinforcement learning as a specific set of tasks in which an agent has to learn how to recover from mistakes on its own without relying on episodic reset\n- Good comparisons to other methods\nWeaknesses\n- I am not sure how clear it is the importance of SLRL problems. The examples described of a robot exploring another planet or a rescue robot are closely related to the concept of safe reinforcement learning or safe exploration [1], where some constraints have to be respected. Moreover, I would think that similar problems are addressed in classical robotics motion planning literature.\n\n[1] Garc\u0131a, Javier, and Fernando Fern\u00e1ndez. \"A comprehensive survey on safe reinforcement learning.\" Journal of Machine Learning Research 16.1 (2015): 1437-1480. - Figure 2 positioning is not good. It\u2019s hard to understand what the point of Figure 2 is until you reach Section 7.\n- Both PointMass and TableTop training/testing environment sound very similar to the test bed environments used in meta learning papers, where the dynamics or the goals change slightly in the environment. For this reason, I would expect that Meta Learning algorithms like MAML would perform well in environments like tabletop, have the authors tried something similar?\n- Figure 5 is never linked, I guess it should be linked somewhere around section 7.3?\n Overall, this paper introduces SLRL and shows how current algorithms are not capable of autonomously recovering in an environment in which they have to reach the goal but are not provided with resets from bad states. It evaluates current algorithms and proposes a new weighted adversarial learning approach to overcome this problem. QWALE is sound and the idea of weighting the examples based on the Q-function trained in the source environment makes sense.\nThe main limitation of this work is that it\u2019s very similar to other approaches like the ICLR 2022 paper Autonomous Reinforcement Learning [2] which introduces a similar setting in which environment resets are rare or not available at all, and I am not sure how useful it is to the community.\n\n[2] Sharma, Archit, et al. \"Autonomous Reinforcement Learning: Formalism and Benchmarking.\" ICLR 2022.",
            " The paper motivates single life RL setup involves pre-training on prior experience and generalizing another setting with novel states and that it is different from episodic or reset-free RL. As the objective is to perform well in single life on novel setting, the paper proposes to bias the exploration at test time to the known distribution at training time. For this, GAIL with the discriminator weighted with Q values while training on prior data is used. QWALE trained policy generalizes to new initial position of mug in Tabletop organization, wind in pointmass, hurdles in half cheetah and new combination of task in Franka-Kitchen environment. The paper is well-written and structured. The paper discusses the The Q-values from prior dataset allow that generalization to novel unseen scenarios at test time.  \n\nHowever, unlike the motivating example of finding water on Mars where the water would not be found at the same place as a desert on Earth, the experiment setup does not discuss if the goal position would be changed, different from that seen in the prior dataset. The goal remains the same in prior data and \u201cnovel\u201d test setting, for example, in pointmass environment at (100, 0), in Franka kitchen with both microwave and cabinet closed, etc. - SLRL is claimed to be a special case of continual or lifelong learning. SLRL focuses to explore as shown in the prior data and avoids exploring unknowns in the novel setting so that it doesn\u2019t go out of training distribution. How does SLRL method compares to any continual lifelong RL approaches? A comparison\u00a0could demonstrate scenarios where focussing on the prior data for exploration clearly beneficial (or detrimental) to the performance of the agent.\n- Is a fundamental assumption in SLRL that the reward distribution and state which represent goal should have same distribution as the shown in prior trajectories?  It seems that weighting on Q-values from prior data will induce this assumption and limit the possible scenarios in which the policy will generalize to new settings. The authors have discussed the potential limitations and future scope of SLRL. Some aspects of potential negative societal impact of the  work should be included, like if SLRL is used in home or space-probing robot, what the scenarios will be that it could not handle.",
            " The paper introduces the setup of single-life reinforcement learning (SLRL), where the algorithm is given offline data from some environment and then is deployed in a related but slightly different environment; it needs to perform as good as possible in a single trial, leveraging online learning as well as offline data. This differs from both no-reset RL (because we care only about a single trial) and 0-shot generalization in RL (because we can learn along the way).\n\nThen the authors propose QWALE, an algorithm based on GAIL, to tackle the setup of SLRL. They provide an empirical evaluation in 3 different continuous control environments. They compare their approach to several baselines, such as vanilla GAIL, RND, and vanilla SAC. They demonstrate that typically success rate is improved and, more visibly, the speed of reaching the solution is higher.\n Strengths:\n\n[S1] The introduced setup of SLRL seems important, describing a quite natural real-world scenario.\n\n[S2] The paper is well written and easy to follow.\n\n[S3] The empirical evaluation is sound and demonstrates improvements brought by the presented approach (see the Questions section for the discussion about baselines though).\n\nWeaknesses:\n\n[W1] The approach assumes that although there is a shift between source and target environments, the state space is essentially the same. If e.g. visual observations were used and the environment shifted from Earth to Mars, then the discriminator would learn to discriminate based on visual cues, which is not what we want in this approach. I think this is fine, it should be clearly stated as a limitation though.\n\n[W2] The novelty of the method itself is limited. It is based on GAIL and adds a weighting based on the critic network, which is reminiscent of critic filtering in CRR ( https://arxiv.org/abs/2006.15134 ). However, it is used in a novel setup.\n [Q1] I have suggestions for additional baselines:\n- just run SAC in the source environment and evaluate in the target one (without online learning); this is just a sanity check to verify that the shift is challenging enough - the performance should be very poor\n- just train SAC on the target environment, completely from scratch\n- fine-tune SAC, and add a behavioral cloning loss of the offline data as a regularization; this is another way to \u201canchor\u201d the solution in the prior data. I conjecture this should help if only start states are changed (and not the dynamics), and otherwise would probably not work well.\n\n[Q2] I\u2019d suggest providing the details about the algorithm clearly in the main text, including the baseline term b described as \u201cimplementation detail\u201d and details of normalization of Q values.\n\n[Q3] I\u2019d suggest citing the CRR work (mentioned in W2, section Weaknesses).\n Please see W1 (section Weaknesses) and address this in the text."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Negative",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses appreciation for the revision and states that the paper would be an \"interesting addition\" to the literature. They also maintain their original positive score, indicating continued satisfaction.",
            "The review expresses both positive aspects ('very interesting paper', 'novel algorithm', 'significant contributions') and negative aspects ('important concerns', 'claims made in the paper are well supported'). It acknowledges the paper's strengths while raising several critical questions and concerns about the methodology and presentation of results.",
            "The review expresses concerns about the novelty and importance of the introduced SLRL problem, drawing parallels to existing concepts like safe reinforcement learning and meta-learning. It also points out similarities to a previous ICLR paper, questioning the contribution of the work.",
            "The review acknowledges the paper's strengths, such as being well-written and structured, while also raising significant concerns about the experimental setup and limitations of the proposed approach. The language used is objective and balanced, presenting both positive and negative aspects without strong emotional expressions.",
            "The review highlights several strengths of the paper, including the importance of the introduced setup, the clarity of writing, and the soundness of the empirical evaluation. Although weaknesses and questions are raised, the overall tone suggests a positive assessment of the work's value."
        ],
        "tone": [
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses phrases like \"Thank you for the very thorough revision\" and \"I think the paper would be an interesting addition\" which convey a supportive and encouraging tone.",
            "The review presents both positive and critical feedback. It starts by acknowledging the paper's strengths and relevance but then raises several important concerns and questions, creating a balanced perspective. The language is generally polite and constructive, even when pointing out flaws.",
            "The review uses phrases like \"I am not sure how clear it is the importance of SLRL problems\" and \"The main limitation of this work is that it\u2019s very similar to other approaches...and I am not sure how useful it is to the community.\" These indicate a critical assessment of the paper's significance and originality. The reviewer also points out specific weaknesses and areas for improvement.",
            "The review uses formal language and presents both positive aspects (e.g., \"well-written and structured\") and critical questions (e.g., \"How does SLRL method compares to any continual lifelong RL approaches?\"). The reviewer is objective in pointing out potential limitations and areas for improvement, indicating a balanced approach.",
            "The review presents both strengths and weaknesses, indicating a balanced perspective. It uses formal language and provides constructive criticism, suggesting improvements while acknowledging the paper's merits. Phrases like 'seems important,' 'well written,' and 'sound' contribute to a balanced and objective tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer appreciates the thorough revision and finds the paper interesting, while consistently maintaining their original score, indicating no contradiction in their assessment.",
            "The review is consistent because it first acknowledges the novelty and potential of the paper, and then raises specific and valid concerns about the lack of theoretical support, unclear explanations, and missing experimental details. The reviewer provides both positive and negative feedback in a balanced and constructive manner, without contradicting themselves.",
            "The review is consistent in its critique. While acknowledging the strengths of the paper in introducing SLRL and providing comparisons, the reviewer consistently questions the novelty and importance of the SLRL problem setting. The weaknesses raised, such as the similarity to safe RL, meta-learning, and existing literature like Autonomous Reinforcement Learning, all contribute to a coherent argument questioning the significance and community impact of the presented work. There are no self-contradictory statements within the review; the reviewer's concerns are focused on the positioning and novelty of the research rather than its technical soundness, which is acknowledged.",
            "The review consistently questions the novelty of the \"novel setting\" and the limitations of relying on prior data for generalization in truly novel scenarios. The reviewer's points are logically connected and aim to identify the scope and limitations of the proposed SLRL approach.",
            "The review is consistent because the strengths, weaknesses, and questions are logically connected and do not contradict each other. The reviewer provides constructive feedback, acknowledging the paper's contributions while also pointing out areas for improvement."
        ]
    },
    {
        "paper_id": "iclr_2022_DvcMMKmDJ3q",
        "paper_title": "Generating Symbolic Reasoning Problems with Transformer GANs",
        "paper_abstract": "Constructing training data for symbolic reasoning domains is challenging: Existing instances are typically hand-crafted and too few to be trained on directly and synthetically generated instances are often hard to evaluate in terms of their meaningfulness. We study the capabilities of GANs and Wasserstein GANs equipped with Transformer encoders to generate sensible and challenging training data for symbolic reasoning domains. We conduct experiments on two problem domains where Transformers have been successfully applied recently: symbolic mathematics and temporal specifications in verification. Even without autoregression, our GAN models produce syntactically correct instances and we show that these can be used as meaningful substitutes for real training data when training a classifier. Using a GAN setting also allows us to alter the target distribution: We show that by adding a classifier uncertainty part to the generator objective, we obtain a dataset that is even harder to solve for a classifier than our original dataset.",
        "review_ids": [
            "gwDZ_LGzvv",
            "F90-vecqQc",
            "CNSiLVttmwE",
            "9PMP4ub8NNm",
            "lKH5FzVwziY",
            "3tuLK8vcPLw",
            "DRwTWMiDmu4",
            "9EL3oHJxTAa"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I appreciate authors for making clarifications and providing additional details. However, I would keep the score as it is. I would like to mention that multiple other reviewers seem to share the same concerns as me, based on their final official comments, e.g., Reviewer JuGn (\"I still don't find generating syntactically-correct formulae alone to be particularly interesting, since it is easy to randomly generate well-formed formulae.\") and Reviewer qNn9 (\"I'm not excited about just generating syntactically correct problems because a similar task has been approached by symbolic reasoning (as in program synthesis) and even by neural methods (e.g., as the following [1]). Perhaps I might miss something, but I can't find from the paper nor response why the experimental result on symbolic integration is exciting or worth reporting.\").",
            "This paper makes the use of GANs equipped with Transformers to generate new\ninstances of the problems in the symbolic reasoning domains, and demonstrated\nthe usefulness of the idea on two domains: satisfiability prediction of LTL\nformulas and mathematical reasoning on integration and ordinary differential\nequations.  The experiments show that the trained GAN models can produce\nparsable instances on both of the domains and that training on the generated\ndata can improve the accuracy performance of a classification model for LTL\nsatisfiability prediction. ### Strengths\n\n* Simple idea: using GANs for data augmentation.\n\n* Empirically proving that GANs can generate parsable instances and that GANs\n  can generate interesting problems that are hard to solve and contribute to\n  improve the performance of classifiers.\n\n### Weaknesses\n\nI have several concerns and questions as follows.\n\n* I'm not entirely sure of how the generated data are labeled.  Is it done by\n  the GAN discriminator or by another algorithm as labeling LTL formulas in the\n  real dataset with the tool aalta (Appendix B.3)?\n\n  If the latter is employed, it indicates the existence of an algorithm to\n  address the task of interest, so I'm wondering why machine learning is applied\n  to it.  Perhaps ML models might be expected to answer the problems faster\n  than the algorithm, but labeling with the algorithm would be successful only\n  on problems that the algorithm can answer in a given time; thus, it seems hard\n  to label problems that the algorithm cannot solve in a reasonable time,\n  although the ML models should approach such problems.\n\n  For the former, the generated data could be mislabeled.  Then, it is not clear\n  for me why training on such data can produce a model with the performance very\n  close to the model trained on data with correct labels (Table 2).\n\n* I don't think this is the first work to use GANs for data augmentation.  For\n  example, [1] and [2] used GANs for augmenting image data.  Unlike\n  these previous works, the paper addresses sequential data that are textual\n  representations of mathematical expressions, but lacking the discussion and\n  (qualitative) comparison with them makes a challenge and novelty of the paper\n  unclear.\n\n  [1] Antreas Antoniou, Amos J. Storkey, Harrison Edwards.\n  Data Augmentation Generative Adversarial Networks.\n\n  [2] Christopher Bowles, Liang Chen, Ricardo Guerrero, Paul Bentley, Roger N. Gunn, Alexander Hammers, David Alexander Dickie, Maria del C. Vald\u00e9s Hern\u00e1ndez, Joanna M. Wardlaw, Daniel Rueckert:\n  GAN Augmentation: Augmenting Training Data using Generative Adversarial Networks.\n\n\n* The abstract and introduction seem oversold somewhat in that they don't\n  mention that the experiment conducted for symbolic mathematics only\n  confirms the GAN model can produce syntactically correct problems; thus, the\n  full usefulness of the GAN-based data augmentation is still open in the\n  symbolic mathematics domain.  I think it would be nicer to conduct more\n  experiments on symbolic mathematics, which would demonstrate the proposed\n  approach is useful in broader domains.\n\n* The experiments on satisfiability of LTL formulas look interesting, but I'm\n  concerned that it is adequate to be tacked with machine learning.  As written\n  in Appendix B.3, there exists a tool for checking satisfiability of LTL\n  formulas, and it is used to label the (real) dataset. Then, why do we need the\n  trained model for checking satisfiability?  A critical problem with the\n  trained model is that its prediction might be wrong.  How can we use a\n  satisfiability checker that may produce wrong answers?\n\n  For a similar reason, I'm not sure why machine learning is useful for the\n  domain where \"data can ... often be labeled automatically\" (page 8).\n\n* The paper uses GAN and WGAN, but it does not investigate why they make a\n  difference in the experiments (e.g., Table 1).\n\n\n### Minor comments / questions\n\nP2 \"\\neg \\box (access_p0 \\wedge access_p1)\"  I think \\box (globally) should be\nreplaced with \\diamond (eventually) for correction.\n\nP2 \"Mathematical expressions\"  What are these?  Are they different from random functions?\n\nP3 \"continuous domains were\"  where\n\nP4 \"we use the alternative generator loss\"  Why?\n\nP6 \"the origin training data\"  original\n\nP6, Table 1:\n  For GAN, does the use of \\sigmoid_real larger than 0.2 make the fraction of fc larger?\n\nP7 \"we combine both critic and classifier into one Transformer encoder\"  Why are not they separated?\n\nP8: The result in Table 3 seems peculiar to me.  Why does the model trained on\nMixed-e outperform the model trained on LTLbase even when tested on LTLbase?\n\n### Post-Rebuttal\n\nI would like to thank the authors for the additional comments to answer my questions.\nHowever, I still have two major concerns that make me hesitate to accept the paper.\nThe first s that I don't still find the task of generating symbolic expressions interesting.\nThe second is about an application of the approach.  The response from the authors says that neural models may compute solutions faster than classical tools.  I don't disagree with this claim, but the paper doesn't show that the proposed approach is indeed helpful for that task.  I would like to see more discussions and evidence for the story to hold true (e.g., how the augmented data are labeled, whether they can improve the performance of neural models that compute solutions, etc.) The paper addresses a critical problem in symbolic reasoning domains, and the\nexperimental results are promising. However, I think it's not ready for\npublication because of lacking a discussion for practical settings to use the\nproposed GAN-based data augmentation, an evidence to show its generality, and a\ncomparison with the previous work.\n",
            " I thank the authors for their response and for providing additional information about the experiments. I still don't find generating syntactically-correct formulae alone to be particularly interesting, since it is easy to randomly generate well-formed formulae. I would still like to see that the generated data has statistical value (as was shown for LTL).",
            " Thank you for the response.  It addresses my questions in the review, but I still have several concerns.\n\n>> The abstract and introduction seem oversold somewhat in that they don't mention that the experiment conducted for symbolic mathematics only confirms the GAN model can produce syntactically correct problems; thus, the full usefulness of the GAN-based data augmentation is still open in the symbolic mathematics domain. I think it would be nicer to conduct more experiments on symbolic mathematics, which would demonstrate the proposed approach is useful in broader domains.\n> \n> Symbolic integration is not a classification, but a complex translation task. We chose the integration dataset to validate our main GAN findings for generating syntactically consistent formulas on an existing dataset.\n\nI'm not excited about just generating syntactically correct problems because a similar task has been approached by symbolic reasoning (as in program synthesis) and even by neural methods (e.g., as the following [1]).  Perhaps I might miss something, but I can't find from the paper nor response why the experimental result on symbolic integration is exciting or worth reporting.  \n\n[1] Baptiste Rozi\u00e8re, Marie-Anne Lachaux, Lowik Chanussot, Guillaume Lample. Unsupervised Translation of Programming Languages. NeurIPS 2020.\n\n> Even when classical algorithms exist, having neural reasoning engines is still very desirable: classical tools often do not scale very well and suffer from a high computational complexity. Neural models, however, can make fast predictions. These can be checked by a classical tool in a fraction of the time that it would need to come up with a solution.\n\nDo the authors mean  checking predictions (without evidence) is (much) faster than computing solutions?  Then, I'm not convinced with this claim.  Can the authors provide references to prove it?  How broadly is the claim applicable?  If the claim is true, then why don't practitioners just run two processes of the checker by supposing the answer of a problem is satisfiable in one process and unsatisfiable in the other?  If this worked well, it would be quite surprising for me that we get the correct answer with at most twice of machine resource that is necessary to run one checking process, and that we could get the  answer very quickly by running the two processes in parallel.\n\n>> P4 \"we use the alternative generator loss\" Why?\n>\n> Training is not successful otherwise.\n\nWhy not successful?  How did the authors come up with the alternative loss, and why do the authors consider it can be a replacement?",
            " > The long-term motivation of this line of work is the enhancement of classical algorithms for symbolic reasoning with neural reasoning engines, which can make fast predictions. Note that verifying a prediction is often much easier than computing a solution. \n\nHow would the authors create a supervised dataset? Are the authors suggesting that they would create a supervised dataset by generating a number of example problems using a GAN, (quickly) filter results using a verifier and then use solvers to find solutions? If computing solutions is hard, if this scalable?",
            "This paper aims to generate training data for symbolic reasoning by training GANs based on Transformers. Specifically, the paper explores two methods --- standard GAN and Wasserstein GAN --- which has a Transformer as an encoder and a decoder, respectively. Training these models on symbolic reasoning data that is randomly generated --- LTL and Symbolic mathematics --- is found to generate high quality formulas. ** Update after author responses ** After the author clarified with the format of the data and results, my problems have resolved in part and thus I updated the score. My concern in the motivation of the paper still remains. The paper claims that synthetic dataset generated from rules is limited in the coverage so that it is valuable to construct synthetic dataset from neural models --- however, it is still not clear to me if experiments demonstrate that the resulting data from neural models are shown to have better coverage and higher quality than data from rules.\n\nThe model introduced in the paper is well-executed, and based on the generated data attached in the submission, the data quality appears to be good enough. There are still a few concerns I have in the motivation of the problem and experiments.\n\nFirst, the problem setup is not convincing to me and is not well-motivated in the paper. If data for the symbolic reasoning --- which is not precisely defined in the paper --- can easily be generated at scale using rules, as done for the base data in the paper, what is the reason for training a neural model to generate more data? How is it inherently different from generating more using rules, as data, either generated automatically from rules or generated by neutral models, is equally artificial?\n\n~~Second, although the paper claims that the generated data is high quality, it seems to have quite bad quality to me. I checked Supplementary materials and most formulas contain many repetitions like \u201c&&&&&&&GXaG>!\u201d and are very hard to interpret. It is very difficult to find good examples as listed in Section 4.2.1, so there is a high likelihood that these good examples are cherry-picked.~~\n\nThird, the experiments in Section 4 do not demonstrate that the generated data has high quality and effectively replaces the base data. For example, Table 2 shows that training on generated data achieves performance that is comparable to the model trained on the original data, but not better. This is related to my first point in the motivation of the problem - if the original data can be obtained automatically at scale and generated data from the proposed model is not significantly better than the original data, is there a justification for not using the original data?\n\n The model introduced in the paper is well-executed, and based on the generated data attached in the submission, the data quality appears to be good enough. However, in my opinion, there are more fundamental issues in the motivation of the problem and whether experiments successfully justified the usefulness of the generated data from the proposed model.",
            "Generating symbolic reasoning training data using transformer GANs. Use classifier uncertainty in the generator objective to generate samples that are harder for the classifier to solve than the original data. The problem is very interesting, but the empirical results could be improved. The problem is interesting and the paper is well motivated.\n\nAbstract: What do they authors mean by \u201csynthetically generated instances are often hard to evaluate in terms of their meaningfulness\u201d?\n\nWhat do the authors mean by \u201ctraining on randomly generated data carries the risk of training on meaningless data or the risk of introducing unwanted biases\u201d? Provided that the conclusion follow logically, can the authors elaborate on what they mean here? \n\n\u201cWe show that training directly in the one-hot encoding space is possible when adding Gaussian noise to each position\u201d. Why use Gaussian noise? The authors should be more clear about why they add noise to the real samples? Presumably this is to make distinguishing the real samples from the fake ones non-trivial? This is only explained later on and is a bit confusing.\n\n\u201con which a classifier can successfully be trained on\u201d. Do you test on an established dataset?\n\n\u201cThe generator\u2019s input is a real scalar random value with uniform distribution [0, 1] for each position.\u201d It is not clear here what is mean by \u201ceach position\u201d. I assume this is each symbol in the input? But this is not clearly explained.\n\n\u201cThe position-wise padding mask is copied from the real data during training, so the lengths of real and generated formulas at the same position in a batch are always identical.\u201d The first time you refer to **the** padding mask it is not clear what this is or where it comes from?\n\n\u201cStill, both generators are able to produce a large fraction of fully correct temporal specifications, which we find surprising\u201d Could there be overfitting? Are the results the same across many runs? Is 0.3 a large fraction? How does this compare to generating examples randomly? This would make a good baseline (even if your models perform worse).\n\nFigure 4: Interesting results.\n\nThe satisfiability classifier results are interesting. What would be better tho, is to show that training on the generated data improved results on an established dataset. \n\nIt is interesting that you can learn on data generated using the LTLbase 10k dataset and perform better on the validation set than training directly on the LRLbase (and perform similarly well when training on the whole LTLbase dataset). However, it\u2019s clear that the model has overfit to the LTLbase 10k. How did you decide to stop training? Does this happen for all runs? It would be helpful to see the training and test curves while the model is training. Additionally, it would be good to see a graph with multiple runs.\n\nFor all results in the paper, it would be best to perform multiple runs and report the standard deviations. \n\nIn the section titled \u201cGAN with included classifier\u201d: What are you classifying? Are you predicting satisfiability? This is not clear. How do you know the satisfiability of the generated samples? Is there a 50/50 split of satisfiable and unsatisfiable examples?\n\nAre there the same number of training examples in the LTLbase, Uncert-e and mixte-e datasets?\n\nTable 3: Are you training and testing on different splits of the dataset? It would help to add the std? The results somewhat suggest that training to increase uncertainty does produce slightly more challenging problems, but it\u2019s hard to tell because it\u2019s not clear how much results vary between runs and it\u2019s not clear if the differences are statistically significant. \n\nTable 3: It\u2019s very important to know how train and test samples were split for Uncert-e. Uncert-e being more difficult to classify could also be explained by lack of variation and the samples you tested happening to be in a different mode to those in the training set. What is the average length of problems in each data set?\n\nIs there a qualitative difference between samples generated with and without the uncertainty loss? What makes the problems harder?\n\nDoes training on Uncert-e improve performance on LTLbase (more so than training on generated?) Is there a standard dataset that you can show improvement on?\n\nWhy are there not more quantitative results on the integration examples? It\u2019s clear that a classifier can already perform very well on the LTL tasks, perhaps it would be easier to see improvements on the function integration task? It does not appear that there are any results for this in the main text? \n\nGeneral: The paper does not clearly separate LTL results and symbolic math results. This makes some results harder to parse.\n\nGeneral: This model does not generate a supervised training dataset since you still need to use existing algorithms/ programs to compute the labels/targets. This could be a problem for datasets where the solution is intractable and would also suggest that you already have a model capable of solving the problem for which you are generating the data. What is the long term motivation of this work if you either (a) cannot generate labels/targets or (b) can already use existing algorithm to solve these problems.\n The problem is really interesting. \n\n\nCorrectness: \nThere are problems with the experimental results. If the authors can add the suggested results and show that their results are statistically significant, I would be very happy to increase my score. \n\nNovelty: The approach also lacks novelty, only proposing an additional loss which is not clearly described. However, their application is very interesting.",
            "The authors apply (W)GANs with transformer encoders for data-augmentation in two symbolic domains: LTL and function integration. There are many interesting findings, three of which stand out. First, in both domains, the network learns to generate syntactically-correct examples roughly 30% of the time. Second, for LTL, a GAN trained on a dataset of size 10k can produce a much larger dataset, such that training on the new dataset is almost as good as training on the original distribution (when evaluating on the original distribution). Third, by also rewarding the generator for confusing a classifier, they can generate new problems that are harder to classify than those in the original distribution. This is a strong paper overall: innovative, well-written, and potentially important.\n\n- I found it awkward that the prose stressed that two domains would be considered, in some cases even listing symbolic mathematics (i.e. function integration) first, even though the integration domain received much shorter shrift and most of the experiments seem to be LTL only. I think the paper would be stronger with two domains throughout rather than one. \n\n- How many examples were in LTLBase? It might be nice to see an estimate of the effective size of the \"Generated\" dataset with respect to the original distribution.\n\n- I see that in S4.2 you are starting with only 10k samples from LTLBase, but what are you training on exactly for S4.1? If a larger dataset than in S4.2, did you do a similar check for duplicates between the generated examples and the training dataset? How many (if any) of the 30% that are syntactically valid were seen during training?\n\n- It is not clear to me whether the last paragraph of S4.1 is describing a different training regime than the paragraph preceeding it.\n\n- The first sentence seems more suitable for a blog post than a conference paper; in particular, the claim that deep learning is \"on the verge\" of something sounds unscientific. Also, the word \"transitioning\" may not be appropriate, since presumably deep learning will continue to be applied to e.g. image recognition.\n\n- I particularly liked the approach in S4.3 in which an additional objective is introduced that shifts the generated distribution away from the original one. In general, the true objective of data augmentation may be broader than simply modeling the specific distribution of training examples one has at hand. Have the authors considered other \"knobs\" to add that would allow generating more diverse examples, that may provide useful augmentation either for the original distribution or for out-of-distribution evaluations?\n\n- Minor comment: I did not find that it added much to the paper to consider both GANs and WGANs. I would suggest only discussing WGANs.\n This is a strong paper overall: innovative, well-written, and potentially important. I think it will be of interest to anyone applying machine learning in data-sparse symbolic domains."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the authors' efforts but maintains their original score, indicating a neutral overall sentiment. The reference to other reviewers' concerns reinforces this neutrality.",
            "The review expresses several concerns and questions about the paper's methodology, novelty, and practical applications. Phrases like \"I have several concerns,\" \"I'm not entirely sure,\" \"it seems hard,\" \"it is not clear for me,\" \"I don't think,\" and \"The result in Table 3 seems peculiar to me\" indicate a critical and questioning stance. The final paragraph reinforces this with statements like \"I still have two major concerns that make me hesitate to accept the paper\" and \"I think it's not ready for publication.\"",
            "The reviewer expresses a lack of interest in the paper's approach ('I still don't find generating syntactically-correct formulae alone to be particularly interesting') and suggests a key weakness ('it is easy to randomly generate well-formed formulae'). The reviewer also indicates a desire for additional evidence of statistical value, implying the current data is insufficient.",
            "The reviewer expresses concerns and criticisms throughout the review, using phrases like \"I still have several concerns,\" \"I'm not excited,\" \"I can't find ... why the experimental result ... is exciting or worth reporting,\" and \"I'm not convinced.\" The reviewer also poses multiple critical questions and requests further justification.",
            "The review poses questions and seeks clarification, without expressing strong positive or negative opinions.",
            "The review expresses concerns about the motivation and experimental validation of the paper. Phrases like \"not convincing to me\", \"not well-motivated\", and questioning the justification for the approach indicate a negative sentiment.",
            "The review expresses several concerns about the paper's methodology, clarity, and experimental results. The reviewer points out issues with the explanation of concepts, lack of statistical significance in the results, and the absence of comparisons with established datasets. The phrase \"problems with the experimental results\" and the statement that the approach \"lacks novelty\" contribute to the overall negative sentiment.",
            "The review expresses overall positive sentiment, describing the paper as \"strong overall: innovative, well-written, and potentially important.\" The reviewer also states, \"I think it will be of interest to anyone applying machine learning in data-sparse symbolic domains.\""
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical",
            "Critical",
            "Neutral",
            "Critical",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is balanced, acknowledging the authors' efforts (\"I appreciate authors for making clarifications and providing additional details\") while also reaffirming their position and referencing concerns from other reviewers to support their decision.",
            "The review adopts a critical tone by directly pointing out weaknesses and expressing doubts about the paper's claims. It uses phrases like \"several concerns,\" \"not entirely sure,\" \"lacking the discussion,\" \"abstract and introduction seem oversold,\" \"I'm concerned that it is adequate to be tacked with machine learning,\" and \"seems peculiar to me\" to highlight the perceived shortcomings. The reviewer also poses several challenging questions that expose potential flaws in the methodology and reasoning.",
            "The tone is critical, as indicated by phrases like 'I still don't find...particularly interesting' and 'I would still like to see,' which suggest the reviewer has reservations and is not fully convinced by the authors' work.",
            "Critical",
            "The review uses objective language and focuses on understanding the proposed methodology. The questions are direct and relevant, indicating a desire for clarity rather than criticism or support.",
            "The reviewer uses direct and questioning language, challenging the paper's claims and experimental setup. Phrases like \"what is the reason for\", \"How is it inherently different\", and \"is there a justification for\" demonstrate a critical tone.",
            "The review uses direct and questioning language, highlighting specific weaknesses and areas for improvement in the paper. Examples include: \"What do the authors mean by...?\", \"Why use Gaussian noise?\", \"The authors should be more clear\", \"It's very important to know...\", \"It would be best to perform multiple runs and report the standard deviations.\", \"This model does not generate a supervised training dataset since you still need to use existing algorithms/ programs to compute the labels/targets.\" The reviewer also points out a lack of clarity and novelty.",
            "The review provides both positive and critical feedback. While praising the paper's innovation and potential, it also points out areas for improvement, such as the imbalance in domain coverage and clarity in experimental details. The language is generally formal and constructive."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer appreciates the authors' clarifications but maintains their original score, citing that other reviewers share similar concerns, thus showing a consistent stance.",
            "The reviewer consistently questions the motivation and practical value of using GANs for symbolic reasoning tasks, especially given existing classical methods. The reviewer's concerns about novelty, experimental limitations, and lack of practical evidence are maintained throughout the review and post-rebuttal, showing a consistent line of reasoning.",
            "The reviewer consistently expresses their opinion that generating syntactically-correct formulae alone is not particularly interesting and wants to see statistical value in the generated data, maintaining this stance even after the authors' response.",
            "The review is consistent in its critical assessment of the paper. The reviewer initially expresses concerns about the overselling of the approach and the lack of convincing evidence, particularly in the symbolic mathematics domain.  After the authors' response, the reviewer remains unconvinced and further probes the justifications provided, questioning the claims about the benefits of neural models and the choice of alternative loss. The reviewer's line of questioning is logical and focused on seeking stronger evidence and clearer explanations, without contradicting any previous statements.",
            "The review is consistent because it raises logical questions and concerns based on the initial statement about the difficulty of computing solutions compared to verifying predictions. The reviewer's questions about dataset creation and scalability directly stem from the presented motivation and highlight potential challenges in the proposed approach.",
            "The review is consistent in its critique regarding the motivation and justification of the paper. The reviewer repeatedly questions why neural models are used to generate data when rule-based methods are available and the experiments do not convincingly demonstrate the superiority or necessity of the generated data. While the reviewer initially expressed concerns about data quality, this point was retracted, and the core concern about the motivation and justification for the approach remains consistent throughout the review.",
            "The review is consistent in its critique, pointing out areas for improvement while acknowledging the interesting problem. The reviewer provides constructive feedback and suggestions for strengthening the paper's clarity, rigor, and justification. There are no self-contradictory statements.",
            "The review is consistent because while it raises several points for improvement and questions about the paper's details (like domain balance, dataset sizes, clarity of descriptions, and minor stylistic points), it consistently maintains a positive overall assessment of the paper. The reviewer explicitly states multiple times that it is a 'strong paper', 'innovative, well-written, and potentially important', and expresses liking specific aspects of the approach. The criticisms are constructive and aimed at strengthening the paper, not contradicting its overall value."
        ]
    },
    {
        "paper_id": "iclr_2021_ZKyd0bkFmom",
        "paper_title": "Parametric Copula-GP model for analyzing multidimensional neuronal and behavioral relationships",
        "paper_abstract": "One of the main challenges in current systems neuroscience is the analysis of high-dimensional neuronal and behavioral data that are characterized by different statistics and timescales of the recorded variables. We propose a parametric copula model which separates the statistics of the individual variables from their dependence structure, and escapes the curse of dimensionality by using vine copula constructions. We use a Bayesian framework with Gaussian Process (GP) priors over copula parameters, conditioned on a continuous task-related variable. We improve the flexibility of this method by 1) using non-parametric conditional (rather than unconditional) marginals; 2) linearly mixing copula elements with qualitatively different tail dependencies. We validate the model on synthetic data and compare its performance in estimating mutual information against the commonly used non-parametric algorithms. Our model provides accurate information estimates when the dependencies in the data match the parametric copulas used in our framework. Moreover, even when the exact density estimation with a parametric model is not possible, our Copula-GP model is still able to provide reasonable information estimates, close to the ground truth and comparable to those obtained with a neural network estimator. Finally, we apply our framework to real neuronal and behavioral recordings obtained in awake mice. We demonstrate the ability of our framework to 1) produce accurate and interpretable bivariate models for the analysis of inter-neuronal noise correlations or behavioral modulations; 2) expand to more than 100 dimensions and measure information content in the whole-population statistics. These results demonstrate that the Copula-GP framework is particularly useful for the analysis of complex multidimensional relationships between neuronal, sensory and behavioral data.",
        "review_ids": [
            "WCwtFmssM4z",
            "a9SuatVOYRh",
            "j0dDF_7j-Tk",
            "ItIEBJmeoia",
            "wwjJdozAGUQ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This manuscript models the conditional joint distribution over variables by using Copula models and copula vines.  The experimental data shows that when the observed variables are highly correlated that the proposed approach improves estimation of entropy over competing benchmark approaches (MINE and KSG) when the variables are highly correlated. Synthetic results demonstrate good improvements, and application to real scientific data seems promising.\n\nStrengths:\nApplication to real, novel scientific data shows potential utility of the model.\nSynthetic results show a good improvement over competing methods, albeit in a limited setup (highly correlated variables)\nMixtures of copulas seems an effective way to produce model complexity, and by linking it to a GP can make sure that it's smooth over the conditioned variable x.\n\nWeaknesses:\nNot all modeling steps are clear.  In particular, the interaction between the GP prior and the model selection step is undescribed.  Since there are multiple ways to do this, needs a full description.\nNo comparison to more neuroscience focuses techniques.\nExperimental setup and utility is not fully described.\nLacks ablation studies to elicit key model components.\n\nQuestions:\nThe choice to make the link function on the different copula families all dependent on the same $f$ seems like a strange and limiting choice.  Why was this choice made?\n\nThe choice to model calcium trace level and not neural spiking seems mathematically convenient for this method, but it's not clear to me that this is the correct scientific choice.  Typically, calcium imaging traces are preprocesses to extract spikes.  Is there a scientific rationale for using the raw data?  Or was this primarily motivated by avoiding discrete measurements that would be harder to model in the copula?\n\nPlease describe more of the scientific setup.  For example, why is there any relationship to licks outside of the reward?\n\nWhat is the scientific question on this neuroscience application?  Why is entropy relevant, rather than using one of the many predictive problems?\n\nUpdate after author response:\nMost of my methodological concerns have been address (except for the ablation studies).  The scientific application here is not super well-motivated.  It would improve the article greatly to show a greater utility (or at least, clearly describing future utility for answering scientific questions).  It is mentioned that \"A full application of the method to study the dependence of contextual signals in mouse visual cortex will be the focus of a follow-up publication.\"  That's vague; it would be nice to at least clearly discuss how this could be used to facilitate or enhance these scientific experiments.\n",
            "This largely seems reasonable, and the additional details have enhanced the manuscript. \n\nRe: \"Our copula mixture models are parameterized with 2K-1 Gaussian processes (s), so each copula element depends on its own  (K independent GPs in total) + the mixture of copulas is parameterized by (K-1) additional mixing parameters.\"  That makes way more sense; however, you should revise that section for clarity, because that is not how I read the section, nor is it when I reread it.  I would add a comment under bivariate copula families because the notation is overloaded.\n\nRegarding \"A big advantage of Bayesian models is that the role of each of the components (copula elements) can be studied without the need for ablation.\"  I strongly disagree.  You definitely can post-hoc analyze these components, but there are strong effects of the prior and it's unclear which part of the model is necessary.  It is standard in the Bayesian modeling literature to show improvements in predictions or estimations as you add or remove parts of the model (e.g., a hierarchical structure improves over an independent structure). ",
            "The authors develop a Gaussian process vine copula model, very much in the flavor of the modeling approach of Lopez-Paz et al. (2013). The improvement to the earlier work seems to be a framework for flexible copula modeling including a copula mixture model, approximate inference, model selection, and calculation of mutual information. The paper on its own is a modest improvement on existing work and is both an engineering accomplishment and has the potential for a useful model. The paper is exceptionally well-written and clear. I found it a breeze to read and I credit the authors for that. However, both the validation and the motivation for the model (namely characterizing the probabilistic relationships for neural and behavioral variables) seems particularly thin and could be substantially improved. The paper falls short on these points to such a degree that I am hesitant to recommend acceptance since it notably impacts my evaluation of the significance of the works\n\n## Major points:\n\nFirst, the authors claim (page 7, paragraph 2) that their model out-performs all non-parametric methods. This is by no means obvious form Fig2b,c. Moreover, If the model performs \"similarly\" to MINE, then it is maybe not worth using when a convenient technique already exists. What is the advantage of the present model?\n\nSecond, the authors show that the estimation of mutual information is only unbiased for a narrow range  of distributions (Gaussian or transformed Gaussian for small dimensions according to Fig 2) and fails for the heavy-tailed Student's-T. However, many neural and behavioral variables are themselves heavy-tailed and the authors did not demonstrate that the real data are sub-gaussian.\n\nThird, it seems like there was a wasted opportunity with this paper. The authors spent $\\approx$ 2/3 of a page discussing the estimation of mutual information without motivating why that was a good example metric that could be derived from their model. This modeling framework is an opportunity to determine virtually any expectation over the entire distribution and it is entirely possible that MI is neither all that interesting, nor does it play to the strengths of the model. They then describe changes  to the pairwise distributions of variables from the copula model but we didn't need the copula model to estimate.\n\nFinally, what are we to make of the real data results *vis-a-vis* the validation experiments with simulated data in Fig 2? Besides what I mentioned above regarding the tails of the real data, its not clear that the variance explained wouldn't behave differently. Could the authors report the variance explained for their simulation experiments?\n\n## Minor points: \nThe authors state (page 7, last paragraph) That the \"stimulus-related changes in the joint variability of the two neuronal signals are commonly described as _noise correlations_.\" But, isn't that the definition of _signal correlations_?\n\nthe authors state (page 8, paragraph 5) \n>The Copula-GP \u201cestimated\u201d (dashed line) almost perfectly matches the \u201cintegrated\u201d result, which suggests that the model was able to tightly approximate both $p(u^x|x)$ and $p(u^x)$, and, as a result, $I(x, \\{u|^x_{i<N} \\})$.\n\nHowever, it is not clear at all that the later statement regarding $I(x, \\{u|^x_{i<N} \\})$ follows from the former. In fact Figure 2 demonstrates that the integrated result may not estimate $I(x, \\{u|^x_{i<N} \\})$ well at all.",
            "- This is an interesting neuroscience application where Copula estimation has shown to be effective. \n\n- Having said that, I believe that the technical novelty is minimal. To the authors' credit, they did not claim a huge theoretical edge. They honestly reported the findings of the paper. \n\n- The paper is well written. Ideas flow very neatly. \n\n- p2: \"It was previously shown that such a combination of parametric copula models with GP priors ...\": Precisely.. Therefore, given the work by Hernandez-Lobato et al. (2013), as well as the works which have eventually built on top of it, novelty of the proposed method is minimal. I am not saying the application is not useful though.  \n\n- Along the same lines from above, equations (2), (3) and (4) which depict the core of the method, are all taken from seminal previous works of copula mixture estimation. \n\n- p3: \"Since none of the aforementioned families alone could describe such conditional dependency, we combined multiple copulas into a linear mixture model\": Is it possible to elaborate a little bit more on whether this is actually the best technical choice here? For instance are there any side effects (e.g. computational) resulting from using this linear mixture? \n\n",
            "The authors exploit the expressive power of Copula mixtures to model time-varying multi-modal data, and employ Gaussian Processes to model the time-varying copula parameters. They demonstrate the efficacy of their method using information theoretic metrics on a synthetic dataset and a real-world joint neural-behavioral dataset from a neuroscience experiment.  Results demonstrate that the proposed techniques are comparable to the state of the art nonparametric methods, while being more scalable due to the use of stochastic optimization based methods that are commonly used with parametric methods. \n\nThe paper was quite thorough in the motivation, development and empirical analysis of the proposed technique. The use of Copulas, that are commonly employed in the Finance community, but are rare in statistical neuroscience, should interest the more theoretically inclined reader. Given the accelerating trend of collecting long-term joint neural and behavior in experimental neuroscience, the authors make an interesting and timely contribution to the statistical neuroscience literature.\n\nI found that the motivations of the paper were difficult to extract from the Introduction, as there was substantial use of jargon and the intuition behind the technical results were not accessible. To encourage the adoption of their methods in the neuroscience community, the authors should consider improving the readability of their manuscript by making it more friendly to the non-statistician reader. \n\n"
        ],
        "sentiment": [
            "Neutral",
            "Neutral",
            "Negative",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses both positive and negative aspects of the manuscript, acknowledging strengths and weaknesses without leaning heavily towards a positive or negative overall assessment. The reviewer also poses questions indicating areas needing clarification.",
            "The review expresses a mix of agreement and disagreement. It starts by acknowledging improvements but then points out areas needing clarification and expresses strong disagreement with a specific claim.",
            "The reviewer expresses hesitation to recommend acceptance due to concerns about validation, motivation, and significance of the work. Phrases like \"modest improvement,\" \"particularly thin,\" \"falls short,\" and questioning the advantages of the model contribute to the negative sentiment.",
            "The review acknowledges the paper's strengths (well-written, interesting application) but also points out a lack of technical novelty and raises questions about the methodological choices. This mix of positive and negative aspects results in an overall neutral sentiment.",
            "The review expresses overall positive feedback, highlighting the paper's thoroughness, the interesting use of Copulas, and the timely contribution to the literature. While it points out areas for improvement, the overall assessment is favorable."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The review presents both strengths and weaknesses of the paper, using phrases like \"Strengths:\" and \"Weaknesses:\" to structure the feedback. The reviewer also asks clarifying questions, indicating a desire to understand the work better rather than simply criticizing it. The update after the author response shows a willingness to adjust the assessment based on new information.",
            "While acknowledging improvements, the review includes phrases like \"you should revise that section for clarity\" and \"I strongly disagree,\" indicating a critical stance. The reviewer also points out potential issues with the model's interpretation and the need for further validation, highlighting areas of concern.",
            "The review directly challenges the authors' claims, questions their methodology, and points out inconsistencies between the model's performance and the authors' interpretations. The reviewer uses phrases like \"by no means obvious,\" \"maybe not worth using,\" and \"fails for\" to express their criticism.",
            "The tone is balanced, as it combines praise for the writing and honesty of the authors with criticism regarding the novelty and methodological choices. Phrases like \"To the authors' credit\" and \"I am not saying the application is not useful though\" demonstrate a balanced perspective.",
            "The review acknowledges the strengths of the paper (thoroughness, interesting methodology, timely contribution) while also offering constructive criticism (difficult to extract motivations, substantial use of jargon). This balanced approach indicates a neutral and objective tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it provides a balanced assessment, highlighting both strengths and weaknesses of the manuscript. The criticisms are constructive and focused on improving the clarity, completeness, and scientific motivation. The update after author response logically follows the initial review, acknowledging improvements while maintaining some concerns.",
            "The review is consistent because it acknowledges the improvements in the manuscript while also pointing out specific areas that need further clarification and revision. The reviewer expresses disagreement on a particular point but provides a clear justification for their stance, maintaining a consistent and constructive tone throughout the review.",
            "The review is consistent in its assessment. It acknowledges the paper's strengths in writing and technical aspects but consistently criticizes the weak validation, thin motivation, and questionable claims of outperformance. The reviewer's concerns about the limitations of the model and the lack of strong justification for its application are reiterated throughout the major and minor points, leading to a coherent and consistent negative evaluation regarding the paper's significance and suitability for acceptance.",
            "The review is consistent because it acknowledges the positive aspects of the application and the writing quality, but consistently points out the lack of significant technical novelty. The reviewer repeatedly emphasizes that the method builds upon existing works and equations, thus maintaining a consistent argument about the minimal technical contribution despite the application's usefulness and clarity of presentation.",
            "The review is consistently positive about the technical contribution and novelty of the method, while suggesting improvements for readability and accessibility to a broader audience. The reviewer highlights both strengths and weaknesses without contradicting themselves."
        ]
    },
    {
        "paper_id": "nips_2021_nlEQMVBD359",
        "paper_title": "SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness",
        "paper_abstract": "Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Do-Guk Kim, Jinwoo Shin",
        "review_ids": [
            "u2ZbliDay8c",
            "gWt4J_NkFWL",
            "gcKF8jWmN92",
            "YfWeBigznNX",
            "F3sYK3s_772",
            "PiVWRwEva-Q",
            "ibmuagIiXCE"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper looks to improve upon SmoothAdv - a methodology that employs adversarial training on a smoothed classifier to further improve certified robustness. The methodology employed looks to tackling overconfident examples rather than adversarial examples by utilising MixUp. They demonstrate that we can combine MixUp with SmoothAdv to improve certified accuracy.  UPDATE:\nThe authors have made clearer the motivation of overconfident examples, as a result I am increasing my score to 6. \n\nStrengths:\n- This paper demonstrates we can combine MixUp and adversarial training to improve certified accuracy for smoothed classifiers. \n\nWeakness and Questions:\n- One main concern I have is that the improvements shown in Tables 1 and 2 seems marginal, for example SmoothMix obtains 55.3% accuracy for CIFAR-10 when $\\sigma$=0.25 whereas for Consistency it is 55.2%. Given these small margins, the authors should also report error bars on the approximate certified accuracy.\n\n- The main motivation regarding why overconfident examples should be penalised is talked about in Section 3.1. I found this section a bit confusing to read, in particular the two bullet points seems speculative.  I've read bullet point one several times and I'm still unclear what it is trying to say. Further, both bullet points refers to Figure 2 which pinpoints to just two examples. To ensure these are not cherry-picked or anecdotal examples, the authors should show statistically that smoothed classifiers is overconfident towards the direction of adversarial perturbation. \n\n- It seems throughout when SmoothAdv was used in conjunction, the attack used was only one-step. This has been to known to cause gradient obfuscation / catastrophic overfitting especially when the perturbation radius is large, given that the adversarial example here is unrestricted it would be good to see the results for multi-step attacks as well. Otherwise it is unclear what the one-step attack is doing.\n\n- From Figure 5/6 it would appear that SmoothMix performs worse when the certification radius is smaller compared to SmoothMix and Consistency. This doesn\u2019t seem desirable, can the authors explained why this is? \n Yes",
            " Thank you for the follow-up answers, clarifications and experiments. I think most of my concerns have been adequately addressed. In particular, I now understand more about the paper's motivation, although I must also state that yet some of the claims could still be up for discussion (as other reviewers also pointed out their somewhat speculative nature).\n\nGiven this, I still consider this paper should be accepted, and so I will keep my rating.\n",
            " Thank you for your response and clarifications.",
            " I appreciate the authors for providing a detailed response. See below for my remaining concerns.\n\n**Re Q1.** I get the point of using average certified radius (ACR) as it is the most commonly-used metric in the literature, but that does not justify the disadvantages of the metric. I believe the main argument is whether these disadvantages lead to a false interpretation of results and how practical is the incorporation of distant adversarial examples during training. Further, the author's response of \"samples with zero ACR scores are unutilized in the analysis\" is unclear as the definition of ACR (Line 212-213) states that it's calculated for only the correctly classified samples. Moreover, the highest improvement for SmoothMix is in cases with $\\sigma=1$, but that leads to a large drop in testing accuracy. I understand an inherent trade-off between robustness and performance, but this trade-off seems more prominent for SmoothMix than the Gaussian baseline.\n\n**Re Q2.** The authors mention that SmoothMix can boost previous methods when the task is more challenging, e.g., when higher  $\\sigma$ is used. The underlying principle behind adversarial examples is making infinitesimal perturbations to a sample. Does using higher $\\sigma$ break this adversarial criterion? ",
            "The paper proposes a new method for training models to achieve better certified L-2 robustness. To this extent, SmoothMix generates adversarial examples by relaxing the perturbation budget and minimizes a mixup loss. The proposed procedure identifies instances classified with high confidence and near-off-class samples as causes of limited robustness in smoothed classifiers and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Finally, the method obtains improved certified accuracy for certain radii \u03b5 that each adversarial perturbation must be in.  Strengths\n1. The proposed method is motivated and well-explained.\n2. Evaluation results using standard datasets, such as MNIST (Table 1),  Cifar-10 (Table 2), and ImageNet (Table 4), show the effectiveness of SmoothMix across different smoothing factors.\n3. Extensive ablation studies aid in understanding the utility of SmoothMix.\n\nOpen concerns\n1. Average certified radius (ACR) is used to quantify the effectiveness of SmoothMix. However, it may be sensitive to particular adversarial samples, i.e., a few generated adversarial instances could have a high ACR score and thus can mislead the global interpretation of ACR. Analyzing the distribution of ACR scores might shed more light on this point.\n2. SmoothMixs' performance on the certified accuracy at different radii and epsilons is not consistent across datasets. In addition, it achieves marginal improvement in most cases. This is observed even for easier datasets like MNIST, where we see the difference gap getting bigger for higher epsilons (this is also arguable as to how practical it is to generate adversarial samples using such larger perturbations). Yes, the authors have clearly detailed the limitations and the broader societal impact of the work.",
            "The paper offers a novel training scheme\u2014called SmoothMix\u2014for fitting deep net classifiers with improved adversarial robustness while maintaining accuracy to the extent possible. The key idea is to combine the mixup loss with smooth classifiers. The former interpolates the input sample with its adversarial perturbation, designed to \u201cmislead\u201d the smoothed classifier. The effectiveness of the proposed idea was studied on MNIST, CIFAR10, and ImageNet, showing competitive performance to that of state-of-the-art adversarial defense strategies (Gaussian smoothing, stability training, adversarial smoothing, MACER, and consistency).  Originality\n\nThe idea of combining the mixup loss with adversarial smoothing is novel. Also, it is nicely motivated. First, it was demonstrated (visually) that adversarial perturbations of a smoothed classifier may have semantic changes that either transform the input to another class or removing relevant content for the current class. Second, the smoothed classifier tends to assign to adversarially-crafted examples higher confidence scores compared to the clean input.\n\nMotivated by the above observations the authors hypothesized that miscalibration (although not defined precisely) degrades the certified robustness of smoothed classifiers. They propose to penalize over-confident predictions by interpolating the original image (and label) with its unrestricted adversarial example, assigning uniform confidence to the latter. This approach encourages the smoothed classifier to produce a uniform confidence score for adversarial samples.\n\nQuality\n\nThe paper is well written, the exposition of the idea is clear, the experiments follow existing literature.\n\nMain concern\n\nWhile the method attains state-of-the-art performance, it is hard to judge whether the new results are significantly better than existing ones, e.g., they are only slightly better than adversarial smoothing. It will be interesting to see a detailed performance analysis, e.g., a report of class-conditional performance metrics could perhaps communicate better the advantages of the proposed method.    \n\nSignificance\n\nCertified adversarial robustness is an active area of research that sheds light on the robustness of deep nets. The proposed method achieves state-of-the-art performance on benchmark datasets. It is likely that---at least in the short term---this new method will have a noticeable impact. It will be a plus to explain that the SmoothMix approach has an additional hyper-parameter to tune (\\alpha).",
            "The paper observes a connection between the confidence with which a smooth model classifies an input and the robustness radius with which the input's prediction can be certified. In particular, the paper argues that the model's over-confidence on some inputs can be a source for detrimental effects on the certified radius of other (nearby) instances. Inspired by this, the paper proposes to find over-confident near off-class samples (somewhat) near training instances, and conduct mixup training on this pair of instances as a way of improving the certified accuracy of the resulting smoothed classifier. The proposal can be regarded as another way of adversarial training that is better crafted for smoothed classifiers. Experiments conducted on MNIST, CIFAR and ImageNet show marginal but consistent improvements in certified accuracy.  Strengths:\n+ The paper's object of study, i.e. modifying the training of smooth classifiers for enhancing the classifier's certified accuracy, is of interest to the adversarial robustness and certification communities, and in someway of interest for the deep learning community in general.\n+ The paper's writing is mostly clear, following a narrative that properly exposes the paper's motivation, relation to previous works and the rationale behind the experiments that are conducted. Figures are also well-built and clear.\n+ The experimental protocol is standard and sensible.\n+ Experimental results are mostly consistent across factors being studied (datasets, parameters, etc.).\n+ The paper's motivation and formulation (although as I expose next I do not entirely understand or agree with) draws on previous works but is for the largest part original.\n\nWeaknesses that affect my rating:\n+ In general I find two main issues with the paper: (1) the motivation is still not completely clear to me, and (2) the experimental results are somewhat not significant enough. In particular, I find the motivation rather troubling, as I am not entirely sure about the over-confidence item raised in the paper: is this over-confidence truly a common phenomenon in *smoothed* classifiers? Given the relation between smoothed classifiers and \"Lipschitzness\" I would have initially thought of the opposite, and so I think this (fundamental) claim in the paper would require experimental validation. Further, while the paper repeats several times that the adversaries are \"unconstrained\" in practice that does not appear to be the case, precisely because part of the importance of finding adversarial examples is that they are near the original inputs, so perhaps a more precise term for this would simply be adversarial examples with looser constraint. \n\nNext, I outline some questions that better expose what I find unclear about the paper, and so I would be thankful for further explanation.\n+ Eqn. (8) implies that the adversarial examples that the optimization procedure most likely finds are the ones for which the label $y$ has low score. How is this quantity related to \"over-confidence\", could it not be the case that simply the classifier will be not confident towards $y$ but uniformly confident towards the rest of the classes? Why would over-confidence emerge here?\n+ I do not understand the claims made in the bullet point of L138. Fig. 2 compares apparently two cases: (a) when the input suffers some modifications so that the label changes and (b) when the modifications to the input can be simply described as removing characteristic features of the class being described (the deer's antlers, in this case). How are these two cases related to how sensible it is for the classifier to keep its low confidence to the original class? Is the paper stating that the classifier's confidence for the true class when making predictions over the original input should be low because it should be aware of how few changes would be required to change the semantic class? I do not think I would agree with such a premise: even though zebras and horses look very much alike, when perceiving a zebra we can be rather certain it is a zebra and not a horse, precisely because of its *very* distinctive features.\n+ In general, while the experimental results are mostly consistent, the improvements are rather marginal, and so I am unsure whether they are significant.\n+ My understanding of one of this work's main contributions is observing a relation between the certification of a classifier and its calibration. Because of this observation, the paper proposes to improve calibration through a mixup-inspired procedure so that certification is also improved. Is my understanding correct? If so, would not a valid baseline be other previous calibration techniques? Or is there some issue with this approach? If there is no issue, how do other calibration techniques perform towards improving certification?\n\n\nWeaknesses that do not affect my rating but should be addressed if possible:\n+ L6: of smoothed classifier\n+ L8: it trains convex combinations? or *on* convex combinations? \n+ In Fig. 1's caption: \"mixup\" or \"the mixup\"?\n+ L37: one of important\n+ L57: they goes\n+ L85: construct\n+ L142: \"their\" or \"its\"?\n+ L144: have\n+ L146: \"a semantically off-class samples those to be labeled as the uniform confidence\"\n+ L151: those\n+ Fig. 2's caption is not self-contained: what the authors want to express with the figure is unclear from the caption. In particular, what is meant by \"in/out-of-class translation\"? I think this caption requires more explanation.\n+ In L159, I do not understand what the paper means by \"mixing the uniform confidence to them\". What does \"uniform confidence\" mean, and to whom is it being \"mixed\"? Similarly, in L166 the paper says \"direct supervision of the uniform confidence on it\", which I do not understand. Could the authors please clarify what was meant by these expressions?\n+ Table 1's caption: \"those\"\n+ The tables reporting the experiments follow a protocol of adding bold-face and underline that I think is confusing: usually whenever readers see a bold-faced number (I think) they think it is the largest, which is not the case in this paper. I would urge the authors to reconsider this protocol. (For instance the results of other methods are never underlined, which can misguide th reading of results)\n+ Table 1's caption: \"improves\" or \"outperforms\"?\n+ Fig. 4's caption: \"plots\"\n+ L195: trainig\n+ Fig. 5's caption: \"plots\"\n+ L230: improve\n+ L240: those\n+ For the experiment reported in L285, how were the adversarial examples computed? Were both restricted or both unrestricted? Or was there a mismatch in this? If there was a mismatch, I think the results are not interpretable, as the process of finding an adversary for the Gaussian model is constrained and so biased towards small values, while the one for SmoothMix is not.\n+ I encourage the authors to rename this method, as there is a paper with the same name: \"SmoothMix: a Simple Yet Effective Data Augmentation to Train Robust Classifiers\", Lee et al., CVPR20 \n Yes, they have addressed it."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses both positive aspects ('Strengths') and negative aspects ('Weakness and Questions') of the paper. While the reviewer acknowledges the improvement in certified accuracy, they also raise concerns about the marginal improvements, unclear motivation, and potential issues with the experimental setup.",
            "The reviewer explicitly states that they \"consider this paper should be accepted\" and will \"keep my rating\". This indicates a positive overall assessment.",
            "The reviewer expresses gratitude, indicating a positive sentiment.",
            "The review expresses remaining concerns and questions the authors' justifications and interpretations of results. Phrases like \"disadvantages of the metric\", \"false interpretation of results\", \"unclear\", \"large drop in testing accuracy\", and \"break this adversarial criterion\" indicate a critical and questioning stance.",
            "The review expresses overall positive sentiment. It highlights several strengths of the paper, including the method's motivation, clear explanation, effectiveness demonstrated through evaluation results, and comprehensive ablation studies. While it raises some concerns, the strengths outweigh the weaknesses.",
            "The review acknowledges the novelty and motivation of the proposed method, the clarity of the writing and experiments, and the potential impact of the research. While it raises a concern about the significance of the improvement over existing methods, the overall tone suggests a positive assessment.",
            "The reviewer expresses concerns about the paper's motivation, experimental results, and clarity. Phrases like \"not completely clear to me\", \"somewhat not significant enough\", \"I do not understand\", and \"improvements are rather marginal\" indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The review uses direct questions and critical feedback to point out weaknesses in the paper. Phrases like 'One main concern I have,' 'a bit confusing to read,' 'seems speculative,' and 'This doesn\u2019t seem desirable' indicate a critical tone. The reviewer also suggests specific improvements like reporting error bars and showing statistical evidence.",
            "The reviewer expresses gratitude for the authors' efforts (\"Thank you for the follow-up answers, clarifications and experiments.\") and acknowledges that their concerns have been \"adequately addressed\". While mentioning remaining concerns, the overall tone is encouraging and in favor of acceptance.",
            "The phrase \"Thank you\" suggests a supportive and appreciative tone.",
            "The tone is critical due to the direct questioning of the authors' methodology and results. Phrases like \"does not justify\", \"disadvantages\", \"unclear\", and raising concerns about a \"large drop in testing accuracy\" convey a critical evaluation.",
            "The review adopts a balanced tone by acknowledging both the strengths and weaknesses of the paper. It uses phrases like 'Strengths' and 'Open concerns' to clearly delineate positive and negative aspects. The language is objective and constructive, avoiding overly critical or effusive language. The reviewer provides specific examples and suggestions for improvement, indicating a desire to help the authors strengthen their work.",
            "The review provides both positive feedback (e.g., \"novel idea\", \"nicely motivated\", \"well written\", \"achieves state-of-the-art performance\") and constructive criticism (e.g., \"hard to judge whether the new results are significantly better\", \"It will be interesting to see a detailed performance analysis\"). This blend of positive and negative feedback indicates a balanced tone.",
            "The review identifies weaknesses in the paper's motivation, experimental results, and clarity, posing critical questions and suggesting improvements. Phrases like \"I find the motivation rather troubling\", \"I do not understand the claims made\", and \"the improvements are rather marginal\" contribute to this critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially raises concerns about marginal improvements, unclear motivation, one-step attack, and performance at small radii. The update acknowledges that the authors clarified the motivation, leading to a score increase. However, the reviewer still maintains the other concerns in the 'Weakness and Questions' section. This indicates a consistent evaluation where the reviewer acknowledges improvements while still highlighting remaining weaknesses, rather than contradicting themselves.",
            "The reviewer expresses that most of their concerns have been addressed and maintains their positive recommendation for acceptance, despite acknowledging that some claims might still be speculative. The reviewer's final decision aligns with their overall assessment.",
            "The review is a short positive statement acknowledging the author's response and clarifications. There are no contradictory points within this short review.",
            "The review is consistent as it raises valid concerns and questions based on the authors' response to previous queries (Q1 and Q2). The reviewer logically argues against certain aspects of the authors' methodology and explanations, without presenting any self-contradictory statements. The concerns raised in both Re Q1 and Re Q2 are distinct and relevant to the topic, showing a consistent line of reasoning.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper without contradicting itself. The reviewer acknowledges the positive aspects such as clear motivation, good explanation, effective evaluation, and helpful ablation studies. Simultaneously, the reviewer raises valid concerns about the evaluation metric (ACR) and the inconsistent/marginal performance across datasets. These points are presented as separate aspects of the paper's evaluation, leading to a consistent and balanced review.",
            "The review is consistently positive, highlighting the novelty, motivation, quality, and significance of the work. The reviewer's concern is framed as a suggestion for improvement rather than a criticism of the core method or its results. There are no contradictory statements within the review.",
            "The review is consistent because it clearly separates strengths and weaknesses, providing specific and logically connected points without contradictions. The reviewer maintains a balanced and constructive tone throughout the text."
        ]
    },
    {
        "paper_id": "iclr_2018_ByJbJwxCW",
        "paper_title": "Relational Multi-Instance Learning for Concept Annotation from Medical Time Series",
        "paper_abstract": "Recent advances in computing technology and sensor design have made it easier to collect longitudinal or time series data from patients, resulting in a gigantic amount of available medical data. Most of the medical time series lack annotations or even when the annotations are available they could be subjective and prone to human errors. Earlier works have developed natural language processing techniques to extract concept annotations and/or clinical narratives from doctor notes. However, these approaches are slow and do not use the accompanying medical time series data. To address this issue, we introduce the problem of concept annotation for the medical time series data, i.e., the task of predicting and localizing medical concepts by using the time series data as input. We propose Relational Multi-Instance Learning (RMIL) - a deep Multi Instance Learning framework based on recurrent neural networks, which uses pooling functions and attention mechanisms for the concept annotation tasks. Empirical results on medical datasets show that our proposed models outperform various multi-instance learning models.",
        "review_ids": [
            "Hk2mNy-gG",
            "rkcWXX9gf",
            "Hyu6DTogG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper addresses the classification of medical time-series data by formulating the problem as a multi-instance learning (MIL) task, where there is an instance for each timestep of each time series, labels are observed at the time-series level (i.e. for each bag), and the goal is to perform instance-level and series-level (i.e. bag-level) prediction.  The main difference from the typical MIL setup is that there is a temporal relationship between the instances in each bag.  The authors propose to model this using a recurrent neural network architecture.  The aggregation function which maps instance-level labels to bag-level labels is modeled using a pooling layer (this is actually a nice way to describe multi-instance classification assumptions using neural network terminology).  An attention mechanism is also used.\n\nThe proposed time-series MIL problem formulation makes sense.  The RNN approach is novel to this setting, if somewhat incremental.  One very positive aspect is that results are reported exploring the impact of the choice of recurrent neural network architecture, pooling function, and attention mechanism.  Results on a second dataset are reported in the appendix, which greatly increases confidence in the generalizability of the experiments.  One or more additional datasets would have helped further solidify the results, although I appreciate that medical datasets are not always easy to obtain.  Overall, this is a reasonable paper with no obvious major flaws.  The novelty and impact may be greater on the application side than on the methodology side.\n\nMinor suggestions:\n\n-The term \"relational multi-instance learning\" seems to suggest a greater level of generality than the work actually accomplishes.  The proposed methods can only handle time-series / longitudinal dependencies, not arbitrary relational structure.  Moreover, multi-instance learning is typically viewed as an intermediary level of structure \"in between\" propositional learning (i.e. the standard supervised learning setting) and fully relational learning, so the \"relational multi-instance learning\" terminology sounds a little strange. Cf.:\nDe Raedt, L. (2008). Logical and relational learning. Springer Science & Business Media.\n\n-Pg 3, a capitalization typo: \"the Multi-instance learning framework\"\n\n-The equation for the bag classifier on page 4 refers to the threshold-based MI assumption, which should be attributed to the following paper:\nWeidmann, N., Frank, E. & Pfahringer, B. 2003. A two-level learning method for generalized multi-instance problems. In Proceedings of the 14th European Conference on Machine Learning,\nSpringer, 468-479.\n(See also: J. R. Foulds and E. Frank. A review of multi-instance learning assumptions. Knowledge Engineering Review, 25(1):1-25, 2010. )\n\n- Pg 5, \"Table 1\" vs \"table 1\" - be consistent.\n\n-A comparison to other deep learning MIL methods, i.e. those that do not exploit the time-series nature of the problem, would be valuable.  I wouldn't be surprised if other reviewers insist on this.",
            "This paper proposes a framework called 'multi-instance learning', in which a time series is treated as a 'set' of observations, and label is assigned to the full set, rather than individual observations. In this framework, authors propose to do set-level prediction (using pooling) and observation level predictions (using various attention mechanisms). \nThey test their approach in a medical setting, where the goal is to annotate vital signs time series by clinical events. Their cohort is 2014 adults time series (average length 4 time steps), and their time series has dimension of 21 and their clinical events have dimension of 26. Their baselines are other 'multi-instance learning' prior work and results are achieved through cross-validation. A few of the relevant hyper-parameters are tuned and some important hyper-parameters (i.e. number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. \n\nOriginality - I find the paper to be very incremental in terms of originality of the method. \n\nQuality and Significance - Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments. Given that results are reported via cross-validation and without a true held-out dataset, and given that a number of hyperparameters are not even tuned, it is difficult to be confident that the differences of all the methods reported are significant. \n\nClarity - The writing has good clarity.\n\nMajor issues with the paper: \n- Lack of reliable experiment section. Dataset is too small (2000 total samples), and model training is not described with enough details in terms of hyper-parameters tuned. \n",
            "==== Post Rebuttal ====\nI went through the rebuttal, which unfortunately claimed a number statements without any experimental support as requested. The revision didn't address my concerns, and I've lowered my rating.\n\n==== Original Review ====\nThis paper proposed a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. The paper also explored integrating RMIL with various attention mechanisms, and demonstrated its usage on medical concept prediction from time series data.\n\nThe biggest technical innovation in this paper is it combines recurrent networks like Bi-LSTM with MIL to model the relations among instances. Other than that, the paper has limited technical innovations: the pooling functions were proposed earlier and their integration with MIL was widely studied before (as cited by the authors); the attention mechanisms are also proposed by others.\n\nHowever, I am doubtful whether it\u2019s appropriate to use LSTM to model the relations among instances. In general MIL, there exists no temporal order among instances, so modeling them with a LSTM is unjustified. It might be acceptable is the authors are focusing on time-series data; but in this case, it\u2019s unclear why the authors are applying MIL on it. It seems other learning paradigm could be more appropriate.\n\nThe biggest concern I have with this paper is the unconvincing experiments. First, the baselines are very weak. Both MISVM and DPMIL are MIL methods without using deep learning features. It them becomes very unclear how much of the gain on Table 3 is from the use of deep learning, and how much is from the proposed RMIL.\n\nAlso, although the authors conducted a number of ablation studies, they don\u2019t really tell us much. Basically, all variants of the algorithm perform as well, so it\u2019s confusing why we need so many of them, or whether they can be integrated as a better model.\n\nThis could also be due to the small dataset. As the authors are proposing a new MIL learning paradigm, I feel they should experiment on a number of MIL tasks, not limited to analyzing time series medical data. The current experiments are quite narrow in terms of scope.\n"
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer states that the proposed formulation 'makes sense,' the approach is 'novel,' and the paper is 'reasonable with no obvious major flaws.' The reviewer also appreciates the thoroughness of the experiments and the inclusion of results on a second dataset.",
            "The review expresses concerns about the paper's originality, the quality and significance of the experiments due to the small cohort size and lack of hyperparameter tuning, and the reliability of the experiment section. Phrases like 'very incremental,' 'difficult to reliably access quality,' 'difficult to be confident,' and 'lack of reliable experiment section' indicate a negative assessment.",
            "The reviewer explicitly states that the revision 'didn't address my concerns' and they 'lowered my rating.' The original review also expresses doubts and concerns about the paper's approach and experiments."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses encouraging language such as 'nice way,' 'very positive aspect,' and 'greatly increases confidence.' The reviewer also provides constructive suggestions for improvement in a polite and helpful manner.",
            "The tone is critical, pointing out specific weaknesses and shortcomings in the paper, using phrases like 'very incremental,' 'difficult to reliably access quality,' 'lack of additional dataset,' 'difficult to be confident,' and directly stating 'Major issues with the paper.' These phrases clearly convey a critical perspective on the work.",
            "The review uses critical language, pointing out weaknesses in the paper's technical innovation ('limited technical innovations'), questioning the appropriateness of using LSTM ('unjustified'), and expressing concerns about the experiments ('unconvincing experiments', 'baselines are very weak', 'small dataset', 'experiments are quite narrow')."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as it maintains a positive tone while providing constructive criticism and suggestions for improvement. It highlights the strengths of the paper, such as the novel approach and comprehensive experiments, and points out minor areas for improvement without contradicting its overall positive assessment.",
            "The review is consistent. The reviewer raises concerns about the originality and the experimental setup due to the small dataset and lack of hyperparameter tuning. These points are consistently negative about specific aspects of the paper (originality and experimental validity) while also acknowledging the clarity of writing as a positive aspect. There are no contradictions within the review.",
            "The reviewer's concerns in the original review, such as weak baselines, questionable use of LSTM for general MIL, and narrow experimental scope, were not addressed in the rebuttal. The reviewer consistently maintained a critical stance and lowered the rating because the rebuttal lacked experimental support to alleviate these concerns."
        ]
    },
    {
        "paper_id": "nips_2021_GuTIBjOSIw8",
        "paper_title": "When Are Solutions Connected in Deep Networks?",
        "paper_abstract": "Quynh N. Nguyen, Pierre Br\u00e9chet, Marco Mondelli",
        "review_ids": [
            "0E04SqfJvbh",
            "nAWnxF_tZV",
            "C-vFjJnlcJy",
            "qNQDZ0l8M4K",
            "EMNkpRLYqq",
            "m1721dAByVC"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors propose a theoretical explanation to the mode connectivity phenomena observed in neural networks. Specifically, it is shown that any two points in parameter space may be connected by a piece-wise linear path of low loss.  Technically this is done by assuming that there exists a tradeoff between feature quality and over parameterization at every layer. The authors note that this tradeoff assumption is milder than the dropout stability assumption made in previous work. Given this assumption, the authors improve upon the conditions for the existence of level set connectivity by requiring that the last two hidden layers only contain O(\\sqrt{N}) neurons, instead of O(N).  I am mainly concerned with the actual significance of the results, and with the writing/presentation in general. \nAs with the results themselves, i am a bit unsure as to how significant the results really are, given the strong assumption on the quality/over parameterization tradeoff. Concretely, Theorem 4.1 is proven by constructing sub networks connecting a set of neurons from each layer, to the output. The bound in Theorem 4.1 therefore depends on the error that these subnetworks can achieve (Eq 8), which crucially depend on the quality of the set of features chosen. It is not clear to me how significant this bound is given this assumption. (this is also true for the dropout stability assumption in prior work).   It appears to me that the main novelty of the paper is by formalizing a stronger assumption (which is eq 8 needs to be small) than dropout stability (the validity of both is not clear), and using it to get stronger results. \n\nIn addition, i find it hard to judge the impact of this paper due to the lack of discussion on why the subject matter is of any relevance. Besides it being intellectually intriguing to some, what is the significance of the findings in this paper, practically or otherwise?  To be clear, i feel this is more of a presentation issue than anything, and could be cleared out by the authors with some additional discussion.\n\nSome additional comments:\n\n- The authors mention that \"This improvement comes at the cost of ensuring a weaker guarantee (connectivity of SGD solutions, as opposed to connectivity of all sublevel sets)\". Can the authors expand on this further, how are the results in the paper specifically relevant for solutions found by SGD? Is the intention that the connecting path is found by SGD? This is not clear from the draft.\n\n- While i appreciate figure 1 and the Proof techniques section, i feel that the readability of the proof of Theorem 4.1 could and should be improved. This is especially true since it is part of the main draft. For instance, i find it hard to parse the sentence \". Now, set the\nincoming weights of the neurons ....  to the corresponding values given by....\" right before Eq 13. This could be made much clearer by some additional figures similar to fig 1.\n\n----------Post Rebuttal\n\nThe authors have addressed my concerns to an adequate degree, hence i have updated my score accordingly and recommend acceptance. \n They are addressed ",
            "The paper introduces an upgrade of the existing research on the existence of low-loss paths connecting minima (or random points) on the loss surface of a neural network. The main idea is to bound the loss of the points along the path by the loss of the sparse subnetworks of the original network. The technique is demonstrated to be more widely applicable as compared to the existing dropout-stability technique. Moreover, the connection to memorization ability is shown with smaller requirements to the width of the layers, which is also demonstrated in the experiments (by the price of considering only SGD solutions and not the full parameter space). The theory gives an insight on the trade-off between quality of features and overparametrization of the layers.\nThe empirical evaluation demonstrates that the proposed approach can find connection path on each level and with significantly better approximation than the dropout stability technique. Also it requires less overparametrization for precise estimations.  The paper is clearly written and provides the proofs for all the presented results. Empirical evaluation demonstrates the validity of the proposed technique also showing the advantages compared to the current state-of-the-art.\n\nSome minor points can improve the understanding of the interesting details:\n- Clearer discussion on memorization, including the notion of memorization used and how it is connecting to the low-loss-paths\n- Clearer discussion on why only SGD solutions should be considered in the theory and what are the possible problems with it (will the parameter configurations built for the low-loss-path always an SGD solution for example?)\n\nPossibly there is a typo in eq.4 (second max in RHS)\n\nOverall, the theory is clear and well explained, the empirical results are good, so my suggestion would be to accept the paper for the conference.\n\n---\nI am satisfied with the rebuttal discussion and not changing my score. The authors included a detailed discussion of limitations, all of which seem to be viable.\n\nAs a theoretical paper it does not have any direct societal impact.",
            " Thanks for the explanations. I see this is weaker than dropout stability, but the implication on the level of overparametrization is unknown. I think the message is more clear now. ",
            " I thank the authors for their explanations and clarifications.\n\n1 - So basically the main idea is that larger amount of neurons means larger memorization and the ability to apply the proposed technique depends on the amount of the neurons - do I understand it correctly?\n\n2 - I guess this is written slightly confusing in the contributions part of the introduction. If the main theorem 4.1 already proves the existence of the low loss path for any two points, why the corollary for the technique introduces some restrictions?\n\n3 - I understand now. I would suggest to think on the improvement of the readability of this formula (maybe breaking it in parts and giving separate notations can help).",
            "1. The authors prove a mild condition for mode connectivity for deep nets. The proof is a natural generalization of (Kuditipudi et al., 2019) and the condition required in this paper is provably weaker than the dropout stability. The authors also demonstrate in experiments that their condition can still hold when dropout stability fails.\n\n2. The author provides a nice conceptual connection between their results and the memorization capacity of neural nets. In detail, they show if the product of widths of the last two layers is $\\Omega(N)$, then solutions are connected via a low-loss path with minimal assumptions.  This paper made a nice observation that the proof technique in (Kuditipudi et al., 2019) could be generalized in the following sense: for a fixed subset of the neurons per layer, to find a low-loss path, one doesn't have to rescale the original weights. Instead, it's possible to optimize weights in all further layers, thus further reducing the loss. Despite the simplicity of the idea, this new construction actually gives paths of much lower cost, as demonstrated in Figure 2 and 3.\n\nThis paper is clearly written and I enjoyed reading this paper. The setting of the experiments is clearly stated and I checked the proof of the main theorem and it sounds correct to me. The topic of mode-connectivity in deep learning is closely related to the interest of Neurips community. As the authors remarked, the mystery of mode-connectivity is still unsolved because we don't know why SGD find networks satisfying the assumption or if this is in general true for deep nets. However, I think this paper makes a solid step towards understanding mode-connectivity. Thus I suggest accepting this paper.\n\nOther comments:\nI really appreciate the authors' effort in making Figure 1, which helps readers to understand the notation of subnetwork used in this paper. Similarly, I think the paper will benefit from making several graphical demonstrations for the proof sketch, e.g., the operations you take to move from the right to the right subfigure in Figure 1. Yes, in Sec 7.",
            "This paper studies the connectivity of two parameters such as two SGD solutions of the deep neural networks. The general result subsumes several conditions including dropout stability, memorization capacity, and linear separable features. In particular, the theory shows that the connectivity occurs when the last two layers have $\\Omega(\\sqrt{N})$ neuron. The theory is validated by numerical experiments.   Overall, I think the main message is interesting, and the presentation is clear. Since the main contribution claimed in this paper is on the milder overparametrization, the assumption (A1-b) on the feature quality deems further justifications. This assumption possibly implicitly put stringent requirements on the level of overparametrization of previous layers. The discussion might also help understand the difference between deep nets and two-layer nets where $\\Omega(N)$ neurons are necessary.  The limitations are discussed in the paper. The potential negative societal impacts are not available, as this paper focuses on the mathematical theory for the solutions of deep networks. "
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer initially had concerns but states that the authors addressed them adequately, leading to a recommendation for acceptance and an updated (positive) score.",
            "The review expresses overall positive sentiment, highlighting the paper's clarity, well-explained theory, and good empirical results. The reviewer concludes with a recommendation to accept the paper.",
            "The reviewer expresses gratitude ('Thanks for the explanations') and acknowledges improvement ('the message is more clear now').",
            "The reviewer expresses gratitude and understanding, indicating a positive reception of the authors' work.",
            "The reviewer explicitly states they \"enjoyed reading this paper\" and \"suggest accepting this paper.\" They also highlight the paper's clarity, correctness, and relevance to the NeurIPS community.",
            "The reviewer states that the main message is interesting and the presentation is clear. They also acknowledge the paper's discussion of limitations."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Supportive",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The review presents both positive and negative feedback, starting with an assessment of the paper's strengths and weaknesses, then offering specific suggestions for improvement. The final statement indicates a shift to a positive stance after the rebuttal, suggesting the initial critique was constructive.",
            "The tone is supportive, evidenced by phrases like \"clearly written,\" \"provides the proofs,\" \"empirical evaluation demonstrates the validity,\" \"theory is clear and well explained,\" \"empirical results are good,\" and \"my suggestion would be to accept the paper.\"",
            "The reviewer's tone is supportive, showing appreciation for the author's efforts and acknowledging the clarity of the message. The reviewer is also providing constructive feedback in a positive manner.",
            "The reviewer uses phrases like \"I thank the authors,\" \"do I understand it correctly?\" and \"I would suggest to think on the improvement\" indicating a desire to help improve the paper rather than harshly criticize it.",
            "The reviewer uses positive language such as \"nice conceptual connection,\" \"nice observation,\" \"clearly written,\" and \"solid step.\" They also offer constructive suggestions for improvement, indicating a desire to help the authors strengthen their work.",
            "The review expresses both positive aspects (interesting message, clear presentation) and critical points (need for further justification of assumption A1-b, discussion on the difference between deep nets and two-layer nets). This balanced approach indicates a neutral but thorough evaluation."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially raises concerns regarding the significance of the results and the clarity of presentation. However, after the rebuttal, the reviewer explicitly states that their concerns have been adequately addressed and consequently recommends acceptance. This indicates a consistent evaluation process where initial concerns are resolved through author feedback, leading to a change in the recommendation.",
            "The review is consistently positive, highlighting the paper's strengths such as the upgrade of existing research, clear writing, and good empirical results. While suggesting minor improvements like clearer discussions on memorization and SGD solutions, the reviewer's overall assessment remains positive, leading to a recommendation for acceptance. The satisfaction with the rebuttal further reinforces the consistency of the positive evaluation.",
            "The reviewer expresses gratitude for explanations, acknowledges a weakness compared to another method, points out an unknown implication, and concludes that the message is clearer. These points are logically connected and do not contradict each other. The reviewer's stance is consistent throughout the review.",
            "The review is consistent as it presents a logical flow of questions and suggestions for clarification and improvement, without any self-contradictory statements. The reviewer starts by seeking clarification on a core concept, then points out a potential inconsistency in the paper's presentation, and finally acknowledges understanding and offers a constructive suggestion. This progression is coherent and does not contain any internal contradictions.",
            "The review is consistently positive, highlighting the paper's contributions, clarity, and relevance. The reviewer praises the generalization of existing work, the conceptual connection, and the experimental validation. The reviewer explicitly recommends accepting the paper, which aligns with the overall positive assessment and detailed comments.",
            "The review is consistent because it acknowledges the strengths of the paper, such as the interesting main message, clear presentation, and validation through numerical experiments.  It also raises a specific concern about the justification of assumption (A1-b) and suggests further discussion on the implications of this assumption on overparametrization and the difference between deep and two-layer networks. The reviewer's points are constructive and do not contradict each other, providing a balanced assessment of the paper."
        ]
    },
    {
        "paper_id": "iclr_2018_BkS3fnl0W",
        "paper_title": "Semi-supervised Outlier Detection using Generative And Adversary Framework",
        "paper_abstract": "In a conventional binary/multi-class classification task, the decision boundary is supported by data from two or more classes. However, in one-class classification task, only data from one class are available. To build an robust outlier detector using only data from a positive class, we propose a corrupted GAN(CorGAN), a deep convolutional Generative Adversary Network requiring no convergence during training. In the adversarial process of training CorGAN, the Generator is supposed to generate outlier samples for negative class, and the Discriminator as an one-class classifier is trained to distinguish data from training datasets (i.e. positive class) and generated data from the Generator (i.e. negative class). To improve the performance of the Discriminator (one-class classifier), we also propose a lot of techniques to improve the performance of the model. The proposed model outperforms the traditional method PCA + PSVM and the solution based on Autoencoder.",
        "review_ids": [
            "HJDhuQtlM",
            "Sks502_lG",
            "Hy2FsQKef"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The idea of using GANs for outlier detection is interesting and the problem is relevant. However, I have the following concerns about the quality and the significance:\n- The proposed formulation in Equation (2) is questionable. The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence. However, then what is learned though the proposed formulation? Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable.\n- In addition to the above point, I guess the expectation is needed as the original formulation of GAN. Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined.\n- In experiments, although the authors say \"lots of datasets are used\", only two datasets are used, which is not enough to examine the performance of outlier detection methods. Moreover, outliers are artificially generated in these datasets, hence there is no evaluation on pure real-world datasets. To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments.\n- As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments.\n  Although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data, this is not always correct.\n  Please see the paper:\n  -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)\n  This paper shows that the performance gets even better for higher dimensional data if each feature is relevant.\n  I recommend to add some distance-based outlier detection methods as baselines in experiments.  \n- Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others). Otherwise in practice how to set these parameters to get better results is not obvious.\n\n* The clarity of this paper is not high as the proposed method is not well explained. In particular, please mathematically formulate each proposed technique in Section 4.\n\n* Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high.\n\nMinor comments:\n- P.1, L.5 in the third paragraph: architexture -> architecture\n- What does \"Cor\" of CorGAN mean?\n\nAFTER REVISION\nThank you to the authors for their response and revision. Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation.",
            "The idea of the paper is to use a GAN-like training to learn a novelty detection approach. In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution. The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers. To establish such a behavior, the authors propose early stopping as well as other heuristics. \n\nI like the idea of the paper, however, this paper needs a revision in various aspects, which I simply list in the following:\n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \"inlier accepted fraction\" is not well motivated in my opinion. This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data. The latter is important for the GAN-like training.\n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. \n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)\n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)\n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)\n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ())\n* The term \"AI-related task\" sounds a bit too broad\n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection.\n* Where is Table 1?\n* There are quite a lot of typos.\n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating. The clarity of the paper has improved but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)",
            "This paper addresses the problem of one class classification. The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class.\n\nThe suggested techniques are nice and show promising results. But I feel a lot can still be done to justify them, even just one of them. For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values. But, in the experimental section results are shown only for a  single value, alpha_new=0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested. \n\nThe writing of the paper is also very unclear, with several repetitions and many typos e.g.:\n\n'we first introduce you a'\n'architexture'\n'future work remain to'\n'it self'\n\nI believe there is a lot of potential in the approach(es) presented in the paper. In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the proposed method's theoretical foundation, experimental validation, and overall originality. Phrases like \"questionable formulation,\" \"not enough to examine the performance,\" \"not convincing,\" and \"insufficient experimental evaluation\" indicate a negative sentiment.",
            "The reviewer states that the paper 'needs a revision in various aspects' and lists several significant shortcomings, including a lack of comparison with state-of-the-art methods, insufficient experiments, and poorly motivated heuristics. The reviewer also mentions 'quite a few heuristic tricks' and questions the use of a cross-entropy loss for the autoencoder. The final statement indicates that the reviewer maintains their negative rating even after revision.",
            "The review points out several weaknesses in the paper, including the lack of justification for techniques, unclear writing, repetitions, and typos. While acknowledging the potential of the approach, the overall assessment is critical."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, as evidenced by direct critiques of the methodology, such as \"The proposed formulation in Equation (2) is questionable\" and \"the originality is not high.\" The reviewer also points out specific shortcomings in the experimental design and clarity of the paper.",
            "The review uses direct and critical language, such as 'not well motivated,' 'not sufficient,' 'does not make much sense,' and 'lacking realistic datasets.' The reviewer also points out specific weaknesses in the methodology and experimental setup.",
            "The tone is critical due to the reviewer's direct feedback on the paper's shortcomings. Phrases like 'But I feel a lot can still be done to justify them,' 'The writing of the paper is also very unclear,' and the listing of specific typos indicate a critical assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently points out weaknesses in the methodology, specifically regarding the formulation of the GAN approach, the limited experimental evaluation, lack of comparison with baselines, and clarity of the proposed method. The reviewer's concerns are logically connected and maintain a consistent critical stance throughout the review, even after revision, focusing on the insufficient experimental evaluation.",
            "The review is consistent because while the reviewer initially states liking the idea, the rest of the review consistently points out significant flaws in the methodology, experiments, and motivation, requiring major revisions. The 'After revision statement' reinforces this negative assessment by stating that the clarity improved but the core issues remain, leading to the same rating.",
            "The review is consistent in its assessment. It acknowledges the potential of the proposed techniques while consistently pointing out weaknesses in the experimental justification (single parameter values tested), clarity of writing, and lack of theoretical depth. The reviewer's points all contribute to a coherent message about the paper's strengths and areas for improvement."
        ]
    },
    {
        "paper_id": "iclr_2021_pAq1h9sQhqd",
        "paper_title": "Stochastic Canonical Correlation Analysis: A Riemannian Approach",
        "paper_abstract": " We present an efficient stochastic algorithm (RSG+) for canonical correlation analysis (CCA) derived via a differential geometric perspective of the underlying optimization task. We show that exploiting the Riemannian structure of the problem reveals natural strategies for modified forms of manifold stochastic gradient descent schemes that have been variously used in the literature for numerical optimization on manifolds. Our developments complement existing methods for this problem which either require O(d3) time complexity per iteration with O(1t) convergence rate (where d is the dimensionality) or only extract the top 1 component with O(1t) convergence rate. In contrast, our algorithm achieves O(d2k) runtime complexity per iteration for extracting top k canonical components with O(1t) convergence rate. We present our theoretical analysis as well as experiments describing the empirical behavior of our algorithm, including a potential application of this idea for training fair models where the label of protected attribute is missing or otherwise unavailable.",
        "review_ids": [
            "1mDMWe9VUve",
            "9cxwZ5Ouedz",
            "bUrYiJNWEff",
            "wfYblg_7hRr",
            "yMX_CbHRtO7",
            "D_YudmtLrJr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "There are some points that the authors may be able to quickly clarify:\n\n1) by `\"asymptotically converge\": does it mean when the number of samples goes to infinity?\n\n2) by \"ensuring\" the diagonal of S to be nonzero, what exactly does it mean?. A constraint such as S_ii \\neq 0 may not be nontrivial in practice (and nonconvex in fact). How is this enforced without affecting the convergence proof? By optimizing over the log, you also need some nonnegativity constraint on S_ii, I suppose---otherwise log is not defined. The reviewer is wondering are you using the log-barrier approach? Not that log barrier works for convex sets, but S_ii\\neq 0 is not convex. Even if the set is convex, the barrier parameter affects the approximation accuracy a lot. It is unclear how this approach affects the overall analysis since this information was not disclosed before.  In addition, your algorithm descriptions in the main text and in the supplemental material seem not to reflect this point.  The reviewer feels that this explanation seems not to be very convincing.",
            "Comment: \u00a0\u00a01. *paper is packed*\n\nThe reviewer understands page limitation may give challenges for preparing the manuscript. But the clarity bar shouldn\u2019t be lowered because authors have a lot of materials to present. \n\n2. *Improve readability: contributions, formulations, etc*\u2028\n\nThe reviewer has taken a look at page 3 again per the authors\u2019 request. This page is particular hard to follow. First, the reviewer does not think SO, Gr, and St are clearly defined. If the authors are using some conventionally used notations from somewhere else, one standard way is to cite the origin. A better way is perhaps to give the exact definitions in the appendix. Saying SO is a manifold of \u201cspecial matrices\u201d does not help clarify what is the mathematical definition---and the argument that this is \u201cstandard\u201d in certain community may not help neither. Without definitions of these manifolds, the reviewer could only guess what the authors meant to write here.\n\nBesides notation discrepancy, page 3 also have many comments that does not lead to clear understanding.\n\nFor example. In Remark 1\n\nit seems that U, V are constrained in equation (2) rather than \"arbitrary\" matrices.  The decomposition adds more structural constraints on to U, V, but the remark did not explain why this structural constraint is a good constraint. Is it without loss of generality, or it is a compromise for computational efficiency? It may be helpful to clarify these points.\n\nThis is perhaps related to Theorem 1. Theorem 1 only says that E goes to zero when Eq 3b is satisfied. But Theorem 1 does not say when Eq. 3b can be satisfied. If the reviewer understands correctly, the manifold parameterization for U and V may make Eq 3b infeasible, since the search space has been shrunk. This was not quite articulated in the paper. More importantly, if Eq 3b is not satisfied, then the computed U, V may not be desired solutions. How to deal with this situation?\n\nFor the paragraph `` Intuition behind the decomposition''\n\nThe last couple of sentences may be the most useful. But this claim was not clearly seen at this stage.  \n\n\u2028\u2028\u00a03. *Equivalence of (1) and (3) was not clearly shown.*\u2028\u2028\u00a0\n\nThe expression change is indeed minor. But how large is the change in essence? The reviewer\u2019s concern is that this decomposition may have changed the problem to an extent that is not easy to quantify. The question is how much the problem has been changed?Is the approximation good? \u2028\u2028\u00a0\n\n4. *Theorems presented in a bit abrupt way.*\u2028\u2028\u00a0\n\nSome suggestions: Perhaps an equivalence lemma could appear after eq 3. This may be part of Theorem 1 or modified version of Theorem 1. Proposition 1 may be a bridge, and may not need to appear in the main text. Proposition 2 may be more clearly explained how it is used to compute Eq 3, rather than just stating it here without too much explanation. The same applies to Prop 5. How is this used in *this* submission? This is perhaps more important than simply stating this proposition.\n\n\u2028\u00a05. *Even the algorithm does not appear in the main text.*\u2028\u2028\u00a0\n\nThe reviewer does not think readers could follow the \u201cfull\u201d pseudocode to reproduce the algorithm. Indeed, detailed explanations do not add too much value. But vague description may not either.\u2028\u2028\u00a0\n\n6. *Color code is confusing.*\u2028\u2028\u00a0\n\nThis may be just the reviewer\u2019s personal opinion. The authors may keep it. It does not affect the score that the reviewer gives.\u2028\u2028\u00a0\n\n7. *Main Contribution*\u2028\u2028\u00a0\n\nUnfortunately, the reviewer still feels the paper hard to follow. Some major re-organization may help improve the clarification level.\u2028\u2028 The proof of Theorem 1 may be enhanced to justify the reformulation.\n\n\u00a08. *Unclear why the reformulation in (3) is a good approximation for CCA.*\u2028\u2028\u00a0\n\nThe reviewer has checked the proof of Theorem 1. Two major concerns. First, again, this proof is only for sub-Gaussian data, but CCA is a generic tool that can be applied as a deterministic method. Second, C(\\tilde{X}_k) \\rightarrow I_k is unjustified. Can it be I_k? how far is it from I_k? Assuming it approaches I_k is not \u201cconsistency proof\u201d. \u2028\u2028\u00a0\n\n9. *\u201cSO(n)\u201d: what is \u00a0\u201cspecial\u201d here?*\u2028\u2028\u00a0\n\nSee the reviewer\u2019s comment under Q2\u2028\u2028\u00a0\n\n10. *Why is there a Q in the middle?*\u2028\u2028\u00a0\n\nThe above explanation makes sense. The Q matrix is automatically full rank. The unclear part is how the upper triangular structure is preserved in the optimization? In addition, even if the upper triangular structure is preserved, S can still be rank deficient. So overall A=QS may still be rank deficient. How is this addressed? \u2028\u2028\u00a0\n\n11. *How high-level description connects CCA with PCA using this reformulation*\u2028\u2028\u00a0\n\nsee comments on Theorem 1\n",
            "The paper presents an approach to find canonical directions in a streaming fashion, i.e. without direct calculation of covariance matrices (which becomes hard when the number of examples is large). This solution to that task is not obvious, because the objective function of CCA, together with whitening constraints, does not allow simple additive decomposition.\n\nFirst, the optimization task is reformulated as a task over certain Riemannian manifolds, and a natural initialization is suggested. It is shown that under a certain assumption, this initialization is already a solution of good quality. Then, a natural minimization algorithm is presented, which is based on stochastic gradient descent on a Riemannian manifold. The key aspect of the algorithm is a combination of 2 types of gradient, the gradient for top-k principal vectors and standard gradient.\n\nThe experimental part shows that the algorithm successfully solves CCA in a streaming fashion. Also, it can be effectively combined with deep feature learning (Andrew, 2013) to find common features for multi-view representation learning tasks. Experiments look convincing.",
            "This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices, a stochastic gradient-based optimization on a Riemannian manifold is provided reducing the computational complexity from $d^3$ to $d^2k$.\n\n\nStrength:\nCCA is a classic and still important method, especially in combination with deep neural networks (e.g., multi-view learnings). \nThe proposed method enables the applications of CCA to high-dimensional vectors with small memory.\nThis makes it easier to use CCA as an objective function of deep neural networks, which is trained on GPUs.\nExperiments show the benefits of their proposed method, in particular, the computational speed is 5-10 times faster than the existing method (MSG).\n\nWeakness:\nAlthough the authors claim that their proposed method captures more correlation than MSG, two of the three datasets in which their method is superior (MNIST and CIFAR) are not realistic setting (i.e., correlation between left/right half of images).\n\nQuestion:\nIs it possible to make a (numerical) comparison with Yger+2012, which reformulates CCA as an optimization on the generalized Stiefel manifold?",
            "main contribution\n\nThis work offers a stochastic CCA algorithm based on Riemannian optimization approach\n\nStrength\n\n- The paper offers a theory-backed algorithm for CCA under the assumption that the two views are sub-Gaussian. The complexity-accuracy tradeoff of the algorithm seems to be appealing according to the experiments.\n\nWeakness\n\nOverall, the biggest concern is readability. The paper is very packed and many treatments seem to be unbalanced. Important details are missing, and proofs seem to be hastily. The paper may need some re-packaging and re-organization before its core technical contents could be easily followed.\n\n- readability. The biggest concern of the work is that it is very hard to read. This creates a lot of barriers in understanding key aspects of the paper, e.g., contributions, formulations, novelty of the proof, just to name a few. The key formulation and the definitions of the manifolds were not clearly defined. The equivalence of (1) and (3) was not clearly shown. The theorems were presented in a bit abrupt way. Even the algorithm does not appear in the maintext but the appendix (which is also hard to read). The color code is used in a way that is a bit confusing (does ICLR allow writing in different colors?).\n\n- Clarify about the contribution. It is hard to clearly see how much is the contribution. The proofs seem to be short and most of the proofs are presented using \u201cpropositions\u201d cited from existing papers. If the authors think the contributions lie in reformulating the CCA problem as a PCA problem, then the reformulating part is perhaps the contribution. But from the current writing it is hard to follow how the reformulation comes through and how the reformulation enables using these existing propositions to prove convergence of the proposed algorithm.\n\n- It is unclear why the reformulation in (3) is a good approximation for CCA. Some attempts for justifying this were offered in Theorem 1, but it was based on the assumption X and Y are sub-Gaussian, which is at best a special case, even if the proof is correct (which, due to the current organization of the paper, is hard to read and fully understand).\n\n- \u201cSO(n): group/manifold of nxn special orthogonal matrices.\u201d what is the definition of \u201cspecial\u201d here? are they the commonly understood orthogonal matrices?\n\n- The paper has this upper triangular structure of the S matrices but this point seems to have no detailed explanation. If one understands U = \\tilde{U}S as the QR decomposition, then indeed S is upper triangular, but why is there a Q in the middle? Why is this Q useful?\n\n- the \u201chigh-level\u201d description of the algorithm says the idea is to connect CCA with PCA using this reformulation, but this point was not clearly explained.\n\n- Complexity. From the current writing, it is very hard to see how the complexity of the algorithm is calculated. The gradients needed are tabulated in the supplementary materials, but it is very hard for a reader to directly see why the algorithm saves computational complexity.\n\n- It is also unclear how the convergence and convergence rate analyses come together. These parts may need to be elaborated.\n",
            "1. Paper summary:\n\nThis paper proposes a method for solving linear CCA on high dimensional data. Linear CCA has a closed form solution. The solution requires a whitening step that costs O(d^3). This makes it not applicable to data in high dimensional spaces, e.g. representations learnt by deep networks. \n\nTo resolve the issue, the authors propose a reformulation of linear CCA which decomposes the transformation matrix U (V) into a product of three matrices. Those three matrices have the following properties:\n\n- Their initial values can be obtained by efficient streaming PCA on original view matrix X (Y). Streaming PCA costs O(d^2 * k) only where k is the top k eigenvectors.\n\n- They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold.\n\n2. Strong points of the paper:\n\nThe new linear CCA formulation justifies the rationale of batch CCA training.\n\nUnder Gaussian distribution assumption:\n- The absolute difference between correlation found by original linear CCA and stochastic one is bounded.\n- The convergence of the training process is proven.\n\nThe experiments are performed on different aspects:\n- Recovering groundtruth transformations on MNIST, CIFAR and Mediamill data sets.\n- Learning deep features on MNIST data set.\n- Improving fairness in deep learning by adding CCA term to the loss function.\n\n3. Weak points of the paper:\n\nThe theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution.\n\nMost propositions are from other papers."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The review raises several concerns about the clarity, correctness, and completeness of the authors' explanations and methodology. Phrases like \"not very convincing,\" \"unclear how this approach affects the overall analysis,\" and questions about the validity and implementation of the proposed method indicate a negative sentiment.",
            "The review expresses numerous concerns about the paper's clarity, notation, and justification of key steps. Phrases like \"paper is packed,\" \"hard to follow,\" \"does not lead to clear understanding,\" \"equivalence was not clearly shown,\" \"presented in a bit abrupt way,\" \"still feels the paper hard to follow,\" and \"unclear why the reformulation is a good approximation\" indicate a negative sentiment.",
            "The review expresses positive feedback about the paper's approach, solution, algorithm, and experimental results. Phrases like 'solution to that task is not obvious,' 'natural initialization is suggested,' 'natural minimization algorithm is presented,' 'algorithm successfully solves CCA,' and 'Experiments look convincing' indicate a positive sentiment.",
            "The review expresses positive sentiment by highlighting the strengths of the paper, such as the importance of CCA, the method's ability to handle high-dimensional vectors, and the speed improvements shown in the experiments.",
            "The review expresses several concerns about the paper, including readability issues, lack of clarity regarding the contribution and novelty, unclear justifications for approximations, and missing details in the proofs and algorithm explanation. The reviewer uses phrases like 'biggest concern is readability,' 'hard to read,' 'hard to clearly see,' 'unclear why,' and 'not clearly explained,' indicating a negative sentiment towards the paper's presentation and content.",
            "The review highlights several strong points of the paper, including a novel formulation of linear CCA, theoretical guarantees under specific assumptions, and comprehensive experiments. While acknowledging a weak point (strong Gaussian assumption), the overall assessment leans positively due to the identified strengths."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The tone is critical due to the direct questioning of the authors' methodology and explanations. Phrases such as \"what exactly does it mean?,\" \"not very convincing,\" and pointing out potential issues with the convergence proof and algorithm implementation demonstrate a critical stance.",
            "The tone is critical, pointing out specific weaknesses and areas needing improvement. The reviewer uses direct and challenging questions, such as \"what is 'special' here?\" and \"Why is there a Q in the middle?\" which reflects a critical assessment of the paper's content and presentation.",
            "The tone is supportive as it highlights the strengths and novel aspects of the paper's approach, such as the non-obvious solution, natural initialization, and successful experimental results. The reviewer acknowledges the challenges and praises the paper's ability to overcome them.",
            "The tone is balanced as it acknowledges both the strengths and weaknesses of the paper. It points out the benefits of the method while also raising concerns about the realism of the datasets used in the experiments. The reviewer also poses a question suggesting a potential comparison with another method, indicating a constructive and critical approach.",
            "The review points out several weaknesses in the paper, using phrases like 'biggest concern,' 'important details are missing,' 'proofs seem to be hastily,' 'hard to read,' 'not clearly defined,' and 'unclear why.' These phrases indicate a critical tone, as the reviewer is actively identifying and highlighting flaws in the paper's content and presentation.",
            "The review adopts a balanced tone by clearly delineating both the strong and weak points of the paper. It presents the paper's contributions objectively while also pointing out limitations, resulting in a neutral and fair assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "No",
            "Yes"
        ],
        "consistency_reason": [
            "The review consistently raises questions and seeks clarifications on specific aspects of the paper, without presenting contradictory statements or viewpoints. The reviewer is asking for more details and explanations on certain points, which is a consistent line of inquiry.",
            "The review is consistent. All comments consistently point to the lack of clarity and rigor in the paper, focusing on unclear notations, missing definitions, abrupt presentation of theorems, and insufficient justification for the proposed method. The reviewer repeatedly expresses difficulty in understanding the paper's contributions and reasoning.",
            "The review is consistent as it provides a positive and coherent summary of the paper's contributions, methodology, and experimental results. It highlights the problem addressed, the proposed solution, and the validation through experiments, without any contradictory statements or opinions.",
            "The review is consistent because it highlights both the strengths of the paper, such as the importance of the method and its computational advantages, and weaknesses, such as the limited validity of the experimental setting for demonstrating improved correlation capture. These points are independent and do not contradict each other, providing a balanced and consistent evaluation.",
            "The review mentions a strength regarding the theory-backed algorithm and complexity-accuracy tradeoff. However, this positive point is significantly undermined by a large number of weaknesses focusing on critical issues like readability, clarity, missing details, and lack of justification. The reviewer repeatedly emphasizes the difficulty in understanding the paper's core aspects, contributions, and proofs, making the initial strength seem less impactful and almost irrelevant given the substantial flaws in presentation and clarity.",
            "The review is consistent as it highlights both the strengths and weaknesses of the paper without contradicting itself. The reviewer acknowledges the contributions and limitations of the proposed method in a balanced manner."
        ]
    },
    {
        "paper_id": "iclr_2020_B1lOraEFPB",
        "paper_title": "Transition Based Dependency Parser for Amharic Language Using Deep Learning",
        "paper_abstract": "Researches shows that attempts done to apply existing dependency parser on morphological rich languages including Amharic shows a poor performance. In this study, a dependency parser for Amharic language is implemented using arc-eager transition system and LSTM network. The study introduced another way of building labeled dependency structure by using a separate network model to predict dependency relation. This helps the number of classes to decrease from 2n+2 into n, where n is the number of relationship types in the language and increases the number of examples for each class in the data set. Evaluation of the parser model results 91.54 and 81.4 unlabeled and labeled attachment score respectively. The major challenge in this study was the decrease of the accuracy of labeled attachment score. This is mainly due to the size and quality of the tree-bank available for Amharic language.  Improving the tree-bank by increasing the size and by adding morphological information can make the performance of parser better.",
        "review_ids": [
            "rye8rHBmsH",
            "H1xIpYAstB",
            "H1ek-2mRKr"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper describes a dependency parser for Amharic text trained on the Yimam et al. 2018 treebank.\n\nThe proposed method is an unlabelled arc-eager transition-based dependency parser followed by a dependency label classifier. Both models are based on LSTM architectures. Unfortunately there is no clear description of these models, which makes it difficult to understand their structure.\n\nThe experimental section contains a questionable selection on the train/test split (which should have been done blindly before looking at the results) and lacks any baseline comparison with previous approaches. The authors argue that performing arc prediction and arc labeling in two separate stages is beneficial, but they don't compare with a method that perform both actions at the same time.\n\nThe motivation of the paper is also questionable: the introduction section describes some of the peculiarities of the Amharic language, such as being morphologically rich, but the proposed method seems completely generic and does not address or exploits any of these peculiarities. If the authors want to argue that Amharic would benefit from specialized parser architecture, the methods and results reported in this paper fail to provide evidence for this position.\n\nThe citation format is also broken.\n",
            "The paper claims to build a transition-based dependency parser for Amharic. To produce a parse tree, the paper takes a two-stage approach, first producing the structure of the tree, followed by annotating each individual arcs on the tree. The paper also explores attention and finds it useful for improving the performance.\n\nI am giving a score 1, because the paper lacks precision and the contribution is not significant.\n\nThe major problem of the paper is precision. There is a lack of formal descriptions of the parser, e.g., the representation of the input and output, what each individual block in the figures are doing, how the arc-eager transition parser works. The text description is certainly helpful, but without formal descriptions, it is impossible to reproduce the results in the paper. Besides, without setting up a precise language/notation, the paper is not easy to follow.\n\nIn terms of contribution, the paper begins with the motivation that Amharic has certain language characteristics that are absent in languages, such as English, where most research effort has been spent. However, the paper still uses a parser for English, and does not utilize the special features of Amharic. Generic architectural modification, such as adding attention, has been explored, but this has little contribution on its own and has little to do with Amharic.\n\nIn addition to the above, other minor weaknesses of the paper includes typos, poor experimental practices, missing citations and related work, and other wording issues.\n\nIn particular, choosing the split of the data set that achieves the best result, as quoted below, is concerning.\n\n\"We experimented on 60/40, 70/30 and 80/20 train-test splitting ratios ... Therefore, we select 70/30 train-test splitting ratio throughout the experiments.\"",
            "* overview \n- This paper describes a model for transition-based dependency parser, and tested on the Amharic treebank. It proposes a modification that is different from the common practice of combining the transition with dependency relation, namely to predict the transitions first, then the dependency relations.\n    \n* strengthens\n - Unfortunately, I can't find any strength in this paper. It is more like a course project than a research paper for ICLR.\n\n* weaknesses\n- The main idea of first unlabeled parsing then predicting labels is hardly novel, and it generally won't help, since the predicted labels could be useful features for determining the transitions. There is also no experiment to compare the proposed model to normal parsing model with 2n+2 predictions to support the author's claim.\n- Generally, the architecture of the model is poorly explained. For example, it seems that the \"embeddings\" for the words are just one-hot index vectors (since the number of dimensions are the same as the vocabulary size), which is not what we mean by embeddings. It is also unclear what the dot product of the embeddings means. The attention layer is again not explained at all, e.g. what are the input and output.\n- The splitting of train/test set does not make sense, the selection should not be influenced by the results at all. There is no reason not to use the standard split in UD. \n- I don't see what is the challenge in the discussion section, since UAS is by definition lower than LAS.\n- There numerous typos and grammar mistakes, and the citation format is broken.\n- In many places, the quoted presumably Amharic words are not shown."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses multiple concerns, including a lack of clear model description, questionable experimental design, absence of baseline comparisons, questionable motivation, and broken citation format. These issues collectively indicate a negative assessment of the paper.",
            "The reviewer explicitly states they are giving a score of 1, indicating a negative assessment. The review is filled with criticisms about the paper's lack of precision, insignificant contribution, and various weaknesses, such as typos and poor experimental practices.",
            "The reviewer expresses strong criticisms regarding the paper's novelty, experimental design, clarity, and writing quality. Phrases like \"I can't find any strength in this paper,\" \"hardly novel,\" \"poorly explained,\" \"does not make sense,\" and \"numerous typos and grammar mistakes\" indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses critical language such as \"questionable\", \"lacks\", and \"fail to provide evidence\", indicating a negative evaluation.",
            "The review uses strong negative language such as \"lacks precision,\" \"contribution is not significant,\" \"major problem,\" \"impossible to reproduce,\" \"does not utilize,\" \"little contribution,\" \"minor weaknesses,\" and \"concerning.\" These phrases clearly indicate a critical tone.",
            "The review adopts a critical tone by directly pointing out flaws in the paper's methodology, explanation, and presentation. Specific criticisms are raised using phrases such as \"It is more like a course project than a research paper,\" \"the main idea...is hardly novel,\" \"poorly explained,\" \"does not make sense,\" and \"citation format is broken.\""
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its criticism of the paper. It points out several weaknesses, including lack of clarity in the method description, questionable experimental setup (train/test split, lack of baselines), weak motivation, and even minor issues like broken citation format. All these points consistently suggest that the reviewer finds the paper to be lacking in several key aspects and needs significant improvement. There are no contradictory statements within the review.",
            "The review is consistent because all the points raised by the reviewer support the negative evaluation of the paper. The reviewer argues that the paper lacks precision, has insignificant contribution, and suffers from several minor weaknesses, all leading to the low score of 1.",
            "The review is consistently negative. The reviewer explicitly states the absence of strengths and lists multiple weaknesses, all contributing to a negative evaluation of the paper's quality and suitability for ICLR."
        ]
    },
    {
        "paper_id": "nips_2022_8U5J6zK_MtV",
        "paper_title": "LobsDICE: Offline Learning from Observation via Stationary Distribution Correction Estimation",
        "paper_abstract": "We consider the problem of learning from observation (LfO), in which the agent aims to mimic the expert's behavior from the state-only demonstrations by experts. We additionally assume that the agent cannot interact with the environment but has access to the action-labeled transition data collected by some agents with unknown qualities. This offline setting for LfO is appealing in many real-world scenarios where the ground-truth expert actions are inaccessible and the arbitrary environment interactions are costly or risky. In this paper, we present LobsDICE, an offline LfO algorithm that learns to imitate the expert policy via optimization in the space of stationary distributions. Our algorithm solves a single convex minimization problem, which minimizes the divergence between the two state-transition distributions induced by the expert and the agent policy. Through an extensive set of offline LfO tasks, we show that LobsDICE outperforms strong baseline methods.\n",
        "review_ids": [
            "ObYFZoUjcaZ",
            "K_LZtNGvLtn",
            "w7v4k09DfE",
            "nLerHrGMyPU",
            "-Vr0RjgaH_e",
            "-kyuFsdgEcE",
            "Zob7bR-B5QP"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I really appreciate the authors for additional comparison and the extension to the $\\gamma=1$ case, and I will raise my score.\n\nFor the poor empirical performance of using the new sampling strategy, I think it is expected that the empirical performance may not be better than the original uniform sampling strategy in the discounted case because most of the hyperparameters are tunned based on the old one. \n\nHowever, I do think current offline imitation learning algorithms that are DICE based are problematic because the issue is totally ignored by the community and few works are trying to fix it. I think the authors should discuss this in the next version or in the future to draw people's attention, so the theoretical derived algorithms match the practical implementations. \n\n",
            " Thanks for the response. I will keep my score unchanged.",
            " Thank you for your reply. I agree that the extension to DemoDICE is not straightforward, hence my vote for acceptance.",
            " This paper deals with offline LfO with imperfect demonstrations. The main contribution of this paper includes solving a distribution matching problem without  requiring an inverse dynamics model. The reduction of the problem is simple. But the algorithm and the theory heavily borrow from DemoDICE[15]. and the contribution of this paper is incremental. - Strengths\n  - The paper reduces the regular minimax problem in matching state distribution to a single convex minimization problem to make the proposed algorithms more stable than prior work.\n  - There are sufficient baselines in the experiments to support the performance of LobsDICE.\n\n- Weaknesses\n  - The algorithm and the theory heavily borrow from DemoDICE[15]. Figure 1 shows that the performance gap between LobsDICE and other baselines becomes larger as the level of stochasticity increases. And there is no gap when the environment is nearly deterministic. \n\nHowever, the Mujoco environments in Figure 2 are deterministic within my knowledge. So how to explain the performance of LobsDICE in figure 2? N/A",
            " This paper propose an offline learning from observation (LfO) algorithms, which imitates the expert policy by stationary distribution matching without interacting with the environment. The authors presented how to transfer the original complex minimax optimization problem to a simple convex minimization problem in detail, which avoids the instability issue in the original DICE based algorithms. \nEmpirical experiments show the effectiveness of the proposed algorithms and shows competitive performance compared with baseline algorithms. - **Originality**.  \nThe proposed algorithm is based on previous DICE techniques including DualDICE, OptiDICE for offline policy evaluation or improvement, and the authors extended the idea to the offline learning from observations settings. As far as I know, the techniques are not new but there are very few prior works that consider extending the idea to the offline LfO setting. \n\n-  **Quality**.  \nThe paper is technically sound and the empirical experiments are conducted thoroughly. One concern I have is that, as the authors mentioned, there are some concurrent works (SMODICE) related to this topic. In the appendix the authors discussed potential drawbacks of the concurrent work, while there are no empirical experiments to validate and support the claim, which I think the authors should make clear comparison and discussion in both the experiments and the related work sections (I checked with their paper, it seems that they already release the code?). \n\n- **Clarity**.  \nOverall the paper is well presented and it is easy to follow. The derivation presented in the main content are easy to follow if the readers know the basic DICE techniques in the previous work.\n\n- **Significance**.  \nThe presented techniques are a little bit incremental, but I think the proposed algorithm is a good addition to the offline LfO settings. \n\n\n I have a long existing fundamental question for this line of works, which try to use DICE techniques to perform offline imitation learnings, ever since ValueDICE, and also for this work.\n\n If we consider Equation (1) or final Equation (5) in the paper for offline imitation learning, what is the distribution of $\\bar{d}^{E}(s, s^\\prime)$, $d^{I}(s,a)$ or $d^{E}(s,a)$? Are they the **discounted** stationary distributions (the distribution would involve initial state distribution and a discounted factor $\\gamma < 1$) or just the **average** visitation stationary distribution ($\\gamma =1$ or the distribution you just uniformly sample from the empirical dataset).\n\nThis question is important, because when we consider to use the **discounted** stationary distribution $\\bar{d}(s, s^\\prime)$ or $d(s, a)$ of the current learning policy to match the target distributions ($d^{I}(s, a)$ or $\\bar{d}^{E}(s, s^\\prime)$), the target distributions should also be the **discounted** ones. As a result, $d^{I}(s, a)$ or $\\bar{d}^{E}(s, s^\\prime)$ must be the **discounted distribution**, which are not the **same as the distributions we uniformly sample from the empirical dataset**. \n\nFrom what the authors describe in the paper, I think the authors just use the empirical data distribution as $d^{I}(s, a)$ or $\\bar{d}^{E}(s, s^\\prime)$, which are actually not right because the uniformly sampling procedure would give us the average visitation stationary distribution ($\\gamma = 1$). \n\nThere are two ways to fix the flaw:\n1. Consider to define $\\bar{d}(s, s^\\prime)$ or $d(s, a)$  as the average visitation stationary distribution ($\\gamma = 1$), thus Equation (7) is different, and the whole derivation of this paper need to change. \n\n2. Consider to change the sampling procedure such that $d^{I}(s, a)$ or $\\bar{d}^{E}(s, s^\\prime)$ are the discounted stationary distribution, which involves the initial distribution $p_{0}(s)$. One possible simple solution is to sample transition data according to $\\gamma^t$, where $t$ is the time step. \n\n-----\nEmpirically I think there might not be significant impact of above concern, and please correct me if I misunderstand anything in the paper, and I will change my score. \n\n\n\n\n\n\n\n\n   The authors adequately addressed the limitations and there are no negative societal impact. ",
            " This paper algorithmically extends recent advances to the DICE literature in the offline RL setting to the IFO setting. They present a novel extension to match state-transition distributions as well as a more stable minimization objective to achieve SOTA performance for Offline IFO on the Mujoco benchmark task.  ### Strengths\n1. Technical contribution: Extends much of the recent advancements in the DICE literature to the IFO setting to do (s, s') distribution matching. \n2. Definitely excited to see the stabilization of the DICE objective to be a single minimization than a min-max. On a side note, this makes me wonder if this sheds some light into scaling up to higher dimensional benchmarks?\n3. Strong experimentation showing the effective state-transition minimization with state-of-the-art performance on the Mujoco benchmarks. I know D4RL does not provide datasets for Humanoid but would be interesting to fully finish up the benchmark suite for IFO.\n\n### Weaknesses\n1. One small nitpick would be compare against an Offline IL method such as MILO [1] slightly modified for IFO (train discriminator on (s, s') rather than (s, a)). This would be a comparison against a principled offline method for adversarial IL. This would provide more proof of the stability contribution of this work of having just a minimization objective rather than a min-max objective - a benefit the authors mention throughout the text.\n\n\n[1] Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage, 2021  This is not a critique, but from the experiments it looked like you used 5 expert demonstrations. I'm curious about how low could you go? BC is able to achieve expert performance with as low as 1 trajectory in these environments (I understand BC also gets actions so has a definite advantage), but it would be really cool to see how LobsDICE does with less and less expert demonstrations.  Yes the authors have sufficiently addressed limitations and societal impacts in their paper.",
            " The authors propose an algorithm for offline learning from _observations_, i.e. for learning from observed expert trajectories where only the states, not the actions can be observed. Additionally, similar to some prior work, the authors assume access to a large set of data from suboptimal policies, but containing actions.\n\nOverall, the paper follows the DemoDICE paper very closely, both in structure of the paper as well as in terms of the algorithm, but with the crucial difference than DemoDICE assumes access to expert actions.\n\nInterestingly, the optimization target for both algorithms ends up looking exactly alike:\n\nEquation (23) from LobsDice: \n\n$$\n\\widetilde{\\mathcal{L}}(\\widetilde{\\nu})=(1-\\gamma) \\hat{\\mathbf{E}}\\_{s\\_{0} \\in D_{0}}\\left[\\widetilde{\\nu}\\left(s_{0}\\right)\\right]+(1+\\alpha) \\log \\hat{\\mathbf{E}}\\_{x \\in D^{I}}\\left[\\exp \\left(\\frac{1}{1+\\alpha} \\widehat{A}_{\\widetilde{\\nu}}\\left(s, a, s^{\\prime}\\right)\\right)\\right]\n$$\n\nEquation (17) from DemoDice: \n\n$$\n\\widetilde{L}(\\nu ; r):=(1-\\gamma) \\mathbb{E}\\_{s \\sim p\\_{0}}[\\nu(s)]+(1+\\alpha) \\log \\mathbb{E}\\_{(s, a) \\sim d^{U}}\\left[\\exp \\left(\\frac{A\\_{\\nu}(s, a)}{1+\\alpha}\\right)\\right]\n$$\n\nwith, as far as I can tell, the only difference being whether the discriminator is learned on tuples $(s,s')$ or $(s,a)$.\n\nHowever, as far as I can tell, it is not obvious a-priori that the solutions will end up looking the same, hence making the derivations in the paper necessary. \n\nPerformance is evaluated against several baselines (BC, BCO, DemoDICE with inverse dynamics model, OPOLO) on both a tabular environment and several MuJoCo environments (Hopper, Walker2d, Ant and HalfCheetah), showing good performance. ## Strenghts\n\n* I found the writing accessible and good to follow, as well as the mathematical derivation well explained. I do think the paper might be a bit harder to follow if one is not familiar with similar algorithms (e.g. the *DICE family), so a short recap of their their core ideas in the background section would be helpful. Similarly, a recap of DemoDICE would be helpful and make the close connection clearer.\n* The topic of offline IL is relevant, as is the extension to LfO\n* I liked the experimental section and found both the figures and discussion easy to follow as well as insightful.\n\n## Weaknesses\n\n* I think the presentation of the derivation of the algorithm could be slightly improved. While each step was easy to follow, it took me a while (including going back and forth between LobsDICE and other *DICE papers) to get a decent high level motivation for the individual steps. It certainly does not require any major changes, but maybe a bit of additional signposting could help.\n* I think the relationship to DemoDICE should be made clearer. The authors already mention that DemoDICE solves a related problem, but I would have found it helpful if also the similarities and differences in the solution would be discussed.\n* One could argue that the \"delta\" in terms of research compared to DemoDICE is limited. However, I believe there is sufficient difference to warrant publication.  Please see my suggestions above regarding small changes to the presentation of the paper. \nI don't have any question and I think this is a solid paper which should be accepted.\nI'm not rating it stronger than \"accept\", due to the similarity (in approach and solution) to DemoDICE.  N/A."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Negative",
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer appreciates the authors' work and indicates they will raise their score, which signals a positive overall assessment.",
            "The reviewer states they will keep their score unchanged, indicating no significant positive or negative shift in their assessment.",
            "The reviewer explicitly states their agreement and votes for acceptance, indicating a positive sentiment.",
            "The review expresses concerns about the incremental contribution of the paper, stating that the algorithm and theory heavily borrow from existing work. The reviewer also points out inconsistencies in the experimental results, questioning the performance of the proposed method in deterministic environments.",
            "The review acknowledges both strengths and weaknesses of the paper. It praises the technical soundness and clarity while also raising a significant theoretical concern. The reviewer is open to changing their score based on the authors' response, indicating a neutral stance.",
            "The review expresses excitement and appreciation for the paper's technical contributions, strong experimentation, and potential for stabilization of the DICE objective. The reviewer uses positive language such as \"excited to see\", \"strong experimentation\", and \"state-of-the-art performance\".",
            "The reviewer states \"I think this is a solid paper which should be accepted.\" and expresses positive feedback on the writing, experimental section, and relevance of the topic."
        ],
        "tone": [
            "Supportive",
            "Neutral",
            "Supportive",
            "Critical",
            "Balanced",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer expresses appreciation, offers constructive criticism, and encourages the authors to address a specific issue, indicating a supportive stance. Words like \"appreciate\" and phrases like \"I think the authors should discuss this\" suggest a desire to help improve the work rather than simply criticizing it.",
            "The language is straightforward and lacks strong emotional coloring. The sentence is a simple statement of intent.",
            "The reviewer expresses agreement and supports the acceptance of the work, suggesting a supportive tone. The phrase \"hence my vote for acceptance\" directly conveys support.",
            "The review uses phrases like \"contribution of this paper is incremental\" and poses direct questions challenging the validity of the experimental results (\"how to explain the performance of LobsDICE in figure 2?\"). The identification of weaknesses and questioning of results indicates a critical tone.",
            "The review presents both positive and negative aspects of the paper. It uses phrases like \"technically sound\" and \"well presented\" but also points out \"a little bit incremental\" and raises a \"fundamental question.\" The tone is critical but also acknowledges the potential value of the work and invites a response from the authors.",
            "The reviewer expresses enthusiasm for the work and offers constructive suggestions for improvement, indicating a supportive stance. Phrases like \"Definitely excited to see\" and \"would be really cool to see\" demonstrate a positive and encouraging tone. The reviewer also frames their suggestions as \"nitpick\" and \"not a critique\".",
            "The review provides both strengths and weaknesses of the paper, giving constructive criticism while also acknowledging the paper's value. The reviewer uses phrases like \"should be accepted\" and \"good to follow\" indicating a positive yet critical assessment."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it starts with appreciation for the authors' improvements and indicates a score increase. It then provides a reasonable explanation for the observed poor empirical performance, linking it to hyperparameter tuning. Finally, it raises a valid and constructive point about a broader issue in the field of DICE-based offline imitation learning and suggests addressing it in future work.  There are no contradictory statements or conflicting opinions within the review.",
            "The review is consistent as the reviewer clearly states they are keeping their score unchanged after considering the response. There are no contradictory statements within this short review.",
            "The reviewer states that the extension to DemoDICE is 'not straightforward' and uses this as a reason for acceptance. This implies that the reviewer sees the complexity or non-trivial nature of the extension as a positive aspect, possibly indicating novelty or significant contribution, thus justifying their vote for acceptance. The review is consistent in linking the perceived difficulty with a positive recommendation.",
            "The review is consistent because the reviewer's points are aligned and focused on the incremental contribution and a specific question about the experimental results. The reviewer consistently mentions the similarity to DemoDICE and raises a question about the performance in deterministic environments.",
            "The review is consistent in its assessment. It highlights both the strengths of the paper, such as clarity, technical soundness, and empirical evaluation, and a potential weakness concerning the theoretical foundation related to the distribution matching in offline imitation learning. The reviewer's concerns are presented as questions and suggestions for improvement rather than contradictions to the positive aspects mentioned.",
            "The reviewer's strengths and weaknesses are logically connected and do not contradict each other. The weaknesses are presented as suggestions for improvement and further validation of the paper's contributions, rather than criticisms that undermine the strengths. The reviewer acknowledges the novelty and performance of the method while suggesting additional experiments for a more comprehensive evaluation and deeper understanding of the method's capabilities.",
            "The review consistently points out the similarity to DemoDICE, but acknowledges the novelty in addressing learning from observations and finds the derivations and experimental results to be valuable. The reviewer suggests improvements for clarity and highlighting the relationship to DemoDICE, but ultimately recommends acceptance, indicating a consistent positive assessment despite the incremental nature of the work."
        ]
    },
    {
        "paper_id": "iclr_2021_Qm8UNVCFdh",
        "paper_title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions",
        "paper_abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.",
        "review_ids": [
            "U_W5QKLhvhB",
            "HfT4A1qCpH9",
            "3yD8e2U9rcU",
            "nKbMstJujAI"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper proposes to improve upon unsupervised representation learning for various downstream vision tasks by leveraging human motion and attention (gaze) information. The authors collect a large spatio-temporal dataset with gaze and body motion labels for this task. They train a network to jointly predict the visual focus of attention in scenes and body motion besides visual instance recognition via an NCE loss to learn good visual representations. They show large improvements in accuracy of many different visual recognition downstream tasks with their approach versus the SOTA MOCO approach, which uses visual information only.\n\nPros:\nThe work is novel and considers a new dimension to solving the problem of representation learning, which hasn't been explored before. It explores supervising neural networks to predict humans' motion and visual focus of attention. This is biological motivated by human beings' similar learning strategies. The novel datatset containing both gaze and body motion labels can be useful to the research community for other tasks beyond visual representation learning. The authors show consistent improvements with their proposed method of incorporating knowledge of gaze and body motion versus using visual information only for many downstream tasks they consider. The experimental section is fairly thorough (except for an important missing experiment as explained below). \n\nCons:\nThe authors argue that one of the disadvantages of the current approaches for unsupervised representation learning is that they use ImageNet-type datasets that require significant effort for curation and cleanup. In contrast to this, the authors' proposed approach of requiring large amounts of data with expensive TOBII eye-tracking glasses and body-IMUs from multiple subjects and with special calibration and syntonization procedures seems even more cumbersome and less accessible to ordinary practitioners of AI in the real world. How do the authors justify this? To clearly show the superiority of their dataset versus ImageNet, the authors should also include the results of MOCO trained on ImageNet for each of downstream tasks shown in their paper.\n------\n\nPost Rebuttal:\nI thank the authors for their response and additional experiments to show the performance of MOCO trained on ImageNet for the various downstream tasks considered in this paper. It is evident from the results that the authors presented in Table 5 that their best method (using their multimodal data) performs worse than MOCO trained only on ImageNet with InfoNCE. Hence, while this current work has some interesting novel insights of theoretical value, I don't think the complete proposed method of data collection and training is very practical or broadly scalable. It is likely to be of limited practical applicability.  In contrast to the authors' proposed cumbersome method of collecting annotated data using expensive gaze and motion sensors, cell phones and cameras are nowadays ubiquitous and image and video data is routinely uploaded to the internet by users all over the world. Using such abundantly available existing data on the web, which can often times simply be downloaded for free without any annotations, is what I believe is likely to be a much more practical and broadly applicable approach to solving the problem of representation learning via self-supervision. This concern is also shared by Reviewer 3. \n\nOn weighing the various pros and cons of the proposed approach, I will maintain my previous rating.",
            "The main aim of the paper is to make use of human interaction/motion to learn a visual  representation that can be re-used for classic visual tasks such as depth estimation. The authors claim that by encoding interaction and attention cues in the self-supervised representation, the method can outperform visual-only state-of-the-art methods. To study the interaction element, the authors attach sensors like Inertial Movement Units (IMUs) to the limbs of subjects and monitor their reaction to visual events in daily life. The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants which include synchronized streams of images, body part movements, and gaze information.\n\nPaper Strengths:\nThis is a simply awesome paper. Idea is novel, well-validated, and well-written. The result is strong.\n+ Novel intuition: The idea of the paper is intuitive, where it proposes to incorporate body part movements and gaze information in learning visual representations. Attention does play an impact in many tasks like action recognition and scene classification, which might benefit from the proposed representation learning. Also, in case of tasks like depth estimation and future prediction of dynamics, it is insightful to use body movement since it encodes temporal changes.\n+ Experimental setup and ablation studies: The intuition of the authors to incorporate body part movements and gaze information in their representation has been well justified by the experimental setups and ablation studies. The importance of using each objective in the representation learning has been effectively demonstrated by showing its impact on various target tasks.\n+ Performance: The authors have shown that the representation, trained using movement and attention supervision, outperforms the visual-only representations in all the tasks mentioned in the paper by a range of 1.3% to 7%\n+ Dataset: The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants. This is a novel dataset with synchronized streams of images, body part movements and gaze information.\n\nPaper Weaknesses:\n- Objective function selection: In the ablation studies of body parts, we see how the removal of a body part can affect the performance on the target tasks. However, it is still not clear how each individual part may fare since the performances do not vary greatly from each other. The exclusion of the torso results in a lower error for the depth estimation, but an explanation for why exactly would that be the case may be beneficial, since intuitively it may seem that the complete movement of the body should result in better performance.\n",
            "\n# Paper Summary \n\nThe paper uses a combination of visual, human gaze and human motion sensors to build representations that perform better on downstream tasks such as action recognition, physics prediction and depth estimation than representations extracted from solely visual input. The paper announces the release a new data set of aligned visual images, eye gaze fixations and IMU motion readings from test subjects walking around an environment. Representations are computed using three different forms of information simultaneously. Given a visual input, the system tries to predict the location of eye gaze in image frame coordinates, whether each of 6 groups of motion detectors are active or not (head, torso, legs, etc.) and the result of a more traditional auxiliary visual pretext task. In this work, the paper uses \u201cinstance discrimination\u201d where representations of augmented versions of a specific image are pushed close together in latent space and far away from augmentations of other images. Tests on diverse benchmarks show that the gaze and motion prediction improve over visual pretext tasks alone and that there is a small benefit to using both together, but it is not additive. The paper also shows the benefit of gaze and motion is present for two different visual auxiliary tasks.\n\n\n# Pros and Cons \n\nThe paper defines a new way of obtaining self-supervised representations for a variety of core computer vision tasks which is important and highly relevant to the ICML community. \n\nThe paper clearly describes prior work and situates its contributions well within this space. \n\nThe paper evaluates the proposed augmented loss function on a diverse collection of standard benchmarks to test their hypothesis including scene classification, action recognition, future motion, walkable surfaces and depth estimation. \n\nTable 1 provides a clear ablation study showing that predicting gaze and motion improve downstream performance on diverse benchmarks by non-trivial amounts:  ~7% , 3.5%, 1%, 1% and reduce RMSE  on depth estimation by 2% or so.  \n\nIn the first benchmark, adding either attention or movement increases results by 6%, but adding both only improves results by 7%, not 14%! This suggests that there is a lot of redundant information between gaze and motion sensors which is a surprising as superficially they seem like very different modalities. \n\nIt is interesting and surprising that eye-gaze, where a person is looking in an image, is a useful feature for tasks such as depth prediction. This suggests there are features that are easy to compute (compared to interactions between robots and physical world)  that significantly improve network intuitions about what is important in images.  \n\nFrom table 4, it seems like Torso movement is more informative for scene classification, neck more informative for action recognition, arms for dynamics and arms for walkability. It is interesting that different parts are more or less informative for different tasks. \n \nI am a little unsure what inference to draw about the results in table 2 (experiments on Lnce vs. Lae). I guess the main point is that the addition of gaze and motion also improves Lae results, so the effect is not specific to Lnce? It is interesting that gaze and motion are not as helpful with Lae ( 3% gain vs 7% with Lnce) but it is not clear why. Why would there be better complementarity between gaze, motion and NCE? Some thoughts on this would be interesting. \n\n\n# Recommendation  \n\nAccept - it is an interesting and surprising result that adding self-supervised tasks for other modalities significantly improves self-supervised representations and that gaze and motion are redundant with each other.  \n\n \n# Questions  \n\nHow are alpha, beta and gamma set? Hmm, see from appendix A.4.1 these are 0.09, 0.01 and 0.9 respectively indicating that visual loss is the primary driver here and the others are acting more like regularizations on the representation. It is a bit surprising that the other modalities have so large an effect given their small weights in the loss function. Were any other forms of regularization used such as dropout or weight decay?  Didn\u2019t see anything about this in the appendix. \n\nSection 5.3.2 It seems surprising that gaze, a 2 dimensional quality needs a 512 D embedding. Is this some sort of one-hot encoding over a matrix of locations? \n\n \n# Feedback  \n\nSection 3: \u201cfeature extractor to embed a more detailed representation\u201d \u2026 I don\u2019t know if it is more detailed, but hopefully, it is more invariant, at least to the augmentations applied, ideally to all non-semantic attributes of the image. \n\nI guess it is implied that the baseline \u2018vis\u2019 approach in the paper  is identical to the \u2018vis\u2019 in He et al 2020, but it would be nice if this was explicitly in the caption for Table 1.  \n\n ",
            "The paper proposes an interesting video dataset with body-part movement signal, gaze signal. And they also treat these signals together with infoNCE/or AE as self-supervised signals. With random initialization on ResNet18, their model is better than the model only with infoNCE/AE. \n\nI agree with the author that their supervision is working and working together with infoNCE. However, this supervision does not come for free like Rotation/Speed/Contrastive learning. For example, how do you apply your signal on the same 'egocentric' video like 'Epic-kitchen' dataset?\n\nPros: a new dataset contributing to the 'affordance' society, a novel and useful supervision signal.\n\nCons: Even though the signal is useful, it does not come for free like SpeedNet, Rotationnet or infoNCE, they still need to capture the information when recording the data.\n\nIf the author can make the supervision more free?(self-supervision), I can change my rating.\n\n"
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Neutral"
        ],
        "sentiment_reason": [
            "The reviewer acknowledges the theoretical novelty but ultimately concludes that the proposed method is impractical and of limited applicability, particularly compared to existing methods using readily available data. The reviewer explicitly states that the authors' method performs worse than MOCO trained on ImageNet, reinforcing the negative sentiment.",
            "The review contains overwhelmingly positive language such as \"simply awesome paper,\" \"novel intuition,\" \"well-validated,\" \"strong result,\" and \"outperforms.\" The weaknesses mentioned are framed as areas for improvement rather than critical flaws.",
            "The review expresses overall positive feedback, highlighting the paper's novelty, clear presentation, comprehensive evaluation, and interesting findings. The recommendation to 'Accept' further reinforces the positive sentiment.",
            "The review expresses both positive aspects (novel dataset, useful signal) and negative aspects (supervision not 'free'). The overall sentiment is therefore neutral."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer raises serious concerns about the practicality and scalability of the proposed method, using phrases like 'cumbersome method,' 'limited practical applicability,' and 'performs worse than MOCO.' The tone is critical of the method's real-world viability.",
            "The reviewer consistently uses positive and encouraging language. Phrases like \"simply awesome paper\" and \"well justified\" clearly convey support. The weaknesses are presented constructively, aiming to help improve the paper.",
            "The reviewer uses encouraging language, such as 'interesting and surprising,' and provides constructive feedback and questions to help improve the paper, indicating a supportive and helpful tone. The reviewer also acknowledges the paper's strengths and contributions to the field.",
            "The review provides both 'Pros' and 'Cons' and offers constructive criticism. The reviewer acknowledges the paper's strengths while also pointing out limitations and suggesting improvements, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer's concerns about the practicality and scalability of the proposed method are consistent throughout the review process. Initially, the reviewer questioned the practicality compared to ImageNet. After rebuttal, the reviewer acknowledges the authors' response but highlights that the proposed method performs worse than MOCO on ImageNet, reinforcing the initial concern about practicality and limited applicability. The reviewer maintains the initial rating, indicating a consistent stance.",
            "The review is consistent in its assessment. It praises the paper's novelty, experimental setup, performance, and dataset in the 'Strengths' section. The 'Weaknesses' section raises a valid question about the interpretation of ablation studies, specifically regarding the impact of individual body parts and the unexpected result with torso exclusion. This is a constructive criticism and does not contradict the overall positive evaluation of the paper's strengths.",
            "The review is consistently positive, highlighting the paper's novelty, relevance, and strong experimental results. The reviewer recommends acceptance and raises constructive questions and feedback points that do not contradict the overall positive assessment. The reviewer expresses surprise at some findings, but these are presented as interesting observations rather than criticisms.",
            "The review is consistent in praising the contribution of the dataset and supervision signal, but consistently points out that the supervision is not as 'free' or broadly applicable as other self-supervised methods because it requires specific data collection during recording."
        ]
    },
    {
        "paper_id": "iclr_2019_SJlh2jR9FX",
        "paper_title": "Learning with Reflective Likelihoods",
        "paper_abstract": "Models parameterized by deep neural networks have achieved state-of-the-art results  in  many  domains.  These  models  are  usually  trained  using  the  maximum likelihood principle with a finite set of observations. However, training deep probabilistic models with maximum likelihood can lead to the issue we refer to as input forgetting. In deep generative latent-variable models, input forgetting corresponds to posterior collapse---a phenomenon in which the latent variables are driven independent from the observations. However input forgetting can happen even in the absence of latent variables.  We attribute input forgetting in deep probabilistic models to the finite sample dilemma of maximum likelihood.  We formalize this problem and propose a learning criterion---termed reflective likelihood---that explicitly prevents input forgetting. We empirically observe that the proposed criterion significantly outperforms the maximum likelihood objective when used in classification under a skewed class distribution.  Furthermore, the reflective likelihood objective prevents posterior collapse when used to train stochastic auto-encoders with amortized inference.  For example in a neural topic modeling experiment, the reflective likelihood objective leads to better quantitative and qualitative results than the variational auto-encoder and the importance-weighted auto-encoder.",
        "review_ids": [
            "BkeaOAeg1E",
            "SJlHDE5ARQ",
            "S1xoTX5ARQ",
            "r1lJnt8AAQ",
            "rJlp3F0d2Q",
            "rJl5OkRLnm",
            "HklxkbeU3X",
            "r1xoJGzy3X"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "Thanks for your responses and revisions, authors.  I do find the finite sample derivation in Equations 3-6 of the new draft to be clearer.  It is an interesting observation.  \n\nUnfortunately, my \"biggest issue\" of the lack of rigor still stands.  While the derivations are rigorous in the new draft (i.e. I follow the algebra), I don't find the motivations and logic behind them to be.  I don't find them to have a clear flaw per se, but I find them hand-wavy.  Overall, it's very hard for me to swallow that there is a deficiency in maximum likelihood learning and that Equation 7 fixes this deficiency in just ~two pages of exposition.  Moreover, there are still no theorems, clear mathematical definitions, or some simulations.  For instance, can you show how the reflective likelihood changes analytically tractable / closed-form solutions?  What would the 'reflective OLS estimator' be?  These simpler, classical cases need addressed before I could be convinced that it fixes the problem.  ",
            "But you are now addressing something else.",
            "In the original version, you wrote: \"Contributions. We identify a peculiarity in maximum likelihood learning that causes the input forgetting problem in Section 2.1. We then propose a new learning criterion to mitigate this issue.\"  The intend was to address \"a peculiarity in ML learning\".  But, we are addressing something else. This is a change of main theory (story) to me.\n\n\n\n",
            "The authors modified the paper, but did not explain how my concern is addressed. Although the theory is changed, the objective function remains the same. I am not convinced that the issue is addressed. \n\nIn addition, the purpose of the rebuttal process is to provide authors with the opportunity to clarify misunderstandings, NOT to change the main theory. If the main theory is flawed, the paper cannot be accepted in this round. ",
            "Summary:\n\nThis paper proposes maximizing the \u201creflective likelihood,\u201d which the authors define as: E_x E_y [log q(y|x) - \\alpha log q(y)] where the expectations are taken over the data, q is the classifier, and \\alpha is a weight on the log q(y) term.  The paper derives the reflective likelihood for classification models and unsupervised latent variable models.  Choices for \\alpha are also discussed, and connections are made to ranking losses.  Results show superior F1 and perplexity in MNIST classification and 20NewsGroups modeling.\n\nPros:\n\nI like how the paper frames the reflective likelihood as a ranking loss.  It does seem like subtracting off the marginal probability of y from the conditional likelihood should indeed \u2018focus\u2019 the model on the dependent relationship y|x.  Can this be further formalized?  I would be very interested in seeing a derivation of this kind.    \n\nI like that the authors test under class imbalance and report F1 metrics in the experiments as it does seem the proposed method operates through better calibration.\n\nCons:\n\nMy biggest issue with the paper is that I find much of the discussion lacks rigor.  I followed the argument through to Equation 3, but then I became confused when the discussion turned to \u2018dependence paths\u2019: \u201cwe want our learning procedure to follow the dependence path\u2014the subspace in \u0398 for which inputs and outputs are dependent. However this dependence path is unknown to us; there is nothing in Eq. 1 that guides learning to follow this dependence path instead of following Eq. 3\u2014the independence path\u201d (p 3).  What are these dependence paths?  Can they be defined mathematically in a way that is more direct than switching around the KLD directions in Equations 1-3?  Surely any conditional model x-->y has a \u2018dependence path\u2019 flowing from y to x, so it seems the paper is trying to make some stronger statement about the conditional structure?\n\nMoving on to the proposed reflective likelihood in Equation 4, I could see some connections to Equations 1-3, but I\u2019m not sure how exactly that final form was settled upon.  There seems to be a connection to maximum entropy methods?  That is,   E_x E_y [log q(y|x) - \\alpha log q(y)] = E_x E_y [log q(y|x)] + \\alpha E_y [ -log q(y)] \\approx E_x E_y [log q(y|x)] + \\alpha H[y], if we assume q(y) approximates the empirical distribution of y well.  Thus, the objective can be thought of as maximizing the traditional log model probability plus an estimate of the entropy.  As there is a long history of maximum entropy methods / classifiers, I\u2019m surprised there were no mentions or references to this literature.  Also, I believe there might be some connections to Bayesian loss calibration / risk by viewing \\alpha as a utility function (which is easy to do when it is defined to be data dependent).  I\u2019m less sure about this connection though; see Cobb et al. (2018) (https://arxiv.org/abs/1805.03901) and its citations for references.   \n\nThe data sets used in the experiments are also somewhat dissatisfying as MNIST and 20NewsGroups are fairly easy to get high-performing models for.  I would have liked to have seen more direct analysis / simulation of what we expect from the reflective likelihood.  As I mentioned above, I suspect its really providing gains through better calibration---which the authors may recognize as F1 scores are reported and class imbalance tested---but the word \u2018calibration\u2019 is never mentioned.  More direction comparison against calibration methods such as Platt scaling would be make the experiments have better focus.  It would be great to show that this method provides good calibration directly during optimization and doesn\u2019t need the post-hoc calibration steps that most methods require. \n\nEvaluation:  While the paper has some interesting ideas, they are not well defined, making the paper unready for publication.  Discussion of the connections to calibration and maximum entropy seems like a large piece missing from the paper\u2019s argument.  ",
            "This paper is technically flawed. Here are three key equations from Section 2. The notations are simplified for textual presentation:  d \u2013 p_data; d(y|x) \u2013 p_d(y|x); m(y|x) \u2013 p_theta(y|x)\n\nmax E_x~d E_y~d(y|x) [ log m (y|x) ]                 \t\t\t\t               (1) \nmax E_x~d { E_y~d(y|x) ) [ log d(y|x) ]}  -  E_y~d(y|x) [ log m (y|x) ]}        (2)\nmax { E_y~d [  log  (y) ]  -  E_y~d  log E_x~d(x|y) [ m (y|x) ]}                        (3)\n\nFirst error is that the \u201cmax\u201d in (2) and (3) should be \u201cmin\u201d. I will assume this minor error is corrected in the following.\nThe equivalence between (1) and (2) is correct and well-known. The reason is that the first entropy term  in (2) does not depend on model.  The MAJOR ERROR is that (1) is NOT equivalent to (3). Instead, it is equivalent to the following:\n\n min { E_y~d [  log d (y) ]  -  E_y~d  E_x~d(x|y) [ log m (y|x) ]}                     (3\u2019)\n\nNotice the swap of \u201cE_x\u201d and \u201clog\u201d. By Jensen\u2019s nequality, we have \n\n log E_x~d(x|y)  m (y|x) ]  > E_x~d(x|y) [ log m (y|x)\n -  E_y~d  log E_x~d(x|y)  [ m (y|x) ]    < -  E_y~d  E_x~d(x|y) [ log m (y|x) ]                    \n\nSo, minimizing (3) amounts to minimizing a lower bound of the correct objective (3\u2019). It does not make sense at all.\n",
            "The paper proposes a modification of maximum likelihood estimation that encourages estimated predictive/conditional models p(z|x) to have low entropy and/or to maximize mutual information between z and x under the model p_{data}(x)p_{model}(z|x).\n\nThere is pre-existing literature on encouraging low entropy / high mutual information in predictive models, suggesting this can indeed be a good idea. The experiments in the current paper are preliminary but encouraging. However, the setting in which the paper presents this approach (section 2.1) does not make any sense. Also see my previous comments.\n\n- Please reconsider your motivation for the proposed method. Why does it work? Also please try to make the connection with the existing literature on minimum entropy priors and maximizing mutual information.\n\n- Please try to provide some guarantees for the method. Maximum likelihood estimation is consistent: given enough data and a powerful model it will eventually do the right thing. What will your estimator converge to for infinite data and infinitely powerful models?",
            "You write \"Maximizing Eq. 1 can be achieved by maximizing either Eq. 2 or Eq. 3 or both\", and this seems to be crucial to your motivation for the proposed method. However this statement is false. It's true that the true conditional model p(y|x) is a solution to eq 1,2 and 3, but the converse does not hold: There are many solutions to equation 3 that do not maximize equation 1. You are basically claiming that maximum likelihood is not a consistent estimation method, contradicting all of the statistical literature. Please clarify your motivation for the proposed method, and let me know if I'm misunderstanding."
        ],
        "sentiment": [
            "Negative",
            "Neutral",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses significant concerns about the paper's rigor and the justifications provided. Phrases like \"biggest issue' of the lack of rigor still stands\", \"I don't find the motivations and logic behind them to be\", \"I find them hand-wavy\", and \"hard for me to swallow\" clearly indicate a negative assessment. The reviewer also points out the absence of key elements like theorems, definitions, and simulations, further reinforcing the negative sentiment.",
            "The statement is a factual observation without expressing a positive or negative opinion.",
            "The reviewer expresses concern about a 'change of main theory (story),' indicating a significant issue with the paper's consistency and focus.",
            "The reviewer expresses dissatisfaction with the authors' response, stating they did not explain how their concern was addressed. Phrases like \"I am not convinced\" and \"If the main theory is flawed, the paper cannot be accepted\" indicate a negative sentiment.",
            "The review expresses several concerns about the paper's rigor, clarity, and experimental setup. Phrases like \"lacks rigor,\" \"I became confused,\" \"not sure how exactly that final form was settled upon,\" \"somewhat dissatisfying,\" and \"not well defined\" indicate a negative sentiment. The reviewer also states the paper is \"unready for publication.\"",
            "The review identifies major technical flaws in the paper, using phrases like 'technically flawed,' 'MAJOR ERROR,' and 'does not make sense at all.'",
            "The review expresses concerns about the paper's motivation and theoretical guarantees. While acknowledging the potential of the approach and encouraging experimental results, the reviewer finds the setting in section 2.1 nonsensical and requests a reconsideration of the motivation and connection to existing literature. The request for theoretical guarantees also indicates a critical perspective.",
            "The reviewer directly states that a key claim is 'false' and suggests the authors are contradicting established statistical literature, indicating a negative evaluation."
        ],
        "tone": [
            "Critical",
            "Neutral",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical and questioning. The reviewer directly challenges the authors' claims with phrases like \"hard for me to swallow that...\" and poses pointed questions about the implications of their method (e.g., \"can you show...?\", \"What would the...?\" ). The repeated use of \"I don't find\" also contributes to the critical tone.",
            "The statement is direct and objective, lacking emotional coloring or specific stylistic features that would indicate a particular tone.",
            "The reviewer uses phrases like 'change of main theory (story)' and 'addressing something else,' which directly point out a flaw and express disagreement with the paper's direction. The use of 'But, we are addressing something else' highlights a disconnect between the stated intention and the actual content.",
            "The tone is critical, using phrases like \"did not explain,\" \"I am not convinced,\" and directly stating that the paper cannot be accepted if the main theory is flawed. The reviewer also points out what they perceive as a misuse of the rebuttal process.",
            "The review adopts a critical tone by directly pointing out flaws in the paper's arguments, mathematical derivations, experimental design, and connections to existing literature. Specific criticisms include the lack of rigor in defining \"dependence paths,\" the unclear derivation of the reflective likelihood, the absence of discussion on maximum entropy methods, the use of easily solvable datasets, and the lack of direct comparison with calibration methods. The concluding statement that the paper is \"unready for publication\" reinforces the critical tone.",
            "The reviewer directly points out errors and inconsistencies in the paper's equations and logic, using phrases like 'First error is that...', 'The MAJOR ERROR is that...', and highlighting the incorrect equivalence between equations.",
            "The reviewer uses phrases like \"does not make any sense\" and \"Please reconsider your motivation.\" The direct and questioning nature, along with the demand for guarantees, points to a critical tone.",
            "The reviewer's tone is critical, employing phrases like 'this statement is false' and 'contradicting all of the statistical literature' to directly challenge the authors' reasoning and methodology."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it acknowledges the improvement in the clarity of derivations but maintains the main concern about the lack of rigor in the motivations and logic behind them. The reviewer consistently points out the need for more rigorous justification, mathematical definitions, and empirical evidence to support the claims.",
            "The review is consistent as it presents a single, clear point about the author addressing a different topic. There are no contradictory statements within this short review.",
            "The reviewer points out an inconsistency between the initially stated contribution of addressing 'a peculiarity in ML learning' and what the reviewer perceives as the current focus of the paper, which is 'something else'. This indicates a change in the main theory or story of the paper, leading to inconsistency.",
            "The reviewer consistently argues that their concern was not addressed despite the authors modifying the paper. The reviewer supports this by stating that while the theory changed, the objective function remained the same, and emphasizes that rebuttal is for clarification, not for major theory changes, reinforcing the point that the issue remains unaddressed in the current revision.",
            "The review is consistent because it acknowledges some positive aspects of the paper, such as framing the reflective likelihood as a ranking loss and testing class imbalance. However, the review consistently points out significant weaknesses, including a lack of rigor in the discussion, unclear definitions (e.g., 'dependence paths'), insufficient justification for the proposed method, missing connections to relevant literature (maximum entropy, calibration), and weak experimental validation. The evaluation section summarizes these negative points and concludes that the paper is not ready for publication, maintaining a consistent critical stance throughout.",
            "The review is consistent because the reviewer clearly identifies errors in the equations, explains the nature of these errors (minor and major), and provides justifications for their claims using mathematical reasoning (equivalence between (1) and (2), non-equivalence between (1) and (3), and application of Jensen's inequality). The reviewer's arguments are logically connected and do not contradict each other.",
            "The review is consistent because it acknowledges the potential of the proposed method by referencing existing literature and preliminary encouraging results, while also consistently pointing out weaknesses in the motivation, theoretical grounding, and presentation of the method, requesting improvements in these specific areas. There is no contradiction in acknowledging potential while criticizing current shortcomings and asking for improvements.",
            "The review is consistent because the reviewer clearly identifies a logical flaw in the author's reasoning. The reviewer argues that the author's claim about maximizing equations is false and explains why, based on established statistical principles related to maximum likelihood estimation. The reviewer's points are logically connected and do not contradict each other."
        ]
    },
    {
        "paper_id": "nips_2021_OQLCPvYnMOv",
        "paper_title": "Submodular + Concave",
        "paper_abstract": "Siddharth Mitra, Moran Feldman, Amin Karbasi",
        "review_ids": [
            "YKbyNcTcGD8",
            "orqzmcBi0W_",
            "0j66jZpXGHF",
            "LMAW43hUcyI",
            "HXcc1dPxrGB"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies maximization of submodular + concave functions over solvable polytopes. This problem forms a new class of non-convex optimization problems, for which no theoretical results have been obtained. Depending on the conditions of the problem, the authors develop several algorithms and prove that they return approximately optimal solutions. Experiments demonstrate the practical utility of the proposed algorithms. \n\n--- After rebuttal ---\n\nI appreciate the authors' kind response. My technical concerns have been resolved. However, I still think the experimental comparison in running time is important to clearly demonstrate the effectiveness of Algorithm 4 since $\\nabla \\bar{G}$ usually requires more computation costs than $\\nabla \\bar{G}$, as mentioned in the response. Despite the lack of experiments, the theoretical contributions are great and I think the paper is above the acceptance threshold. Therefore, I keep my score of 6.   The paper addresses a new class of optimization problems, which are motivated by some machine learning applications (e.g., DPP + clustered). The theoretical guarantees are reasonable and strong. Overall, I think this is a good paper. There are, however, some concerns and comments as listed below: \n\n1. Since each algorithm and its analysis are explained only briefly, it is hard to see how novel and significant each of them is. Algorithms 1 and 2 seem to be similar to continuous greedy and measured continuous greedy. Regarding the analyses of them, are there some special techniques required to deal with the additional concave function? Regarding Algorithms 3 and 4, are there existing studies that provide their bases, or are they newly developed for submodular + concave maximization?\n\n2. I would like the authors to show how to guarantee outputs of Algorithm 2 are included in $P$. \n\n3. I think the class of quadratic functions considered in Section 4.2.1 is interesting. I would appreciate it if the authors could present relevant previous studies on non-convex quadratic programming, if any, and explain advantages of the submodular + concave framework in the quadratic-programming context. \n\n4. In Figure 3, algorithms are compared based on the number of iterations. What if those are compared based on running times?\n\nMinor comments: \n\n- It is better to define $\\bar 0$ and $\\bar 1$ in Section 2.  The limitations are described. The paper seems to have no negative societal impacts. ",
            "Authors consider the problem maximizing a function that can be written as the addition of submodular and concave functions. They provide multiple Frank-Wolfe-based algorithms for this problem with different approximation factors.   1)Author motivates the subject from a theoretical and practical point of view. But all the experiments are on synthetic data. It would be great if they can add simulation for an application on a real dataset.\n\n2)In the introduction, the author reviewed the previous works carefully.\n\n3)the author successfully addressed the issue and recommended the algorithms. They proposed this new problem. Their idea to change the frank Wolfe algorithm to come up with a better approximation factor is interesting. Although, if they can characterize what type of functions can be written as sub+concave or, for example, come up with the condition on second-order derivative, which is equivalent to writing a function as sub+concave that would be very interesting.\n\n4)The authors successfully provide simulation results for their algorithm.\n\n5)The paper is well-organized and well-written.\n\n6)The proof is mathematically correct.\n\n7)the code is included. \n\nOverall, the idea is interesting, and the author's way of solving the problem is interesting too.\n\n\n----------- Post Rebuttal------------- \n\nI have read all the comments, and I don't change my score. The authors discussed the limitations and potential negative social impact of their work.\n\n",
            "This paper studies a class of optimization problems where the goal is to maximize the sum of a DR-submodular function and a concave function. Depending on whether the DR-submodular function is monotone, the concave function is monotone or non-negative, and the constraint set is down-closed, the authors have proposed 4 algorithms with various approximation ratios for the DR-submodular and concave parts of the objective function. Finally, this work provides a set of experiments to compare the performance of the proposed algorithms with baseline algorithms for DR-submodular maximization and concave maximization.  The paper has many strengths that are listed below:\n- Theoretical analysis of the previously unstudied framework of DR-submodular + concave maximization and providing new algorithms (inspired by the algorithms for DR-submodular maximization) that obtain separate approximation ratios for the DR-submodular and the concave part.\n- Comprehensive set of numerical experiments to highlight the performance of the proposed algorithms and compare/contrast them with the previously known algorithms for either DR-submodular maximization or concave maximization.\n- Great review of all the previous works that are either studying a similar framework or using similar techniques and ideas.\n- Easy to follow proofs (in the appendix) for all the claims and results of the paper.\n\nOn the other hand, this work has some limitations as follows:\n- In my view, the framework of DR-submodular + concave maximization is not well motivated in the paper. While the DR-submodular + linear setting is well-motivated in prior works, I'm not sure how important this extension to concave functions is (aside from the additional theoretical challenges to obtain results). It seems like the chosen concave functions in all the provided experiments are also arbitrarily chosen and not inspired by a practical application.\n- One of the most important subclasses of this framework is \"monotone non-negative DR-submodular - monotone non-negative convex\" maximization which is a penalized formulation of constrained DR-submodular maximization with convex constraints. Out of the 4 proposed algorithms, the Gradient Combining Frank-Wolfe algorithm (with approximation ratios 1/2 and 1 for the DR-submodular and concave parts respectively) is the only one that could be used for this problem. Given the specific structure of the concave function in this example (negative and monotone decreasing), I think it is possible to obtain a better approximation ratio for the DR-submodular part.\n- Most of the ideas for designing the proposed algorithms are heavily inspired by prior works on submodular maximization and although the theoretical analysis for the DR-submodular + concave is not exactly similar to prior results, it could be considered a straightforward extension of previous proofs (particularly for the first two proposed algorithms). \n**********************************\nPost-Rebuttal Update: Thanks to the authors for their careful responses to the raised questions. I totally agree that \"the addition of a concave function can be seen as the usage of a concave regularizer for the maximization of DR-submodular functions\" and this is basically the \"monotone non-negative DR-submodular - monotone non-negative convex\" case that I mentioned earlier. For instance, a knapsack or budget constraint $c^Tx\\leq 1$ could be enforced in the penalized form as $C(x)=-P(c^Tx)$ where $P$ is a monotone and convex function (e.g., $P(z)=exp(z)$). I think a separate discussion of this framework and obtaining an improved approximation ratio $\\alpha>\\frac{1}{2}$ for this special case would be interesting. No, however, I don't think neither was really needed to be addressed. In terms of limitations, Table 1 clearly highlights the settings that each of the proposed algorithms could be applied to, and therefore, it is easy to infer the problems that are not addressed by this work. Also, this paper is mainly a theoretical work and does not raise any potential ethical concerns.",
            " Here is some reference which explicitly used up-concavity beyond DR-submodularity. The author applied Frank-Wolfe to some up-concave function appearing in CVaR optimization to get $(1-1/e)$-approximation. I hope this helps.\n\nWilder, B. (2018). Risk-Sensitive Submodular Optimization. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). \nhttps://ojs.aaai.org/index.php/AAAI/article/view/12121\n",
            "This paper studies the maximization of a nonconvex function in the form of $F = G + C$ on a convex set $P \\\\subseteq [0,1]^n$, where $G$ is DR-submodular and $C$ is concave. For various settings (monotonicity/nonnegativity of $G$ or $C$, $P$ is a down-closed/general convex set, the gradient oracle is for only $F$/both $G$ and $C$, etc.), the authors developed the first approximation algorithms.\n\nThe algorithms exploit some ideas from submodular function maximization such as Frank-Wolfe, measured continuous greedy, etc. The algorithms have provable fine approximation guarantees, i.e., they achieve $\\\\alpha$- and $\\\\beta$-approximation for $G$ and $C$, respectively. These are stronger guarantees than the standard approximation results in submodular function maximization. Furthermore, the authors proved even tighter approximation ratios (i.e., $\\\\beta \\\\approx 1$) if the individual gradients of $G$ and $C$ are available.\n\nThe authors conducted the numerical experiments in (nonconvex) quadratic programming and D-optimal design. The proposed algorithms achieved better objective values compared to the baseline methods.  Overall, I enjoyed reading this paper. The class of nonconvex functions is reasonable and interesting because we can obtain constant-factor approximation guarantees. The paper is well-structured and easy to follow (if one is familiar with submodular/convex analysis). The authors provided effective algorithms for various settings. Each proposed algorithm nicely extends the known algorithm for submodular or concave maximization. The experiment with real-world data shows the efficacy of the proposed algorithms. In summary, the paper made significant theoretical contributions to nonconvex optimization using submodular maximization techniques, and the results are supported by real-world experiments. I would recommend accepting it.\n\n## Minor comment\nThe class of submodular+concave is contained in the class of up-concave functions (i.e., concave along nonnegative directions). Can we understand the results from the up-concavity of objective functions? I guess Greedy Frank-Wolfe also works for up-concave functions with the same approximation ($\\\\alpha=\\\\beta=1-1/e$), but not sure for the rest. It would be helpful if the authors have comments on the relation to up-concavity. N/A"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states that the paper is \"above the acceptance threshold\" and that the \"theoretical contributions are great.\" They also mention the theoretical guarantees are \"reasonable and strong\" and conclude by saying \"this is a good paper.\"",
            "The reviewer expresses positive feedback such as 'interesting idea,' 'successfully addressed the issue,' 'algorithm is interesting,' 'paper is well-organized and well-written,' and 'proof is mathematically correct.' The reviewer maintains their score even after the rebuttal.",
            "The reviewer acknowledges the paper's strengths, such as theoretical analysis, comprehensive experiments, a great review of previous works, and easy-to-follow proofs. The post-rebuttal update shows the reviewer is satisfied with the authors' responses and no longer feels the limitations are critical.",
            "The reviewer is providing helpful information and expressing a positive attitude towards assisting the author.",
            "The reviewer states they 'enjoyed reading this paper' and concludes with 'I would recommend accepting it.' They also highlight the paper's significant theoretical contributions and the efficacy of the proposed algorithms."
        ],
        "tone": [
            "Balanced",
            "Supportive",
            "Balanced",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "While ultimately positive, the review presents both strengths (novel problem, strong theoretical guarantees) and weaknesses (lack of experimental comparison, unclear novelty of algorithms). The reviewer also poses questions and suggestions for improvement, indicating a constructive and balanced approach.",
            "The reviewer provides constructive feedback and uses positive language like 'great,' 'interesting,' and 'successfully.' They also acknowledge the authors' efforts and contributions.",
            "The review presents both strengths and limitations of the paper, using phrases like \"The paper has many strengths that are listed below\" and \"On the other hand, this work has some limitations as follows.\" The reviewer also offers constructive criticism and suggestions for improvement, indicating a balanced perspective. The post-rebuttal update maintains a respectful and objective tone.",
            "The reviewer uses phrases like \"I hope this helps,\" indicating a desire to be supportive and helpful to the author. The act of providing a relevant reference also contributes to the supportive tone.",
            "The reviewer uses positive language such as 'reasonable and interesting,' 'well-structured and easy to follow,' 'effective algorithms,' 'nicely extends,' and 'significant theoretical contributions.' The recommendation for acceptance further reinforces the supportive tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer initially raised concerns about the novelty and significance of algorithms, guarantee of Algorithm 2 outputs, context of quadratic functions, and comparison based on running times. After rebuttal, the reviewer acknowledges that their technical concerns have been resolved and maintains a positive stance, emphasizing the strong theoretical contributions and acceptance worthiness of the paper. The reviewer's suggestion for running time experiments is presented as an improvement rather than a critical flaw, and the overall sentiment remains consistent in favor of the paper's acceptance.",
            "The review is consistently positive, praising various aspects of the paper such as the motivation, literature review, proposed algorithm, simulation results, writing, proofs, and code. The suggestions are constructive and do not contradict the overall positive assessment. The reviewer maintains their score after rebuttal, reinforcing the consistency.",
            "The reviewer maintains a consistent stance throughout the review and post-rebuttal. Initially, they point out both strengths and weaknesses of the paper, focusing on the theoretical contribution and experimental validation as strengths, while questioning the motivation and novelty as weaknesses. In the post-rebuttal, the reviewer acknowledges the authors' explanation and reinforces their point about a specific subclass, suggesting further exploration but not retracting any previous points or introducing contradictions. The reviewer's opinion evolves slightly in understanding but remains consistent in the overall assessment of the paper's contributions and areas for potential improvement.",
            "The review is consistent as it provides a single, clear message by pointing to a relevant reference without any contradictory statements.",
            "The review is consistently positive, highlighting the paper's strengths in theoretical contributions, algorithmic design, experimental validation, and clarity. The reviewer explicitly recommends acceptance and the minor comment is a suggestion for improvement, not a criticism or contradiction."
        ]
    },
    {
        "paper_id": "iclr_2022_e0jtGTfPihs",
        "paper_title": "Signing the Supermask: Keep, Hide, Invert",
        "paper_abstract": "The exponential growth in numbers of parameters of neural networks over the past years has been accompanied by an increase in performance across several fields. However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications, since hardware requirements increased accordingly. \nTackling both issues, we present a novel approach that either drops a neural network's initial weights or inverts their respective sign. \nPut simply, a network is trained by weight selection and inversion without changing their absolute values.\nOur contribution extends previous work on masking by additionally sign-inverting the initial weights and follows the findings of the Lottery Ticket Hypothesis.\nThrough this extension and adaptations of initialization methods, we achieve a pruning rate of up to 99%, while still matching or exceeding the performance of various baseline and previous models.\nOur approach has two main advantages.\nFirst, and most notable, signed Supermask models drastically simplify a model's structure, while still performing well on given tasks.\nSecond, by reducing the neural network to its very foundation, we gain insights into which weights matter for performance. \nThe code is available on GitHub.",
        "review_ids": [
            "eAINb6vcAp4",
            "KAMCyDvDWQ",
            "bOYOmtl35SH",
            "sJZHycX0mJ",
            "qjs8VSws3Jy"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This work extends to (Zhou et al., 2019) and (Ramanujan et al., 2019) to allow sign flipping in the supermasks, without updating the magnitudes of weights. The main technical contributions are the new thresholding-based training method and the new weight initialization scheme that considers the supermasks. Empirical results show better performance on small conv nets and also residual networks on CIFAR-10/100 datasets. Firstly, this paper is written clearly and easy to follow. The main technical contributions are clear. Performance improvements over (Zhou et al., 2019) and (Ramanujan et al., 2019) are clear.\n\nHowever,  there are several concerns I have for this work.\n\n1. I think more detailed ablation study is needed for the experiments. For example, it is imaginable that the threshold tau will have an influence on the number of remaining parameters but the corresponding experiments are lacking. Having some will be nice. Also, the new training technique is always coupled with the new initialization method. Ablation study to decouple the effect of these two contributions will be good for the readers to understand the which one is more important. This is crucial because in (Ramanujan et al., 2019) the authors also investigated a certain form of scaling of the parameters which is found to improve the performance significantly.\n\n2. In the abstract, the authors posed this work as tackling the issue of difficult interpretation and training/usage in real-world applications of highly overparameterized networks due to their size. However, I don't think the proposed method save anything during training, e.g., memory, computations (in FLOPs) or time because it uses latent weights.\n\nMinor point:\n - In table 2, should the baseline accuracy for conv 8 be 82+% instead of 72+%? Overall I think this work proposes an interesting extension to previous supermask works but lack necessary experiments, making the results less convincing.",
            " I appreciate the author's response. I think there is value to this work, though I still feel that the present paper has a lot of room for improvement in providing concrete demonstrations of this value. Original supermasks are certainly not appropriate as the only performance baselines, but I suspect there may be other benefits of this type of approaches beyond what is measured in the paper. I would like to give this work the benefit of the doubt, and have raised my score accordingly. Nonetheless, I would encourage authors to show concrete new insights derived from the signed supermask model as that would further strengthen this work.",
            "This paper introduces \"signed supermasks\", which builds on top of the supermasks line of work of finding binary masks on untrained networks that result in good performance. This work extends the original supermask by adding the ability to flip the sign of the weights. The proposed method learns parameters of a mask, and converts the mask parameters into a tertiary mask through the use of two thresholds. The paper further proposes to use ELU activation function and a ELUS initialization scheme that is more tailored to supermask training. Performance is compared against fully-trained models and prior work on supermasks on MNIST, CIFAR10, and CIFAR100, and the proposed method shows competitive results. **Strength:**\n\n- This paper is well written and the proposed method is clearly described. \n- The proposed method is fairly novel, and the resulting high performance and sparsity level of the Conv 2,4,6,8 models are inspiring. \n\n**Weakness:**\n\n- The paper motivates the signed supermask as a way to improve the interpretability of the trained model. While this would be very interesting, I find that the supporting interpretability analyses to be lacking in substance. The mask visualizations are limited to the first layer of a FC network on MNIST, and cannot be directly extended to more layers or other architectures. The observations of layer-wise pruning ratios are interesting, but are not new and have been well studied in the pruning literature. \n\n- Another motivation is in terms of efficiency due to sparsity and compression of the final trained model. To this point, I think it is more appropriate to compare performance to other tertiary network methods, as the supermask baselines have much less capacity due to maintaining the sign at initialization. Is signed supermask a competitive method for training tertiary networks? How does it compare to other tertiary methods in terms of the performance and efficiency tradeoff? Testing the limits of how barebones can a network be is an interesting question, and it may very well be the case that the signed supermask pushes this limit, but it needs to be made clear through a quantitative comparison to similar methods rather than just a discussion.\n\n- Comparisons to the other supermask baselines are missing in the ResNet models for some reason.\n This paper presents an interesting new approach of training tertiary neural networks with high sparsity levels. The paper is well written and several of the results look promising and would be of interest to the community. However, I take some issue with the motivations of this paper. As a method that is proposed to produce insights on neural network training and enable interpretablility of networks, I find a lack of new insights provided in the paper. As a method that improves efficiency and compressibility, I find it to be very intriguing and promising, but the paper currently lacks the appropriate performance comparisons with other tertiary neural networks to fully convince me of its advantages.\n\n### Update after rebuttal\n\nThank you to the author for the update. I agree that there is value in training the most bare-boned network that we can. Even though not all the value has been demonstrated in the present work, I think it will be useful in many, perhaps unexpected ways in the future. Although this is not really discussed, I'm particularly interested in how this form of training can act as a particular form of regularization that is orthogonal to most other forms. I'll raise my score from 5 to 8 and encourage the authors to continue in this direction.",
            "This paper discusses the signed supermask that can improve the model accuracy of untrained neural networks significantly while enhancing sparsity compared to the original supermask idea. The authors introduce \"-1\" as an additional mask value to enable flipping a sign of an initialized weight and suggest related activation functions and fixed threshold hyperparameters to achieve sparse networks. Analysis on sparsity and corresponding accuracy is given for various CNN models including ResNet models. The role of batch normalization for large models is also studied. Previous studies on supermask are interesting because they provide insights that even untrained models have chances to achieve reasonable model accuracy only if we can find such supermask in advance. This reviewer is not sure whether minor improvements over the original supermask idea are critical if a large amount of training is still required.\n\nIf the authors can reveal that the model accuracy can be significantly improved even for untrained models compared to previous supermask, it would be helpful to understand the inherent characteristics of the neural networks initialization. But as shown in Table 1, compared to the work by Zhou et al, even test accuracy is slightly degraded even with additional \"-1\" mask while 'normal training' is still required. The original idea of Supermask is meaningful because it shows a new insight that untrained model has already a sub-network of reasonable accuracy. If the authors want to claim that we can generate a new Supermask generation method, it would not produce additionally new insight. Such concern is obvious for residual networks. Without training batch norm parameters, it is unavoidable to see noticeable accuracy degradation. Then, batch-norm hyperparameters are known to be affine hyperparameters that are superior to normal weights in terms of expressive power. Since the existence of Supermask is already known, the focus of new research in Supermask would need to be finding Supermask efficiently with minimal efforts. Otherwise, Supermask would not be practical in the field. Unfortunately, as indicated in Table 1, training time to find Supermask is quite slow. Then, why we don't just perform usual pruning method to achieve better model accuracy?\n\nComparison on the previous works is missing. At least, this paper needs to include \"What's Hidden in a Randomly Weighted Neural Network?\" in CVPR 2020. Since this paper introduces only marginal improvements over original Supermask work, this reviewer cannot find significantly new insights from this paper. Supermask needs to involve much higher model accuracy (than previous works) or minimal efforts to be computed. ",
            "This paper proposes Signed Supermask,  an extension of the original Supermask work (Zhou2019) for finding more efficient untrained subnetworks. Instead of learning a binary mask, Signed Supermask claims and shows that adding another dimension -1 to the masks leads to higher sparsity with higher accuracy. The main contribution of this paper is the introduction of weight flipping, as an improvement over the existing approaches. The method is simple and effective. The empirical experiments validate the effectiveness of the proposal.  ##########################################################################\n\nPros: \n\n \n\n1. The paper proposes a method to find untrained subnetworks that can approach the performance achieved by trained networks. The performance improvement achieved by allowing weights to flip is interesting.\n\n \n\n2. Overall, the paper is well written. Readers can directly grasp the main idea of the paper.\n\n\n\n##########################################################################\n\nCons: \n\n1. My main concern is the motivation and the usage of the proposed method.  As the authors said in the introduction \"However, due to their sheer size, the networks not only became difficult to interpret but also problematic to train and use in real-world applications...\", I expect a method that can either reduce the number of trained parameters or require a fewer number of FLOPs to be proposed. While the authors claim that the subnetworks discovered are untrained, I find that it requires training mask matrices with the same size as the model weights. Considering that straight-through estimator is used to estimate gradients, I suppose the backward pass is also dense and thus leads to no acceleration for training. Compared with directly training a dense network, I suppose the overall training FLOPs required by Signed Supermask is similar. The larger \"TT / Epoch\" over the baseline in Table 1 confirms my concern. I believe it is necessary to explain the benefits of Signed Supermask compared with training a dense network and prune or directly training a sparse network. With the sparsity learned by Signed Supermask, I believe even directly training a sparse neural network with static or dynamic sparsity (e.g., Mocanu et al [1], Evci et al [2], Liu et al [3]) can have similar performance, while with much fewer training FLOPs. I encourage the authors to clarify this.\n\n2. There are some related works that are missing in the paper, e.g., Diffenderfer & Kailkhura [4] (accepted by ICLR 2021) and Chijiwa [5] (accepted by NeurIPS 2021). Since the number of works on this topic is quite small, I expect a good submission should at least do a good related work job by introducing them. What's more, as they are aiming to search for untrained subnetworks without adding another dimension, comparisons with these works are encouraged. The current experiments only include comparisons with Zhou2019 and Ramanujan2019 with small convolutional networks, which is too small to draw a solid conclusion. For instance,  Ramanujan2019, [4] and [5] all provide results on large scale dataset ImageNet.\n\n3. As I understand, the performance improvement of Signed Supermask comes from (1) allowing weight flip, (2) ELUS initialization. I expect ELUS  is a universal method that can improve all the related works. However, I didn't see any ablation study of these two components. It is not clear to me if the improvement is caused by one of them or both of them. \n\n4. As mentioned by the authors, the two threshold hyperparameters in equation (1) control the sparsity level. I expect to see more experiments with how these thresholds influence the sparsity level and the corresponding performance of Signed Supermask.\n\nMinor typos: \n\nInstead of starting with a new paragraph, there is an inexplicable blank after some lines. E.g., 2rd paragraph on page 1; 1st paragraph on page 7.\n\nA space is missing before this sentence \"The gradient is estimated\" on page 3.\n\nReference:\n\n[1] Mocanu, Decebal Constantin, et al. \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science.\" Nature communications 9.1 (2018): 1-12.\n\n[2] Evci, Utku, et al. \"Rigging the lottery: Making all tickets winners.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Shiwei  Liu, et al. ''Sparse training via boost-ing pruning plasticity with neuroregeneration.'' NeurIPS, 2021.\n\n[4] Diffenderfer, J., & Kailkhura, B. (2021). Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. ICLR, 2021.\n\n[5] Chijiwa, Daiki, et al. \"Pruning Randomly Initialized Neural Networks with Iterative Randomization.\" NeurIPS, 2021.\n\n While the performance achieved by untrained NN is interesting, the motivation and experiments need more work. "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses concerns about the lack of detailed ablation studies and the method's practical benefits during training. The reviewer uses phrases like \"several concerns,\" \"experiments are lacking,\" and \"results less convincing.\"",
            "The reviewer states they 'appreciate the author's response' and believe 'there is value to this work'. They also express a willingness to 'give this work the benefit of the doubt' and have raised their score, indicating a positive overall sentiment.",
            "The review expresses a positive sentiment overall, starting with acknowledging the paper's strengths (well-written, novel method, inspiring performance) and ending with raising the score and encouraging further work. The reviewer appreciates the potential value of the approach, even if not fully demonstrated in the current work.",
            "The reviewer expresses several concerns about the paper's novelty and practical significance. Phrases like \"minor improvements,\" \"slightly degraded,\" \"not produce additionally new insight,\" \"noticeable accuracy degradation,\" and \"cannot find significantly new insights\" indicate a negative sentiment.",
            "The review expresses multiple concerns regarding the paper's motivation, experimental setup, missing related works, and lack of ablation studies. Phrases like 'My main concern', 'I believe it is necessary to explain', 'I expect a good submission should at least do a good related work job', and 'It is not clear to me' indicate a critical assessment."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review points out specific weaknesses in the experimental design and claims made by the authors. The reviewer uses direct criticisms like \"I think more detailed ablation study is needed\" and \"I don't think the proposed method save anything during training.\"",
            "The reviewer uses encouraging language like 'I would like to give this work the benefit of the doubt' and 'I would encourage authors'. While they point out areas for improvement, the overall tone is aimed at helping the authors strengthen their work.",
            "The review presents both strengths and weaknesses of the paper in a structured manner. While the reviewer raises concerns and criticisms, they also acknowledge the paper's merits and potential. The final update shows a shift towards a more supportive tone.",
            "The review uses critical language, directly questioning the value and novelty of the research. Phrases such as \"I'm not sure whether minor improvements are critical,\" \"Such concern is obvious,\" and \"why we don't just perform usual pruning method\" demonstrate a critical tone. The reviewer also points out missing comparisons to relevant prior work.",
            "The tone is critical due to the direct questioning of the paper's claims, identification of missing elements (related works, ablation studies), and suggestions for improvement ('I encourage the authors to clarify this'). The reviewer uses phrases that directly challenge the validity and completeness of the work."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it first acknowledges the strengths of the paper (clarity, contributions) and then logically proceeds to point out weaknesses, mainly the lack of experimental validation to support the claims. The concerns raised are directly related to the paper's claims and contributions, and the overall assessment aligns with these concerns. There are no self-contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because it acknowledges the potential value of the work while also pointing out areas for improvement, specifically the need for concrete demonstrations of this value. The reviewer's decision to raise the score is presented as being based on the potential and benefit of the doubt, which aligns with the identified weakness of lacking concrete demonstrations. The encouragement to show concrete new insights further reinforces the consistent message about the need for stronger evidence of the work's value.",
            "The review is consistent in its overall assessment. Initially, the reviewer pointed out weaknesses regarding the motivations (interpretability and efficiency comparisons) and missing comparisons in ResNet models, while also acknowledging the strengths of the paper (well-written, novel, promising results). In the update after rebuttal, the reviewer shifts focus to the value of training 'bare-boned networks' and its potential as a form of regularization, increasing the score. This is not a contradiction but rather a shift in emphasis and appreciation of a different aspect of the work's value after reflection and potentially considering the author's rebuttal. The reviewer's core positive sentiment about the novelty and potential of the work remains consistent throughout, even as the specific aspects of value highlighted evolve.",
            "The review consistently argues that the paper presents only marginal improvements over the original supermask idea and questions the significance and practicality of these improvements, especially considering the need for training and the lack of substantial accuracy gains compared to prior work. The reviewer's concerns are focused on the limited novelty and practical value of the proposed method.",
            "The review is consistent because the pros and cons are logically separated, and the cons outweigh the pros leading to a negative overall assessment which is reflected in the summary. The reviewer provides clear reasons for their concerns and suggestions for improvement, without contradicting themselves."
        ]
    },
    {
        "paper_id": "iclr_2022_w8HXzn2FyKm",
        "paper_title": "Finite-Time Error Bounds for Distributed Linear Stochastic Approximation",
        "paper_abstract": "This paper considers a novel multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction, in which each agent evolves according to its local stochastic approximation process which depends on the information from its neighbors. The interconnection structure among the agents is described by a time-varying directed graph. While the convergence of consensus-based stochastic approximation algorithms when the interconnection among the agents is described by doubly stochastic matrices (at least in expectation) has been studied, less is known about the case when the interconnection matrix is simply stochastic. For any uniformly strongly connected graph sequences whose associated interaction matrices are stochastic, the paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. For the case of interconnection matrices being stochastic, the equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication. Both the cases with constant and time-varying step-sizes are considered. In the case when the convex combination is required to be a straight average and interaction between any pair of neighboring agents may be uni-directional, so that doubly stochastic matrices cannot be implemented in a distributed manner, the paper proposes a push-type distributed stochastic approximation algorithm and provides its finite-time bounds for the performance by leveraging the analysis for the consensus-type algorithm with stochastic matrices.",
        "review_ids": [
            "4D9fadhgIdP",
            "5FNpb8WG1Pg",
            "rAewWd8SN2c",
            "Xghi-gfqGZP",
            "trmfZMv5bBW",
            "OZarjj7kzP_",
            "-WBCYDnB3ZE",
            "OLhEcHw5S-c",
            "6OcJyhUJA0Y"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Please highlight any changes made to the paper to address the concerns raised above.",
            " The issues raised in my review remain unresolved. I find the paper lacks what I would consider to be mathematical rigour. I pointed out several specific examples, along with more extensive explanations -- but none of these have been addressed. If it were a matter of one or two points, and the authors had made some effort towards addressing those points, I would be willing to try and explain in more detail, but since no effort has been made, I am not convinced further effort on my part would be fruitful.",
            "This paper studies the problem of distributed linear stochastic approximation for a group of agents over time-varying directed communication networks (to be more specific, the setup is decentralized). The authors propose two decentralized algorithms, (1) consensus-based linear stochastic approximation using row-stochastic mixing matrices (Eq. (1)), and (2) a push-sum type algorithm using column-stochastic mixing matrices (Eq. (9)). They further provide results on the asymptotic and finite-time mean-square errors to the equilibrium point of the (proper) ODE for each proposed algorithm. The manuscript considers the linear stochastic approximation problem (Srikant & Ying (2019)) for a multi-agent system. The authors revisit similar setups and results as Doan et al. (2019) for the cases where the agents interact over time-varying directed communication networks. Below, the strong and weak points of the paper are pointed out.\n\nStrengths:\n- The authors propose a push-sum counterpart for the distributed linear stochastic approximation under time-varying directed networks.\n- The manuscript made a nontrivial contribution to generalize the convergence result in Doan et al. (2019) for a more complicated communication setup, time-varying directed networks.\n- The authors provide a detailed analysis in the appendix for both proposed algorithms.\n\nWeaknesses:\n- The lack of any theoretical comparisons to the literature prevents the readers from understanding the technical differences between the results of this work and the existing theoretical results (e.g., Doan et al. (2019)). The reviewer suggests providing a detailed table, comparing the results in this work in terms of the constants shown in the convergence rate. This is a crucial part of such a theoretical paper.\n- The authors introduced RL and specifically TD as examples of the theory developed in this paper. This reviewer suggests adding an illustrative synthetic example over a sequence of directed graphs besides additional discussions on the two proposed algorithms for that example.\n- Another serious issue with this work is Assumption 6. Even after reading Appendix C, this reviewer is not convinced that Assumption 6 is a proper assumption. Indeed, the limit in Assumption 6 does not exist for multiple real-world problems. For example, consider a periodic change of $\\pi_t$, given periodic communication networks. Given that all agents reach consensus on the same $\\theta_t^i$, this reviewer suggests dropping Assumption 6 and modifying the statement in Theorem 2 to some time-varying ODE with $\\pi_t^i$ in the definition.\n- This reviewer finds the introduction lengthy, yet not sufficiently informative. Most of the discussions address the issues for time-varying directed networks, while the discussions on the linear stochastic approximation are limited. This reviewer strongly encourages the authors to add examples and/or motivations on the importance of this problem for TD (or generally RL) problems.\n- As a minor issue, before using an acronym, please first mention the full form, for example, TD learning which is mentioned several times, is the acronym for temporal difference learning.\n- One of the main issues of this manuscript is that it takes the reader a long time to find the main objective, which is reaching consensus on the equilibrium point of some ODE, and why this is important. The authors elaborate on the setup details, and after proposing the first algorithm (Eq. (1)), they mention that local $\\theta_t^i$ converges to $\\theta^\\star$ which is the solution to some ODE problem (Eq. (5)). The authors do not explain why reaching consensus on the solution of the ODE is the main objective, i.e., it is not quite clear where the ODE appears in this problem. To understand the role of the ODE, the reader is required to read the Appendix. \n- For a non-expert reader, finding the differences between the two proposed algorithms (Eq.(1) and Eq, (9) is not easy. This reviewer suggests the authors take some time to improve the presentation of the two algorithms and highlight the advantages of one to the other (e.g., in a table). The manuscript in its current format is confusing, in the sense that the importance of the second algorithm compared to the first algorithm is vague. This reviewer believes that the authors should specifically address the cases wherein a column stochastic matrix is not guaranteed. This requires clear explanations.\n- On a more technical note, it is not clear to this reviewer why in Eq. (1) the update rule is established based on $b^i$, while in Eq.(9) considers a convex combination of neighbors\u2019 $b^j$. In summary, this work extends the existing results on the finite-time error bounds for the problem of distributed (decentralized) linear stochastic approximation to a more challenging communication setup. The paper is marginally novel and the contributions are fair. However, the manuscript has two main issues, (1) the comparisons to the previous works are missing, and (ii) the structure of the paper, as well as the presentation of its results, require some work.",
            "An interesting submission dealing with distributed stochastic approximation driven by Markovian noise, in which the communication topologies considered among agents are captured by a stochastic matrix (in contrast to the doubly stochastic matrix studied in the related literature). Both asymptotic and finite-time error bounds are established and it was shown that the algorithm converges to some unspecified convex combination of the equilibrium points of local tasks. This paper finally proposes a push-type distributed stochastic approximation algorithm and provides its finite-time bounds for the performance by leveraging the analysis for the consensus-type algorithm with stochastic matrices. All analysis and error bounds are directly applicable to distributed TD algorithms.\n\n Originality: This paper studies distributed Markovian stochastic approximation algorithms with stochastic interconnection matrices, which has not yet been covered in the related literature. The results are a bit different than what is known for related algorithms having doubly stochastic interconnection matrices. Further, a push-type algorithm is proposed for decentralized implementation.\n\nQuality. The results are technically sound and theoretically supported. The algorithm proposed is fresh and new as well as interesting.\n\nClarity. The paper is well written and easy to follow. The results are nicely organized and the contributions relative to existing works are clear too.\n\nSignificance. The results are important and complement the existing distributed Markovian stochastic approximation results which have only focused on doubly stochastic interconnections among agents. The tools developed for analysis may also be useful for further use. Numerical simulations are suggested to compare the proposed algorithm and existing ones as well as validate the proved finite-time error bounds for both constant/time-varying stepsizes.\n\n--------------\nAfter rebuttal: After reading the other reviews and authors response, I have updated my score. \n",
            " Agreed that $A$ needs to be stable is the only (explicit) assumption. However, to choose $\\alpha_0=\\frac{\\gamma_{\\max}}{0.9}$, we require $\\gamma_{\\max}$, and as the authors point out, for a general stable $A$ it may not be possible to have an explicit expression for $\\gamma_{\\max}$, which implies that $\\gamma_{\\max}$ has to be computed first and then plugged into $\\alpha_0=\\frac{\\gamma_{\\max}}{0.9}$ to achieve the desired convergence rates. How do we compute $\\gamma_{\\max}$? Is it faster than rates of the main SA itself? It would be great if the authors can comment on this.",
            " Thanks for the clarifications. It will be great if the following are also elaborated.\n\n(1) If the agents are collaborating, and okay with sharing parameters (via consensus), why have the rewards private or why does it hurt to share the rewards? In other words, it understandable having the rewards strictly private (but states common) in a competitive setting, but in a collaborative one. \n\n(2) By letting $\\gamma_{\\max}=\\frac{1}{a_{\\min}}$, we are assuming that we know the minimum eigenvalue of $A$ (at least in case when there are no imaginary components in the eigenvalues). Is this not a very strong assumption? \n\n\n\n",
            "This paper studies multi-agent distributed optimization under general consensus-type interactions between agents. The main contribution of the paper is to study linear stochastic approximation in a multi-agent setup without using bi-directional communication among agents. Non-asymptotic upper bounds on the associated error function, in a mean squared sense, are derived.  The paper studies a concrete problem in distributed optimization in a multi-agent setting, where consensus-type dynamics prevail to allow various agents reach a global equilibrium point, which is often the globally optimal solution of some convex problem. Maintaining a consensus algorithm using bi-directional communication is rather straightforward and, by now, well-understood, but as the authors argue, relaxing this assumption poses some challenge. \n\nThe paper is well-written and well-organized. Problem formulation and results are presented clearly and precisely. Though generally well-organized, the paper uses 3.5 (out of 9) pages of the main text for introductory, non-technical part, which suggests that the organization is more suitable for a journal submission than a conference. In this limited review time, I was unable to check the proofs. Nonetheless, the derived error bounds make perfect sense, and they look correct. \n\nMy main concern about the paper is that it is marginally relevant for a learning conference like ICLR. To establish connection to ML and RL, the authors provide pointers to some literature studying distributed multi-agent RL problems using distributed optimization frameworks. First, I personally believe that this literature does not represent well distributed, multi-agent RL. Second, the paper addresses a challenge existing in generic distributed optimization, but not in RL. In other words, this challenge does not arise because of the RL nature of the problem, but rather comes from the networked nature of the generic distributed optimization. The problem studied here is a good fit for networked/distributed optimization venues. As such, it might get a reasonable attention if published in NeurIPS and ICML, but I think it will receive much less attention from the ICLR community. This is the main reason behind my low score. \n\n\nMinor points: \n\n- terms such as \u201cmean-square\u201d and \u201cmean squared\u201d are used to denote the same notion. \n\n- p. 3: double stochastic matrix => doubly stochastic matrix \n This is a well-written paper studying a multi-agent distributed optimization under general consensus-type interactions between agents. I personally think that it is not of relevance for ICLR, while it might be on interest to some larger learning conferences such as NeurIPS. ",
            "This paper addresses a consensus problem in stochastic approximation, which has application to policy iteration in reinforcement learning. Specifically, the authors address the case where the interaction graph between agents is stochastic, but not doubly stochastic. Under certain assumptions, the authors prove convergence of their proposed algorithm in a certain sense.  The topic of the proposed paper is certainly significant, and the extension to the general non-doubly stochastic case is important. The proofs appear to be solid at first glance, although I have certainly not verified all 47 pages of the supplementary material. Despite the significance of the topic, however, the paper is not well-conceived or well-written. \n\nFor example, the introduction is 3 pages long and is mostly talking about how doubly stochastic matrices are not good enough to represent certain systems and discussing related literature. While it is appropriate to place the results in the context of existing literature, much of this exposition is repetitive and is undercut by the fact that the authors fail to provide a numerical example/simulation illustrating a non-doubly stochastic model. Furthermore, this extensive introduction means the authors have less space to define the problem and present the results.\n\nThis brings up the second problem with the paper, which is the lack of exposition and rigour in the definition of the problem and presentation of results. Specifically, the problem being solved is not defined. The exposition begins on page 4 with the proposed approximation/consensus rules. Yet it is unclear what the purpose of these dynamics is and what problem they are solving. I had to do significant literature survey to even understand what problem the authors were trying to solve. This lack of exposition continues to the main results in Thms 3/5. These results are presented as inequalities proving convergence of the proposed dynamics in a certain sense. Yet it is unclear to me exactly in what sense these inequalities prove convergence or what that convergence would imply about whether the proposed dynamics solve a particular problem. For example, the approximation state $\\theta$ appears on both the left and the right. Does the inequality imply that the approximation state is converging to the true state or not.\n\nThis brings up my third concern which is that a certain lack of rigour impedes understanding, verification and interpretation of the results. For example, the theorem statements are not self-contained and rely on implicit assumptions and definitions of variables mentioned earlier in the exposition. This is particularly problematic since several variables are defined differently at different times. For example, $A$ is initially part of the definition of the dynamics, but then is later taken to be some property of the sequence $\\pi_t$. As a result, there are numerous terms in the Thm statements for which must guess at the meaning. This is further impeded by a length series of assumptions included in the Thm statement which depend on terms which do not actually appear in the theorem statement. See the notes for specific concerns.\n\nMy final concern is that there is no validation or illustration of the proposed approach on a numerical example. This is rather unusual and makes it particularly difficult for the reader to verify or interpret the results.\n\n- Is assumption 1 required to hold for any $X_t$?\n- What are all these assumptions on? The use of assumptions is generally not rigorous, but the particular usage here does not even make it clear what these properties of which variables are being assumed. \n- the $\\forall X$ in Assumption 3 is unclear.\n- Which $A$ is used in assumption 3.\n- Which $A$ is used in assumption 4. Also assumption 4 has some discussion which is not part of the assumption.\n- The statement of Thm1 is not complete\n- The use of both fixed and time-varying step sizes are unclear. I was initially under the impression that the time-varying step could be unknown or stochastic, but it seems to be prescribed. What is the motivation here?\n- What is the diameter of a graph in Thm 3.\n- why does $i$ not appear in the definition of $\\hat w_{t}^{ij}$\n- Typos: \"decaying expect for\" \"decreasing\" \"that consensus interaction\" \"cannot be directly apply\"\n\n In summary, the paper may contain publishable results, but is not sufficiently clear to understand what those results are, let alone verify them. In addition, the lack of a numerical illustration seriously undercuts the claims of significance. To be publishable, the authors should focus on clearly defining the problem, presenting the solution and its significance, and illustrating on a numerical example. ",
            " The paper presents a finite time analysis of distributed linear stochastic approximation (non-doubly stochastic interconnection matrix) in the presence of Markovian Noise. Strength:\n               Attacking a hard setting.\n\nWeakness:\n1. A major weakness of the paper is the lack of numerical examples in the distributed TD setting. Furthermore, even a simpler toy example demonstrating the interplay between the various quantities in the Theorems is missing.\n\n2. As the paper mentions, the value of many constants cannot be computed, however, it is sort of the inherent difficulty of the setting itself. While it is not a weakness that should limit the evaluation of the paper per se, it is nevertheless important to be highlighted and I thank the authors for the same.\n\n\n\nPossible Technical Issues: It will be great if the authors can clarify the following.\n\n* (Question 1) Why is that $b^i$ keeps changing with agents and yet $A$ is the same across agents? To elaborate, let us consider TD with linear function approximation, wherein $\\phi(s)$ is the feature for state $s$, then $A=\\phi(s_t)\\left(\\beta\\phi(s_{t+1})-\\phi(s_t)\\right)^\\top$, which is based on the transition from state $s_t$ to state $s_{t+1}$. In a distributed RL like setting, different agents could possibly be transitioning from different states, i.e., we would have $s^i_t$ to state $s^i_{t+1}$ and hence $A^i=\\phi(s^i_t)\\left(\\beta\\phi(s^i_{t+1})-\\phi(s^i_t)\\right)^\\top$. This is where it would have helped to have a motivating example. \n\n\n\n*  (Question 2)  Consider the stochastic approximation in $2$-dimensions for just a single agent. Let $A=\\left[\\begin{matrix}-\\gamma_{\\max} ,0 ; 0 ,-\\gamma_{\\min}\\end{matrix}\\right]$ and $b=\\left[\\begin{matrix}0 ;0  \\end{matrix}\\right]$. In this case $\\theta_*=\\left[\\begin{matrix}0 \\\\ 0 \\end{matrix}\\right]$, and assume $\\theta_0=\\left[\\begin{matrix}1 \\\\ 1  \\end{matrix}\\right]$. In below, step sizes $\\alpha_t=\\frac{\\alpha_0}{t+1}$, and $\\theta_t=\\left[\\begin{matrix}\\theta_t(1) \\\\ \\theta_t(2)  \\end{matrix}\\right]\\in \\mathbb{R}^2$. \n\n$\\theta_{t+1}(1)=\\theta_t(1)-\\alpha_t\\gamma_{\\max}\\theta_t(1),$\n\n$\\theta_{t+1}(2)=\\theta_t(2)-\\alpha_t\\gamma_{\\min}\\theta_t(2),$\n\nLet us just look at $\\theta_{t+1}(2)$, \n\n$\\theta_{t+1}(2)=\\Pi_{k=0}^{t}\\left(1-\\frac{\\gamma_{\\min}\\alpha_0}{k+1}\\right) \\theta_{0}(2)$\n\n$|\\theta_{t+1}(2)|=\\Pi_{k=0}^{t}\\left(1-\\frac{\\gamma_{\\min}\\alpha_0}{k+1}\\right)|\\theta_{0}(2)|=\\Pi_{k=0}^{t}(1-\\frac{\\gamma_{\\min}\\alpha_0}{k+1})$\n\nNow using the fact that for for small $x$, we have $e^{-2x}\\leq(1-x)$ we have (for sufficiently small $\\gamma_{\\min}\\rightarrow 0$ and for $t\\geq 1$):\n\n$|\\theta_{t+1}(2)|\\geq e^{-2\\gamma_{\\min}\\alpha_0\\sum_{k=0}^{t}\\frac{1}{k+1} }$\n\n$=e^{-2\\gamma_{\\min}\\alpha_0}e^{-2\\gamma_{\\min}\\alpha_0\\sum_{k=1}^{t}\\frac{1}{k+1} }$\n\n$\\geq e^{-2\\gamma_{\\min}\\alpha_0}e^{-2\\gamma_{\\min}\\alpha_0\\ln(t) }$\n\n$= \\frac{e^{-2\\gamma_{\\min}\\alpha_0}}{t^{2\\gamma_{\\min}\\alpha_0} }$\n\nPicking $\\alpha_0= \\frac{\\gamma_{\\max}}{0.9}$, we have \n\n\n$|\\theta_{t+1}(2)| \\geq \\frac{e^{-2\\gamma_{\\min}\\alpha_0}}{t^{\\frac{2\\gamma_{\\min}\\gamma_{\\max}}{0.9}} }$\n\nWe can always let $\\gamma_{\\max}=1$, to get $\\frac{1}{t^{\\frac{2\\gamma_{\\min}}{0.9}}}$, which is arbitrarily small rate for arbitrarily small $\\gamma_{\\min}$. However, Theorem 3-(2) seems to suggest that we are indeed achieving a $\\frac1t$ rate. Please clarify the same.\n\n\n*  (Question 3) We can think of the constant step size case to be $\\alpha_t=\\alpha_0=\\alpha$. Now, it is intriguing that for the constant step size case we have $\\alpha_0<\\min\\\\{K_1,\\frac{\\log 2}{A_{\\max}\\tau(\\alpha)}, \\frac{0.1}{K_2\\gamma_{\\max}}\\\\}$, where $\\gamma_{\\max}$ appears in the denominator and for the time varying case we have $\\alpha_0\\geq \\frac{\\gamma_{\\max}}{0.9}$, where $\\gamma_{\\max}$ appears in the numerator. Why is this difference on the dependence of step size on $\\gamma_{\\max}$? \n\n While the possible technical issues can be discusses further, the score is mainly due to lack of empirical section which is a major weakness of the paper."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Positive",
            "Neutral",
            "Neutral",
            "Neutral",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review requests information without expressing a positive or negative opinion.",
            "The reviewer states that the issues raised in their previous review remain unresolved and that the paper lacks mathematical rigor. They also express frustration that none of their specific examples and explanations have been addressed, leading them to believe that further effort on their part would be unfruitful.",
            "The review identifies several significant weaknesses, including a lack of theoretical comparison, issues with Assumption 6, a lengthy and uninformative introduction, and problems with the clarity and structure of the paper. The reviewer uses phrases like \"serious issue,\" \"not convinced,\" \"strongly encourages,\" \"main issues,\" and \"confusing,\" indicating a negative sentiment.",
            "The reviewer uses positive language such as \"interesting submission,\" \"technically sound,\" \"theoretically supported,\" \"fresh and new,\" \"important,\" and \"well written.\" The reviewer also states that the results are \"nicely organized\" and that the contributions are \"clear.\"",
            "The review raises a specific technical question about the computation of a parameter, without expressing strong positive or negative opinions about the work as a whole.",
            "The review poses questions and seeks further elaboration, indicating a neutral stance rather than explicit praise or criticism. The reviewer uses polite language ('It will be great if...')",
            "The review acknowledges the paper's strengths (well-written, clear presentation) but expresses a significant concern about its relevance to the ICLR conference. This mix of positive and negative feedback leads to an overall neutral sentiment.",
            "The review expresses significant concerns about the paper's clarity, exposition, rigor, and lack of validation. Phrases like \"not well-conceived or well-written,\" \"lack of exposition and rigour,\" \"a certain lack of rigour impedes understanding,\" and \"not sufficiently clear to understand\" indicate a negative sentiment. The reviewer also points out the absence of a numerical example as a serious drawback.",
            "The review identifies a 'major weakness' as the lack of numerical examples and highlights possible technical issues which require clarification. While acknowledging the difficulty of the setting, the review ultimately lowers the score due to the absence of an empirical section."
        ],
        "tone": [
            "Neutral",
            "Critical",
            "Critical",
            "Supportive",
            "Neutral",
            "Neutral",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The request is phrased politely and professionally.",
            "The reviewer uses phrases like \"issues raised in my review remain unresolved\", \"lacks what I would consider to be mathematical rigour\", and \"no effort has been made\" which indicate a critical and dissatisfied tone.",
            "The reviewer uses direct and critical language to point out flaws in the paper. Phrases like \"lack of...prevents the readers from understanding,\" \"serious issue,\" \"not convinced,\" \"lengthy, yet not sufficiently informative,\" \"manuscript in its current format is confusing,\" and \"requires some work\" demonstrate a critical tone. The reviewer also offers specific suggestions for improvement, but the overall assessment is critical of the current state of the manuscript.",
            "The reviewer expresses encouragement and appreciation for the work. Phrases like \"easy to follow,\" \"results are important,\" and the suggestion for numerical simulations to further validate the findings indicate a supportive stance towards the authors and their research.",
            "The tone is inquisitive and analytical, using formal language and focusing on a specific technical aspect of the paper. The reviewer uses phrases like 'It would be great if the authors can comment on this,' which is polite but doesn't indicate strong support or criticism.",
            "The tone is neutral as it presents specific questions and requests for clarification without expressing strong positive or negative opinions. The language is professional and focused on technical aspects of the work.",
            "The review provides both positive feedback (e.g., \"well-written and well-organized\", \"Problem formulation and results are presented clearly and precisely\") and critical feedback (e.g., \"marginally relevant for a learning conference like ICLR\", \"this literature does not represent well distributed, multi-agent RL\"). This balanced approach indicates a thoughtful and objective assessment.",
            "The tone is critical, highlighting flaws in the paper's structure, definitions, and presentation. The reviewer uses direct and negative language like \"the paper is not well-conceived or well-written\", \"lack of exposition and rigour\", and poses several pointed questions indicating a lack of understanding and raising concerns about the validity and interpretability of the results.",
            "The review uses phrases like 'major weakness,' 'lack of numerical examples,' and 'possible technical issues' indicating a critical assessment. The reviewer also directly poses questions and points out discrepancies, further contributing to the critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review consists of a single sentence that is not self-contradictory. It is a coherent request for revision highlights.",
            "The reviewer consistently maintains their negative assessment of the paper due to unresolved issues from a previous review. They explicitly state that none of their previously raised points have been addressed, leading to their current stance and unwillingness to further engage.",
            "The review is consistent because it clearly outlines both the strengths and weaknesses of the paper, providing specific reasons and examples for each point. The reviewer's overall assessment aligns with the detailed points made in the strengths and weaknesses sections, and there are no contradictory statements or illogical arguments presented. The review offers constructive criticism and suggestions for improvement that directly address the identified weaknesses, demonstrating a coherent and consistent evaluation of the manuscript.",
            "The review is consistently positive, praising the paper's originality, quality, clarity, and significance. There are no contradictory statements within the review. The reviewer updated their score after rebuttal, which implies a reinforcement of their initial positive assessment.",
            "The review is consistent as it raises a valid question about the practical computation of $\\gamma_{\\max}$ and its efficiency in relation to the main algorithm, without contradicting itself.",
            "The review is consistent as it raises valid questions and points for clarification without contradicting itself. The reviewer is asking for further elaboration on specific aspects of the paper, which is a constructive and consistent approach to peer review.",
            "The review is consistent because the reviewer praises the paper's quality and clarity while consistently arguing that the paper's main weakness is its marginal relevance to the ICLR conference. The low score is justified by this lack of relevance, not by any flaws in the paper's quality or methodology. The reviewer clearly distinguishes between the paper's strengths in distributed optimization and its weak fit for a learning conference like ICLR.",
            "The review consistently criticizes the paper for its lack of clarity, rigor, and poor presentation, despite acknowledging the significance of the topic. The reviewer provides multiple specific examples to support this consistent negative assessment of the paper's writing and exposition.",
            "The review is consistent because the identified weaknesses and technical questions logically support the overall assessment, which emphasizes the lack of empirical validation and potential theoretical inconsistencies or areas needing clarification. The reviewer's points are coherent and contribute to a unified critique of the paper."
        ]
    },
    {
        "paper_id": "nips_2021_vBYwwBxVcsE",
        "paper_title": "A self consistent theory of Gaussian Processes captures feature learning effects in finite CNNs",
        "paper_abstract": "Deep neural networks (DNNs) in the infinite width/channel limit have received much attention recently, as they provide a clear analytical window to deep learning via mappings to Gaussian Processes (GPs). Despite its theoretical appeal, this viewpoint lacks a crucial ingredient of deep learning in finite DNNs, laying at the heart of their success --- \\textit{feature learning}. Here we consider DNNs trained with noisy gradient descent on a large training set and derive a self-consistent Gaussian Process theory accounting for \\textit{strong} finite-DNN and feature learning effects. Applying this to a toy model of a two-layer linear convolutional neural network (CNN) shows good agreement with experiments. We further identify, both analytically and numerically, a sharp transition between a feature learning regime and a lazy learning regime in this model. Strong finite-DNN effects are also derived for a non-linear two-layer fully connected network. Our self-consistent theory provides a rich and versatile analytical framework for studying strong finite-DNN effects, most notably - feature learning.\n",
        "review_ids": [
            "hdqAOv1Wt5c",
            "-26w2MH7kV",
            "OhY-8DxyTIw",
            "SswyefLy-aU",
            "_ipd6w_N9v6",
            "h0Y22yAP1zJ"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "In this work, the authors solve for the infinite time mean predictor of a model with weight decay trained via noisy full batch gradient descent. The authors develop a general formalism appropriate for model weights initialized with mean zero and variance related to the weight decay and gradient noise parameters and push this formalism to explicit characterization of the model output for the case of one-hidden-layer linear teacher student CNNs trained with MSE loss and quadratic teacher student fully connected DNNs. In the former case, the authors observe a phase transition between the lazy and feature learning regimes as the model width decreases.  The formalism and solvable model presented are interesting and have the potential to provide important insight into more realistic deep learning systems. At present, however, the manuscript falls short in a few key areas. I am open to increasing my score if these concerns are addressed (potentially using the additional page).\n\n**Relevance to realistic DL models**\n\nThe theoretical and empirical setup presented here is interesting, but quite limited. Even if the formalism requires restrictive assumptions, the paper would be much stronger if the qualitative phenomena exhibited by the solvable models could be explored empirically in more realistic settings. For example, can one see evidence of the spiked-MP phase transition in the spectrum of the weights for realistic models trained with a teacher target? Does the same transition also empirically explain models trained with categorical labels? Do the solvable models make additional predictions that can be investigated empirically in a wider context?\n\n**Relationship to existing literature**\n\nAnother area where the paper could be significantly strengthened is casting the present work in light of previous work. The authors have a fairly inclusive related work section, however there are two qualitative discussions which are important to include.\ni) The discussion of how student weights learn teacher weights presented in Section 4.4 seems quite similar to the analysis in 1809.10374. Both discuss this process in terms of low rank perturbation of a noise matrix and the associated phase transition, following Benaych-Georges & Nadakuditi. It is true that the authors of 1809.10374 were not concerned with connecting this behavior to the lazy / feature transition, however more discussion of the connection to their analysis would be welcomed.\nii) Various mechanisms underlying the lazy/feature learning transition have been discussed in the literature. For example a transition as a function of model normalization was discussed in 1812.07956 and 1906.08034, a transition as a function of learning rate was discussed in 2003.02218, and a transition as a function of weight decay strength is implicit in 2006.08643. How does the transition here relate to those mechanisms? If different, are there measurements in realistic models we can perform to disentangle the key factors underlying the transition in relevant setups? \n\n**Additional questions**\n\nFundamental limitations of formalism v convenient simplification -- It would be nice to emphasize a little more clearly which simplifications are essential for the analysis and which are used for simplicity of presentation. For example, the requirement that the weight initialization is related to the weight decay and gradient noise is essential, however the MSE loss is less essential. Is that correct? Is it correct even if one wants to understand the lazy/feature transition? Perhaps this could be emphasized more explicitly for more of the modeling assumptions?\n\nDo these results hold for both C > n and C < n? If C <n, is the measure in 2 still unconstrained? If \\sigma were 0, \\tilde{K} would not be invertible for C<n so expressions such as 5 would be singular with non-zero sigma, is there any avatar of this discontinuity?\n\nDo these results shed light on the \\sigma-> 0, finite \\gamma limit?\n\n**Minor questions / comments**\n\n- The authors provide a consistency condition for the saddle point approximation, but comment that one cannot reformulate the action with an explicit pre-factor. Can one introduce auxiliary fields (e.g. as in hep-th/0306133) to accomplish this?\n\n- 2106.10165 -- which came out post submission -- might, none-the-less, be nice to reference in a final version, as they also attempt to use corrections to large width to understand the feature learning / lazy transition.\n\n-----------------------------------\n\nAfter the response from the authors and discussion with other reviewers. I have decided to revise my score up.\n\nI appreciate the time the authors took in responding to my questions! I am only partially appeased by my main question -- what aspects of this solvable system are applicable (at least approximately) in realistic systems? The authors point to [27] (\"Implicit Self-Regularization in Deep Neural Networks...\") as a potential example of the spiked MP behavior they are analyzing in realistic setups. Though [27] does exhibit a variety of empirical distributions for spectra, it is not clear to me that they are realizing a version of the lazy feature transition discussed by the authors. That being said, I do think a model that can be analyzed through the non-perturbative lazy/feature transition is attractive. I also appreciated the additional discussion of Lampinen and Ganguli.\n Some limitations, and encouragement for the authors to discuss the limitations, are discussed in the review above. The authors have adequately discussed societal impact.",
            "This work addresses the question of how finite-width DNNs differ from their infinite-width GP limit. This is important since, while the GP limit is a powerful theoretical tool for studying DNNs, it's unable to capture _feature learning_, a key behavior of DNNs. The authors show that the mean predictor of a finite DNN trained with noisy gradients and weight decay corresponds to GP regression on a shifted target, and give an approximation for the shift based on the cumulants of the prior. They apply the framework to two toy models. In particular, for a two-layer linear CNN they derive a large-training-set approximation for the cumulants, and show that as the width increases, there is a phase transition out of the feature-learning regime.    My impression is that this paper makes important contributions \u2013 particularly the characterization of the finite-width DNN as shifted GP regression and the the demonstration of feature learning. However, my familiarity with the topic is limited and the paper is written in a fairly inaccessibly way, which makes it hard for me to fully evaluate the contribution. \n\n\n * I felt that the experimental sections could be made more convincing, since as written, I found the discussion to be imprecise and hard to follow. For example:\n\t * The discussion of how large the training set must be for the CNN vs the GP to \"perform well\" is somewhat vague.\n\t * In section 4.3, the idea is to show that the prediction of $\\alpha$ improves as $n$ grows. But it was unclear if the existence of such $\\alpha$ depends on linearity occurring in the limit $C\\rightarrow \\infty$ anyways, (eqn. E.3)? And there is the fact that the linearity only results from the EK large $n$ limit, so it's confusing to say that the prediction of $\\alpha$ is improving as $n\\rightarrow\\infty$. And there is the approximation from finding $\\alpha_{test}$ using $\\alpha_{train}$. In essence, I didn't follow the discussion well enough to feel confident that what is demonstrated indicates an improved prediction.\n\t * In section two 4.4, I'm uncertain what is meant by saying the phase transition \"becomes sharp as one takes $n, S  \\rightarrow  \\infty$.\" Is this demonstrated? \n\t \n * It would be nice to see some comparison of the predictions of this approach compared to those of a leading-order finite width correction, e.g. [30]. Are these other approaches incapable of exhibiting feature learning?\n * A number of terms and notation are introduced without explanation, and I'm unsure which are standard to someone more familiar with the topic. Perhaps adding additional references to background material could aid the uninitiated. For example, $C$ is not defined when first introduced, SRN is undefined, and I still don't fully understand what \"self-consistent\" means.\n * Minor: \"self consistent\" should be hyphenated everywhere.\n Yes",
            " Thanks very much for your detailed reply, which has helped clarify several things for me. I also appreciate the additional experiments comparing to other theories. My overall assessment is positive and I\u2019d be happy to see this paper accepted. My score remains a 6 and my confidence is increased to 4.",
            " Thanks for comments and the clarification regarding Figure 1, I am maintaining both my rating and confidence score.",
            "The authors develop a self-consistent Gaussian Processes (GPs) theory to account for finite size effects in Deep neural networks (DNNs). The equivalence between GPs and DNNs is a well known result that holds in the so called \u2018lazy learning\u2019 regime in the infinite width/channel limit, where the DNNs\u2019 parameters don\u2019t change during training. In practical contexts, however, DNNs are finitely sized and operate in a different regime, called \u2018feature learning\u2019, where parameters change substantially during training. This paper provides a theoretical framework to account for finite size effects, and so it allows to investigate the feature learning regime.\n\nTo accomplish this, the authors consider the partition function $Z$ associated with the DNN posterior distribution $P(f)$, in the case of training via noisy (full batch) gradient flow with MSE loss. $Z$ is given by an integral over the prior distribution $P_0(f)$, which can be expressed in terms of cumulants. Following their notation, $C$ is the hyper-parameter controlling the over-parametrization. In the infinite-$C$ limit, only the first two cumulants survive, and the standard GP-DNN equivalence is recovered, with the GP kernel given by the second cumulant.\nFor $C$ finite, all cumulants contribute, and the authors have to rely on a saddle point approximation to make progress. By doing so, they arrive at a self-consistent set of equations for the target shift that describes the predictive mean of the DNN. In fact, the predictive mean is given by the same GP-like expression found for the infinite-$C$ limit, though with shifted targets. Next, the authors consider a second order expansion of the action so as to evaluate the posterior covariance.\n\nIn order to test their results, the authors consider two examples of DNN architectures that over-perform their associated GPs. Of the two, they mostly focus on the case of a linear two-layer CNN, in a teacher-student set up.  For this simple architecture, the cumulants can be worked out analytically. However, in order to deal with the series appearing in the expression of $P_0(f)$, the authors consider the limit of infinite number of training data, also known as Equivalent Kernel limit. This leads to an analytical expression for $\\alpha$, the proportionality factor between the discrepancy in prediction and the target value. Numerical verifications confirm their theoretical predictions (showing mismatches decreasing with the number of training samples).\nBy modelling the empirical weight covariance matrix as a Wishart matrix with a rank-one perturbation, the authors are able to detect a feature learning transition for this model. This is marked by the appearance of outliers in the associated spiked Marchenko-Pastur eigenvalue spectrum.\n\nFinally, the authors consider the case of a two-layer FCN with average pooling and quadratic activations, with a rank-1 teacher. They provide the expression of the cumulant generating function, as well as the equation for the targets shift, and leave the analysis to Appendix I.  The overall quality of the paper is good: I found it technically solid, clearly written, original (at least to my knowledge), and all claims are well supported. The organisation of the paper is fine but could be improved (see my comments below).\n\nI have the following comments for the authors:\n\n1. Other works from the literature (see e.g. https://arxiv.org/abs/1906.08034) distinguish lazy and feature learning regimes in the infinite-$C$ limit, according to the scaling of the network's parameters with $C$. In this paper, feature learning appears to be solely a finite size effect (the parameters\u2019 scaling with $C$ gives the lazy learning regime in the infinite-$C$ limit). I wonder if the self consistent theory developed here would work for different scalings of the parameters with $C$, and in which case, as I presume the cumulants too will scale differently with $C$. I would appreciate if the authors could comment on this point.\n\n2. The authors use a saddle point (SP) method in a somewhat non standard way, as explained in the first paragraph of Appendix A.4. I think the paper would gain in clarity with a sentence in the main text that spells out this point. I would also suggest to state more clearly that the approximation assumes $n$ large, with $n$ the training set size, in the paragraph where the SP method is introduced. A further improvement would be to comment on the error induced by this approximation when $n$ is finite, as e.g. to explain the mismatches observed in figure 1 (increasing as $n$ decreases).\n\n3. With the SP method, the authors are able to find the expression for the posterior covariance matrix by expanding the action to quadratic orders. This coincides with the posterior covariance of a GP with a kernel given by $K + \\Delta K$ (with $\\Delta K$ properly defined in eq. (9)). As far as I understand, the term $\\Delta K$ makes this kernel different from the one appearing in the predictive mean with shifted targets. If this is the case, the predictions for finitely sized DNNs cannot be really described as equivalent to a GP regression with shifted targets (while the equivalence in the infinite-$C$ limit is exact). If so, this should be stated clearly in the paper, as to avoid misinterpretations.\n\n4. Although section 4.4 provides a valid and interesting investigation of the feature learning transition in the CNN model, it is not completely clear to me how the proposed analysis fits with the rest of the paper, i.e. if/how the self-consistent shifted target approach is relevant for the task. I would recommend the authors to make this clearer in the main text; as of now, it appears to be more of a stand alone section (also, it is not immediately obvious why the feature-to-lazy learning transition observed in this specific model is of interest, nor is it clear if the same method could be applied to different architectures).\n\n5. The discussion of the FCN architecture presented in the main text is limited to the equation for the target shift, while the rest of it is left to Appendix I. Even there, there is no figure showing a comparison between theoretical predictions and empirical results. I would have expected to see a plot of this kind in the main text (i.e. one similar to fig. 1), as to support the general applicability of the proposed method. Basically, the content of the FCN discussion doesn't seem to be enough for a separate section in the main text. The authors do not put great effort in explaining the limitations of their analysis. In particular, the obstacles for their theory to be applied to different DNN architectures should be discussed with more details.",
            "This paper introduces a new perspective on feature learning in wide neural networks by demonstrating that DNNs trained with noisy gradients and large training data converge to solutions with mean predictions dictated by GP regression with shifted targets when using MSE loss. The authors then consider two toy CNN models, one linear and one non-linear, where they are able to make more analytic progress with their theory, and demonstrate the benefits of of feature learning of wide enough but finite NNs compared to their corresponding infinite-width GPs in terms of sample complextiy on student-teacher tasks. Finally, they also show a phase transition (marked by a critical value of width/channels) in the distribution of eigenvalues empirical covariance of weights in the first layer of a CNN instantiation, between a feature learning regime (where there is an outlier eigenvalue that is associated to the teacher's features) and a non-feature learning regime. Experimental evidence is provided to support these claims.   *originality* The view of trained NNs as GP regression with modified targets is novel as far as I can tell, as are the insights developed on toy CNN models. The paper is clear in how it uses and extends previous work in my opinion.\n\n*quality* I enjoyed reading this paper, and believe it is of high quality. The approach is non-rigorous but I believe technically sound, and the insights that the authors obtain (e.g. the main equations 8 and 16) are interesting. The choice of toy models is also made well, in that they are simple enough to obtain analytic results but also complicated enough to lead to profound results of benefits of feature learning (in terms of sample complexity).\n\n*clarity* the paper is well written and organised, particularly with regards to relation to previous work.\n\n*significance* The paper examines a key question which is of feature learning in wide NNs, which will be of interest to the general NeurIPS community. The new results about trained NNs being GP regression with shifted targets is certainly surprising to me, and I can see these techniques being built upon in future work. \n\nMinor points:\n- The comment on line 20 about 'DNNs map to Bayesian inference on GPs governed by the NTK' is incorrect. https://arxiv.org/abs/2007.05864 shows you need to add in extra randomness to the initialised NN to obtain a Bayesian interpretation to trained NNs in the NTK regime.\n- where -> were in line 332\n- There is another concurrent paper to [41] in line 77, https://arxiv.org/abs/2106.06615, which provides similar results. - I'm a bit unsure what the setup is in Figure 1. E.g. what is C*, what is the input dataset? Also isn't it concerning that as you increase the number of channels \\alpha gets larger (so the discrepancy with the ground truth increases)?\n- This isn't really a limitation, but it's quite hard for someone (like myself) who is not well versed in the techniques used in statistical physics to understand all the details of the author's arguments."
        ],
        "sentiment": [
            "Positive",
            "Neutral",
            "Positive",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer increased their score after the author's response, indicating a positive shift in their assessment. They express appreciation for the authors' efforts and find the analyzed model attractive.",
            "The review expresses both positive and negative aspects. It acknowledges the important contributions of the paper but also points out issues with clarity, experimental sections, and missing explanations.",
            "The reviewer expresses appreciation for the authors' response and additional experiments, states that their overall assessment is positive, and indicates they would be happy to see the paper accepted.",
            "The reviewer expresses gratitude (\"Thanks for comments\") and confirms their satisfaction with the clarification, maintaining their original rating and confidence.",
            "The reviewer states, \"The overall quality of the paper is good: I found it technically solid, clearly written, original (at least to my knowledge), and all claims are well supported.\"",
            "The reviewer expresses enjoyment in reading the paper and believes it is of high quality. They find the insights interesting and the choice of toy models well-made. The reviewer also notes the paper is well-written, organized, and addresses a key question of interest to the community."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Supportive",
            "Supportive",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review offers both criticisms and positive feedback. While initially pointing out shortcomings, the reviewer acknowledges the authors' response and revises their score upward, demonstrating a balanced perspective.",
            "The reviewer uses phrases like \"imprecise and hard to follow,\" \"somewhat vague,\" \"unclear,\" \"confusing,\" and \"didn't follow the discussion well enough.\" These indicate a critical assessment of the paper's clarity and experimental validation.",
            "The reviewer uses phrases like \"Thanks very much,\" \"appreciate the additional experiments,\" and \"happy to see this paper accepted,\" indicating a supportive and encouraging tone.",
            "The reviewer's expression of thanks and affirmation of their rating suggests a supportive and agreeable tone.",
            "While the reviewer expresses overall positive sentiment, they also provide several points of criticism and suggestions for improvement, indicating a balanced assessment. Phrases like \"could be improved\" and specific questions/concerns demonstrate a critical perspective alongside the supportive comments.",
            "The reviewer uses phrases like \"I enjoyed reading this paper\", \"believe it is of high quality\", \"interesting\", and \"well written\". They also highlight the paper's originality and significance, indicating a supportive stance. While providing constructive criticism, the overall tone remains positive and encouraging."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its assessment. It starts by acknowledging the potential of the work but points out key areas for improvement, such as relevance to realistic models and connection to existing literature. The reviewer maintains a constructive tone throughout, asking specific questions and suggesting concrete improvements. The revision of the score upwards after the author's response is a logical progression, indicating that some concerns were addressed, without contradicting the initial assessment of the paper's strengths and weaknesses.",
            "The review is consistent in its assessment. The reviewer acknowledges the potential importance of the work but expresses concerns about the clarity of the writing, particularly in the experimental sections. The reviewer's points are detailed and logically connected, highlighting specific areas where the paper needs improvement. The reviewer's self-declared limited familiarity with the topic explains their difficulty in fully evaluating the paper and justifies their questions and requests for clarification. The final 'Yes' is ambiguous and likely a mistake, but it does not contradict the overall consistent critique provided in the review.",
            "The reviewer expresses positive feedback throughout the review and there are no conflicting statements. The reviewer appreciates the authors' response and additional experiments, maintains a positive overall assessment, and is happy to see the paper accepted. The score and confidence level are also consistent with a positive assessment.",
            "The reviewer maintains their rating and confidence score after the clarification, indicating a consistent stance.",
            "The review is consistent because the reviewer starts with an overall positive assessment of the paper's quality (technically solid, clearly written, original, claims well supported) and all subsequent comments are constructive suggestions for improvement and clarification, rather than criticisms that contradict the initial positive assessment. The reviewer points out areas for improvement in clarity, organization, and completeness, but these are typical aspects of peer review and do not indicate any self-contradiction or inconsistency in the reviewer's evaluation.",
            "The review is consistently positive, praising the paper's originality, quality, clarity, and significance. The reviewer expresses enjoyment in reading the paper and highlights its novel contributions and interesting insights. The minor points raised are constructive suggestions for improvement and do not contradict the overall positive assessment of the paper's consistency and value."
        ]
    },
    {
        "paper_id": "iclr_2022_6VpeS27viTq",
        "paper_title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations",
        "paper_abstract": "Owing much to the revolution of information technology, recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. Yet those publicly accessible information also raises a fundamental issue concerning Intellectual Property, that is, how to precisely control legal or illegal exploitation of a dataset for training commercial models. To tackle this issue, this paper introduces and investigates a new concept called ''learnability lock'' for securing the process of data authorization. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to encrypt data samples so that they become ''unlearnable'' by machine learning models with negligible loss of visual features. Meanwhile, authorized clients can use a specific key to unlock the learnability of the protected dataset and train models normally. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.  ",
        "review_ids": [
            "PXbHqZGEb3G",
            "g0pEKWLHLm8",
            "hv8uDNuWSw",
            "PADrOjHWXIZ",
            "nuwonUt4A1",
            "26nqwH1Ccax",
            "c8Gc6gGKQQt",
            "C-UE2JaaHtB",
            "3m86ELjHm95"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "- The paper tackles the problem of 'unlearnable examples': to perturb the images of a labeled dataset $D_c$ to obtain $D_p$ with the desiderata (a) training models on $D_p$ leads to models with significantly lower performance (b) image perturbations are constrained to some $\\epsilon$-ball and (c) with the correct 'secret key' (learnable parameters in this case), one should approximate recover $D_c$.\n- The approach `learnability lock' to make the examples unlearnable involve a bi-level minimization objective over the original network parameters $\\theta$ and parameters $\\psi$ of a 'secret key' network $g_\\psi$ (which could be a linear layer or an invertible ResNet).\n- Experiments demonstrate among other things: (i) training networks on $D_p$ leads to poor performances and on unlocked $D_c$ to high performances (ii) the proposed approach is more robust to augmentation strategies (specifically adversarial training) compared to previous approaches. **Strengths**\n\n1\\. Well-motivated problem and reasonable results\n- Given the prevalence of data being shared and made accessible everyday, it becomes crucial certain authorized uses e.g., preventing malicious entities on training a facial classifier on provided data. This paper takes an interesting step towards addressing this issue. The authors also show certain benefits of the approach e.g., controlling access to who can train models on the given data.\n\n2\\. Writing\n- I highly appreciate the clarity in writing. The paper was clear for the most part and easy to follow. \n\n**Major Concerns**\n\n1\\. Significance of contributions over prior work\n- My first concern is the significance of the contributions over prior work and most notably with [1]. Similar to proposed approach, [1] also propose a bi-level minimization approach to introduce class-specific additive perturbations to prevent training on the perturbed dataset. My sub-concerns: \n- (a) it seems that the improvement of the proposed approach is primarily addressing a specific failure mode -- being more robust to adversarial training on the perturbed dataset. Apart from that, [1] seems to outperform the proposed approach in few other scenarios e.g., when comparing test accuracy (Table 1) [1] appears to outperform (esp. on CIFAR-100 and ImageNet).\n- (b) As for the technical contributions itself, the proposed approach also shares a bi-level minimization problem similar to [1]. But instead of learning fixed additive per-class perturbations, a network $g_\\psi$ is trained to perturb the inputs. Could the authors comment on technical benefits over previous approach? I see that in Table 9 remarks the benefits as \"adv. train\" (which is supported) and \"stealthiness\" (which is unclear; aren't both methods evaluated at a comparable $\\epsilon$?)\n\n[1] \"Unlearnable Examples: Making Personal Data Unexploitable\" Huang et al., ICLR '21\n\n2\\. Optimization formulation\n- I was also a bit unclear (or rather could not intuit) on how the optimization objective (3.1) makes the examples unlearnable.\n- Specifically, wouldn't the optimal $g_\\psi$ be an identity function which would lead to original 'clean' accuracy? In other words, if one were to take a pretrained $\\theta$, I reckon $\\psi$ would correspond to identity function?\n- I'm also wondering how the approach learns good parameters $\\psi$ in spite of an explicit term to maximize the loss of training on the perturbed dataset?\n- I would appreciate if the authors slightly elaborated (to complement an already nice discussion below eq 3.1) on the optimization problem.\n\n**Minor Concerns**\n\n1\\. Table 1\n- I think the results in Table 1 is comparable to previous approaches (e.g., Table 1 of [1])? I would appreciate the authors also added corresponding rows (or alternatively a more descriptive table in appendix) so that direct comparison with prior work when possible. As a reader, I had to flip back and forth to compare the numbers.\n\n2\\. Unauthorized use of perturbed dataset for other tasks\n- Are the authors aware whether training on $D_p$ fails only the envisioned task (e.g., 10-way multiclass classification over predefined CIFAR classes)? It seems problematic if the same images could be used for other tasks (e.g., face recognition) unforeseen by the creator.\n\n**Nitpicks**\n* \"Control over Single Class\": What is the accuracy of the model on locked bird class? What is the number in the heatmap of Fig. 2? - Overall, the paper tackles a reasonably well-motivated idea of introducing perturbations to prevent unauthorized training over a labeled dataset. The authors show that the approach achieves this objective and is additionally more robust when compared to previous works.\n- My biggest concern is the significance of improvements over prior work, which appears limited to a highly specific failure mode. I would be glad to increasing my score if the authors could address this.",
            " Thanks for the response, this does address some of my concerns.",
            "Paper presents an idea of \u201cLearnability lock\u201d, a system that aims to control learnability of individual datapoints or data classes. The system builds on top of recent work on data learnability, where, in essence, noise is applied to individual datapoints in a way to disrupt learning generalisable features. From the description, it appears that it works as a poisoning or a backdoor attack, where the objective is to correlate non-generalisable features. \n\nNow, learnability lock describes a mechanism that enables unlearnable features, whilst at the same time allowing the user to revert unlearnable noise if authorisation is acquired. Authors evaluate Learnability lock with a number of datasets and network architectures to find that it outperforms its competitors in presence of defences.\n\n\n Strengths:\n+ Interesting setting and a challenging threat model\n+ Thorough evaluation \n\nWeaknesses:\n+ Narrative being very far from reality \n+ Unclear contributions over and above Huang et al. \n\n\nFirst, I want to mention that the paper is very well written, thoroughly evaluated and presents a convincing technical story for an ML audience. Yet, for people more familiar with Computer Security, and \u201cdo not roll your own crypto\u201d principle, it presents a rather weak argument as to why any protection is provided with such an encryption scheme at all. In fact, the results from adversarial training tell that the system design is broken and the paper contradicts its own story. \n\nSecond, Carlini et al. in \u2018Is Private Learning Possible with Instance Encoding?\u2019 talk about lack of rigorous notions for privacy and use of ad-hoc arguments in instance encoding schemes such as InstaHide. It seems that this paper follows the same premise and provides no justifications for its, rather strong, claims. I am very confused about the setting of the paper. \n\n```\nExisting methods include training ML models on encrypted data, where the sensitive information could be hidden through cryptographic approaches in order to prevent malicious manipulation (Hesamifard et al., 2017; Ding et al., 2021). However, those kinds of data processing methods normally do not preserve visual properties from the raw data and thus affect normal use. For example, it does not make sense for one to send a fully encrypted and unrecognizable \u201cselfie\u201d photo to friends just to make sure the photo will not be exploited without authorization.\n```\n\nIndeed, if the goal is confidentiality and control of ones secrets, it does make sense to send data encrypted, a de-facto standard in private end-to-end communication. There is a branch of cryptography focusing on controlled disclosure of information if needed e.g. Partially Revealing Cryptography or even Deniable Cryptography. What is more, it does make sense to send ones \u2018selfie\u2019 encrypted over vetted channels and theoretically sound protocols \u2014 that way privacy is not going to be violated. In fact, the paper does discuss that data transfer is a potential security risk, yet argues to use a mechanism that leaks data by default:\n\n```\nFrom the data providers\u2019 perspective, this means that any mistake conducted in the data transfer procedure could lead to potential privacy/security risks. This raises great challenges in securely distributing sensitive data from the data providers to the clients.\n```\n\nIt took cryptographic community decades to establish best practices, develop theoretically and practically rigorous attacks and defences. Currently, there are international standards, competitions, and good understanding of privacy guarantees of different encryption mechanisms. \u2018\u2019Rolling your own crypto\u2019\u2019 leads to mistakes. Thus, I am unable to understand how the approach proposed in this paper improves upon the state-of-the-art in secure data transfer in Machine Learning.  \n\nIn light of the above, I was unable to understand the threat model in which the paper proposes to provide a sensible mechanism to protect ones privacy, limiting its contributions over and above Huang et al. I also did not understand how the proposed approach innovates over a scheme constructed using standard cryptography toolbox like AES to make learning impossible.\u00a0 If the authors are able to clarify these concerns, I would be happy to revise my review.\n\n\n\n\n\n Paper presents an interesting construction for controlled learnability, but the setting of the paper seems to be a bit misleading. Paper argues that learnability lock provides privacy through their encryption scheme, yet in the same paper authors break their own scheme. Paper also provides no cryptographic underpinning as to why any privacy is provided at all.",
            " Dear authors, thank you very much for your update, I think it clarifies my concern about the setting. Updating the score. \n\nAside from that, I was wondering if there was a particular reason why W and B have to be learned? Aside from cases where the inverse does not exist, could they not be inverted directly?",
            " I would like to thank the Authors for their clarifications, all my questions were addressed.\n\n",
            " Thank you very much for your response. \n\nI am afraid the revised paper still has not addressed my concerns \u2014 I think that the abstract/introduction/conclusion are misleading. I do not think that use of the word encryption or encryption scheme is applicable here at all, and instead class-wise poisoning is more applicable. In fact, in the comments you claim IP, a digital rights management setup (DRM), whilst in the introduction of the paper you talk about encryption and protecting data ``in transit`` (?).  You will discover that most modern DRM works using standard cryptographic tools. \n\nAs I mentioned before, there exist subfields of cryptography that enable the tasks you described e.g.``revealing encryption`` or ``deniable cryptography``, that both protect data and reveal some of its properties. If you follow down that path and want to claim secrecy, I think you need to consult the relevant literature.  \n\nAs I said before, if you change the setting of the paper i.e. relevant parts of abstract/intro/conclusion, I will change the score. ",
            " Thanks for the clarifications. I think most of my concerns are properly addressed. The unlearnable rate indeed seems to be a big challenge at the moment. And if a few unlearnable can break the entire dataset, then it will be a more powerful data poisoning attack and a big threat to deep learning in general. From the data protection, authorization, or IP protection perspective, I believe the authors have done quite good work compared to Huang et al. (2021). Learnability lock appears like the type of techniques people need to protect/authorize their social media data. I believe this work should be of significant interest to the community.\n\nI am happy to increase my rating to 8 (apparently, there is no 7 :)).",
            "This paper introduces the idea of \u201clearnability lock\u201d for data authorization. This paper along with three prior works mentioned in the paper are all creative works recently proposed for data protection, an important research topic that has been investigated much before.  Compared to the \u201cunlearnable examples\u201d work of Huang et al. 2021, this work proposes to use inevitable linear/convolution transformations to formulate the noise. This was demonstrated to have two benefits that seem to address two important limitations of previous works: 1) making the unlearnable noise more controllable so \u201clearnability\u201d can be flexibly locked and unlocked; 2) the noise seem can survive (to some extent) adversarial training now. Strong points:\n1. An advanced exploration of an important research direction. The idea of \u201clearnability lock\u201d appears fairly interesting, although it is motivated by the \u201cunlearnable examples\u201d idea. The \u201clearnability lock\u201d concept is one step closer to what these works try to achieve: giving data owners some real control of their own data without damaging (much) the original content.\n\n2. The proposed invertible noise generation method looks simple and effective, should be easily applied in real-world scenarios.\n\n3. The proposed method holds two advantages (invertible/controllability and robustness to adv train) over the prior work and these two advantages seem to be quite important for real-world usage.  \n\n4. The paper is well written, pleasant to read. The experiments are comprehensive. \n\nWeak points:\n1. It is not very clear why invertible transformation improves robustness to adversarial training. Although it explains somewhere, conceptually, multiplicative noise is more effective than additive noise, I think some experiments can be conducted to dig a bit deeper on this point. Looking forward to some explanations from the authors.\n\n2. The robustness to adaptive adversarial training part in the appendix is also important. I suggest the authors provide a paragraph of analysis in the main text as well. Also, some experiments in the appendix are not mentioned in the main text.\n\n3. The experiment structure looks quite similar to Huang et al. 2021, I suggest the author acknowledge their work/setting in the experimental setting. \n\n4. The 100% unlearnable rate seems to be a remaining limitation of your and Huang\u2019s work. Any thoughts on this?\n Important research on data protection/authorization, simple yet very effective method, some level of technical improvements over prior work but addresses two key limitations, rich experiment, good writing. A quite solid paper overall. ",
            "The paper proposes a new concept called \"learnability lock\" for securing distribution of sensitive data. The idea is to apply an invertible adversarial transformation when releasing the data, which makes the data \"unlearnable\" for machine learning models, but also preserves the visual properties of the data. Therefore, unauthorized practitioners can not use the data for training ML models, however authorized users can use specific key to invert adversarial perturbation and make the data \"learnable\" again. \n\nThe authors propose two invertible transformations to craft adversarial perturbations: linear pixel-wise transformation and convolutional functional transformation based on invertible ResNet. Numerous experiments demonstrate the effectiveness of the proposed transformations in both securing the data (making the data unlearnable when transformation is applied) and unlocking the transformation (making the data learnable when the transformation is inverted).  The idea of using invertible adversarial transformation for securing the data is interesting and novel. The paper is very well written and structured. Numerous experiments are conducted to show effectiveness of the method for breaking the ML models when \"secured\" data is used and for training accurate ML models when the inverse transformation is applied. The authors also show that their method is more robust to adversarial learning than state-of-the-art additive perturbation methods. The method is shown to be robust to strong data augmentation and filtering techniques which are often used as defense methods. Finally, the authors conduct experiment to show that when two different adversarial transformations are computed, their keys can not be used to unlock each other's transformation. \n\nI have two questions for the authors: \n1. What data was used to train the attacker model (f_theta)? Also, I can see that the ResNet-18 model was used for computing the attack, did you try to use other backbones? Does the attack still transfer to other model in this case? It would be great to elaborate on that somewhere in Experiments section. \n2. In section 4.3 it is said \"we train two learnability locks for each transformation separately\". When learning two transformations from the same data, how can one control that the transformations will be different and that the model will not converge to the same solution twice? \n Overall, I believe that the idea of the paper is interesting and novel, the results are appealing. The writing is very clear and structured, the method is compared to other attacks and defenses and is shown to outperform previous methods. I vote for accepting the paper. "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Positive",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review expresses several major concerns, including the significance of the contributions over prior work and the optimization formulation. The reviewer also points out that the proposed approach seems to only improve upon a specific failure mode and that prior work outperforms it in other scenarios. The tone is critical, suggesting a lack of confidence in the paper's novelty and technical soundness.",
            "The reviewer expresses gratitude ('Thanks') and acknowledges that their concerns have been addressed.",
            "The review expresses significant concerns about the paper's threat model, cryptographic underpinnings, and overall contribution. Phrases like \"weak argument,\" \"broken system design,\" \"no justifications for its, rather strong, claims,\" and \"unable to understand\" indicate a negative sentiment.",
            "The reviewer expresses gratitude for the authors' update and states that it clarifies their concern, leading to an updated (presumably improved) score. This indicates a positive shift in the reviewer's assessment.",
            "The reviewer expresses gratitude to the authors and confirms that all their questions were addressed, indicating satisfaction with the revisions.",
            "The reviewer explicitly states that the revised paper has not addressed their concerns and finds the abstract/introduction/conclusion misleading. The reviewer also expresses disagreement with the terminology used (encryption vs. class-wise poisoning).",
            "The reviewer explicitly states that their concerns are addressed and expresses happiness to increase the rating. Phrases like \"quite good work\", \"significant interest to the community\", and \"happy to increase my rating\" indicate a positive sentiment.",
            "The review expresses positive feedback, highlighting the paper's interesting idea, effectiveness, advantages, and good writing. Phrases like 'An advanced exploration,' 'simple and effective,' 'quite important,' 'well written, pleasant to read,' and 'A quite solid paper overall' indicate a positive sentiment.",
            "The review expresses a positive overall assessment, highlighting the novelty and effectiveness of the proposed method, the clarity of writing, and the strong experimental results. Phrases like 'interesting and novel,' 'results are appealing,' 'very clear and structured,' and 'I vote for accepting the paper' clearly indicate a positive sentiment."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Supportive",
            "Supportive",
            "Supportive"
        ],
        "tone_reason": [
            "The review uses phrases like \"My first concern\", \"I was also a bit unclear\", \"It seems problematic\", and \"My biggest concern\" to express reservations and criticisms. The reviewer also uses direct questions to challenge the authors' claims and understanding of the problem. The final statement explicitly mentions a willingness to increase the score if the concerns are addressed, highlighting the current dissatisfaction.",
            "The reviewer uses positive language ('Thanks', 'does address') indicating a supportive stance towards the work being reviewed.",
            "The tone is critical, using phrases like \"Narrative being very far from reality,\" \"Unclear contributions,\" \"presents a rather weak argument,\" and directly questioning the paper's claims and justifications. The reviewer also points out contradictions within the paper, further reinforcing the critical tone.",
            "The reviewer expresses gratitude ('thank you very much') and frames their question as a point of 'wondering,' suggesting a desire to understand the authors' choices rather than directly criticizing them. This exhibits a supportive and collaborative tone.",
            "The reviewer uses phrases like \"thank the Authors\" and \"all my questions were addressed,\" which conveys a supportive and appreciative tone.",
            "The reviewer uses phrases like 'I am afraid the revised paper still has not addressed my concerns,' 'I do not think that use of the word encryption or encryption scheme is applicable here at all,' and 'misleading' which indicate a critical tone. The reviewer is directly challenging the author's choices and suggesting specific changes.",
            "The tone is supportive, evident in phrases like \"Thanks for the clarifications\", \"I think most of my concerns are properly addressed\", and \"I believe this work should be of significant interest to the community\". The reviewer also provides constructive feedback and acknowledges the value of the work.",
            "The tone is supportive, offering constructive criticism and encouragement. The reviewer points out strong points and suggests improvements in a helpful manner, using phrases like 'Looking forward to some explanations from the authors' and 'I suggest the authors provide a paragraph of analysis.'",
            "The tone is supportive, as evidenced by the reviewer's encouragement of the authors ('It would be great to elaborate on that somewhere in Experiments section') and the concluding statement recommending acceptance ('I vote for accepting the paper'). The reviewer frames their questions constructively, aiming to improve the paper rather than criticize it."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer clearly outlines both the strengths and weaknesses of the paper. The reviewer appreciates the motivation and clarity of the paper, while raising valid concerns about the significance of the contribution over prior work and some aspects of the methodology. The different sections of the review (Strengths, Major Concerns, Minor Concerns, Nitpicks) all contribute to a coherent overall assessment without contradicting each other.",
            "The review is consistent as it expresses gratitude and acknowledges that some concerns have been addressed. There are no contradictory statements within this short review.",
            "The reviewer consistently criticizes the paper's security and privacy claims, arguing that it lacks cryptographic rigor, misrepresents existing secure data transfer methods, and proposes a weak \"roll your own crypto\" solution. The reviewer's confusion about the threat model and setting is also a consistent point throughout the review.",
            "The reviewer expresses satisfaction with the authors' update and raises a new question, which is consistent with a constructive review process.",
            "The review is consistent as the reviewer expresses satisfaction that their questions were addressed and thanks the authors for clarifications. There are no contradictory statements.",
            "The review is consistent because the reviewer clearly states their concern about the misuse of the term 'encryption' and suggests alternative framings like 'class-wise poisoning' or concepts from subfields of cryptography. The reviewer's arguments are logically connected and focused on this central point, offering specific examples and suggestions for improvement.",
            "The review is consistently positive, expressing satisfaction with the authors' response and highlighting the significance and relevance of the work without any contradictory statements.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper without contradicting itself. The reviewer acknowledges the novelty and importance of the research, praising its contributions and effectiveness while also pointing out areas for improvement. The weaknesses are presented as constructive criticisms to enhance the paper further, not as fundamental flaws that undermine its value. The overall tone remains positive, concluding that it is a 'quite solid paper overall', which aligns with the identified strengths and constructive weaknesses.",
            "The review is consistently positive, praising the novelty, clarity, and experimental validation of the paper. The reviewer raises constructive questions for improvement but ultimately recommends acceptance, aligning with the positive tone throughout the review."
        ]
    },
    {
        "paper_id": "iclr_2018_ByJHuTgA-",
        "paper_title": "On the State of the Art of Evaluation in Neural Language Models",
        "paper_abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.\n      ",
        "review_ids": [
            "S1Mw8jBef",
            "rJTcBCtxG",
            "HkGW8A2gG"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications, and indeed the results here are interesting as they favour relatively simpler structures.\n\nI have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.",
            "The authors did extensive tuning of the parameters for several recurrent neural architectures. The results are interesting. However the corpus the authors choose are quite small, the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn.\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens. This will use significant resources and is much more difficult, but it's also really valuable, because it's much more close to real world usage of language models. And less tuning is needed for these larger datasets. \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. ",
            "The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space. Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks.\nThis is a significant result in language modeling and a milestone in deep learning reproducibility research. The paper is clearly motivated and authoritative in its conclusions but it's somewhat lacking in detailed model or experiment descriptions.\n\nSome further points:\n\n- There are several hyperparameters set to the \"standard\" or \"default\" value, like Adam's beta parameter and the batch size/BPTT length. Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified.\n\n- The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection). Second, the phrase \"additive skip connections combining outputs of all layers\" has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?).\n\n- Fully evaluating the \"claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization.\n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance.\n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table. The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces. For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters.\n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \"standard\" or \"conventional\" LSTM implementation (e.g., as provided in optimized GPU libraries). In addition to further discussion on this point, this result also suggests evaluating other recently proposed \"minor changes\" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016)\n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \"in which there is further sharing of masks between gates\" and the one with \"independent noise for the gates,\" as described in the footnote. There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help."
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer expresses a 'favourable impression' of the paper and acknowledges the importance and interesting results of the study.",
            "The reviewer expresses concerns about the validity of the conclusions due to the small dataset size, suggesting the results might not generalize well. The reviewer also suggests more experiments, implying the current work is insufficient.",
            "The review expresses a positive overall sentiment towards the paper, highlighting its significance in language modeling and deep learning reproducibility research. Phrases like \"significant result,\" \"milestone,\" \"clearly motivated and authoritative,\" and \"significant conceptual step forward\" indicate a favorable assessment."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses positive language ('favourable impression,' 'important,' 'interesting') and expresses hope that another reviewer will provide more specific feedback, indicating a generally supportive stance.",
            "The review uses phrases like \"quite small\", \"variance of the estimate will be quite high\", \"I suspect whether the same conclusions could be drawn\", and \"it's better to do some experiments\", indicating a critical evaluation of the work's methodology and scope.",
            "The review is balanced, offering both praise and constructive criticism. While acknowledging the paper's strengths (e.g., \"significant result,\" \"clearly motivated\"), it also points out areas for improvement (e.g., \"somewhat lacking in detailed model or experiment descriptions,\" suggestions for further experiments, and requests for clarification). The reviewer uses polite and professional language throughout."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because the reviewer expresses a positive view of the paper, highlighting its importance and interesting results. The reviewer's statement about hoping for another reviewer with more specific domain knowledge is not contradictory but rather a reflection of their own expertise and a desire for a more comprehensive review process.",
            "The review is consistent. The reviewer acknowledges the interesting results obtained through extensive parameter tuning but raises concerns about the small dataset size and its impact on the generalizability of the conclusions. The reviewer then suggests concrete steps to improve the study, such as using larger datasets and evaluating on downstream tasks. These suggestions directly address the initial concern about the dataset size and aim to strengthen the validity and real-world relevance of the research. There are no contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent because it acknowledges the paper's significant contribution and strengths while also providing constructive criticism and suggestions for improvement. The reviewer points out areas where the paper could be more detailed and clearer, but these points are presented as suggestions for enhancement rather than contradictions to the positive overall assessment of the work."
        ]
    },
    {
        "paper_id": "iclr_2022_87Ks7PvYVJi",
        "paper_title": "Offline Decentralized Multi-Agent Reinforcement Learning",
        "paper_abstract": "In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition probabilities calculated from the dataset can be much different from the transition probabilities induced by the learned policies of other agents, creating large errors in value estimates. Moreover, the experience distributions of agents' datasets may vary wildly due to diverse behavior policies, causing large difference in value estimates between agents. Consequently, agents will learn uncoordinated suboptimal policies. In this paper, we propose MABCQ, which exploits value deviation and transition normalization to modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the biased transition probabilities of next states. They together encourage agents to discover potential optimal and coordinated policies. Mathematically, we prove the convergence of Q-learning under the non-stationary transition probabilities after modification. Empirically, we show that MABCQ greatly outperforms baselines and reduces the difference in value estimates between agents. ",
        "review_ids": [
            "qYntxq7bNY",
            "svGa2EWpdn-",
            "apHmidbdfjJ",
            "Q-zAbFMoHNA"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper studies an offline MARL setting, with a focus on the decentralized case, and discovers that the difference between transitions in the dataset and other agents can be very large. Authors propose two techniques, value deviation and transition normalization, to modify the transition probabilities. Experimental results validate its performance improvement over BCQ on multi-agent MuJoCo tasks.\n Pros:\n1. The problem studied in this paper is very interesting, with a clear motivating example emphasizing that in the decentralized offline MARL setting, the difference between transition probabilities in the dataset and other agents can be large and harm the performance. Overall, the paper is well-written.\n2. The theoretical analysis shows that the proposed operator is able to converge.\n\nCons:\n1. There is a recent paper about offline MARL, MA-ICQ (Yang et al., 2021) which should be compared or at least discussed in the paper.\n2. The assumption in Theorem that gamma<r_min/(2r_max-r_min) seems too strong in the case that r_max>>r_min. \n3. The experimental evaluations are not sufficient as it only compares MABCQ with baselines on a mixed dataset. How does MABCQ perform on other types of datasets?\n\n This paper studies an interesting offline decentralized MARL setting. However, relevant works are not cited and discussed, and experimental evaluation can be improved.",
            "The authors propose a scheme to modify the transition probabilities in the learning process in a fully decentralised MARL setting. The authors show that the method is still converging to the optimal solution. They also provide some experiments on synthetic environments. It is not clear to me when the setting described by the authors occurs in practical cases. I think that the paper lacks a clear applicative example in which the framework proposed by the authors occurs.\n\nThe assumption that the transition probabilities are deterministic for the environment seems to be a bit restrictive. Again, if the author could provide a practical example, it would help to motivate the setting described by the authors.\n\nIn the first formula of page 5, the first two terms cancel out. I suggest to provide a more direct formula for this probability, e.g., P_B_i(s'|s,a_i) = 1/|S|. Moreover, the presentation of these modifications of the probabilities should be more formally defined with a proper symbol for each one of them.\n\nProposition 1 is misleading. If I understood correctly the theorem holds if one applies Q-learning over the previously modified transitions. It should be mentioned in the theorem statement.\n\nThe title of Section 3.3 (Implementation) is misleading. First, I would have mentioned which are the characteristics and the assumptions required for a generic methodology to be compliant with your proposed approach. Second, this seems a specific case of the application of your method rather than its implementation.\n\nI think that the experiments, even if they present some difference in the expected values of the agents' objective, do not provide any statistical significance of the fact that the proposed method is somehow providing an improvement over the existing ones (other than DDPG).\n\nIt would also be interesting to see the performance of fully centralized learning, to understand how much we are losing from the fact that we are learning in a decentralised manner. The topic is interesting, but the setting lacks a strong applicative example and the formalism of the paper should be improved before it is worthed for publication. Moreover, I think that the statements sometimes are hard to understand. Therefore, I suggest the author should try their best to improve the readability of the paper. Moreover, the experiments should provide some statistical significance that the proposed method is better than the state of the art.",
            "This paper proposes a method named MABCQ to utilize offline data in the MARL environment via (1) value deviation and (2) transition normalization. These two techniques are used to correct the bias in the individual observation of the transition probability and improve the discovery of optimal policy. The authors showed the convergence of the Q-learning under the non-stationary transition probabilities after modification.\n\n I appreciate the ideas proposed by the authors. However, a lot of descriptions in the submission are very vague and the statements are mathematically imprecise. I also have some concerns regarding the generality of the proposed framework. Please find my detailed comments below.\n\n(1) A lot of sentences are very vague and imprecise (without introducing the framework in a rigorous) way. To name a few examples:\n\n(1-A) It's stated in the abstract that \"However, the transition probabilities calculated from the dataset can be much different from the transition probabilities induced by the learned policies of other agents...\" This really depends on the information observability of the agents. If an individual agent could observe the states and actions of all agents in the system, then this won't be an issue.\n\n(1-B) In the first paragraph, the authors keep switching between \"agents\" and \"agent\" and sometimes mentioned \"offline RL\", it is difficult to understand if you are always talking about the multi-agent setting or moving to talk about the single-agent setting at some point.\n\n(1-C) The authors start with \"The main challenge of offline RL is the extrapolation error\" in the second paragraph, are you referring to the single-agent case, or multi-agent case, or both? Later in the paragraph, the authors mentioned \"... ignore to correct the transition bias\" without proper explanation of what you mean by transition bias.\n\n(1-D) The authors mentioned multiple times on \"decentralized multi-agent environment\" which is confusing to me. Based on my understanding of the paper, the proposed learning procedure is decentralized (choice of the agent or the coordinator) but this has nothing to do with the environment.\n\n(1-E) The second paragraph on page 2 does not make sense to me before talking about what is observable and what is not abservable to each individual agent.\n\n(1-F) I don't understand the sentence \"If the transition probabilities of high-value next states are extremely low...\"\n\n(2) Set up in Section 3.1:\n\n(2-A) The problem description is not mathematically rigorous. The authors should specify the dimension or the space.\n\n(2-B) Why the reward only depends on the state (not the action)?\n\n(2-C) It reads to me that the centralized training and decentralized execution scheme could still be applied to this setting. In this case, the bias issue mentioned in the motivation should be solved.\n\n\n A lot of descriptions in the submission are very vague and the statements are mathematically imprecise. I do not think the submission is ready for acceptance with its current presentation.",
            "This paper studies a MARL offline setting, where each agent is trained independently. value deviation and transition normalization techniques are proposed to modify the transition probabilities. Experimental results on four benchmarks show MABCQ's performance improvement over BCQ. Originality: The problem the paper studies is important and interesting. \n\nQuality:\n1.Value deviation aims to increase the transition probabilities of the high-value next states, but it will increase the value estimate of the current state and action. It can worsen the performance in bad datasets, which is discussed in the CQL paper.\n2.The evaluation of MABCQ is not sufficient, the dataset seems to be \"replay\" in RL domain. How does  MABCQ perform in other datasets: Random, Medium, Expert?\n3.The interpretation of the effectiveness of transition normalization in Figure 3 is not solid. The difference between the maximum value and minimum value can not quantify the coordination of agents, it would be better to show a real example.\n\nSoundness:\n1. A recent MARL offline RL paper \"Believe What You See: Implicit Constraint Approach\nfor Offline Multi-Agent Reinforcement Learning\" is not discussed and compared.\n2. The weakness of the proposed heuristic method is not discussed, is there any other method that can improve the coordination of agents?\n This paper studies a novel problem in MARL area. However, the proposed value deviation method is not well motivated, the experimental evaluation is only done in replay datasets, and other marl offline paper are not cited and discussed. Thus I recommend the rejection."
        ],
        "sentiment": [
            "Neutral",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses both positive aspects ('The problem studied in this paper is very interesting', 'Overall, the paper is well-written', 'The theoretical analysis shows that the proposed operator is able to converge') and negative aspects ('relevant works are not cited and discussed, and experimental evaluation can be improved', 'The assumption in Theorem ... seems too strong', 'The experimental evaluations are not sufficient').",
            "The review expresses concerns about the paper's clarity, applicability, formalism, and experimental validation. Phrases like \"not clear to me,\" \"lacks a clear applicative example,\" \"misleading,\" \"hard to understand,\" and \"should provide some statistical significance\" indicate a negative sentiment.",
            "The reviewer expresses concerns about the paper's clarity, mathematical rigor, and generality. Phrases like 'very vague and imprecise,' 'mathematically imprecise,' and 'not ready for acceptance' indicate a negative sentiment.",
            "The reviewer recommends rejection, citing issues with motivation, experimental evaluation, and lack of comparison to related work."
        ],
        "tone": [
            "Balanced",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review presents both positive and negative feedback. It acknowledges the paper's strengths (interesting problem, clear motivation, well-written, theoretical convergence) and weaknesses (missing related work, strong assumption in theorem, insufficient experimental evaluation). This balanced approach suggests a neutral and fair assessment.",
            "The review points out several flaws in the paper, including unclear applicability, misleading statements, lack of statistical significance in experiments, and poor readability. The reviewer uses direct feedback and suggestions for improvement, indicating a critical tone.",
            "The review uses direct criticisms and questions, pointing out specific instances of vagueness and lack of rigor. Examples include 'A lot of sentences are very vague and imprecise,' 'I don't understand the sentence,' and posing direct questions about the setup and assumptions of the paper.",
            "The review uses critical language, such as \"not well motivated\", \"not sufficient\", \"not solid\", and \"weakness\". The reviewer also directly states a recommendation for rejection."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review presents both positive aspects (interesting problem, well-written, theoretical analysis) and negative aspects (missing related work comparison, strong theorem assumption, insufficient experiments) without any contradiction. The overall summary is consistent with the detailed pros and cons.",
            "The review is consistent in its critique, focusing on the lack of clear application, insufficient formalism, and weak experimental validation.  The reviewer consistently points out areas for improvement throughout the text without contradicting themselves. The suggestions and criticisms all align with the overall assessment that the paper needs significant revision before publication.",
            "The review is consistent in pointing out the vagueness and imprecision of the paper throughout, providing specific examples to support this main point. The reviewer consistently criticizes the lack of clarity and mathematical rigor in different parts of the paper, without contradicting themselves.",
            "The review consistently identifies weaknesses in the paper across quality and soundness aspects, such as potential performance issues with value deviation, insufficient evaluation datasets, weak interpretation of results, missing related work, and lack of discussion on limitations. These criticisms logically lead to the final recommendation of rejection, indicating a consistent negative assessment."
        ]
    },
    {
        "paper_id": "iclr_2022_pgkwZxLW8b",
        "paper_title": "Efficient Image Representation Learning with Federated Sampled Softmax",
        "paper_abstract": "Learning image representations on decentralized data can bring many benefits in cases where data cannot be aggregated across data silos. Softmax cross entropy loss is highly effective and commonly used for learning image representations. Using a large number of classes has proven to be particularly beneficial for the descriptive power of such representations in centralized learning. However, doing so on decentralized data with  Federated Learning is not straightforward, as the demand on computation and communication increases proportionally to the number of classes. In  this  work  we  introduce Federated Sampled Softmax, a novel resource-efficient approach for learning image representation with Federated Learning. Specifically, the FL clients sample a set of negative classes and optimize only the corresponding model parameters with respect to a sampled softmax objective that approximates the global full softmax objective. We  analytically examine the loss formulation and empirically show that our method significantly reduces the number of parameters transferred to and optimized by the client devices, while performing on par with the standard full softmax method. This work creates a possibility for efficiently learning image representations on decentralized data with a large number of classes in a privacy preserving way.",
        "review_ids": [
            "jocMw1-Fg_w",
            "pO_yXIwfGuN",
            "pgSyR9HIb6",
            "Q4_3zIH2SMm",
            "fnD4GR-s73",
            "PvIyXw0G8lt"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for the rebuttal. The authors addressed some of my concerns. However, my main concerns on the novelty/contributions are still not fully addressed. No matter in a centralized or a federated setting, adding the positive classes seems quite obvious and straightforward. No further discussion is provided for federated learning algorithms besides FedAvg. Thus, I will keep my score unchanged.\nAlso, I would suggest the authors make a table of the dataset to make the setup more clear.",
            " 1. The author argues that the accuracy of ImageNet-21k will be close to the full softmax performance when increasing the % of classes. However, there is no real theoretical or experimental proof to support that. The current good results are only obtained on smaller-scale datasets with fewer classes. In machine learning, the phenomenon could be quite different at different scales. Thus it is unknown if we can really scale up the method to a dataset with many classes (which the paper is targeting for). Since there is no theoretical proof, experimental evidence would be necessary. \nEven we can recover the accuracy on ImageNet-21k by increasing %, what would be a sweet point of the %? It is possible that we need a very large % (e.g., 80%) to retain the accuracy (if ever possible). In such cases, the saving would be much smaller compared to existing methods like gradient compression (e.g., TernGrad, Deep Gradient Compression).\n\n2. The authors argue that we can simply use the single and same backbone (MobileNetV3) for comparison. I agree with Reviewer UuaD that extra backbones are needed to show that the generalization ability of the algorithm.\nThe experiments also lack the comparison to several important baselines: \na. Comparing with gradient compression methods (e.g., TernGrad, Deep Gradient Compression). The author mentioned that \"TernGrad quantizes gradients to improve efficiency but all the weights are still required to be transferred when adapting it for FL\" in reply to Reviewer UuaD. Nonetheless, TernGrad can reduce the gradient to 2bits, which is 16x smaller compared to full-precision weights. It is not clear if the proposed method can outperform TernGrad at the same amount of transferred data. TernGrad also has a better convergence proof compared to the proposed method. I would still think a comparison is necessary to show the advantage.\nb. Comparing with using a smaller model. Using a smaller model can also reduce the transferred data by reducing $d$. Reducing the model size and class % can both reduce transfer, but it is unclear which one has a better accuracy-cost trade-off, especially when the authors do not provide results of different backbones (I doubt that using a smaller model will easily provide a good trade-off on SOP and landmark dataset). It is also unclear whether the two dimensions can be well combined.",
            "This paper considers the federated learning problem, especially for the case that there are many classes and each client has a small set of classes. When there are many classes, the neural network has to have many parameters to define the last layer classifier, which makes a huge communication cost. To resolve this issue, the author proposes FedSS, by which each FL client samples a set of classes and defines the loss function only with the sampled classes.  Since unsampled classes are not involved in the client's training process, the client sends only the parameters corresponding to the sampled classes. The authors empirically show the proposed approach can reduce the communication cost without sacrificing performance. \n Pros,\n\nThis paper is well-written and easy to follow.\n\nThe idea is simple.\n\nThe proposed method can reduce the communication cost\n\nCons,\n\nThe most significant weak point is that the proposed algorithm is not free from the privacy issue. Since every sampled set of each client has to include the classes that the client has, the central server can infer the classes the client has. In many federated learning applications, it is strictly prohibited that the server can reveal any information of clients.\n\nIt would be much better to provide how much the algorithm can save the communication cost. Since the main contribution of this work is not improved performance but reduced communication cost, the authors should provide more discussions about the communication cost.\n\nThe authors should discuss with other communication efficient algorithms since the main contribution of this work is saving the communication cost. For instance, there are many works for efficient distributed learning, e.g., TernGrad. \n\nExperiments should include more results considering other data sets and neural net architectures. Since this is not a theory paper, the proposed algorithm can be justified only by experiments. Therefore, ICLR papers are expected to have comprehensive experimental results. \n This paper should be very careful to define a class subsampling for federated learning applications since it can leak clients' private information. Also, this paper requires more comprehensive experiments with considering other communication-efficient algorithms.\n",
            "In this paper, the authors proposed federated sampled softmax (FedSS) for resource-efficient federated image representation learning. When the number of classes is large, the final classification layer could take up a large part of the communication cost during training. FedSS allows subsampling the weights of negative classes to reduce data transfer and leads to similar accuracy compared to the full softmax. Pros:\n1. The paper is generally well written and easy to follow.\n2. FedSS provides a potential solution to reduce the communication cost of the classification layer in federated learning settings.\n\nCons:\n1. Firstly, the proposed method only has a significant saving when the number of classes is huge (Figure 8, >>1000). However, the datasets used in this paper mostly have a limited number of classes (SOP and Landmarks). In this case, the saving is very small (smaller than 15%). On real tasks with many classes (ImageNet-21k), the proposed method failed to match the accuracy compared to the full Softmax (Table 3). And the increase of $|S_k|$ does not seem to close the gap. Therefore, it is questionable if the proposed algorithm has real-life benefits.\n2. The proposed method seems to be a direct application of the sampled Softmax algorithm under a federated learning setting, which restricts the novelty.\n3. The proposed method degrades the accuracy (even in Table 1, the results do not really match). How it behaves compared to just using a smaller model with the overall same model size? Since the communication savings on SOP and Landmarks are small, the smaller model baseline should also have roughly the same performance. The paper is generally well written. However, the experimental results and technical novelty are less convincing. I would lean towards objection for now.",
            "This paper works on \"supervised\" representation learning in a federated learning setting. The main goal is to save the communication cost: by preventing sending the entire fully-connected layer between the server and the clients if the clients only have data from parts of the classes. The authors proposed federated sampled softmax, which is to compute the softmax only over the \"positive\" classes of which a client has data and a small portion of the other negative classes. The authors empirically show that, by doing so, even with a very small set of the negative classes (so small communication cost), the resulting feature network can achieve comparable performance to learning with the conventional softmax.  ===== Strength ======\n1. The paper points out an interesting fact and problem: most of the parameters in a neural net are in the last fully connected (FC) layer, while in a large-number-of-class and non-IID setting, each client may only have data from a small portion of classes. The authors thus accordingly proposed the federated sampled softmax to save the communication cost by local training with only a small number of \"negative\" classes in addition to the \"positive\" classes.\n\n2. The authors conduct experiments on three large-scale datasets. The authors conduct the ablation study on how many negative classes are needed. \n\n==== Weakness =====\n1. The technical contribution is not sufficient. There have been many methods like (Bengio & Sen\u00b4ecal,2008) and [a-c] that subsample the negative classes in the softmax objective. Specifically, in [c], the proposed method did include the positive classes in a minibatch, together with a set of subsampled classes, to compute the softmax. While the authors compare different combinations of positive and negative classes in 3.4, to me, it seems pretty obvious that we should combine the clients' positive classes with subsampled negative classes. Besides, while the authors discussed several advanced methods in 3.3, they ended up choosing uniform sampling. I was wondering if any of these advanced approaches can be approximately implemented in a federated setting to further improve the performance.\n\n[a] Joulin et al., Learning Visual Features from Large Weakly Supervised Data, ECCV 2016\n\n[b] Mikolov et al., Distributed representations of words and phrases and their compositionality, NeurIPS 2013\n\n[c] Hu et al., Learning answer embeddings for visual question answering, CVPR 2018\n\n2. One potential direction to improve the novelty or technical contributions of the paper is to develop a series of methods for different kinds of representation learning in a federated setting, including learning a fully connected layer and metric learning. If we do not consider the computational cost at the client end, will metric learning (without learning the FC layer) outperform the proposed method? In [a], the authors proposed to learn multiple one-vs-all classifiers rather than a softmax classifier. Will it be effective in a federated setting?\n\n3. The experimental setup can be improved. There is no clear description of how many classes, images, local epochs for each dataset. I also have concerns about using the pre-trained features for 4.2 and 4.3, which makes 4.2 and 4.3 like downstream tasks rather than representation learning. I would suggest that the authors include a fixed feature baseline in Table 1 and Table 2 to demonstrate that fine-tuning the feature extractor in a federated setting could outperform the pre-trained features. \n\n4. The authors only apply the proposed method to the FedAvg baseline. Will the proposed method be applicable/effective to more advanced federated learning algorithms like FedProx [d], Scaffold [e], and FedDyn [f]?\n\n[d] Li et al., Federated optimization in heterogeneous networks. In MLSys, 2020\n\n[e] Karimireddy et al., Scaffold: Stochastic controlled averaging for federated learning. In ICML, 2020\n\n[f] Acar et al., Federated learning based on dynamic regularization. In ICLR, 2021\n\n==== Other comments ====\n1. I would suggest that the authors add \"supervised\" into their title, to contrast to many recent works on \"unsupervised\" representation learning.\n\n2. I would suggest that the authors cite some more papers on federated learning. Overall, I enjoy reading the paper as it pointed out an interesting fact and problem. However, the proposed method seems to be too straightforward with insufficient novelty --- negative class subsampling has been widely used and including positive classes of the client seems to be quite intuitive. I have also listed several potential ways to strengthen the paper (please see the main review). I think the current version of the paper is below the acceptance bar, and thus I give a score of \"3\".",
            "This paper investigates the problem of training a good computer vision model in the cross-device federated setting, focusing on classification. For deep networks, when the number of classes gets large ($\\geq 10^3$), the number of parameters is dominated by the classification layers, hence scaling proportionally to the number of classes in the network. The authors propose to alleviate the resulting computation and communication burden by using sub-networks for each client, with a shared feature extractor but with only a smaller number of classes, re-using the sampled softmax proposed by Bengio & Sen\u00e9cal 2008. The resulting algorithm, Federated Sampled Softmax (fedSS), is benchmarked on image classification and image retrieval tasks along with baselines and variants. Experimental results demonstrate the validity of the approach. \n# Significance and impact\n- The problem studied by the authors has not received a lot of attention, save for the previous work cited by the authors, which was restricted to a narrower problem (1 class per client).\n- This paper brings a good practical solution for this problem, building on previous work, which makes it quite significant.\n\n# Writing and clarity\n\n- The paper is very well written and easy to follow, with helpful figures.\n- It could be helpful to provide in appendix a short derivation of the validity of the sampled softmax approximation.\n\n# Quality\n\nThe experiments are well conducted, with adapted baselines, and support well the claims of the paper. I have a few questions and suggestions:\n- in table 2, the proposed method reaches a better performance than fullSoftmax. How do the authors explain this result?\n- The paper mainly focuses on the choice of the classes to include in the sampled softmax, and bring a very good experimental support for the method. It would also have been interesting to study:\n  - different NN architectures (only mobilenet v3 is used)\n  - what is the effect of the proposed method when the number of local epochs increases? This number remains fixed to 1, leading to a very large number of rounds (2k or 5k);\n  - What is the effect of label class heterogeneity on the method? e.g. for the SOP dataset one could study splits with varying heterogeneity. This paper introduces a novel and simple approach to a practical cross-device problem. The experiments are well conducted and support the claims of the paper. I think this paper should be accepted. "
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states that their main concerns regarding novelty and contributions are not fully addressed, and they find the approach \"quite obvious and straightforward.\" They also criticize the lack of discussion on federated learning algorithms beyond FedAvg, leading to an unchanged score.",
            "The review expresses concerns about the lack of theoretical and experimental proof, the scalability of the method, and the absence of comparisons with relevant baselines. Phrases like \"no real theoretical or experimental proof,\" \"unknown if we can really scale up,\" \"experiments also lack the comparison,\" and \"it is not clear\" indicate a negative sentiment.",
            "The review identifies a significant privacy issue with the proposed algorithm and calls for more comprehensive experiments and comparison with other communication-efficient algorithms. The reviewer also uses phrases like \"most significant weak point\" and \"requires more comprehensive experiments,\" indicating a negative assessment.",
            "The reviewer expresses doubts about the real-life benefits of the proposed algorithm, questions its novelty, and points out accuracy degradation. The reviewer concludes with a leaning towards objection.",
            "The reviewer states that the technical contribution is not sufficient, the experimental setup can be improved, and the proposed method seems too straightforward with insufficient novelty. The reviewer also mentions that the current version of the paper is below the acceptance bar, giving it a score of '3'.",
            "The review expresses a positive opinion of the paper, highlighting its significance, clarity, quality of experiments, and concluding with a recommendation for acceptance. Phrases like \"good practical solution,\" \"very well written,\" \"experiments are well conducted,\" and \"support well the claims of the paper\" contribute to this positive sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Critical",
            "Supportive"
        ],
        "tone_reason": [
            "The tone is critical due to phrases like \"concerns are still not fully addressed,\" \"quite obvious and straightforward,\" and \"no further discussion is provided.\" The reviewer is directly pointing out perceived weaknesses in the paper's novelty and contribution.",
            "The review adopts a critical tone by directly questioning the author's arguments and highlighting deficiencies in the methodology and experimental setup. The reviewer uses phrases like \"there is no real theoretical or experimental proof to support that,\" \"the experiments also lack the comparison,\" and \"it is not clear if the proposed method can outperform\" which shows a critical assessment of the work. The reviewer also points out potential limitations and weaknesses in the approach, suggesting further investigation and comparisons are needed.",
            "The reviewer points out a major flaw (privacy issue) and uses strong language like \"strictly prohibited\" and \"very careful.\" The reviewer also uses phrases like \"should include more results\" and \"should discuss with other communication-efficient algorithms\" which indicates critical feedback.",
            "The review uses phrases like \"questionable if the proposed algorithm has real-life benefits,\" \"restricts the novelty,\" \"degrades the accuracy,\" and \"less convincing.\" These phrases indicate a critical assessment of the paper's contributions.",
            "The review uses critical language, pointing out weaknesses in the technical contribution, experimental setup, and novelty of the proposed method. Phrases like 'technical contribution is not sufficient,' 'experimental setup can be improved,' and 'insufficient novelty' indicate a critical tone.",
            "The review demonstrates a supportive tone by praising the paper's strengths, offering constructive suggestions for improvement, and ultimately advocating for its acceptance. Phrases like \"helpful figures,\" \"adapted baselines,\" and the concluding statement \"I think this paper should be accepted\" clearly indicate a supportive stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer acknowledges that some concerns were addressed by the authors in the rebuttal, but maintains that the main concerns regarding novelty and contributions are not fully addressed. The reviewer's decision to keep the score unchanged is consistent with their criticism about the lack of novelty and contribution. There are no contradictory statements in the review.",
            "The review is consistent because it raises concerns about the lack of empirical evidence and comparisons to support the authors' claims. Both points 1 and 2 consistently emphasize the need for more rigorous validation, including theoretical backing, experimental proof on larger datasets, comparison with baseline methods like gradient compression, and exploration of different model architectures. The reviewer's arguments are logically connected and focused on strengthening the paper's justification and experimental evaluation.",
            "The review presents both positive and negative points in separate sections (Pros and Cons) and the criticisms are logically connected and focused on specific areas for improvement without contradicting the positive points. The reviewer acknowledges the strengths while pointing out weaknesses related to privacy, communication cost analysis, and experimental validation, leading to a balanced and consistent assessment.",
            "The reviewer acknowledges the paper's strengths, such as being well-written and proposing a potential solution for communication cost reduction. However, the cons raised, including limited practical benefit due to performance issues on real-world large-scale datasets and questionable novelty, directly support the reviewer's overall negative assessment and leaning towards rejection. The cons outweigh the pros, leading to a consistent negative evaluation.",
            "The reviewer consistently points out that while the problem addressed in the paper is interesting and relevant, the proposed method lacks sufficient technical novelty and contribution. The weaknesses listed all support this central argument, focusing on the lack of novelty compared to existing methods and the need for more rigorous experimentation and exploration of advanced techniques. The final score of '3' (below acceptance bar) aligns with this consistent critique of the technical contribution.",
            "The review is consistently positive, highlighting the significance of the problem, the quality of the proposed solution, the clarity of writing, and the well-conducted experiments. The reviewer's questions and suggestions are constructive and aimed at further improvement, not criticisms of the paper's validity or consistency."
        ]
    },
    {
        "paper_id": "nips_2021_amH9JxZN7C",
        "paper_title": "PreferenceNet: Encoding Human Preferences in Auction Design with Deep Learning",
        "paper_abstract": "The design of optimal auctions is a problem of interest in economics, game theory and computer science. Despite decades of effort, strategyproof, revenue-maximizing auction designs are still not known outside of restricted settings. However, recent methods using deep learning have shown some success in approximating optimal auctions, recovering several known solutions and outperforming strong baselines when optimal auctions are not known. In addition to maximizing revenue, auction mechanisms may also seek to encourage socially desirable constraints such as allocation fairness or diversity. However, these philosophical notions neither have standardization nor do they have widely accepted formal definitions. In this paper, we propose PreferenceNet, an extension of existing neural-network-based auction mechanisms to encode constraints using (potentially human-provided) exemplars of desirable allocations. In addition, we introduce a new metric to evaluate an auction allocations' adherence to such socially desirable constraints and demonstrate that our proposed method is competitive with current state-of-the-art neural-network based auction designs. We validate our approach through human subject research and show that we are able to effectively capture real human preferences.\n",
        "review_ids": [
            "UVCWaST-yt",
            "T5oENu6w98n",
            "IeQ0zFjOh1D",
            "oAPMrPAy9r0"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper studies of designing revenue optimizing auctions subject to the an additional constraint on fairness. Instead of algebraically capturing the fairness of an allocation, the authors train a neural network termed PreferenceNet that assign a fairness score to any allocation. The neural network for strategy-proof, revenue maximizing auction (RegretNet) is trained to trade-off revenue and regret with the preference loss.  The author evaluate this on small instances (2-4 bidders and 2-4 items) and contrast revenue, regret and fairness of simple RegretNet with one constrained to produce fair allocations. \n\nOne primary motivation for a deep neural network based PreferenceNet is that it can capture real world preferences from humans. To explore this aspect, the authors conducted real world studies to get user preference samples and trained a PreferenceNet on that. They also correlate preference learned from humans with mathematical models of preference.   The key idea in this paper is to train a deep neural network for scoring an allocation for fairness. This network is used to score allocations produced by RegretNet which is the deep neural network for computing revenue optimizing incentive compatible allocations. The authors provide empirical evaluation of their idea on small instances. \n\nThe paper is closely related to the paper by Kuo et al. ProportionNet: Balancing Fairness and Revenue for Auction Design with Deep Learning. The key distinction is that paper uses only TVF and the allocation is scored using the mathematical definition, while in this work the authors approximate it using a neural network. \n\nI think this approach is interesting. The authors should explain their approach more fully and also evaluate it on a diverse set of inputs. In particular, fairness can become more restrictive if the advertisers preference for the different goods are very different. So it will be interesting to explore valuations that not identically distributed or with a larger spread of values such as exponential distributions.  \n\nAnother aspect that is worth evaluating is looking into what type of auctions are designed - how is the optimal auction different with the additional fairness constraint. \n\nGiven these concerns, I believe the authors could strengthen paper a lot with a more thorough evaluation of their model. \n\nI also have a minor clarification question. The learning process describes re-training preference net about a few rounds of training regret-net. Is that relevant even for a preference-net trained for real world data. How does that work - does it require new data gathering or some sort of heuristic based approach, what if at all would be the impact of getting that part wrong.\n\nMinor:\n- Many of the references cite the authors as author A et al. It is preferred if the authors could include the full list of authors. \n- the definition of TVF is not clear. What's c, C_k, d^k(j, j')?\n- please clarify that the experiment evaluation was done on bids drawn IID from U[0, 1].\n\nUpdate based on author's response:\nAuthors provided a lot more evaluation of their approach with non-identical distributions, larger instance sizes. The authors should include this new data in the paper as it provides a more thorough evaluation of the approach. For example, we do see larger gaps emerge in the metrics with larger instances.  I had a some more questions about the results. \n1) In some cases RegretNet PCA is lower than PreferenceNet, is this expected. Shouldn't RegretNet perform better?\n2) Can we still say that the two approaches are similar if the PCA is very far from each other or revenue is at times more than 5-6% different?\n Authors do discuss limitations of their approach - in terms of human preference data gathering and model training. Particular for human preference data gathering - authors discussed the noise introduced and how they handled that. The authors also discuss the limitations of the PreferenceNet model and the situations in which it fails. One aspect lacking is looking deeper into the model trained and what it teaches about optimal auction design - this aspect is what makes the RegretNet approach interesting. Alternately the authors could discuss how this approach could scale to instances of arbitrary size. ",
            "This paper covers a somewhat interesting and novel problem: learning constraints of an auction setting, and then learning to run an auction that respects those. Fairness in allocations is cited as a justification for this in practice, where it may be beneficial to learn from a human\u2019s choices to determine how to decide who should win in the auction, for example to determine similar allocations across demographics.\n\nThe approach builds on the RegretNet line of work that learns to run revenue maximizing auctions. The approach broadly is to learn a feasibility constraint that is scored by human subjects, and then use that in the RegretNet architecture to learn to generate auctions that respect that constraint. In this work, the motivating constraint is a binary classification of fairness. The implementation is a composition of learning then constraint, followed by leveraging RegretNet to learn according to that function. \n\nTo support the approach, the authors first compare PreferenceNet to RegretNet trained with the exact constraints, and see similar performance suggesting some amount of usefulness. The authors then train PreferenceNet on responses from human subjects in two surveys regarding the binary qualitative fairness of allocations. \n  The paper delivers on a novel use for machine learning in auctions. The significance of the approach remains a little open to interpretation: I don\u2019t know that fairness is the constraint that makes the most sense for this approach, but there is no reason for the approach to be limited to fairness as the measure used - in such cases is it better to have the biases explicit or implicit as a result of human responses? In some ways it seems that the decision of what is fair is moved from the explicit constraint to the survey question, either explicitly here or implicitly in any use of this approach.\n\nThe paper is generally well written and comprehensive in exploring the performance of PreferenceNet in the fairness area, both comparing to a baseline RegretNet with perfect knowledge of the constraint and then examples trained based on human input. \n\nEvaluating the architecture choices is outside of my area of expertise. \n\nSmall notes:\nFig 1: doesn't seem to be particularly helpful as a comparison, or I am missing the intuition that would give this a half page here.\nLine 193: eval[u]ate The work has one primary potential for negative societal impact: human responses should not be expected to be unbiased. The authors do address this, though perhaps a stronger statement regarding where such approaches would be deemed to be appropriate would be helpful. ",
            "This paper proposes PreferenceNet, which is a new deep learning approach for designing revenue-maximizing auctions that encodes human preferences. PreferenceNet extends RegretNet [Duetting et al. 2019] with modified loss functions and additional constraints.   Overall I find the paper well-written and clear to follow.  The PreferenceNet proposed by the authors extends from RegretNet, with additional MLP layers that are trained to capture implicit human preferences, modified loss functions and constraints. The authors also conducted  two surveys to further study human preferences for auction outcomes, and I find the conduction of the surveys useful and important, as they provide actual feedbacks from human participants. Below are a few questions and concerns I have.\n\nFirst is about the scalability and flexibility of PreferenceNet. In the current formulation, the ground truth labeling function based on the underlying preference ( s(b) ) is constrained to be a binary label, and the input is a whole vector of allocations for the given bid. However, in practice the decision (\"good\" or \"bad\") could be hard to provided by human, especially when the number of items is large. For example, human could be bad at deciding if two vectors of allocations are actually similar or not. This can make the preference feedbacks obtained very noisy. I would also be curious to see more details on experimentally how much additional computation PreferenceNet requires compared to the RegretNet.\n\nSecond is, the deep learning based approach estimates violations of the strategyproofness and fairness constraints using a gradient-based method which is effective during training, but may not accurately reflect the true extent to which strategyproofness or fairness constraints are violated. Therefore I find the design of the loss functions and constraints for PreferenceNet intuitive, but it is unclear theoretically how these affect the generalization bound for strategyproofness. Besides, will the fairness constraints render the optimal auction infeasible in some cases? Is it possible that each agent has incompatible preferences, and PreferenceNet cannot satisfy the overall preferences for all agents? I would be interested in seeing more elaboration on this side.\n\ntypos:\nEq(3) c, C_k undefined\nEq(4) numerator missing index The authors have addressed the limitations and potential negative societal impact of this work. ",
            "This paper describes how we can encode socially desirable constraints in auction mechanisms learned using the regretNet framework.   The paper has two main contributions:  \n(1) A metric that quantifies how well these mechanisms adhere to these constraints.  \n(2) An neural network and a training procedure (called the preferenceNet) that can encode these constraints using exemplars of the desired allocation\n\nThe authors demonstrate the efficacy of the proposed approach by showing that it can match the performance of standard approaches on both synthetic preferences as well as human preferences  Pros:\n- Unlike existing/related work, they attempt to capture socially desirable properties through data (rather than formally defining them).  \n- Results are quite promising - they either match or beat standard approaches (despite only being trained using weaker and noisy signals) \n- Significance: This paper studies an important problem at the intersection of mechanism design and human in the loop learning\n- Clarity: The paper is well written and easy to understand\n\n\nComments:\n- My main concern is the inability of preferenceNet to learn the preference function exactly (which happens to play a key role in the training pipeline)\n- It is unclear how $\\alpha$,  $\\beta$ and $\\gamma$ were chosen.\n- Is there an intuition behind why some constraints are harder than others?\n\nMinor Comments:   \n- typo on line 193: evalate-> evaluate\n- The augmented Lagrangian method is usually the sum of squares of the penalty term (rather than the square of the sum of penalty terms)\n $$\\frac{\\rho_r}{2}\\left(\\sum_{i} rgt_i(w)\\right)^2 => \\frac{\\rho_r}{2} \\sum_{i} \\left(rgt_i(w)\\right)^2$$\n Yes. (For limitations see above)"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Neutral",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states \"I think this approach is interesting\" and concludes that the authors could strengthen the paper with a more thorough evaluation.",
            "The review expresses overall positive sentiment, highlighting the paper's novelty and delivery on a novel use for machine learning in auctions. It also acknowledges the paper's comprehensive exploration of PreferenceNet's performance.",
            "The review contains both positive feedback (well-written, clear, useful surveys) and concerns (scalability, generalization, feasibility of constraints). It doesn't lean strongly positive or negative.",
            "The review expresses overall positive feedback, highlighting the paper's contributions, promising results, significance, and clarity. Phrases like 'quite promising,' 'match or beat standard approaches,' and 'well written and easy to understand' indicate a positive sentiment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The review provides both positive feedback (e.g., \"I think this approach is interesting\") and constructive criticism (e.g., concerns about the evaluation and clarification questions). It acknowledges the paper's strengths while suggesting improvements.",
            "The review adopts a balanced tone, acknowledging the paper's strengths (novelty, comprehensive exploration) while also raising concerns about the significance of the approach and potential societal impact (bias in human responses). It offers constructive criticism and suggestions for improvement.",
            "The review starts with positive remarks but then transitions to specific questions and concerns. It maintains a professional and critical but not overly negative tone, using phrases like 'I would be curious to see' and 'I would be interested in seeing'. The reviewer also mentions that the authors have addressed limitations and potential negative societal impact, showing a balanced view.",
            "The tone is supportive, as evidenced by the use of positive language and constructive criticism. The reviewer lists 'Pros' and provides specific suggestions for improvement in the 'Comments' section."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent. It starts with a summary of the paper, then points out the strengths and weaknesses of the paper. The reviewer suggests concrete improvements, mainly focusing on the need for more thorough evaluation and explanation. The update section shows that the reviewer acknowledges the authors' efforts to address the concerns and still provides further questions and suggestions for improvement, maintaining a consistent line of feedback.",
            "The review is consistent in its assessment. It acknowledges the novelty and interesting problem addressed by the paper, appreciates the approach building on RegretNet, and recognizes the comprehensive evaluation. While raising questions about the specific choice of fairness and potential biases from human input, these are presented as points for consideration and further discussion rather than contradictions to the overall positive evaluation of the paper's contribution and methodology.",
            "The review is consistent as it provides an overall positive assessment initially, followed by specific concerns and questions regarding the proposed method's scalability, theoretical guarantees, and potential limitations. The reviewer appreciates the work but also points out areas for improvement and clarification, maintaining a constructive and balanced tone throughout without contradictions.",
            "The review is consistent as it presents both positive aspects of the paper (Pros) and areas for improvement or questions (Comments and Minor Comments). It acknowledges the strengths of the paper while also raising valid concerns and suggestions, which is a standard and consistent approach in peer reviews. There are no contradictory statements."
        ]
    },
    {
        "paper_id": "iclr_2020_rJlnOhVYPS",
        "paper_title": "Mutual Mean-Teaching: Pseudo Label Refinery for Unsupervised Domain Adaptation on Person Re-identification",
        "paper_abstract": "Person re-identification (re-ID) aims at identifying the same persons' images across different cameras. However, domain diversities between different datasets pose an evident challenge for adapting the re-ID model trained on one dataset to another one. State-of-the-art unsupervised domain adaptation methods for person re-ID transferred the learned knowledge from the source domain by optimizing with pseudo labels created by clustering algorithms on the target domain. Although they achieved state-of-the-art performances, the inevitable label noise caused by the clustering procedure was ignored. Such noisy pseudo labels substantially hinders the model's capability on further improving feature representations on the target domain. In order to mitigate the effects of noisy pseudo labels, we propose to softly refine the pseudo labels in the target domain by proposing an unsupervised framework, Mutual Mean-Teaching (MMT), to learn better features from the target domain via off-line refined hard pseudo labels and on-line refined soft pseudo labels in an alternative training manner.  In addition, the common practice is to adopt both the classification loss and the triplet loss jointly for achieving optimal performances in person re-ID models. However, conventional triplet loss cannot work with softly refined labels. To solve this problem, a novel soft softmax-triplet loss is proposed to support learning with soft pseudo triplet labels for achieving the optimal domain adaptation performance. The proposed MMT framework achieves considerable improvements of 14.4%, 18.2%, 13.1% and 16.4% mAP on Market-to-Duke, Duke-to-Market, Market-to-MSMT and Duke-to-MSMT unsupervised domain adaptation tasks.",
        "review_ids": [
            "Hkg7fUDxFS",
            "rkef5M0RtH",
            "BJeCW-u0tS"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The authors' response addressed my concerns. After reading the reviews and the comments, I choose to stand with the other reviewers.\n\n===================\n\nThis paper uses mean-teacher to ease the noisy pseudo label of clustering methods for domain adaptive Person re-identification task. The authors also propose a variant of triplet loss for soft labels. Experiments show they achieve considerable improvement over state-of-the-art methods.\n\nQuestions:\n1. What's the difference between net 1 and net 2 in Fig. (2)? It seems they are redundant.\n\n2. The results in Table (1) seems to indicate that, in MSMT, if M_t is set to be near the actual identity numbers (1041), the performance will be much better. This makes me suspect that the proposed method benefits from ground truth information of the target domain, which makes the comparison unfair.\n",
            "This paper proposes an unsupervised domain adaptation method for person re-identification. The proposed method handles noises on pseudo labels created by unsupervised clustering. Two networks are used for training, and the estimated confidences of other models are used for the next training iterations. The temporally average model is used for each network to avoid error amplification. Also, soft softmax-triplet loss is proposed to handle soft labels for triplet loss. \n\nThe handling label noises in unsupervised domain adaptation on person re-identification are new. The proposed model produces very high performance and the contribution for person re-identification community is good. \n\nHowever, I would like to see more insights into the proposed model for the contribution of the general deep learning conference. \n\nFirst, this paper lacks a survey of works on handling label noises. For example, \nB.Han, Q.Yao, X.Yu, G.Niu, M.Xu, W.Hu, I.Tsang, M.Sugiyama, Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels, NeurIPS2018. \n\nI could not fully understand why the temporal averaging of model parameters prevents the two models from being the same. I would like to see a theoretical explanation or experimental evidence for this claim. \n\nThe proposed method also uses noisy hard pseudo labels for training, as shown in Eq.(9). \nWhy are the noisy hard labels used? What is the performance when only soft labels are used for model updates?\n\nIn the experiment, \\lambda^t_{id} = 0.5, \\lambda^t_{tri} = 0.8 are used. Why these parameters are different between softmax and triplet losses?\n\np.1 (Zhang et al., 2018b) and p.5 (Zhang et al., 2019a) are missing in references. \n\n\n",
            "After reading the reviews and the comments, I confirm my rating.\n\n=================\n\nThe paper proposes an unsupervised framework to address the problem of noisy pseudo labels in clustering-based unsupervised domain adaptation (UDA) for person re-identification. The noise derives from the limited transferability of source-domain features, the unknown number of target-domain identities, and the imperfect results of the clustering algorithm. \n\nThe proposed framework, Mutual Mean-Teaching (MMT), performs pseudo label re\ufb01nery by optimizing the neural networks under the joint supervisions of off-line re\ufb01ned hard pseudo labels and on-line re\ufb01ned soft pseudo labels. Inspired by the teacher-student approaches (Reference: Tarvainen & Valpola, 2017; Reference: Zhang et al., 2018b), the proposed MMT framework provides robust soft pseudo labels in an on-line peer-teaching manner to simultaneously train two same networks. The networks gradually capture target-domain data distributions and thus re\ufb01ne pseudo labels for better feature learning.\n\nThe main contribution is proposing an unsupervised framework (MMT) capable of tackling the noise problem in state-of-art UDA methods for person re-identification, via producing reliable soft labels in order to achieve better performance. Since the conventional triplet loss cannot properly work with soft labels, a softmax-triplet loss is proposed to enable training with soft triplet labels for mitigating the pseudo label noise.\n\nThe proposed MMT is evaluated on Market1501, DukeMTMC-reID, and MSMT17 datasets with four adaptation tasks: Market-to-Duke, Duke-to-Market, Market-to-MSMT, and Duke-to-MSMT. It outperforms the state-of-the-art methods with significant improvements in terms of Mean average precision (mAP) and Cumulative matching characteristic (CMC). In addition to that, ablation studies conducted to evaluate each component in the proposed MMT framework."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The review acknowledges the authors' response and aligns with other reviewers. It also presents both positive aspects (considerable improvement) and critical questions regarding the methodology and fairness.",
            "The reviewer acknowledges the novelty of the approach, high performance, and good contribution to the person re-identification community. However, the reviewer also suggests improvements, indicating a generally positive but critical stance.",
            "The review highlights the paper's contributions, such as proposing an unsupervised framework (MMT) that tackles the noise problem in UDA methods and outperforms state-of-the-art methods with significant improvements. The reviewer also notes the inclusion of ablation studies, indicating a thorough evaluation."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Supportive"
        ],
        "tone_reason": [
            "The tone is balanced, acknowledging the paper's strengths ('considerable improvement') while also raising specific questions and concerns ('redundant', 'suspect', 'unfair'). The language is professional and objective.",
            "The review presents both positive aspects (novelty, performance, contribution) and areas for improvement (lack of related work, unclear explanation of temporal averaging, justification for noisy hard labels, parameter selection, missing references). This balanced approach characterizes the tone as balanced.",
            "The reviewer uses positive language, such as 'main contribution,' 'reliable soft labels,' 'better performance,' and 'significant improvements,' which indicates a supportive tone towards the paper."
        ],
        "consistency": [
            "No",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review starts with a positive statement but then raises critical questions that undermine the initial positivity, leading to an inconsistent overall message.",
            "The review is consistent because it acknowledges the paper's strengths, such as novelty and good performance in unsupervised domain adaptation for person re-identification, while also pointing out specific weaknesses and areas for improvement. The reviewer raises valid questions and suggestions for enhancing the paper's contribution, such as including a survey of related works, providing theoretical or experimental justification for design choices, and clarifying experimental details. The critique is constructive and focused on improving the paper without contradicting the initial positive assessment of its potential.",
            "The review is a consistent summary of the paper's content and does not contain any contradictory statements or opinions. It accurately describes the paper's contributions, methodology, and evaluation without presenting any conflicting information."
        ]
    },
    {
        "paper_id": "iclr_2021_TYXs_y84xRj",
        "paper_title": "PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection",
        "paper_abstract": "A variety of anchor-free object detectors have been actively proposed as possible alternatives to the mainstream anchor-based detectors that often rely on complicated design of anchor boxes. Despite achieving promising performance on par with anchor-based detectors, the existing anchor-free detectors such as FCOS or CenterNet predict objects based on standard Cartesian coordinates, which often yield poor quality keypoints. Further, the feature representation is also scale-sensitive. In this paper, we propose a new anchor-free keypoint based detector ``PolarNet\", where keypoints are represented as a set of Polar coordinates instead of Cartesian coordinates. The ``PolarNet\" detector learns offsets pointing to the corners of objects in order to learn high quality keypoints. Additionally, PolarNet uses features of corner points to localize objects, making the localization scale-insensitive. Finally in our experiments, we show that PolarNet, an anchor-free detector, outperforms the existing anchor-free detectors, and it is able to achieve highly competitive result on COCO test-dev benchmark (47.8% and 50.3% AP under the single-model single-scale and multi-scale testing) which is on par with the state-of-the-art two-stage anchor-based object detectors. The code and the models are available at https://github.com/XiongweiWu/PolarNetV1",
        "review_ids": [
            "FZ70y38XOH_",
            "V4JzA1A6vB",
            "yrRj5NFcelM",
            "ilc7fO0jFSx"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "# Summary\nIn this paper, the authors propose a simple but effective keypoint-based anchor-free object detection system. The main idea is to replace the Cartesian coordinate with the polar coordinate, compared to the closest related work, FCOS. According to the extensive empirical results, the proposed system achieves a better trade-off between speed and accuracy. \n\nThe overall writing looks good to me. The storyline is consistent and well-motivated. The authors provide enough detail to shed light on the design choices for the state-of-the-art anchor-free detector. The figures and tables are also quite informative. For example, I particularly love the figure 1 because it helps the readers catch up with the most recent progress on keypoint-based object detection frontier. It could be better if the authors could describe more details in the caption. By the way, the black color of rho and theta should be changed into a lighter one.\n\n# Questions\n\n1. scale-sensitive vs. scale-invariant\nIn this paper, the author mentioned the scale-related terminology many times. I wonder if the authors could explain more detail about why PolarNet is scale-invariant? From my perspective, the offset regression is scale-sensitive because the target numbers are strongly related to the actual object size. Even though the proposed method utilizes the corner points to localize objects, the bounding box offset part is still scale-sensitive, right?\n\n2. center-based method + polar coordinate\nI wonder if the authors could try to put the polar coordinate offset regression into a center-based anchor-free detector. For example,  CenterNet[1] regresses the bounding box offset in the Cartesian coordinate system. What if we change the distance encoding to polar coordinate? In section 4.4, the authors claim, \"Speci\ufb01cally, compared with other center-based methods such as CenterNet or FSFA, our method not only extracts features from the central region, but also encodes features from the whole bounding boxes.\" I wonder how much performance gain could be seen if we only change the coordinate system. It could be a helpful ablation study to support this claim.\n\n# Reference\n[1] Zhou, Xingyi, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. \"Objects as points.\" arXiv preprint arXiv:1904.07850 (2019).\n\n---- Post-rebuttal comments----\n\nThe rebuttal and the paper revision address my concerns. I keep my original rating.",
            "##########################################################################\n\nSummary:\n\nThis paper proposes an anchor-free object detector that does bounding box regression in the polar coordinate instead of in the Cartesian coordinate. The motivation of doing this is because there are larger variance in offset vectors in the Cartesian coordinate (the extreme case when a point is on one of the four corners of the bounding box, the offset vector becomes [0, w, 0, h]). The authors propose a solution to regress to the pair of corners (either TL+BR or TR+BL) in the polar coordinate, and select the corner pair that gives the smallest variance during training.\n\n##########################################################################\n\nPros: \n\nExperiment results show using polar coordinate is effective.\n\n##########################################################################\n\nCons:\n\n1. The authors did some analysis on the variance of the offset vector in Section 3.1, however, I think the analysis is not enough. First, the analysis only contains the worst case analysis, that is, the range of offset targets. And the author directly concluded from this: PolarNet \"significantly reduces the variance\" (page 4 last line). What is the ratio of variance under cartesian and polar coordinate to make it \"significant\"? I do not see any number either theoretically proves it or empirical analysis of the variance during training.\n\n2. The term \"keypoint\" used in this paper is confusing. Sometimes the \"keypoint\" refers to corner points of the box (\"Keypoint Position\") and other times the \"keypoint\" simply refers to any point within the bounding box (\"Keypoint Offsets\").\n\n3. The introduction of PolarNet is not clearly presented, specifically there are several confusing points:\n- Section 3.4.1, what is the usage of t_{x,y}? I don't see how t is used during inference.\n- Section 3.5, \"we select the optimal box from b_{x,y} as the final output of the predicted box\", what is \"optimal box\"?\n- Figure 3, why the \"corner supervision\" comes from the feature map? I don't see how corner supervision uses any feature.\n- The proposed corner supervision is simply the L1 loss, and there are methods that already use it with IoU loss. I don't think it is a contribution and section 3.4.2 and 3.4.3 should be combined.\n\n4. Experiments are not solid:\n- There are ways to reduce variance of offsets under cartesian coordinate, e.g. only use points within the center region of the bounding box to learn offset. Such experiment should be compared.\n- The importance of extra loss function is also not studied, what are the benefit of using more losses?\n- From reading Section 3, I feel the method is exactly as applying FCOS + polar coordinate, but Table 2 shows there is still some gap. Where does the extra gain come from?\n- I checked the FCOS paper and found the R101 results in the paper is 43.2 but the number in this paper is 41.5.\n\n##########################################################################\n\nReasons for score: \n\nOverall, I vote for rejecting. This paper proposed an interesting idea, but I think way it is presented is not good enough to be accepted. Specifically, I think the paper still misses analysis on the variance of offset prediction, and also misses some important ablation studies. Furthermore, the paper is not well-written and requires some revision.",
            "This paper proposes a new key-point based object detector, PolarNet, which predicts the distances between key-points and corner pairs (such as top-left and bottom-right pair or top-right and bottom-left pair) on polar coordinates. This is different from other key-point based object detectors such as FCOS which predicts distances between key-points and bounding box boundaries on cartesian coordinates.  The authors claim that the advantage of representing the offsets in the form of polar coordinates is this representation reduces the variance in the offsets, which makes learning easier.\n\nPros:\nThis is an interesting approach and new, to the best of my knowledge, in the context of object detection. The authors show that PolarNet outperforms other approaches such as FCOS and FoveaBox under the same backbone network, ResNet-101. The use of polar coordinates improves the performance of FCOS by more than 4% which shows the effectiveness of the polar coordinates. With a larger backbone and deformable convolution, PolarNet demonstrates state-of-the-art performance among all anchor-free detectors on the challenging COCO dataset.\n\nCons:\nI am confused about the corner supervision in section 3.4.2. It seems to me that the corner supervision is to train PolarNet to predict the offsets which are in polar coordinates. But the authors also apply this to FCOS (FCOS + Centerness + Corner) and compare it with FCOS with polar coordinates (FCOS + Polar).  I don\u2019t understand the difference between them. How do the authors apply corner supervision if FCOS is predicting on cartesian coordinates (i.e. FCOS + Centerness + Corner)? How is that different from FCOS + Polar exactly? Or do I misunderstand the meaning of corner supervision? The authors also mention in the section where they introduce corner supervision that they train a regressor based on corner features. What does corner supervision mean exactly? Does it mean the authors extract features from the corners and use the features together with the key-point features when they predict the offsets? Or do they simply refer to the regression loss function?\n\nThe use of IoU loss seems to a bit redundant. It seems that the $L_{corner}$ already trains the network to predict the offsets. Why do the authors still need the IoU loss? The authors should provide an ablation study to demonstrate how the IoU loss is affecting the performance of PolarNet.\n",
            "Summary:\n\nIn this paper, the author proposes a new anchor-free keypoint based detector, which learns keypoints based on polar coordinates. It can avoid the large variance of learned offsets compared to the existing anchor-free detectors and make bounding box prediction scale-invariant.\n\nReasons for score:\n\nOverall, I vote for accepting. My major concern is about the clarity of the paper and some additional analysis (see cons below). Hopefully the authors can address my concern in the rebuttal period.\n\nPros:\n\n1.This paper proposes a new keypoint based object detector, which represents keypoints based on polar coordinates. It can avoid poor quality keypoints suffered by the existing keypoint based detectors and make regression scale-invariant.\n\n2.Experiments are well thought out and highlight the key advantages of the method over other keypoint based detectors. \n\nCons:\n\n1.In the Section 3.4.2, why use tangent function for angle regression? For tan, its derivative is $\\sec^{2}$. When $|\\theta_{br}-\\theta_{br}^{*}| \\to \\frac{\\pi}{2}$, the gradient is very large. Will this condition happen? In addition, are there some constraints or operations on $\\theta$ to make $\\theta \\in (0, \\frac{\\pi}{2})$?\n\n2.For the training, IoU threshold is adaptively change. Why use this trick? It may lead to unfair comparison with other methods. Could you provide some experiments with fixed IoU threshold?\n\n3.IoU loss is kept in the training. Why keep it? Could you provide some experiments without it to demonstrate its benefits?\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above\n\nSome typos:\n\n(1)3.1: $\\theta \\in \\begin{Bmatrix} 0, & \\frac{\\pi}{2}  \\end{Bmatrix}  \\to \\theta \\in (0, \\frac{\\pi}{2})$\n\n(2)4.1 5th line: four types -> three types "
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer states that the 'overall writing looks good' and that the 'storyline is consistent and well-motivated.' They also express appreciation for the figures and tables, specifically mentioning liking Figure 1. The reviewer concludes by stating that their concerns were addressed and they keep their original rating.",
            "The reviewer recommends rejection, citing insufficient analysis, unclear presentation, and experiments that lack rigor. The reviewer uses strong negative language such as \"not enough\", \"confusing\", \"not clearly presented\", \"not solid\", and \"not well-written\".",
            "The review acknowledges the novelty and potential of the approach ('interesting approach and new'), highlights the performance improvements over existing methods ('outperforms other approaches', 'effectiveness of the polar coordinates'), and recognizes the state-of-the-art performance on a challenging dataset. Despite the concerns raised, the overall tone suggests a positive evaluation of the paper's contribution.",
            "The reviewer votes for accepting the paper and expresses positive aspects of the work, indicating an overall positive sentiment despite pointing out areas for improvement."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Balanced",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer uses positive language such as 'looks good,' 'well-motivated,' 'quite informative,' and 'I particularly love.' They also offer constructive suggestions for improvement rather than outright criticism.",
            "The review is predominantly critical, pointing out several flaws in the paper. Specific criticisms include: lack of variance analysis, confusing terminology, unclear presentation, and experimental shortcomings. The reviewer questions the significance of the contributions and suggests improvements, indicating a critical assessment of the work.",
            "The review presents both positive aspects ('Pros') and negative aspects ('Cons') of the paper. It acknowledges the strengths of the approach while also raising specific concerns and questions about the methodology and experimental setup. The reviewer uses direct questions and critical analysis, but also provides positive feedback, indicating a balanced perspective.",
            "The review presents both positive aspects ('Pros') and negative aspects ('Cons') of the paper. It offers constructive criticism and suggestions for improvement, indicating a balanced perspective."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistently positive and constructive. The reviewer starts with a positive summary, highlighting the good writing and motivation. The questions are aimed at clarifying specific points and suggesting further experiments to strengthen the paper, rather than pointing out fundamental flaws. The post-rebuttal comment confirms that the reviewer's concerns were addressed, maintaining a consistent positive stance throughout the review process.",
            "The review is consistent in its criticism of the paper. While acknowledging a potentially interesting idea and some experimental effectiveness, the reviewer raises several significant concerns in the 'Cons' section regarding insufficient analysis, unclear methodology, confusing terminology, and weak experimental validation. These concerns directly lead to the rejection recommendation in 'Reasons for score', indicating a consistent negative assessment of the paper's quality and presentation.",
            "The review is consistent because the reviewer acknowledges the strengths of the paper (novelty, performance) while raising valid questions and seeking clarification on specific aspects of the methodology (corner supervision, IoU loss). The concerns are presented as points needing further explanation or justification, not as contradictions to the positive aspects.",
            "The review is consistent because the reviewer expresses a positive overall assessment (accept vote, pros) while also providing constructive criticism and questions (cons) to improve the paper. There are no contradictory statements or conflicting viewpoints within the review."
        ]
    },
    {
        "paper_id": "nips_2022_rG7HZZtIc-",
        "paper_title": "D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video",
        "paper_abstract": "Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects. Project page: https://d2nerf.github.io/",
        "review_ids": [
            "Laa0JY-LcQq",
            "4E2lwN9l0Yp",
            "iMrpU4ZP436",
            "ZHmwrHDIg9p",
            "-Hp1TOVqbsb"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " The authors well addressed my concerns. Actually, I really appreciate this paper although the technique is relatively simple.",
            " This paper proposes a method to segment and decouple dynamic objects while recovering the static environment from a monocular video. It adapts from NeRF and its extension Hyper NeRF but with improved handling of shadow regions as well as a loss to promote correct separation of dynamic and static regions. It demonstrates plausible motion segmentation and shadow removal result compared to recent NeRF based methods.\n\n This paper is a solid development for using NeRF to reconstruct from monocular videos with dynamic foregrounds. The paper in general is well-written and the claims are well supported. The adaptation it made to handle shadow and foreground & background separation is elegant and seems to be effective. It definitely holds value to people working on similar problems and deserves publication.\n\nThere are a couple of things I hope the authors could comment on.\n\n(1) In Ln. 226, \u201cTo demonstrate the ability of fully self-supervised scene decoupling, we do not apply any masks when registering real-world images using COLMAP\u201d. In my experience, without feeding masks, COLMAP tends to make wrong estimation of camera poses when the foreground is sufficiently large, which will definitely results in wrong reconstruction of the static background. It seems the proposed do not attempt to update the camera pose during optimization, so the claim above looks confusing to me. \n\n(2) It would be nice if the author also visualizes the depth map of the reconstructed foreground / background.\n\n(3) The results of hyperNeRF looks far worse compared to the proposed method even in the dynamic region. Given that the proposed method seems to have little difference to hyperNeRF at least in the dynamic regions, the current result surprises me a bit. Could the author make additional comments on the possible reasons?\n\n\n see the questions in \"strength and weakness\" The method cannot handle high frequency view-dependent radiance change due to the monocular moving camera setting.",
            " This paper presents D^2NeRF, a self-supervised method that takes a monocular video and learns a 3D scene representation that decouples moving objects, including their shadows, from the static background. In addition, this paper proposes a novel loss to promote the correct separation of phenomena for static and dynamic filed. The authors further propose a shadow field network to detect and decouple dynamically moving shadows. A new dataset was proposed, containing various dynamic objects and shadows. Extensive experiments demonstrate that the proposed method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects. Strength:\n\n* New dataset for static and dynamic field decomposition.\n* Self-supervised method that takes a monocular video and learns a 3D scene representation that decouples moving objects, including their shadows, from the static background. \n* Novel loss to promote the correct separation of phenomena for static and dynamic filed. \n* A shadow field network to detect and decouple dynamically moving shadows\n* SoTA results in several tasks such as decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects.\n\nWeakness:\n* require the accurate camera, suffer from high-frequency view-dependent radiance change, which has been discussed.\n* do not compare with other motion decoupling methods, such as STNeRF[18], NSFF[24] and DynNeRF[11], SIMONe[19], STaR[63], although their experiment setting may be inconsistent with the proposed methods. However, I think in the synthetic dataset, all of them can be reproduced, please add some of their results for a complete comparison.\n* When I refer to the supplementary video, there are still some wrong static and dynamic field decompositions, such as the keyboard. And I think this is a tradeoff. \n* It seems to lack generalizability, just train and test in the same dataset. please refer to the weakness. Add more comparison results with exiting methods.\n* do not compare with other motion decoupling methods, such as STNeRF[18], NSFF[24] and DynNeRF[11], SIMONe[19], STaR[63], although their experiment setting may be inconsistent with the proposed methods. However, I think in the synthetic dataset, all of them can be reproduced, please add some of their results for a complete comparison.\n* It seems to lack generalizability, just train and test in the same dataset. The authors have discussed the limitation, and all of them are a considerable challenge. I have no idea to solve them, and I think it's a trade-off.",
            " This paper proposes a new self-supervised approach for segmenting moving objects from the static background. It tackles with issues encountered during training with several techniques, such as skewed entropy, ray regularization, static regularization, and integrating shadow ratio to separate shadows. ## Strengths\n\n1. Originality: the proposed techniques are insightful and are beneficial to the community, such as skewed entropy and ray regularization.\n2. Quality: The presented qualitative results are of high quality.\n3. Clarity: the manuscript is well-written.\n4. Significance: the task of decoupling dynamic objects and static background is important and the proposed approach tackles the long-standing problem with high-quality results.\n\n## Weakness\n\n1. I think the main weakness comes from the scene-level tuning for the hyparameter of k, $\\lambda_s$, $\\lambda_r$, $\\lambda_{\\sigma^S}$, $\\lambda_\\rho$.\n2. I understand that to push the number, the author may need grid-search for each scene. However, the lack of analysis for the sensitivity of those hyparameters makes it unclear how robust the proposed approach is.\n3. Especially considering in Sec. B of supp, we have 9 sets of parameters for 19 scenes. Even for the same scene (Banana), we have two sets of hyparameters for two tasks (decoupling and novel view).\n4. Besides the per-scene tuning results, I recommend the author to report a set of quantitative results with a single set of hyparameters if possible. 1. I am curious about the robustness of the proposed approach to the hyparameters. See the weakness part. I appreciate the authors's discussions about the limitations of the proposed approach in Sec. 5, which helps the understanding of the suitable scenarios.",
            " This paper introduces a series of techniques to decouple dynamic objects from monocular videos, which present impressive static background recovery with shadow removal.  How to remove the disturbance from dynamic objects/occluders is a long-standing problem in 3D vision domain. I appreciate the simple but valuable design, i.e., the skewed entropy loss with decoupled NeRF and an extra shadow field. However, many previous methods have adopted similar techniques for foregroud/background content separation. Some related literatures are not included in the paper. Besides, it reads like a CVPR-tyle paper. I'm not sure whether it's suitable for NIPS. Strenghts:\n1. The skewed-entropy loss is able to flexibly separate the static and dynamic parts.\n2. Separating the time-varying shadows and the static appearace via a shadow field.\n2. The decouple performance and NVS quality significantly outperforms SOTA.\n 1.  The skewness hyper-parameter $k$ is critical in practice because it determines the proportion of the `dynamic'. And it's sensitive according to the provided experiments. Is it possible to automatically tune the parameter?\n2. The motivation and mechanism of the skew entropy loss is similar to the beta distribution proposed by [1]. As it is regarded one of the major contribution, a thorough comparison and discussion is deserved.\n3. The topic is NVS in dynamic environments. Some related literatures are absent[1][2].\n4. The shadow NeRF concept has been introduced in [3], which should be included in related literature.\n\n[1] Neural Volumes: Learning Dynamic Renderable Volumes from Images \n[2] Neural Scene Graphs for Dynamic Scenes\n[3] Shadow Neural Radiance Fields for Multi-view Satellite Photogrammetry \n\n\n\n\n\n\n"
        ],
        "sentiment": [
            "Positive",
            "Positive",
            "Positive",
            "Positive",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states that the authors 'well addressed' their concerns and expresses appreciation for the paper ('I really appreciate this paper').",
            "The review expresses overall positive sentiment, stating the paper is a 'solid development,' 'well-written,' and its claims are 'well supported.' The reviewer also notes the adaptation is 'elegant' and 'effective,' and concludes the paper 'deserves publication.'",
            "The review highlights several strengths of the paper, including a new dataset, a self-supervised method, a novel loss, a shadow field network, and state-of-the-art results. While weaknesses are mentioned, the overall tone suggests a positive evaluation of the paper's contributions.",
            "The review highlights several strengths of the paper, including originality, quality, clarity, and significance. While it also points out weaknesses, the overall assessment is positive due to the appreciation of the proposed techniques and results.",
            "The review expresses appreciation for the paper's design but raises concerns about novelty, missing literature, and suitability for the conference. The reviewer points out similarities to existing methods and identifies critical parameters that require tuning, suggesting the paper may not be significantly novel or complete."
        ],
        "tone": [
            "Supportive",
            "Supportive",
            "Balanced",
            "Balanced",
            "Critical"
        ],
        "tone_reason": [
            "The reviewer uses positive language like 'well addressed' and 'I really appreciate' which indicates a supportive tone.",
            "The tone is supportive, acknowledging the paper's strengths and offering constructive suggestions. Phrases like 'It would be nice if the author also visualizes...' and 'Could the author make additional comments on the possible reasons?' indicate a desire to help improve the paper rather than harshly criticize it.",
            "The review presents both strengths and weaknesses of the paper, using neutral and objective language to describe each aspect. It offers constructive criticism regarding comparisons with other methods and generalizability while acknowledging the challenges involved.",
            "The review presents both strengths and weaknesses of the paper. It uses objective language and constructive criticism, resulting in a balanced tone.",
            "The tone is critical due to the reviewer's questioning of the paper's novelty ('many previous methods have adopted similar techniques'), pointing out missing literature ('Some related literatures are not included'), and expressing doubt about its suitability for the conference ('I'm not sure whether it's suitable for NIPS'). The use of phrases like 'a thorough comparison and discussion is deserved' and 'should be included' indicates a need for significant improvements."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent because it expresses overall positive feedback. While mentioning the simplicity of the technique, it does not contradict the appreciation for the paper and the fact that the authors addressed the reviewer's concerns.  Appreciating a paper despite a simple technique is a consistent viewpoint.",
            "The review is consistent because it starts with an overall positive assessment of the paper, praising its solid development, well-written nature, well-supported claims, elegant adaptation, effectiveness, and value to the field, ultimately recommending publication.  The reviewer then raises specific questions and suggestions for improvement, which are framed as points for the authors to comment on or consider, rather than contradictions to the initial positive evaluation. The questions are constructive and aim to enhance the paper further, not to undermine its core contributions. The final point about limitations is also presented as an observation, not a contradiction to the paper's strengths.",
            "The review is consistent in its assessment by clearly separating strengths and weaknesses. The weaknesses raised are valid points for improvement and do not contradict the acknowledged strengths. The reviewer maintains a constructive tone throughout the review.",
            "The review is consistent because it clearly outlines both the strengths and weaknesses of the paper. The weakness is focused on the hyperparameter tuning and the lack of robustness analysis, which is a consistent concern throughout the review. The reviewer's curiosity and suggestion for improvement directly relate to this identified weakness, demonstrating a logical and consistent line of reasoning. There are no contradictory statements within the review.",
            "The review is consistent because it presents both positive aspects (strengths) and constructive criticisms (weaknesses) of the paper. The reviewer acknowledges the value of the proposed method while also pointing out areas for improvement and further discussion, such as the sensitivity of a hyperparameter, the similarity to existing methods, and the lack of related literature. There are no contradictory statements within the review; the criticisms are aimed at enhancing the paper rather than refuting its contributions."
        ]
    },
    {
        "paper_id": "iclr_2019_S1xoy3CcYX",
        "paper_title": "Adversarial Examples Are a Natural Consequence of Test Error in Noise",
        "paper_abstract": "    Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to making small modifications of a correctly handled input. At the same time, less surprisingly, image classifiers lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this work, we show that these are two manifestations of the same underlying phenomenon. We establish this connection in several ways. First, we find that adversarial examples exist at the same distance scales we would expect from a linear model with the same performance on corrupted images. Next, we show that Gaussian data augmentation during training improves robustness to small adversarial perturbations and that adversarial training improves robustness to several types of image corruptions. Finally, we present a model-independent upper bound on the distance from a corrupted image to its nearest error given test performance and show that in practice we already come close to achieving the bound, so that improving robustness further for the corrupted image distribution requires significantly reducing test error. All of this suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. This yields a computationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.",
        "review_ids": [
            "HyxZTVo9hQ",
            "HkxpwrWcn7",
            "S1gVOsyuh7"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "The paper tries to make a connection between the functionality of Gaussian noise to adversarial examples. It shows that data augmentation with added Gaussian noise could also improve the model robustness. \nMeanwhile, it shows that in a high dimensional space, even with a small (error) set, its bounding ball of \\epsilon l_p distance could be large. It explains why even with a small test error, the model could still be vulnerable to adversarial examples. \n\nAlthough the paper has some good intuitions and some nice experiments, I find the main conclusion of this paper not very interesting. \nAlthough I partially buy the second point about the high dimensional geometry, this is an obvious observation and does not give rise to much meaningful result for the future work. It would be more interesting to see the different geometry structure of robust versus not robust models. \n\nMeanwhile, though a formal definition of error set is not presented in the paper, it seems the authors are simply dividing the data space to the set where the model gives a correct label, and the \u201cerror set\u201d the other way around. However, since the paper is considering a data distribution (q) rather than a dataset, the separation could be more complicated than that. For instance, a noise image should also in your data space, but does it belong to an error set or not? It isn\u2019t necessarily attached to any labels. Or does your model only consider meaningful images? But what if adding noise simply get you out of the space? It\u2019s better to make this concept clearer. \n\n\nIt is not a surprising result that there is one randomly chosen direction mimicking the performance of adversarial examples as in Figure 2. By \u201ccarefully crafted imperceptible noise\u201d, I assume it means choosing one random sample that will change the model output the most. This is exactly a way of choosing an adversarial example. Since even in high dimensional space, out of a lot of random vectors , one could approximate a target (adversarial) direction.\n\nSimilar explanations also apply to the training with error part. How much more data do you use for the data augmentation? If you use much more data with Gaussian noise than what your use for adversarial training, it is not surprising at all to get a more robust network, with a similar argument as above.\n\n\nminor issue: Section 3 should not be an isolated section.\n",
            "This paper propose an alternative view for adversarial examples in high dimension spaces by considering the \"error rate\" in a Gaussian distribution centered at each test point. However, as mentioned in the related work, adversarial examples through the lens of isoperimetric inequality is not new to this paper; the implication of adversarial sensitivity by error rate in the test-sample-centered-Gaussian in general non-linear case is rather weak; and the empirical results does not show advantage over simple adversarial training against lp constrained adversarial attacks.\n\nHere are some more detailed comments and feedbacks:\n\nThe clarity could be improved by making clear use of notations and define some key terms explicitly:\n\n1. For example, the error rate sometimes refer to the test error, sometimes refer to the error rate under a special distribution centered at a particular test example.\n\n2. Similarly, the distribution q sometimes refer to the original (unknown) input data distribution, but the same notation is also used to refer to this Gaussian distribution centered on each test example. Although the paper says that \"q need not be restricted to the distribution from which the training set was sampled\", it could potentially confuse the reader less if there is a symbol for the \"usual\" test error and a different one for this test-error-under-Gaussian-centered-at-a-particular-test-example.\n\n3. It would be good if the paper could make a formal definition of the problem being studied and explicitly specify the assumptions on the existence of a deterministic target function (concept) and explicitly define the error set E.\n\n4. It seems to assume the input distribution is continuous everywhere but not stated. If for example, the original data is supported on disconnected manifolds separated with low density or even zero-density margins, then the Gaussian distribution centered on test example argument will need to be modified to talk about the intersection of the Gaussian with the data manifold instead. If the paper does decide to make this kind of assumption, some empirical study on the data to verify the fidelity of the assumptions would be great.\n\n5. The paper does not mention how measurement against the error set E. Under the original data distribution, it is natural to measure the error rate with the provided training or test data with labels. However, under each newly formed Gaussian distribution centered at each test point, the labels for the newly sampled examples from this Gaussian is unknown, and since there is no \"ground truth classifier\" for MNIST or CIFAR10 available, it seems impossible to \"calculate\" the true label for those samples, which are needed to calculate the error rate. It is not very clear from the paper how this issue is solved. I'm guess it uses the label from the Gaussian center for all the samples from the Gaussian. While this might be reasonable assumption for Gaussian with tiny variances, it is less clear how reasonable it is for the large variance Gaussian distributions considered in the paper. If this assumption is made, please state it explicitly and empirically or theoretically study how reasonable this assumption is in the regime of variances considered in this paper. If my guessing is wrong, please also explicitly what approach is used to get around this issue.\n\nThe followings are some feedbacks on the contents and ideas of the paper:\n\n6. I think one sentence in the text summarize a large part of the paper very well: \"to measure adversarial robustness is to ask whether or not there are any errors in the linf ball, ... and to measure test error in noise is to measure the volume of the error set in the defined noise distribution\". However, this looks like a rather roundabout approach to attack another problem (measuring volume of an unknown set in a very high dimension space) in order to solve the original problem, while the implication is rather weak (a precise implication can only be obtained for linear separating hyper-plane, while for non-linear classifiers it is much less clear).\n\nNote while exact adversarial robustness is NP-hard, volume estimation in high dimension is not easy (if not harder). For cifar-10, the inputs are in dimension 32x32x3 = 3072, the 1,000 samples used in the paper to estimate the volume of a set in this high dimension seem to be quite inaccurate. I would appreciate if variances could be reported in those studies to show the confidence of the estimations. For imagenets, the inputs are in even larger spaces.\n\nGiven the difficulty (in terms of sample complexity) to estimate the \"error-in-noise\", it might not be very surprising that the noise augmentation does not show advantages to lp constrained adversarial attacks (comparing to adversarial training).\n\n7. In the conclusion, the paper states \"we proved a fundamental relationship between generalization in noisy image distributions and the existence of small adversarial perturbations\". I believe a formal proof is only given to the case of existence of small adversarial perturbations to examples to the \"noisy examples\" from the Gaussian distribution, and in this case, it is a rather direct corollary from the Gaussian Isoperimetric Inequality. For the more \"practical case\" (in the sense that is more related to the usual notion of adversarial examples) of existence of small adversarial examples, it seems only the case of the linear classifier is formally discussed.\n\nMoreover, I'm a little bit worried some important pieces might be lost and create potentially misleading or seemly strong conclusion. Maybe it would be helpful if a concrete example could be given in the paper that shows the full path from the error-in-noise to existence of adversarial example, by showing all the constants involved. I'm a bit confused here because (in order for the isoperimetric inequality to have favorable bounds?) the Gaussian distributions used in error-in-noise seem to have rather large variance. As mentioned in the paper, the majority of the mass in the Gaussian distribution considered will be in a thin sphere of radius sigma * sqrt(n) centered at the test example x. If sigma = 0.1 and dimension n = 3072 (cifar-10), then the radius is around 5.5 (in l2 distance) which is probably quite far from the test point x (is it?). It is then less clear how \"a large majority of this thin sphere far away from x is epsilon close to the error set\" could tightly imply properties of adversarial robustness of x itself in its close vicinity. Maybe a specific example with all the numerical constants spelled out would help illustrate this.\n\nIn summary, I think this paper takes an interesting but roundabout perspective to adversarial robustness, and the implication is weak in the non-linear case. (Potentially because of the weak implication), the suggested approach for defenses by augmenting with noises does not show advantage over adversarial training.",
            "The paper suggests a connection between training with noisy images and the adversarial training. The observation is original to me. It has several cons.\n\n1. The paper is hard to follow because of  too many vague descriptions and unnecessary contrast clauses. Here are some.\n 1) The title of section 4: ERRORS IN NOISE IMPLY ADVERSARIAL EXAMPLES FOR NOISY IMAGE.  \n 2) \"The discussion of high-dimensional geometry suggests that adversarial examples may actually not be in contradiction to high generalization performance. Indeed, high generalization performance does not mean perfect generalization\" The uncertain tone \"may not\" and the vague statement \"does not mean perfect generalization\" make readers hard to get the solid understanding about what the paper tries to say. \n 3)  \"Adversarial training on small perturbations on CIFAR-10 also improved generalization in noise, and training on noise improved robustness to small perturbations.\" \nMany other sentences like the above make the paper not technically sound.\n\n2. It is problematic that the paper uses Euclidean l2 distance to measure the error set and its surface as it is believed that the dataset lives on low-dimensional manifold. Moreover, the adversarial examples often constructed by moving the legal images towards a specific direction rather than adding the Gaussian isotropic noise. \n3. The advocates that using test error in noise as a measure of adversarial robustness  is misleading  as test error in noise has a large number of different combinations: noise type, noise amplitude. One may find one type of test error in noise coinciding with adversarial robustness but  in general it is not a good measure for adversarial robustness because of its varying nature.\n4. Several terms are referred without definitions: errors in noise,  adversarial robustness. From the definition of E_epsilon, it should include the interior of E.\n"
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses a generally negative sentiment, highlighting concerns about the paper's novelty and clarity. Phrases like \"not very interesting,\" \"obvious observation,\" and questions about the paper's assumptions and experimental setup contribute to this negative assessment.",
            "The review expresses several concerns about the paper's novelty, clarity, and the strength of its conclusions. Phrases like \"rather weak,\" \"does not show advantage,\" \"roundabout perspective,\" and \"weak implication\" indicate a negative sentiment.",
            "The review expresses several concerns about the paper's clarity, technical soundness, and methodology. Phrases like \"hard to follow,\" \"vague descriptions,\" \"unnecessary contrast clauses,\" \"not technically sound,\" and \"problematic\" indicate a negative sentiment."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, as evidenced by the reviewer's questioning of the paper's core arguments and methodology. Specific criticisms include the lack of novelty, unclear definitions, and concerns about the experimental setup. Phrases like \"not a surprising result\" and \"it is not surprising at all\" further emphasize the critical tone.",
            "The review provides specific criticisms of the paper's methodology, assumptions, and conclusions. It uses phrases like \"clarity could be improved,\" \"potentially confuse the reader,\" \"does not mention,\" \"not very clear,\" \"rather roundabout approach,\" and \"I'm a little bit worried\" to point out flaws and suggest improvements. The reviewer also questions the validity of the assumptions and the strength of the empirical results.",
            "The tone is critical as the review directly points out flaws and weaknesses in the paper's arguments, writing, and methodology. Terms like \"hard to follow,\" \"problematic,\" and the detailed list of specific issues contribute to the critical tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its critical assessment of the paper. While acknowledging some positive aspects like good intuitions and experiments, it consistently argues that the main conclusion is not interesting or novel enough. The reviewer's points, such as the obviousness of the high-dimensional geometry observation and the potential triviality of the data augmentation results due to increased data volume, all contribute to a coherent critical perspective without contradicting each other.",
            "The review is consistent in its criticism of the paper. It points out several weaknesses, including lack of clarity in definitions and notations, weak theoretical implications especially for non-linear cases, and lack of empirical advantage. All the detailed comments and feedbacks consistently support the reviewer's overall negative assessment of the paper's approach and conclusions. There are no self-contradictory statements or conflicting viewpoints within the review.",
            "The review is consistent in its criticism of the paper. The reviewer points out several weaknesses related to clarity, technical soundness, and definitions, without contradicting themselves. The reviewer starts with acknowledging originality but immediately pivots to listing cons, and all subsequent points are negative and critical of the paper's presentation and methodology."
        ]
    },
    {
        "paper_id": "iclr_2020_B1eCk1StPH",
        "paper_title": "The Generalization-Stability Tradeoff in Neural Network Pruning",
        "paper_abstract": "Pruning neural network parameters is often viewed as a means to compress models, but pruning has also been motivated by the desire to prevent overfitting. This motivation is particularly relevant given the perhaps surprising observation that a wide variety of pruning approaches increase test accuracy despite sometimes massive reductions in parameter counts. To better understand this phenomenon, we analyze the behavior of pruning over the course of training, finding that pruning's effect on generalization relies more on the instability it generates (defined as the drops in test accuracy immediately following pruning) than on the final size of the pruned model. We demonstrate that even the pruning of unimportant parameters can lead to such instability, and show similarities between pruning and regularizing by injecting noise, suggesting a mechanism for pruning-based generalization improvements that is compatible with the strong generalization recently observed in over-parameterized networks.",
        "review_ids": [
            "HkeqIHShKr",
            "H1gBba7itH",
            "BJxKLBXTYH"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper mainly studies the relationship between the generalization error and mean/variance of the test accuracy. The authors first propose a new score for pruning called E[BN]. Then, the authors observe the generalization error and the test accuracy mean/variance for pruning large score weights and small score weights for VGG11, ResNet18, and Conv4 models. From these experiments, the authors observe that pruning large score weights generates instable but high test accuracy and smaller generalization gap compared to pruning small score weights. The authors additionally study some other aspects of pruning (e.g., pruning as a noise injection) and conclude the paper.\n\nOverall, I am not sure whether the observation holds in general due to the below reasons. \n\n- The authors proposed a new score E[BN] and all experiments are performed on this. However, how it differs from the usual magnitude-based pruning in practice is unclear. I would like to know whether similar behavior is observed for the na\u00efve magnitude-based pruning.\n\n- I think that the author\u2019s observation is quite restricted and cannot extend to a general statement since the experiments are only done for pruning small score/large score weights. To verify the generalization and instability trade-off, I believe that it is necessary to examine several (artificial) pruning methods controlling the instability of test accuracies and check whether the proposed trade-off holds. For example, one can design pruning methods that disconnect (or almost disconnect) the network connection from the bottom to the top (i.e., pruned network always outputs constant) with some probability to extremely increase the instability.\n\n- The authors did not report the results for high sparsity.\n\nBesides, I am not sure the meaning of the instability since when the test accuracy of the pruned model is higher than that of the unpruned model, the instability could be large.\n\nOther comments:\n- The first paragraph mentions that the generalization gap might be a function of the number of parameters. However, I think that it is quite trivial that the generalization gap is not a function of the number of parameters while it only provides the upper bound.\n----------------------------------------------------------\nI have read the authors' response. Thanks for clarifying the definition of instability and additional experiments with high sparsity. However, I will maintain my score due to the following concern. \n\nThe remaining concern is that the current evidence for verifying generalization-stability tradeoff is not convincing as the authors presented only some examples having small and large instability (e.g., pruning smallest/largest weights) under the same pruning algorithm. I think that the results would be more convincing if the authors add a test accuracy plots given a fixed prune ratio, whose x-axis is controlled instabilities (e.g., from 10% to 90%) among various pruning algorithms (other than magnitude-based ones, e.g., Hessian based methods). It would be much more interesting if the same instability results same test accuracy even for different pruning algorithms.\n",
            "The paper is an empirical study that looks into the effect of neural network pruning on both the model accuracy as well as the generalization risk (defined as the difference between the training error and the test error). It concludes that while some pruning methods work, others fail. The authors argue that such discrepancy can be explained if we look into the impact of pruning on \"stability\". \n\nThe first major issue I have with the paper is in their definition of stability. I don't believe that this definition adds any value. Basically, the authors define stability by the difference between the test accuracy pre-pruning and post-pruning. This makes the results nearly tautological (not very much different from claiming that the test accuracy changes if the test accuracy is going to change!). One part where this issue is particularly important is when the authors conclude that \"instability\" leads to an improved performance. However, if we combine both the definition of test accuracy and the definition of \"instability\" used the paper, what the authors basically say is that pruning improves performance. To see this, note that a large instability is equivalent to the statement that the test accuracy changes in any direction (there is an absolute sign). So, the authors are saying that the test accuracy after pruning improves if it changes, which is another way of saying that pruning helps. \n\nThe second major issue is that some of the stated contributions in the paper are not discussed in the main body of the paper, but rather in the appendix. For example, the authors mention that one of their contributions is a new pruning method but that method is not described in the paper at all, only in the appendix. If it is a contribution, the authors should include it in the main body of the paper. \n\nThird, there are major statements in the paper that are not well-founded. Take, for example, the experiment in Section 4.4, where they apply zeroing noise multiple times. The authors claim that since the weights are only forced to zero every few epochs, the network should have the same capacity as the full network (i.e. VC capacity). I disagree with this. The capacity should reduce since those weights are not allowed to be optimized and they keep getting reset to zero every few epochs. They are effectively as if they were removed permanently. \n\n ",
            "This paper studies a puzzling question: if larger parameter counts (over-parameterization) leads to better generalization (less overfitting), how does pruning parameters improve generalization? To answer this question, the authors analyzed the behaviour of pruning over training and finally attribute the pruning's effect on generalization to the instability it introduces.\n\nI tend to vote for a rejection because \n(1) The explanation of instability and noise injection is not new. Pruning algorithms have long been interpreted from Bayesian perspective. Some parameters with large magnitude or large importance contribute to large KL divergence with the prior (or equivalently large description length), therefore it's not surprising that removing those weights would improve generalization. \n(2) To my knowledge, the reason why over-parameterization improves generalization (or reduces overfitting) is because over-parameterized networks can find good solution which is close to the initialization (the distance to the initialization here can be thought of as a complexity measure). In this sense, the effect of over-parameterization is on neural network training. However, pruning is typically conducted after training, so I don't think the fact that pruning parameters improves generalization contradicts the recent generalization theory of over-parameterized networks. Particularly, these two phenomena can both be explained from Bayesian perspective."
        ],
        "sentiment": [
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The reviewer expresses significant reservations about the paper's claims and the generalizability of its findings. Phrases like \"I am not sure whether the observation holds in general\" and \"not convincing\" clearly indicate a negative sentiment. The reviewer also maintains their score after reading the authors' response, highlighting persistent concerns.",
            "The review expresses significant concerns about the paper's methodology, definitions, and the validity of its claims, using phrases like 'major issue,' 'I don't believe that this definition adds any value,' 'nearly tautological,' 'not well-founded,' and 'I disagree with this.' These indicate a critical and negative assessment of the work.",
            "The reviewer states 'I tend to vote for a rejection' which indicates a negative sentiment towards the paper."
        ],
        "tone": [
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review adopts a critical tone by pointing out several limitations and weaknesses in the paper's methodology and conclusions. The reviewer uses phrases like \"quite restricted,\" \"cannot extend to a general statement,\" \"did not report,\" and \"not convincing\" to express their critique. The suggestions for improvement are framed as necessary steps to address these shortcomings.",
            "The tone is critical, evident in the reviewer's direct challenges to the paper's methodology and conclusions. Phrases such as 'The first major issue I have,' 'I don't believe that this definition adds any value,' 'what the authors basically say is,' and 'I disagree with this' demonstrate a critical and challenging stance.",
            "The review expresses disagreement with the paper's novelty and reasoning, using phrases like 'not new', 'not surprising', and 'I don't think'. The reviewer also points out perceived flaws in the paper's understanding of over-parameterization and its relationship to pruning."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The reviewer's feedback is consistent across the initial review and the follow-up after reading the author's response. The core concern remains the lack of convincing evidence for the generalization-stability tradeoff, specifically the need for experiments with controlled instability across different pruning algorithms, not just within one algorithm by varying pruning scores. The reviewer acknowledges the authors' efforts but maintains their stance, indicating a consistent line of reasoning.",
            "The review is consistent in its criticism of the paper. The reviewer raises three major issues and provides detailed reasoning for each, maintaining a negative stance throughout the text without any self-contradiction.",
            "The reviewer consistently argues against the novelty and significance of the paper's findings. They state that the explanation of instability is not new and that the premise of the paper (that pruning improving generalization contradicts over-parameterization theory) is flawed. Both arguments are supported by referencing existing Bayesian interpretations and theories related to over-parameterization and pruning."
        ]
    },
    {
        "paper_id": "iclr_2022_8svLJL54sj8",
        "paper_title": "Automatic prior selection for meta Bayesian optimization with a case study on tuning deep neural network optimizers",
        "paper_abstract": "The performance of deep neural networks can be highly sensitive to the choice of a variety of meta-parameters, such as optimizer parameters and model hyperparameters. Tuning these well, however, often requires extensive and costly experimentation. Bayesian optimization (BO) is a principled approach to solve such expensive hyperparameter tuning problems efficiently. Key to the performance of BO is specifying and refining a distribution over functions, which is used to reason about the optima of the underlying function being optimized. In this work, we consider the scenario where we have data from similar functions that allows us to specify a tighter distribution a priori. Specifically, we focus on the common but potentially costly task of tuning optimizer parameters for training neural networks. Building on the meta BO method from Wang et al. (2018), we develop practical improvements that (a) boost its performance by leveraging tuning results on multiple tasks without requiring observations for the same meta-parameter points across all tasks, and (b) retain its regret bound for a special case of our method. As a result, we provide a coherent BO solution for iterative optimization of continuous optimizer parameters. To verify our approach in realistic model training setups, we collected a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, our method is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods.",
        "review_ids": [
            "d1IEzpn0Fen",
            "PqPTl_MfL2O",
            "xgngFKZvUD0",
            "SlbKfSmACQo",
            "9GOaIhix4m_",
            "DsgJW48s8ud"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            "This paper is concerned with speeding up Bayesian optimization by using evaluation data from previous, related tasks defined over the same configuration space. The authors propose to model the data from each experiment (or \"task\") by independent Gaussian processes, which all share the same mean and covariance function. This surrogate model can be learned from past data.\n\nThe paper also presents experiments on a fairly simple search space of 4 optimizer parameters. This is done for a bunch of datasets and NN models. And there is a pretty simple extension of theoretical results from (Wang, 2018b). The problem of \"warmstarting\" HPO by making use of data from previous experiments is an obvious idea, and it has seen a large amount of past work, much of which the authors of this submission do not seem to be aware of, neither apparently was (Wang, 2018b) which seems more of a theoretical paper. In particular, there is quite a lot of work which uses GP models and scales linearly in terms of the number of past experiments, contrary to what is stated in the introduction. Two of the most interesting ones are maybe [1], [2]. The authors here cite (Perrone, 2018), which has these citations and more, so it is pretty odd the authors do not mention (or compare against) any of them.\n\nGiven the straightforward nature of what is proposed here (a setup closely related to what is done in [3]), I'd be quite surprised if for example [1] would not outperform it. After all, the assumption that data from experiments on quite different models can be modeled by the same mean and covariance function, is pretty strong. There are all sorts of issues with this idea, for example what if data from some tasks is much larger than data from others? Moreover, in what is proposed here, the surrogate model parameters do not even seem to be adapted to the current task, even as data from it becomes available. Here, methods like [1], [2] seem much more compelling to me, as they try to for example rank previous experiments by closeness to the current one. [1] is doing this without having to define any meta-features of the dataset, and also of course without relying on observations at the same configurations (given you model your data with a GP, you should certainly not need that anyway).\n\nThe experiments are not meaningful, because essentially all relevant prior work is missing for comparison. The authors more or less compare their proposal (in two variants) against a bunch of baselines, as if there was no revelant prior work. In fact, they even seem to invent on their own methods to compare against, such as \"MIMO\", in a way which has never been used for transfer HPO. Why? Please read about and compare against relevant prior work. Given they cite work (e.g., Perrone 2018), they should have been aware.\n\nApart from that, I also do not get much out of the experimental setup. Why was it chosen that way? Does it have any practical relevance? Does anybody else use this learning rate schedule, or was it just made up for this paper? I also did not find a discussion of a pretty critical point: how are the datapoints chosen for tasks you offline train on? In order to be realistic, these would have to be active choices themselves, because that is data we could have been obtained by running BO on them. Instead, my suspicion is that past data was sampled randomly, which would correspond to pure exploration (random search). Such data is obviously more valuable to obtain a good surrogate model fit, but also more expensive to obtain in the real world (one would have to run random search).\n\n[1] Feurer etal: Practical Transfer Learning for BO, https://arxiv.org/abs/1802.02219\n[2] Wistuba etal: Two-stage transfer..., ECML 2016\n[3] Golovin etal: Google Vizier, KDD 2017 This paper proposes a simple idea for wam-starting BO by fitting the parameters of a GP surrogate model on past data. Unfortunately, a lot of relevant prior work is ignored here and not compared against. Instead, the proposed approach is compared against simple baselines, as well as methods that mostly seem to have been made up (such as \"MIMO\").\n",
            " Thank you for your thoughtful comment.\n\nMost of my questions and concerns are resolved.\n\nHowever, in particular, a relatively small search space is a weak point of this work, while I do not agree with that point.\n\nI think four or five hyperparameters of interest are a good design choice.\n\nBeyond such a search space, I cannot imagine which search space in hyperparameter optimization is appropriate for this meta-Bayesian optimization scenarios, and which search space in neural architecture search is the most effective for finding an optimal architecture.\n\nBut, if you find some realistic and large space as a target search space, this paper would be better.\n\nBest regards,\n\nReviewer jdBZ.",
            "This paper suggests a meta Bayesian optimization strategy that optimizes free parameters of GP including a prior function and noise variance, where multiple sets of historical observations are given. In particular, the proposed method chooses a free parameters using one of three approaches: (i) optimizing a marginal likelihood, (ii) measuring KL divergence, (iii) considering both marginal likelihood and KL divergence. The authors finally show the theoretical analyses on regret bounds and the numerical results on hyperparameter optimization. ### Reasons to Accept\n\n+ It is well-written and well-organized.\n+ It solves a very interesting problem, which transfers a history to the current task in Bayesian optimization setup.\n+ Compared the work by Wang et al. (2018b), it solves more realistic setups.\n+ It provides promising numerical results and sound theoretical results.\n\n### Reasons to Reject\n\n- I do not think that it degrades the contributions much, but four-dimensional search space is relatively small, compared to other Bayesian optimization or hyperparameter optimization papers.\n- Following the above point, is there any specific reason why the authors use four-dimensional search space? I do not think this algorithm is not scalable. Moreover, for example, batch size can be one of the meta-parameters to be optimized.\n\n### Questions to Authors\n\n1. Can you elaborate why the proposed method does not train a GP model every iteration, e.g., every $t = 1, \\ldots, T$? I think that it can be possible without (relatively) expensive computational costs.\n1. H* NLL does not use a matching dataset, right? If you did not use multi-task GP regression, which has an additional input to indicate task information, does H* NLL (i.e., optimizing Equation (2) with $D_N$) work appropriately?  I think that this paper addresses an interesting problem and suggests a novel method as described above. Thus, I would like to recommend acceptance.",
            " I remain highly unconvinced about this work, and the author feedback does not really address my concerns. This works seems \"to challenge the status quo\" quite a bit, combining it with an extremely simple setup and a highly unconvincing empirical evaluation, against pretty esoteric methods.\n\nThe authors did not reply to my request for sensible competitive baselines.\n\nI'd also like to point out that the authors confuse \"Gaussian process\" with \"covariance function\". The former is a random process of dependent variables (unless it is a noise process), the latter is a model for a random process. What they are doing is they share a covariance and mean function across different tasks, each of which is assigned a GP. This is one of the simplest baselines one could imagine, and if it was properly compared to competitive methods (like Feurer etal, or old work from Wistuba etal), it would likely fare not well. Instead, it is still not clear to me how the authors picked the methods to compare against, some of which (MIMO) are totally unrelated to transfer HPO.",
            " HyperBO assumes the tasks are independent given the hyperparameters, unlike typical metalearning approaches which assume tasks are related. This allows for an efficient Kronecker decomposition of the kernel and thus linear, rather than cubic scaling, across tasks. \n\nUsing this model, HyperBO performs BO as usual; maximize the acquisition function to obtain the next point to evaluate. HyperBO also makes the critical assumption of an offline pre-training of hyperparameters on a representative set of completed tasks; during optimization itself the hyperparameters are fixed. \n I have a few key concerns about this paper. \n\n- Why fixed hyperparameters? This is clearly the bottleneck of Metalearned BO, and if these hyperparameters are learned offline, this seems to (A) somewhat eliminate the strength of HyperBO which is the linear scaling per task ---obviously this still helps significantly during the offline training, but still a point of concern of mine, and (B) seems not robust, especially if the set of representative completed tasks is heavily biased. \n\n- HyperBO, in the experiments, uses the PI acquisition function. Is there a particular reason why this is? PI is quite greedy (even more than EI), so is there any intuition as to why PI is appropriate in this situation. \n\n- In Figure 2b, I am somewhat concerned about the empirical performance of HyperBO. Though it beats the baselines, it does so in a 4D search space, using thousands of tasks; this seems like overkill. The error bars are also all over the place. This is somewhat unfair of me to ask for I admit, but I am curious if a much simpler approach involving restricting the search space (given that it is fixed) will help (see the paper \u201cLearning search spaces for Bayesian optimization, Perrone et al., 2019). I feel like there is definitely enough data for this to make a difference. \n\n- Also, the experiments only really concern one optimization problem involving optimizer hyperparameters. Though this one experiment is quite impressive in terms of the data involved, iit would be nice to see another experiment (say for tasks that might be easier like tuning a random forest). \n I have some concerns about the assumptions used in the methodology, as well as the experiments, which leave a number of open questions. In particular, the fixing of GP hypers seems to largely remove the need for scaling, which is the primary strength of HyperBO. Furthermore, though the experimental set up uses a large amount of data to achieve somewhat unconvincing results in my mind, and only one optimization problem is presented (though worth noting, is thoroughly analyzed). Thus, I can't recommend acceptance at the time. ",
            "This paper presents a Bayesian optimization method based on meta-BO. The motivation is tasks can share the same parameter structure and this shared information, e.g. correlation between tasks, can be transferred to new and similar tasks. An example is to optimize the the hyper-parameters of a same optimizer across different architectures and different datasets. This problem is a very important one in the community of Bayesian optimization and a reasonable method can lead to a potentially dramatic decrease in the required computation, especially when the objective function is very expensive. This work tries to overcome limitations of existing methods. For example, the method proposed in this work does not need to evaluate all objective functions associated with all tasks on the same parameters. The reviewer appreciates the authors putting effort into the empirical evaluation of the proposed method. However, the proposed approach is not interesting to the Bayesian optimization community and is trivial to some degree. The reviewer believes that the targeting problem presented in this work is a very important one and an effective method could be of great practical value. \n\nIn the abstract, authors claim that \"data from similar functions\" could lead to a better prior for GP. Obviously a better prior for GP is desirable and that is why the marginal likelihood is used to optimize  parameters of a GP. From such a claim, it is expected that an efficient method for BO will be presented by exploring novel similarities between tasks. However, throughout this paper, there is no definition of a similarity between tasks and tasks are treated as independent. This raises my concern on this work's novelty, which is my biggest concern on this paper.  \n\nAuthors claim that the critical difference between this work and standard BO algorithms is the initial learning process in line 2 of Algorithm 1. The corresponding likelihood of this approach is given in eq(2). I do not get the point how this approach is different from existing GP modeling and eq(2) is simply the unnormalized marginal likelihood for all data points since all tasks are assumed to be independent. Such a formulation is not only trivial to the GP community , but also to the empirical Bayes community. \n\n\nAdditional (minor) issues:\n1. the graphical model for GP in Figure 1 is wrong\n2. there exist a lot of inconsistencies in this paper. In assumptions section, it is assumed the variance is known however the variance is a hyper-parameter in the marginal likelihood. \n3. lots of claims and statements are superfluous. For example, authors claim one limitation of existing approaches is the total number of BO iterations must be set in a manual way. However, throughout this paper, the number of iterations is still pre-defined. What is the point of saying this is a limitation while not touching it at all? Another example, authors claim  \"interpretability of intermediate steps\" is lost in existing methods, however, this problem is not touched either. \n4. Another contribution of this paper is a tuning dataset. I can see the value of such a dataset, however, failing to explicitly describe the required computation resources makes claiming this being a contribution less convincing. The proposed method is trivial. The theoretical part presented in this paper is very minimal and incremental. "
        ],
        "sentiment": [
            "Negative",
            "Positive",
            "Positive",
            "Negative",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses significant concerns about the paper's novelty, awareness of prior work, and experimental setup. Phrases like 'straightforward nature,' 'pretty strong' (referring to assumptions), 'not meaningful,' and 'all relevant prior work is missing' indicate a negative sentiment.",
            "The reviewer expresses gratitude for the author's comment, indicates that most concerns are resolved, and offers constructive suggestions for improvement, indicating a generally positive outlook.",
            "The reviewer expresses overall positive sentiment, explicitly recommending acceptance and highlighting the paper's strengths such as being well-written, addressing an interesting problem, offering improvements over existing work, and providing promising results.",
            "The reviewer uses phrases like \"highly unconvinced\", \"does not really address my concerns\", \"highly unconvincing empirical evaluation\", and criticizes the authors' understanding and methodology, indicating a negative sentiment.",
            "The reviewer expresses 'key concerns' about the paper's methodology and experiments. They state that the results are 'somewhat unconvincing' and ultimately 'can't recommend acceptance at this time.'",
            "The negative sentiment is conveyed through phrases such as \"not interesting to the Bayesian optimization community\", \"trivial to some degree\", \"raises my concern on this work's novelty\", \"not only trivial\", and \"minimal and incremental\"."
        ],
        "tone": [
            "Critical",
            "Supportive",
            "Supportive",
            "Critical",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The tone is critical, using phrases such as 'I'd be quite surprised if,' 'assumption that ... is pretty strong,' 'experiments are not meaningful,' 'they even seem to invent on their own methods to compare against,' and 'I also do not get much out of the experimental setup.' The reviewer directly questions the validity and relevance of the research.",
            "The reviewer uses phrases like \"Thank you for your thoughtful comment\" and \"Best regards,\" demonstrating a supportive and respectful tone. While they express a disagreement, they do so politely and offer a suggestion for improvement, further reinforcing the supportive nature.",
            "The reviewer uses encouraging language (e.g., \"interesting problem\", \"promising numerical results\", \"novel method\") and explicitly states they would \"like to recommend acceptance.\"",
            "The review employs direct and critical language, such as \"highly unconvinced\", \"does not really address my concerns\", \"confuse\", and \"simplest baselines one could imagine\". The reviewer also points out flaws in the methodology and comparison to other methods, indicating a critical tone.",
            "The review uses phrases like 'key concerns,' 'bottleneck,' 'somewhat eliminate the strength,' 'seems not robust,' 'overkill,' 'somewhat unconvincing results,' and 'can't recommend acceptance,' indicating a critical evaluation of the paper's weaknesses.",
            "The tone is critical as the reviewer directly points out flaws in the paper, such as the incorrect graphical model, inconsistencies in assumptions, and superfluous claims. The reviewer uses phrases like \"I do not get the point\" and directly questions the authors' reasoning and contributions."
        ],
        "consistency": [
            "Yes",
            "No",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent in its criticism of the paper. It repeatedly points out the lack of awareness of prior work, the simplicity of the proposed method, the strong assumptions it makes, and the inadequacy of the experimental evaluation due to missing comparisons with relevant prior work. The reviewer's arguments are focused and do not contradict each other, maintaining a negative stance throughout the review.",
            "The reviewer first states disagreement with the point that a small search space is a weak point, arguing that 4-5 hyperparameters are reasonable. However, the reviewer then suggests that finding a realistic and large search space would make the paper better, implying that a larger search space is indeed desirable, which contradicts the initial disagreement.",
            "The review is consistent because despite mentioning a minor concern about the search space dimension in the 'Reasons to Reject' section, the reviewer ultimately recommends acceptance. The negative points are framed as questions and do not fundamentally contradict the positive assessment given in 'Reasons to Accept'. The reviewer explicitly states that the minor concern 'does not degrade the contributions much' and concludes with a recommendation for acceptance, indicating a consistent positive overall evaluation.",
            "The review is consistent in its negative assessment of the paper. It starts with a strong statement of being 'highly unconvinced' and then provides several reasons to support this stance. The reviewer criticizes the simplicity of the approach, the unconvincing empirical evaluation, the lack of sensible baselines, the authors' misunderstanding of Gaussian processes, and the inappropriate choice of comparison methods. All these points consistently contribute to a negative evaluation of the work.",
            "The review consistently raises concerns about the methodology and experiments of HyperBO, focusing on the fixed hyperparameters, acquisition function choice, empirical performance, and limited experimentation. The conclusion to reject is aligned with these concerns.",
            "The review is consistent in its negative assessment of the paper. It starts by acknowledging the importance of the problem but quickly pivots to criticizing the novelty and significance of the proposed method. The reviewer consistently argues that the approach is trivial, lacks novelty, and has several flaws in its presentation and claims, providing specific reasons and examples to support this overall negative evaluation. There are no self-contradictory statements within the review; the arguments logically flow to support the conclusion that the method is not a significant contribution."
        ]
    },
    {
        "paper_id": "iclr_2022_TvMrYbWpa7",
        "paper_title": "Instance-Adaptive Video Compression: Improving Neural Codecs by Training on the Test Set",
        "paper_abstract": "We introduce a video compression algorithm based on instance-adaptive learning. On each video sequence to be transmitted, we finetune a pretrained compression model. The optimal parameters are transmitted to the receiver along with the latent code. By entropy-coding the parameter updates under a suitable mixture model prior, we ensure that the network parameters can be encoded efficiently. This instance-adaptive compression algorithm is agnostic about the choice of base model and has the potential to improve any neural video codec. On UVG, HEVC, and Xiph datasets, our codec improves the performance of a low-latency scale-space flow model by between 24% and 26% BD-rate savings, and that of a state-of-the-art B-frame model by 17 to 20% BD-rate savings. We also demonstrate that instance-adaptive finetuning improves the robustness to domain shift. Finally, our approach reduces the capacity requirements on compression models. We show that it enables a state-of-the-art performance even after reducing the network size by 72%.",
        "review_ids": [
            "GFDXQ6tEc_V",
            "0pq6btZHAd",
            "kjz86jYMbH",
            "z3fBb9dR3G",
            "dSZEuSlRptt",
            "_Vy1qIkqRH0"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " Thank the author for replying to my questions and targeted modifications.\n\nOnly applied experiments, in principle, the workload is not enough, and there is a lack of more in-depth theoretical analysis. However, the author has also made some contributions to promote the development of small model application in the field  of video compression. I would like to keep the initial rating.",
            " Thank the authors for the modification of the paper and the response.\n\nThe answers from the authors addressed my concerns, especially the experimental results of the breakdown of the total rate.\n\nHowever, I still think that the novelty and contribution are limited as the video compression architecture and the instance-adaptive scheme are both borrowed from previous works. Considering that it is an interesting attempt on video compression and it achieves good performance, and the experiments analyze lots of informative things, I would like to keep the initial rating as \"6: marginally above the acceptance threshold\"",
            "This paper tried to close the domain gap between the training and testing dataset for the learned video compression. An instance-adaptive video compression approach is proposed by updating the model parameters of the video codecs and the optimal parameters are transmitted to the decoder. Besides, the authors modeled the updated model parameters under a mixture prior model, which further reduces the bit cost. Experimental results on several datasets demonstrated the effectiveness of this approach.  -Strengths\n1. Good motivations. The domain gap between the training and testing datasets existed in the practical applications and it is important to solve this problem. \n2. The proposed approach is straightforward and neat. Transmitting the model parameters significantly improves the compression performance. \n3. Comprehensive analysis. A lot of results are provided in the ablation study and it is very helpful. \n\n-Weaknesses\n1. Novelty. This paper extended the previous work Rozendaal et al. (2021) to video compression and may have limited novelty. The extension from image compression to video compression is expected. The major modification is the prior model for entropy coding, which is not enough. \n2. Another concern is computational complexity. This new approach increases a lot of computational burdens, which is another issue. This method is even slower than HM in some cases. \n3. The authors only evaluate the proposed method on the UVG and Class B datasets, which are two high-resolution(1920x1080) videos. How about the experimental results on other datasets, like Class c or D?\n4. There are also some related works that should be discussed.   (a) Content Adaptive and Error Propagation Aware Deep Video Compression. Lu et al. (b) Efficient Video Compression via Content-Adaptive Super-Resolution. Khani et al.  (c) Online-Trained Upsampler for Deep Low Complexity Video Compression. Klopp et al. \n\n The main concern is the novelty as a similar approach has been used in image compression. Basically, it is a borderline paper and I am open to other reviews. ",
            "This paper proposed a video compression algorithm based on instance-adaptive learning. Experiments show that this method can significantly improve the performance of existing learned video compression codec with acceptable decoding complexity and latency. Strength:\n\nThis paper is an extention of instance-adaptive image compression to video compression. Main advantage of the proposed method is that it's compatible with many existing learned video compression frameworks and the increase of computational complexity at the decoder side is negligible. Besides, the experiments are sufficient and the performance gain is convincing. \n\nWeakness:\n\nThe overall novelty seems limited since the instance-adaptive method is from existing work with no primary changes. Here are some main questions and concerns:\n\n1). How many optimization steps are used to produce the final reported performance in Figure.1 as well as in some other figs and tables? \n\n2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison?\n\nBesides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison:\n\nGuo Lu,\u00a0et al.,\u00a0\"Content Adaptive and Error Propagation Aware Deep Video Compression.\"\u00a0ECCV\u00a02020.\n The paper is well written in general. However, the overall contribution may not be sufficient enough to reach the acceptance threshold.",
            "This paper proposes a video compression algorithm based on instance-adaptive learning. On each video sequence to be transmitted, the proposed method finetunes a pretrained compression model. The parameters' change are transmitted to the decoder with little bit-rate overhead. The experiments show the outstanding performance of the proposed method. The paper introduces an interesting on-line parameter updating scheme into video compression. The experiments show good RD performance and the analyses on the trade-off between encoding time and RD performance show the flexibility of encoding time.\n\nIn my point of view, the paper may have the following weakness:\n\n1. Limited novelty and contribution.  The video compression architecture is borrowed from scale-space flow (SSF), and the parameter updating scheme follows the similar idea on image compression (Rozendaal et al., 2021), and other detailed technics are also from previous works, e.g., spike-and-slab prior (Johnstone & Titterington, 2009; Rozendaal et al., 2021). Although it is new and interesting to explore the effectiveness of these technics in learned video compression, the reviewer thinks that the novelty and contribution are limited. \n\n2. The ablation studies are missed. How many overhead bits are consumed due to the parameters' update? What is the proportion of the bits used for encoding quantized updates \\bar{\\theta}, quantized codes \\bar{z} and and the prior p(\\bar{\\theta})? These are necessary to analyze the proposed method.\n\n2. Some representative existing works are missed from related works and experimental comparisons. For instance,\n\nCheng, Zhengxue, et al. \"Learning image and video compression through spatial-temporal energy compaction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.\n\nHabibian, Amirhossein, et al. \"Video compression with rate-distortion autoencoders.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019.\n\nLin, Jianping, et al. \"M-LVC: multiple frames prediction for learned video compression.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nYang, Ren, et al. \"Learning for video compression with hierarchical quality and recurrent enhancement.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\nLiu, Haojie, et al. \"Learned video compression via joint spatial-temporal correlation exploration.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 07. 2020.\n\nLu, Guo, et al. \"Content adaptive and error propagation aware deep video compression.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\nHu, Zhihao, et al. \"Improving deep video compression by resolution-adaptive flow coding.\" European Conference on Computer Vision. Springer, Cham, 2020.\n\nYang, Ren, et al. \"Learning for video compression with recurrent auto-encoder and recurrent probability model.\" IEEE Journal of Selected Topics in Signal Processing 15.2 (2021): 388-401.\n\nGolinski, Adam, et al. \"Feedback recurrent autoencoder for video compression.\" Proceedings of the Asian Conference on Computer Vision. 2020.\n\n + The method is interesting and effective.\n\n- The contribution and novelty are limited.\n\n- The experiments (and ablations) are incremental.\n\nOverall speaking, the reviewer thinks that this paper stands at the borderline, but marginally above the acceptance threshold.",
            "This paper mainly introduces a video compression algorithm. The main feature is that it can simultaneously optimize the codec model parameters while compressing the transmitted video, and dynamically adapt to different video styles. The author's main contributions are:\n\na. In this paper, the instance-adaptive data compression method is applied to the compression of complete video sequences. During the test of each video, by fine tuning the model parameters of the codec, the robustness to deal with various fields of video is improved. This method can reduce the model capacity, because a smaller network may suffice to model a single instance, and relax the assumption that the distribution of training set and test set always match. It can be applied to any neural network video codec model.\n\nb. This paper demonstrates the effectiveness of the method through experiments on two different architectures: scale-space flow with low delay setting and B-frame based model. Advantages:\n\nThis paper introduces an instance-adaptive video compression method, which determines a pretrained neural compression model on each video sequence to be transmitted, adjusts the network parameters, and is transmitted together with the latent coding. It is verified by the latest scale-space flow model and B-frame compression model, and some bitstream savings are achieved.\n\nDisadvantages:\n\na The key idea of this paper comes from the instance-adaptive data compression, which is only applied to the scale-space flow model and B-frame compression architecture. The method of this paper is lack of innovation.\nb The difference between the model used and the scale-space flow described in the appendix: the size of the first layer convolution kernel is changed from 5 * 5 to 3 * 3. No reasonable explanation was given.\nc In the experiment, only UVG and class B of the standard test sequences were compared. How does it compare with FVC on the HEVC Class C and HEVC Class D ?\nd There are some spelling and grammatical problems in the paper, such as page 3, the third line of the paragraph above formula 2, parameter \u201ccan be be transmitted very effectively\u201d. In the references on page11, the abbreviation of the conference \u201cSiPS\u201d should be capitalized, just as \u201cDvc\u201d should be expressed by \u201cDVC\u201d.\ne The reference format of the paper is not uniform. For example, on page10, the author of the seventh is inconsistent with other formats. Make sure that some references have been officially published, such as the last one on page 11, which has been published on ICLR. Please check others. On page 12, the conference name of the third reference should be capitalized in accordance with other references. On page 11 there is a problem with the format of the 10th reference. Please check other parts carefully. This paper applies the instance-adaptive image compression method to video compression, and proves the effectiveness of the instance-adaptive data compression method in video through the implementation on two existing models, but I want to weak accept it because of the following problems:\n\na The method of this paper is lack of innovation. In fact, most of the work is done by combining other people's methods. There is no doubt that this paper applies the method of I-frame adaptive compression to video and combines it with other better models to get a certain effect. Authors need to highlight their innovative contributions.\n\nb In addition, there are many errors in the references of the manuscript, such as inconsistent format, author abbreviations, etc."
        ],
        "sentiment": [
            "Neutral",
            "Positive",
            "Neutral",
            "Neutral",
            "Negative",
            "Negative"
        ],
        "sentiment_reason": [
            "The review expresses gratitude for the author's revisions but also points out weaknesses in the work (lack of workload and theoretical analysis). It concludes by acknowledging the author's contributions while maintaining the initial rating, resulting in a neutral overall assessment.",
            "The reviewer expresses satisfaction with the authors' modifications and responses, stating that their concerns were addressed. They also acknowledge the paper's interesting attempt and good performance, leading to a positive overall sentiment.",
            "The review presents both strengths and weaknesses of the paper, with a concluding statement indicating a borderline acceptance. The reviewer highlights positive aspects like good motivation and comprehensive analysis, but also points out concerns about novelty and computational complexity, resulting in a balanced assessment.",
            "The review acknowledges strengths (performance gain, compatibility) but also points out weaknesses (limited novelty, performance at low bitrate) and raises questions, leading to a neutral overall sentiment.",
            "The review identifies multiple weaknesses including limited novelty, missing ablation studies, and omitted related works. While acknowledging the method's effectiveness, the reviewer ultimately views the paper as 'marginally above the acceptance threshold,' suggesting a negative overall assessment.",
            "The review expresses concerns about the paper's lack of innovation, experimental validation, and presence of errors. Phrases like \"lack of innovation,\" \"no reasonable explanation,\" and \"many errors\" indicate a negative assessment."
        ],
        "tone": [
            "Balanced",
            "Balanced",
            "Balanced",
            "Balanced",
            "Critical",
            "Critical"
        ],
        "tone_reason": [
            "The review uses a mix of positive and negative feedback. It starts with \"Thank the author\" (positive), then points out flaws (\"workload is not enough\", \"lack of more in-depth theoretical analysis\"), but also acknowledges contributions (\"made some contributions\") before concluding with maintaining the initial rating (neutral). This balanced approach indicates a considered and fair assessment.",
            "The review acknowledges the paper's strengths (addressed concerns, interesting attempt, good performance, informative experiments) while also pointing out limitations (limited novelty and contribution due to borrowed ideas). This creates a balanced tone that isn't overly critical or excessively supportive.",
            "The review uses objective language to describe the paper's strengths and weaknesses. Phrases like \"Good motivations,\" \"straightforward and neat,\" and \"Comprehensive analysis\" are used to highlight positive aspects, while concerns are expressed with phrases like \"limited novelty,\" \"increases a lot of computational burdens,\" and \"The authors only evaluate...\" The final statement, \"Basically, it is a borderline paper and I am open to other reviews,\" reinforces the balanced perspective.",
            "The review presents both strengths and weaknesses of the paper. It uses phrases like 'Main advantage' and 'performance gain is convincing' to highlight positive aspects, but also points out 'The overall novelty seems limited' and raises 'main questions and concerns', showing a balanced perspective.",
            "The review uses phrases like 'limited novelty and contribution,' 'ablation studies are missed,' and 'incremental' to point out shortcomings. The direct identification of missing works and necessary analyses contributes to a critical tone.",
            "The review points out specific weaknesses and shortcomings in the paper, such as \"The key idea of this paper comes from...\", \"No reasonable explanation was given\", and \"There are some spelling and grammatical problems\". The reviewer also uses direct questions and suggestions for improvement, demonstrating a critical stance."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review acknowledges both positive aspects (author's responsiveness and contribution) and negative aspects (limited workload and lack of theoretical analysis) but maintains the initial rating. This indicates a balanced assessment rather than a contradiction.",
            "The reviewer acknowledges the authors' modifications and addressed concerns, especially regarding experimental results. While pointing out the limited novelty due to borrowed architecture and scheme, the reviewer appreciates the interesting attempt, good performance, and informative experiments, leading to a consistent conclusion of maintaining the initial rating marginally above the acceptance threshold.",
            "The review is consistent because it acknowledges the strengths of the paper, such as good motivation and comprehensive analysis, but also points out significant weaknesses, primarily the lack of novelty due to its extension from image compression to video compression and limited improvement beyond prior work. The reviewer's conclusion that it is a borderline paper aligns with the identified weaknesses outweighing the strengths in terms of novelty and impact.",
            "The review is consistent because the strengths and weaknesses are logically presented, and the final conclusion about the contribution being potentially insufficient is based on the identified weakness of limited novelty. The reviewer acknowledges the positive aspects of the paper while also raising valid concerns and suggesting improvements, leading to a balanced and consistent assessment.",
            "The review is consistent because it presents both positive aspects (effectiveness, interesting method) and negative aspects (limited novelty, missing ablations, incomplete comparisons) of the paper in a balanced way, without contradicting itself. The reviewer acknowledges the method's effectiveness while criticizing its novelty and experimental rigor, leading to a nuanced overall assessment that the paper is borderline but marginally above the acceptance threshold.",
            "The review is consistent because the reviewer clearly outlines both the advantages and disadvantages of the paper, and the final recommendation of 'weak accept' is logically supported by the identified weaknesses, particularly the lack of significant innovation and issues with experimental validation and presentation (references, kernel size explanation)."
        ]
    },
    {
        "paper_id": "iclr_2022_l8It-0lE5e7",
        "paper_title": "Implicit Bias of Adversarial Training for Deep Neural Networks",
        "paper_abstract": "We provide theoretical understandings of the implicit bias imposed by adversarial training for homogeneous deep neural networks without any explicit regularization. In particular, for deep linear networks adversarially trained by gradient descent on a linearly separable dataset, we prove that the direction of the product of weight matrices converges to the direction of the max-margin solution of the original dataset. Furthermore, we generalize this result to the case of adversarial training for non-linear homogeneous deep neural networks without the linear separability of the dataset. We show that, when the neural network is adversarially trained with  $\\ell_2$ or $\\ell_{\\infty}$ FGSM, FGM and PGD perturbations, the direction of the limit point of normalized parameters of the network along the trajectory of the gradient flow converges to a KKT point of a constrained optimization problem that aims to maximize the margin for adversarial examples. Our results theoretically justify the longstanding conjecture that adversarial training modifies the decision boundary by utilizing adversarial examples to improve robustness, and potentially provides insights for designing new robust training strategies.",
        "review_ids": [
            "dB-yjLUP9Ey",
            "uh2MtzVXN-4",
            "RcXbqBusnoP",
            "lPQN9EfxXdj",
            "m1K8I-2zenx"
        ],
        "review_writers": [
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer",
            "official_reviewer"
        ],
        "review_contents": [
            " I thank the authors for their responses and improvements to the paper. I encourage the authors to incorporate the remainder of details from their responses into the paper. I keep my recommendation for acceptance.",
            "*Implicit Bias of Adversarial Training for Deep Neural Networks*\nexplores how minimizing the exponential loss (i.e., $l(x) = e^{-x}$)\nof a homogeneous neural network (i.e., a neural network such that $$\nf = a_L W_L \\circ \\sigma_L \\circ \\cdots \\circ \\sigma_2 \\circ a_1 W_1 =\n\\prod^L_{k=1} a_k^c (W_L \\circ \\sigma_L \\circ \\cdots \\circ \\sigma_2 \\circ\nW_1 $$ for activation functions $\\sigma_L, \\ldots, \\sigma_2$, weights\n$W_L, \\ldots, W_1$, $a_L, \\ldots, a_1 > 0$, and $c \\geq 1$) on samples\nwith perturbations that maximizing the loss influences the optimized\nneural network's weights. Specifically, this paper proves that, for an\nexponential loss and a multi-c-homogeneous neural network, the limit point for $\\frac{W}{\\lVert W \\rVert}$\nwith respect to the gradient flow \n$$\n\\frac{dW}{dt} = - \\left( \\frac{d\\tilde{\\mathcal{L}}}{\\partial W} \\right)^T\n$$\nof the adversarial training objective\n$$\n\\tilde{\\mathcal{L}} = \\frac{1}{n}\\sum^n_{i=1}\\ell(x_i + \\delta_i(W), y_i)\n$$\nunder $\\ell_2$-FGM, FGSM, $\\ell_2$-PGD, and $\\ell_\\infty$-PGD is along the Karush-Kuhn-Tucker (KKT) point \nof the constrained minimization problem\n$$\n\\min_{W_1, \\ldots, W_L} \\frac{1}{2} \\lVert W \\rVert^2_{\\ell_2} \\text{ s.t. } \\tilde{\\gamma_i} \\geq 1\n$$\nwhere $\\tilde{\\gamma_i} = y_i f(x_i + \\delta_i(W))$ and $W = (W_1, \\ldots, W_L)$.\n\nThis theorem demonstrates that---for a class of neural networks and adversarial perturbations---adversarial training has\nan implicit bias that can be expressed in closed form. This result provides an important contribution to understanding \nhow adversarial training improves adversarial robustness.\n *Implicit Bias of Adversarial Training for Deep Neural Networks*\nanswers a prevalent question in the theory of adversarial robustness:\nhow exactly (in closed mathematical form) does adversarial training\nimprove adversarial robustness? The paper proves a theorem stating\nadversarial training produces an implicit bias on the normalized\nweights $\\frac{W}{\\lVert W \\rVert}$ (with a more precise statement in\nSummary Of The Paper). The majority of the paper is clearly written\nand correct. The paper's results place a milestone in the theory of\nadversarial robustness. However, the proof of Theorem 2 may have\npotential errors (which may be from my confusion on some statements\nand notation). Adversarial training in linear neural networks is a corollary to Theorem 5,\nso any potential errors in Theorem 2 do not invalidate Theorem 5's results.\n In addition, there are several statements and notation\nthat are vague and ill-defined in the theorems. This makes the exact\nstatement of the theorems difficult to ascertain.\n\n## Potential errors in the proof of Theorem 2\n\nLemma 5 requires a constant step size $1/\\beta(r, \\bar{r}, \\epsilon)$\nfor\n$$ \\beta(r, \\bar{r}, \\epsilon) = r^{3L}L^2\\left[ \\alpha + \\beta +\n\\frac{\\epsilon}{\\bar{r}^L} \\left( 2\\alpha + \\beta + frac{\\alpha\n\\beta}{\\bar{r}^L} \\right) \\right].$$\nThe constant step size implies in Lemma 5 that\n$$\\max_k \\lVert W_k(t) \\rVert > r(t)$$\nfor some $t$. However, in Lemma 2, the step size $\\eta(t)$ is not constant where\n$$\\eta(t) = \\min \\{ 1, \\beta(r(t), \\bar{r}(t), \\epsilon) \\}$$\nand\n$$r(t+1) = r(t) + \\mu(t)$$\nfor $W(t+1) \\not\\in \\mathcal{S}(r(t) - \\mu(t))$. As the $\\eta(t)$ decreases whenever \n$W(t+1) \\not\\in \\mathcal{S}(r(t) - \\mu(t))$, the step size $\\eta(t)$ \ndecreases at $\\mathcal{S}(r(t) - \\mu(t))$ and not at $\\mathcal{S}(r(t))$.\nIt is not obvious that Lemma 5 holds as the step size is not constant in $\\mathcal{S}(r(t))$.\nThis is particularly important as a Lemma 6 and Theorem 2 assume that\n $\\max_k \\lVert W_k \\rVert_F \\to \\infty$. Could you please provide a short proof\nthat $\\max_k \\lVert W_k \\rVert_F \\to \\infty$ under the assumptions of Theorem 2?\n\nIt is also unclear how statements under\nEquation 39 hold as a result of $\\max_k \\lVert W_k \\rVert_F \\to \\infty$. Why does \n$$U_k \\Sigma_k \\Sigma_k^T U_k^T \\to V_{k+1}\\Sigma^T_{k+1}\\Sigma_{k+1}V^T_{k+1}?$$\nWhy does this imply $\\Sigma_k \\Sigma^T_k$ and $\\Sigma^T_{k+1}\\Sigma_{k+1}$ are\n\"approximately the same\"? How is \"approximately the same\" defined? Why do all layers \nhave rank 1?\n\n## Vague language, ill-defined statements, and undefined definitions\n\nThe paper suffers from several instances of vague language. For\ninstance, \"alignment phenomenon\" on pages 6, 15, and 17 are not\ndefined in the paper. Although I can figure out your intended\ndefinition, it makes Theorem 2's precise statement difficult to\nascertain.  Other instances include \"approximately the same\" on page\n15 and improper use of limits on Equation 52.  In addition, the\nassumptions in the theorems sometimes do not match the proofs.  The\nproof of Theorem 2 assumes only a logistic loss function while Theorem\n2's statement assumes a broader class of loss functions.\n\n## Limited experimental results\n\n**These comments did not impact my review recommendation.**\n\nIn many theoretical papers, experimental sections typically\nquantitatively measure the difference between general theoretical\nstatements and common real-world cases. For example, experiments show\nhow theorems in compressed sensing deviated from typical use cases. In\nthe paper, a particular area for improvement is the experimental\nsection. This section show the training accuracy and the normalized\nmargin. Both plots will clearly increase as a result of adversarial\ntraining and do not add any useful information to the paper. Do the\nweights' singular values diverge to infinity in practice? Does Theorem\n5's statement on the normalized weights still occur when you use other\nlosses and neural networks or remove Assumptions 2 and 4?\n *Implicit Bias of Adversarial Training for Deep Neural Networks*\ncontributes a milestone result to the theory of adversarial\nrobustness. The majority of the paper is clearly written and\ncorrect. However, the Adversarial Training for Linear Neural Networks\nportion of the paper has several flaws: potential errors, vague\nlanguage and ill-defined statements (such as \"approximately the same\"\nand improper use of limits in Equation 52), and use of undefined\nnotation and definitions (such as alignment phenomenon). Nevertheless,\nthe contributions are significant and novel, and the paper would\nreceive an accept if Adversarial Training for Linear Neural Networks\nis amended or removed. \n",
            "This paper characterizes the bias of adversarial training toward specific minimum-norm solutions or KKT points of a particular optimization problem. Their results generalizes the work of Li et al 2020 by proving the directional alignment with the adversarial max-margin solution for deep linear models for L2 perturbations (Theorem 2) as well as convergence in direction for homogenous networks for L2 FGM, FGSM, and L2, Linf PGD perturbations (Theorem 5). Strengths:\n- The results are novel and extend prior theoretical results.\n- To the extent I have verified, the proofs are correct.\n\n\nMinor comments:\n- Theorem 5: one limitation of this result is that it depends on the adversarial perturbation as part of the constraints in Eq 19. That is in comparison with related results of Li et al 2020 and Faghri et al 2021 that their characterizations make the difference between the solution for various Lp-norm perturbations clear. Understandably, Theorem 5 is a more general result for homogenous models but it would still be useful to derive prior linear results as corollaries of Theorem 5.\n- Assumption 4 can easily be false for large perturbation sizes. The footnote says similar assumptions have been made in prior work but those were not about separability of adversarial examples. Can you provide more justification for this assumption?\n- Figure 1: This is an interesting plot confirming the increase in adversarial margin. Can you plot FGSM and PGD on the same plot? I understand that the adversarial margin for the two is different because the corresponding optimization problem is different. However, a natural question is, is there a relation between the two problems?\n- Page 9, Trade-off between standard and adversarial accuracy: I\u2019m not sure I understand the theoretical argument of this part. Is there a concrete result based on Theorem 5?\n- Section 4: Have you verified these results for varying epsilon size (other than 16/255)? How about other network architectures? Is there a challenge in doing so?\n This paper makes a solid theoretical contribution. It could be improved with more empirical verification of the results.",
            "The paper studies the adversarial training problem under deep linear network classifiers and standard L_2 and L_\\infty perturbations. The paper's main result suggests that in the linearly separable case the adversarially trained model via gradient descent will asymptotically converge to the max-margin solution. Some extensions of this result to homogenous neural networks with exponential loss function have been provided. The paper also performs some preliminary numerical experiments to support the theoretical results. This paper focuses on the convergence behavior of adversarial training methods. The paper tries to show the implicit bias of adversarial training in a simplified setting with a deep linear neural network and linearly separable data for a binary classification problem. Under this setting, the paper proves the gradient descent algorithm will converge asymptotically to the max-margin solution. Later, the paper extends this result to homogeneous neural network functions with the exponential loss function and a separation assumption in Assumption 4. Overall, the paper targets an interesting question and proves some useful results on the behavior of adversarial training problems. However, I have some comments on the assumptions made to simplify the analysis, some of which seem to be quite restrictive and limit the results' application to real adversarial training problems.\n\nRegarding the paper's theoretical setting, I think some of the assumptions are quite strong. In section 3.1, the analysis is limited to deep linear networks and linearly separable data in binary classification. Also, the convergence result is an asymptotic guarantee which does not bound the iteration complexity of finding the max-margin solution. The convergence guarantee to the max-margin solution also holds for the standard training algorithm, which questions whether the result can distinguish adversarial training methods from standard training algorithms.\n\nWhile the results in section 3.1 are written clearly, I think section 3.2 lacks a clear presentation and puts several limiting assumptions. First, the gradient steps are replaced with gradient flow which is inconsistent with real adversarial training experiments. Also, the exponential loss function used for theoretical analysis is not used in practical adversarial training experiments. It is not clear whether the results can be extended to standard cross-entropy and squared-error loss functions in deep learning classification problems. Also, Assumption 4 on the separability of adversarial examples is pretty strong and essentially assumes the adversarial training method finds a perfect solution at some iteration t_0, which is too strong given that the paper wants to study the convergence behavior of adversarial training. Therefore, I think the assumptions are too restrictive for a real adversarial learning setting and significantly limit the application of the results to practical deep learning experiments. I will look forward to the authors' responses regarding the reasoning behind these assumptions to give my final score. While the paper shows some insightful results on the convergence of adversarial training for deep linear networks, the assumptions for the analysis of deep nonlinear networks seem too restrictive to me. Also, replacing the gradient steps with the gradient flow seems incompatible with standard adversarial training experiments. The paper would become much stronger after relaxing some of the assumptions and performing the analysis for the actual gradient descent algorithm rather than considering the gradient flow.",
            "This paper aims to understand the training results of adversarial training, and proves that under certain conditions, adversarial training results maximize the margin for the adversarial training samples. Similar results have been observed in the cleaning training of DNN, this paper's contribution is to extend them to the adversarial training settings. The paper seems technically sound (although I don't have the time to go over all the appendix). The results, to be honest, are not surprising, given previous works on standard training. But I believe the rigorous justification presented in the paper is of importance. Some minor concerns:\n1. the notation of loss function is abused. The loss function in (6) takes both x and y as the input, while in Assumption 1, the loss function only takes one argument. I suppose the loss function in (6) means l(x,y;W) = l' (f(x;W)*y) where l' is the loss function in Assumption 1.\n2. I don't quite understand the remark at the top of page 9. LHS of (20) is about the prediction of the clean test sample, the RHS of (20) is about the fitting of the adversarial training sample. How does this inequality relate to the trade-off between robustness and accuracy? Please add more discussion.\n3. Page 21, after eq (81), \"and Lemma 4 can be applied to \". It should be \"and Lemma 9 can be applied to\". The theoretical contribution of this paper is good to me (unless other reviewers find technical errors in the proof). I think it is a good supplement to the current theory of adversarial DNN training. "
        ],
        "sentiment": [
            "Positive",
            "Negative",
            "Positive",
            "Negative",
            "Positive"
        ],
        "sentiment_reason": [
            "The reviewer explicitly states they 'keep my recommendation for acceptance', indicating a positive overall sentiment.",
            "The review identifies potential errors in the proof of Theorem 2, vague language, ill-defined statements, undefined definitions, and limited experimental results. While acknowledging the paper's significant contributions, the criticisms outweigh the positive aspects, leading to an overall negative sentiment.",
            "The review expresses a generally positive sentiment, highlighting the novelty and correctness of the results. It concludes that the paper makes a \"solid theoretical contribution.\"",
            "The review expresses concerns about the paper's assumptions being too strong and limiting the applicability of the results to real-world adversarial training problems. Phrases like 'quite restrictive,' 'limit the results' application,' 'lacks a clear presentation,' 'too strong,' and 'incompatible with standard adversarial training experiments' indicate a negative sentiment.",
            "The reviewer states the paper seems technically sound, the justification is of importance, and the theoretical contribution is good. They also believe it's a good supplement to the current theory."
        ],
        "tone": [
            "Supportive",
            "Critical",
            "Balanced",
            "Critical",
            "Balanced"
        ],
        "tone_reason": [
            "The reviewer thanks the authors and encourages them to incorporate details, using phrases like 'I encourage the authors' which conveys a supportive tone.",
            "The review uses direct criticisms such as \"potential errors in the proof of Theorem 2,\" \"vague language, ill-defined statements, and undefined definitions,\" and \"limited experimental results.\" The reviewer also asks direct questions challenging the authors' reasoning and assumptions.",
            "The review provides both positive feedback (novel results, correct proofs) and constructive criticism (limitations of Theorem 5, concerns about Assumption 4, suggestions for additional experiments and clarifications). The tone is respectful and aims to improve the paper.",
            "The review offers specific criticisms of the paper's methodology, assumptions, and presentation. It questions the practical relevance of the findings due to the restrictive assumptions and suggests improvements, demonstrating a critical evaluation of the work. Phrases like 'assumptions are quite strong,' 'lacks a clear presentation,' 'too restrictive,' and 'incompatible with standard adversarial training experiments' contribute to the critical tone.",
            "The review acknowledges the paper's strengths ('technically sound', 'rigorous justification', 'good supplement') while also pointing out weaknesses and areas for improvement ('not surprising', 'minor concerns'). This mix of positive and negative feedback creates a balanced tone."
        ],
        "consistency": [
            "Yes",
            "Yes",
            "Yes",
            "Yes",
            "Yes"
        ],
        "consistency_reason": [
            "The review is consistent as the reviewer appreciates the authors' responses and improvements, encourages incorporating remaining details, and maintains the recommendation for acceptance. All points align towards a positive assessment and acceptance.",
            "The review is consistent because it identifies both strengths and weaknesses of the paper. While pointing out potential errors in Theorem 2's proof, vague language, and limited experimental results, it acknowledges the paper's significant contribution to the theory of adversarial robustness and ultimately recommends acceptance if the flawed sections are amended or removed. The reviewer's critique and final recommendation are aligned, indicating a consistent evaluation of the paper's merits and shortcomings.",
            "The review is consistent because it highlights both the strengths and weaknesses of the paper without any self-contradictory statements. The reviewer acknowledges the novelty and correctness of the proofs while suggesting specific areas for improvement and further clarification. The minor comments are constructive criticisms and questions, not contradictions to the overall positive assessment of the paper's theoretical contribution.",
            "The review is consistent in its criticism of the paper's assumptions, arguing that they are too restrictive and limit the practical applicability of the theoretical results. The reviewer consistently points out the limitations of the assumptions in different sections of the paper and concludes by reiterating these concerns and suggesting improvements.",
            "The review is consistent because it acknowledges the paper's contribution and technical soundness while pointing out minor concerns for improvement. The reviewer appreciates the rigorous justification even if the results are not entirely surprising, and the raised concerns are specific and constructive, not contradicting the overall positive assessment of the paper's value as a supplement to the existing theory."
        ]
    }
]